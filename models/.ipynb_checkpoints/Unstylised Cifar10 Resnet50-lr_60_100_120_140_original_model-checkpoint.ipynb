{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Contextural Bias of ResNet50 on CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook trains and tests a vanilla ResNet50 model and a stylised ResNet50 model with the CIFAR10 dataset. It includes functions for loading the dataset, turning them into tensors, model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv2d, AvgPool2d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following cell provides a class that loads the CIFAR dataset given the relevant path, processes it into a dictionary format of class labels and content then processes the images into tensors. The class also has helper functions to extract information about the dataset needed for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        super(CifarDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        \n",
    "        classes_list = []\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            classes_list.append(class_name)\n",
    "        classes_list.sort()\n",
    "        self.classes = [dict(class_idx = k, class_name = v) for k, v in enumerate(classes_list)]\n",
    "        \n",
    "\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "        tr = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        img = tr(img)\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_datasets(data_path):\n",
    "    dataset = CifarDataset(data_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used for this experiment are normal CIFAR10 dataset and the stylised version of the CIFAR10 dataset created using AdaIN style transfer.\n",
    "\n",
    "The following cell calls the function created above to load the training, validation and testing datasets of both normal and stylised CIFAR10 and transforms them into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 36000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load normal CIFAR10\n",
    "data_path_train = \"../../cifar/training\"\n",
    "dataset_train = get_cifar_datasets(data_path_train)\n",
    "\n",
    "data_path_val = \"../../cifar/validation/\"\n",
    "dataset_val = get_cifar_datasets(data_path_val)\n",
    "\n",
    "data_path_test = \"../../cifar/testing/\"\n",
    "dataset_test = get_cifar_datasets(data_path_test)\n",
    "\n",
    "print(f\"Number of train samples {dataset_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of val samples {dataset_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of test samples {dataset_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val = DataLoader(dataset_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle = True)\n",
    "\n",
    "# Load stylised CIFAR10\n",
    "data_path_stylised_train = \"../../stylised_cifar32/training\"\n",
    "dataset_stylised_train = get_cifar_datasets(data_path_stylised_train)\n",
    "\n",
    "data_path_stylised_val = \"../../stylised_cifar32/validation/\"\n",
    "dataset_stylised_val = get_cifar_datasets(data_path_stylised_val)\n",
    "\n",
    "data_path_stylised_test = \"../../stylised_cifar32/testing/\"\n",
    "dataset_stylised_test = get_cifar_datasets(data_path_stylised_test)\n",
    "\n",
    "print(f\"Number of stylised train samples {dataset_stylised_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_stylised_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised val samples {dataset_stylised_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_stylised_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised test samples {dataset_stylised_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_stylised_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_loader_stylised_train = DataLoader(dataset_stylised_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_stylised_val = DataLoader(dataset_stylised_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_stylised_test = DataLoader(dataset_stylised_test, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch['image'] torch.Size([32, 3, 32, 32])\n",
      "Shape of batch['cls'] torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Delete before submission\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(8, 8))\n",
    "\n",
    "for batch in data_loader_stylised_train:\n",
    "\n",
    "    print(f\"Shape of batch['image'] {batch['image'].shape}\")\n",
    "    print(f\"Shape of batch['cls'] {batch['cls'].shape}\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        col = i % 4\n",
    "        row = i // 4\n",
    "\n",
    "        img = batch['image'][i].numpy()\n",
    "\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(batch['class_name'][i])\n",
    "        axes[row,col].imshow(np.transpose(img,(1,2,0)))\n",
    "                         \n",
    "        if i >= 15:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining ResNet50\n",
    "\n",
    "The following code defines Resnet50 architecture that will be used to train and test the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resnet building blocks\n",
    "class ResidualBlock(nn.Module): \n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inchannel, outchannel, stride=1): \n",
    "        \n",
    "        super(ResidualBlock, self).__init__() \n",
    "        \n",
    "        self.left = nn.Sequential(\n",
    "            Conv2d(inchannel, outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, self.expansion*outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*outchannel)\n",
    "        ) \n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or inchannel != self.expansion*outchannel: \n",
    "            self.shortcut = nn.Sequential(\n",
    "                Conv2d(inchannel, self.expansion*outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*outchannel)\n",
    "            ) \n",
    "            \n",
    "    def forward(self, x): \n",
    "        out = self.left(x) \n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out) \n",
    "        return out\n",
    "\n",
    "    \n",
    "# define resnet\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ResidualBlock, num_classes = 10):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv2d(3, 64, kernel_size = 3, stride = 1,padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 3, stride = 1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 4, stride = 2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 6, stride = 2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 3, stride = 2)\n",
    "        self.avgpool = AvgPool2d(4)\n",
    "        self.fc = nn.Linear(512*ResidualBlock.expansion, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet50():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the training, validation and testing functions for the experiment. train_part() function trains and updates gradients on each batch of the training set and once that is done, it tests its accuracy on the validation set. Every time the validation test returns a better accuracy than the current maximum, the model is saved and carries on with the next epoch. Then it checks with the learning reate scheduler for any changes in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "print(device)\n",
    "    \n",
    "\n",
    "print_every = 100\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "            scores = model(inputs)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct, accuracy of the dataset is: %.3f %%' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_part(model, train_data, val_data, model_path, optimizer, lr_scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "    val_acc = 0\n",
    "    num_epoch = 2\n",
    "    # Main Loop\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        val_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for i, batch in enumerate(train_data, 0):\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "\n",
    "            # get outputs from the input data and calculate the cross entropy loss\n",
    "            scores = model(inputs)\n",
    "            loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "            # zero and update the gradients and optimise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i, batch in enumerate(val_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['cls'].to(device)\n",
    "\n",
    "                # get the outputs from the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # compute accuracy based on the outputs\n",
    "                _, preds = outputs.max(1)\n",
    "                num_correct += (preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "            if acc > val_acc:\n",
    "                print('saving model')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                val_acc = acc\n",
    "            else:\n",
    "                print('skip model saving')\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla ResNet50 Training\n",
    "\n",
    "The model used in this experiment is ResNet50 with Adam optimiser with learning rate scheduler and default settings. Learning rate changes every 80, 120, 160, 180th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 4.456433\n",
      "[1,   400] loss: 2.114008\n",
      "[1,   600] loss: 2.105654\n",
      "[1,   800] loss: 2.078767\n",
      "[1,  1000] loss: 2.004814\n",
      "Got 816 / 4000 correct (20.40)\n",
      "saving model\n",
      "[2,   200] loss: 1.961299\n",
      "[2,   400] loss: 1.962331\n",
      "[2,   600] loss: 1.932132\n",
      "[2,   800] loss: 1.855734\n",
      "[2,  1000] loss: 1.857134\n",
      "Got 1029 / 4000 correct (25.72)\n",
      "saving model\n",
      "[3,   200] loss: 1.807343\n",
      "[3,   400] loss: 1.832946\n",
      "[3,   600] loss: 1.792323\n",
      "[3,   800] loss: 1.769055\n",
      "[3,  1000] loss: 1.774385\n",
      "Got 1230 / 4000 correct (30.75)\n",
      "saving model\n",
      "[4,   200] loss: 1.729192\n",
      "[4,   400] loss: 1.759562\n",
      "[4,   600] loss: 1.755933\n",
      "[4,   800] loss: 1.733612\n",
      "[4,  1000] loss: 1.711788\n",
      "Got 1267 / 4000 correct (31.67)\n",
      "saving model\n",
      "[5,   200] loss: 1.681814\n",
      "[5,   400] loss: 1.680709\n",
      "[5,   600] loss: 1.636003\n",
      "[5,   800] loss: 1.639341\n",
      "[5,  1000] loss: 1.623239\n",
      "Got 1317 / 4000 correct (32.92)\n",
      "saving model\n",
      "[6,   200] loss: 1.560239\n",
      "[6,   400] loss: 1.537834\n",
      "[6,   600] loss: 1.539239\n",
      "[6,   800] loss: 1.530860\n",
      "[6,  1000] loss: 1.502969\n",
      "Got 1465 / 4000 correct (36.62)\n",
      "saving model\n",
      "[7,   200] loss: 1.468371\n",
      "[7,   400] loss: 1.465547\n",
      "[7,   600] loss: 1.456201\n",
      "[7,   800] loss: 1.450055\n",
      "[7,  1000] loss: 1.425888\n",
      "Got 1791 / 4000 correct (44.77)\n",
      "saving model\n",
      "[8,   200] loss: 1.406347\n",
      "[8,   400] loss: 1.387206\n",
      "[8,   600] loss: 1.369607\n",
      "[8,   800] loss: 1.366390\n",
      "[8,  1000] loss: 1.370336\n",
      "Got 1969 / 4000 correct (49.23)\n",
      "saving model\n",
      "[9,   200] loss: 1.315823\n",
      "[9,   400] loss: 1.342490\n",
      "[9,   600] loss: 1.321459\n",
      "[9,   800] loss: 1.326333\n",
      "[9,  1000] loss: 1.300673\n",
      "Got 2263 / 4000 correct (56.57)\n",
      "saving model\n",
      "[10,   200] loss: 1.210581\n",
      "[10,   400] loss: 1.168900\n",
      "[10,   600] loss: 1.183452\n",
      "[10,   800] loss: 1.181835\n",
      "[10,  1000] loss: 1.127822\n",
      "Got 2389 / 4000 correct (59.72)\n",
      "saving model\n",
      "[11,   200] loss: 1.081520\n",
      "[11,   400] loss: 1.076096\n",
      "[11,   600] loss: 1.052575\n",
      "[11,   800] loss: 1.025023\n",
      "[11,  1000] loss: 0.993988\n",
      "Got 2473 / 4000 correct (61.82)\n",
      "saving model\n",
      "[12,   200] loss: 0.960031\n",
      "[12,   400] loss: 0.962522\n",
      "[12,   600] loss: 0.963236\n",
      "[12,   800] loss: 0.931744\n",
      "[12,  1000] loss: 0.960130\n",
      "Got 2582 / 4000 correct (64.55)\n",
      "saving model\n",
      "[13,   200] loss: 0.870689\n",
      "[13,   400] loss: 0.888889\n",
      "[13,   600] loss: 0.853826\n",
      "[13,   800] loss: 0.885033\n",
      "[13,  1000] loss: 0.925743\n",
      "Got 2694 / 4000 correct (67.35)\n",
      "saving model\n",
      "[14,   200] loss: 0.819875\n",
      "[14,   400] loss: 0.807594\n",
      "[14,   600] loss: 0.831608\n",
      "[14,   800] loss: 0.819734\n",
      "[14,  1000] loss: 0.831143\n",
      "Got 2706 / 4000 correct (67.65)\n",
      "saving model\n",
      "[15,   200] loss: 0.755913\n",
      "[15,   400] loss: 0.766731\n",
      "[15,   600] loss: 0.769057\n",
      "[15,   800] loss: 0.757142\n",
      "[15,  1000] loss: 0.751420\n",
      "Got 2683 / 4000 correct (67.07)\n",
      "skip model saving\n",
      "[16,   200] loss: 0.707823\n",
      "[16,   400] loss: 0.690644\n",
      "[16,   600] loss: 0.704981\n",
      "[16,   800] loss: 0.685399\n",
      "[16,  1000] loss: 0.691201\n",
      "Got 2930 / 4000 correct (73.25)\n",
      "saving model\n",
      "[17,   200] loss: 0.606239\n",
      "[17,   400] loss: 0.652880\n",
      "[17,   600] loss: 0.671606\n",
      "[17,   800] loss: 0.663345\n",
      "[17,  1000] loss: 0.664459\n",
      "Got 1687 / 4000 correct (42.18)\n",
      "skip model saving\n",
      "[18,   200] loss: 0.569636\n",
      "[18,   400] loss: 0.567135\n",
      "[18,   600] loss: 0.644805\n",
      "[18,   800] loss: 0.596691\n",
      "[18,  1000] loss: 0.592439\n",
      "Got 2941 / 4000 correct (73.52)\n",
      "saving model\n",
      "[19,   200] loss: 0.516028\n",
      "[19,   400] loss: 0.562633\n",
      "[19,   600] loss: 0.511358\n",
      "[19,   800] loss: 0.549426\n",
      "[19,  1000] loss: 0.555257\n",
      "Got 2937 / 4000 correct (73.42)\n",
      "skip model saving\n",
      "[20,   200] loss: 0.469306\n",
      "[20,   400] loss: 0.484047\n",
      "[20,   600] loss: 0.474294\n",
      "[20,   800] loss: 0.517964\n",
      "[20,  1000] loss: 0.519012\n",
      "Got 2980 / 4000 correct (74.50)\n",
      "saving model\n",
      "[21,   200] loss: 0.421600\n",
      "[21,   400] loss: 0.470308\n",
      "[21,   600] loss: 0.458774\n",
      "[21,   800] loss: 0.466887\n",
      "[21,  1000] loss: 0.473472\n",
      "Got 2940 / 4000 correct (73.50)\n",
      "skip model saving\n",
      "[22,   200] loss: 0.365180\n",
      "[22,   400] loss: 0.399417\n",
      "[22,   600] loss: 0.419538\n",
      "[22,   800] loss: 0.396938\n",
      "[22,  1000] loss: 0.409766\n",
      "Got 2924 / 4000 correct (73.10)\n",
      "skip model saving\n",
      "[23,   200] loss: 0.360332\n",
      "[23,   400] loss: 0.359251\n",
      "[23,   600] loss: 0.363357\n",
      "[23,   800] loss: 0.375691\n",
      "[23,  1000] loss: 0.415603\n",
      "Got 2961 / 4000 correct (74.02)\n",
      "skip model saving\n",
      "[24,   200] loss: 0.289445\n",
      "[24,   400] loss: 0.338555\n",
      "[24,   600] loss: 0.329938\n",
      "[24,   800] loss: 0.343499\n",
      "[24,  1000] loss: 0.381829\n",
      "Got 2993 / 4000 correct (74.83)\n",
      "saving model\n",
      "[25,   200] loss: 0.257425\n",
      "[25,   400] loss: 0.306482\n",
      "[25,   600] loss: 0.294337\n",
      "[25,   800] loss: 0.302828\n",
      "[25,  1000] loss: 0.359749\n",
      "Got 3009 / 4000 correct (75.22)\n",
      "saving model\n",
      "[26,   200] loss: 0.238260\n",
      "[26,   400] loss: 0.299821\n",
      "[26,   600] loss: 0.276108\n",
      "[26,   800] loss: 0.291159\n",
      "[26,  1000] loss: 0.320653\n",
      "Got 3061 / 4000 correct (76.53)\n",
      "saving model\n",
      "[27,   200] loss: 0.201973\n",
      "[27,   400] loss: 0.292006\n",
      "[27,   600] loss: 0.255333\n",
      "[27,   800] loss: 0.245815\n",
      "[27,  1000] loss: 0.289458\n",
      "Got 2982 / 4000 correct (74.55)\n",
      "skip model saving\n",
      "[28,   200] loss: 0.199223\n",
      "[28,   400] loss: 0.251953\n",
      "[28,   600] loss: 0.248390\n",
      "[28,   800] loss: 0.270465\n",
      "[28,  1000] loss: 0.266873\n",
      "Got 3089 / 4000 correct (77.22)\n",
      "saving model\n",
      "[29,   200] loss: 0.161509\n",
      "[29,   400] loss: 0.213763\n",
      "[29,   600] loss: 0.224356\n",
      "[29,   800] loss: 0.231836\n",
      "[29,  1000] loss: 0.245899\n",
      "Got 3013 / 4000 correct (75.33)\n",
      "skip model saving\n",
      "[30,   200] loss: 0.181631\n",
      "[30,   400] loss: 0.194964\n",
      "[30,   600] loss: 0.200267\n",
      "[30,   800] loss: 0.208952\n",
      "[30,  1000] loss: 0.230782\n",
      "Got 3052 / 4000 correct (76.30)\n",
      "skip model saving\n",
      "[31,   200] loss: 0.156485\n",
      "[31,   400] loss: 0.207465\n",
      "[31,   600] loss: 0.203978\n",
      "[31,   800] loss: 0.197587\n",
      "[31,  1000] loss: 0.202964\n",
      "Got 3058 / 4000 correct (76.45)\n",
      "skip model saving\n",
      "[32,   200] loss: 0.162959\n",
      "[32,   400] loss: 0.141887\n",
      "[32,   600] loss: 0.209292\n",
      "[32,   800] loss: 0.190449\n",
      "[32,  1000] loss: 0.197031\n",
      "Got 3020 / 4000 correct (75.50)\n",
      "skip model saving\n",
      "[33,   200] loss: 0.186504\n",
      "[33,   400] loss: 0.173662\n",
      "[33,   600] loss: 0.185058\n",
      "[33,   800] loss: 0.200734\n",
      "[33,  1000] loss: 0.178192\n",
      "Got 3077 / 4000 correct (76.92)\n",
      "skip model saving\n",
      "[34,   200] loss: 0.143442\n",
      "[34,   400] loss: 0.152233\n",
      "[34,   600] loss: 0.205115\n",
      "[34,   800] loss: 0.180233\n",
      "[34,  1000] loss: 0.196469\n",
      "Got 3063 / 4000 correct (76.58)\n",
      "skip model saving\n",
      "[35,   200] loss: 0.114117\n",
      "[35,   400] loss: 0.142413\n",
      "[35,   600] loss: 0.168467\n",
      "[35,   800] loss: 0.185141\n",
      "[35,  1000] loss: 0.190806\n",
      "Got 3064 / 4000 correct (76.60)\n",
      "skip model saving\n",
      "[36,   200] loss: 0.127694\n",
      "[36,   400] loss: 0.128110\n",
      "[36,   600] loss: 0.190780\n",
      "[36,   800] loss: 0.186930\n",
      "[36,  1000] loss: 0.161678\n",
      "Got 3082 / 4000 correct (77.05)\n",
      "skip model saving\n",
      "[37,   200] loss: 0.112399\n",
      "[37,   400] loss: 0.131394\n",
      "[37,   600] loss: 0.161878\n",
      "[37,   800] loss: 0.143289\n",
      "[37,  1000] loss: 0.148410\n",
      "Got 3061 / 4000 correct (76.53)\n",
      "skip model saving\n",
      "[38,   200] loss: 0.131264\n",
      "[38,   400] loss: 0.127108\n",
      "[38,   600] loss: 0.157053\n",
      "[38,   800] loss: 0.144634\n",
      "[38,  1000] loss: 0.143646\n",
      "Got 3069 / 4000 correct (76.72)\n",
      "skip model saving\n",
      "[39,   200] loss: 0.087129\n",
      "[39,   400] loss: 0.131390\n",
      "[39,   600] loss: 0.129836\n",
      "[39,   800] loss: 0.171138\n",
      "[39,  1000] loss: 0.124321\n",
      "Got 3034 / 4000 correct (75.85)\n",
      "skip model saving\n",
      "[40,   200] loss: 0.127354\n",
      "[40,   400] loss: 0.113152\n",
      "[40,   600] loss: 0.142143\n",
      "[40,   800] loss: 0.132913\n",
      "[40,  1000] loss: 0.148374\n",
      "Got 3098 / 4000 correct (77.45)\n",
      "saving model\n",
      "[41,   200] loss: 0.094628\n",
      "[41,   400] loss: 0.114037\n",
      "[41,   600] loss: 0.142142\n",
      "[41,   800] loss: 0.172321\n",
      "[41,  1000] loss: 0.143534\n",
      "Got 3039 / 4000 correct (75.98)\n",
      "skip model saving\n",
      "[42,   200] loss: 0.079413\n",
      "[42,   400] loss: 0.112414\n",
      "[42,   600] loss: 0.139569\n",
      "[42,   800] loss: 0.158567\n",
      "[42,  1000] loss: 0.123485\n",
      "Got 3063 / 4000 correct (76.58)\n",
      "skip model saving\n",
      "[43,   200] loss: 0.095580\n",
      "[43,   400] loss: 0.109539\n",
      "[43,   600] loss: 0.110926\n",
      "[43,   800] loss: 0.129011\n",
      "[43,  1000] loss: 0.104573\n",
      "Got 3020 / 4000 correct (75.50)\n",
      "skip model saving\n",
      "[44,   200] loss: 0.093301\n",
      "[44,   400] loss: 0.133850\n",
      "[44,   600] loss: 0.118804\n",
      "[44,   800] loss: 0.091726\n",
      "[44,  1000] loss: 0.153528\n",
      "Got 3079 / 4000 correct (76.98)\n",
      "skip model saving\n",
      "[45,   200] loss: 0.097023\n",
      "[45,   400] loss: 0.099819\n",
      "[45,   600] loss: 0.114094\n",
      "[45,   800] loss: 0.110918\n",
      "[45,  1000] loss: 0.109785\n",
      "Got 3104 / 4000 correct (77.60)\n",
      "saving model\n",
      "[46,   200] loss: 0.087028\n",
      "[46,   400] loss: 0.093193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46,   600] loss: 0.127046\n",
      "[46,   800] loss: 0.107422\n",
      "[46,  1000] loss: 0.116163\n",
      "Got 3086 / 4000 correct (77.15)\n",
      "skip model saving\n",
      "[47,   200] loss: 0.092447\n",
      "[47,   400] loss: 0.119922\n",
      "[47,   600] loss: 0.116193\n",
      "[47,   800] loss: 0.102454\n",
      "[47,  1000] loss: 0.110949\n",
      "Got 3023 / 4000 correct (75.58)\n",
      "skip model saving\n",
      "[48,   200] loss: 0.079909\n",
      "[48,   400] loss: 0.104672\n",
      "[48,   600] loss: 0.134328\n",
      "[48,   800] loss: 0.111775\n",
      "[48,  1000] loss: 0.092360\n",
      "Got 3109 / 4000 correct (77.72)\n",
      "saving model\n",
      "[49,   200] loss: 0.065791\n",
      "[49,   400] loss: 0.089282\n",
      "[49,   600] loss: 0.116976\n",
      "[49,   800] loss: 0.120861\n",
      "[49,  1000] loss: 0.138279\n",
      "Got 3035 / 4000 correct (75.88)\n",
      "skip model saving\n",
      "[50,   200] loss: 0.107529\n",
      "[50,   400] loss: 0.103295\n",
      "[50,   600] loss: 0.101267\n",
      "[50,   800] loss: 0.098556\n",
      "[50,  1000] loss: 0.111486\n",
      "Got 3129 / 4000 correct (78.22)\n",
      "saving model\n",
      "[51,   200] loss: 0.067885\n",
      "[51,   400] loss: 0.081413\n",
      "[51,   600] loss: 0.087241\n",
      "[51,   800] loss: 0.112755\n",
      "[51,  1000] loss: 0.108842\n",
      "Got 3103 / 4000 correct (77.58)\n",
      "skip model saving\n",
      "[52,   200] loss: 0.075237\n",
      "[52,   400] loss: 0.101986\n",
      "[52,   600] loss: 0.116338\n",
      "[52,   800] loss: 0.110010\n",
      "[52,  1000] loss: 0.104892\n",
      "Got 3110 / 4000 correct (77.75)\n",
      "skip model saving\n",
      "[53,   200] loss: 0.073034\n",
      "[53,   400] loss: 0.086364\n",
      "[53,   600] loss: 0.111509\n",
      "[53,   800] loss: 0.103249\n",
      "[53,  1000] loss: 0.081309\n",
      "Got 3123 / 4000 correct (78.08)\n",
      "skip model saving\n",
      "[54,   200] loss: 0.083345\n",
      "[54,   400] loss: 0.073001\n",
      "[54,   600] loss: 0.094353\n",
      "[54,   800] loss: 0.092900\n",
      "[54,  1000] loss: 0.085428\n",
      "Got 3053 / 4000 correct (76.33)\n",
      "skip model saving\n",
      "[55,   200] loss: 0.085315\n",
      "[55,   400] loss: 0.074096\n",
      "[55,   600] loss: 0.089466\n",
      "[55,   800] loss: 0.118811\n",
      "[55,  1000] loss: 0.098515\n",
      "Got 3095 / 4000 correct (77.38)\n",
      "skip model saving\n",
      "[56,   200] loss: 0.053941\n",
      "[56,   400] loss: 0.076635\n",
      "[56,   600] loss: 0.111854\n",
      "[56,   800] loss: 0.123662\n",
      "[56,  1000] loss: 0.092644\n",
      "Got 3028 / 4000 correct (75.70)\n",
      "skip model saving\n",
      "[57,   200] loss: 0.077888\n",
      "[57,   400] loss: 0.080455\n",
      "[57,   600] loss: 0.066206\n",
      "[57,   800] loss: 0.096885\n",
      "[57,  1000] loss: 0.081230\n",
      "Got 3085 / 4000 correct (77.12)\n",
      "skip model saving\n",
      "[58,   200] loss: 0.064780\n",
      "[58,   400] loss: 0.096692\n",
      "[58,   600] loss: 0.091713\n",
      "[58,   800] loss: 0.104146\n",
      "[58,  1000] loss: 0.072224\n",
      "Got 3125 / 4000 correct (78.12)\n",
      "skip model saving\n",
      "[59,   200] loss: 0.102538\n",
      "[59,   400] loss: 0.060896\n",
      "[59,   600] loss: 0.086254\n",
      "[59,   800] loss: 0.099827\n",
      "[59,  1000] loss: 0.085691\n",
      "Got 3099 / 4000 correct (77.48)\n",
      "skip model saving\n",
      "[60,   200] loss: 0.080368\n",
      "[60,   400] loss: 0.089737\n",
      "[60,   600] loss: 0.072887\n",
      "[60,   800] loss: 0.091673\n",
      "[60,  1000] loss: 0.086759\n",
      "Got 3112 / 4000 correct (77.80)\n",
      "skip model saving\n",
      "[61,   200] loss: 0.039085\n",
      "[61,   400] loss: 0.031009\n",
      "[61,   600] loss: 0.024620\n",
      "[61,   800] loss: 0.022410\n",
      "[61,  1000] loss: 0.018306\n",
      "Got 3175 / 4000 correct (79.38)\n",
      "saving model\n",
      "[62,   200] loss: 0.010164\n",
      "[62,   400] loss: 0.013731\n",
      "[62,   600] loss: 0.010579\n",
      "[62,   800] loss: 0.010377\n",
      "[62,  1000] loss: 0.009574\n",
      "Got 3191 / 4000 correct (79.77)\n",
      "saving model\n",
      "[63,   200] loss: 0.008902\n",
      "[63,   400] loss: 0.006797\n",
      "[63,   600] loss: 0.005675\n",
      "[63,   800] loss: 0.005551\n",
      "[63,  1000] loss: 0.007411\n",
      "Got 3193 / 4000 correct (79.83)\n",
      "saving model\n",
      "[64,   200] loss: 0.004549\n",
      "[64,   400] loss: 0.005081\n",
      "[64,   600] loss: 0.006272\n",
      "[64,   800] loss: 0.006142\n",
      "[64,  1000] loss: 0.007179\n",
      "Got 3194 / 4000 correct (79.85)\n",
      "saving model\n",
      "[65,   200] loss: 0.003608\n",
      "[65,   400] loss: 0.006416\n",
      "[65,   600] loss: 0.005330\n",
      "[65,   800] loss: 0.006176\n",
      "[65,  1000] loss: 0.004606\n",
      "Got 3192 / 4000 correct (79.80)\n",
      "skip model saving\n",
      "[66,   200] loss: 0.004149\n",
      "[66,   400] loss: 0.004247\n",
      "[66,   600] loss: 0.002850\n",
      "[66,   800] loss: 0.004002\n",
      "[66,  1000] loss: 0.003918\n",
      "Got 3203 / 4000 correct (80.08)\n",
      "saving model\n",
      "[67,   200] loss: 0.004458\n",
      "[67,   400] loss: 0.002668\n",
      "[67,   600] loss: 0.002336\n",
      "[67,   800] loss: 0.003084\n",
      "[67,  1000] loss: 0.002968\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "saving model\n",
      "[68,   200] loss: 0.002137\n",
      "[68,   400] loss: 0.003620\n",
      "[68,   600] loss: 0.001940\n",
      "[68,   800] loss: 0.001687\n",
      "[68,  1000] loss: 0.002926\n",
      "Got 3175 / 4000 correct (79.38)\n",
      "skip model saving\n",
      "[69,   200] loss: 0.002888\n",
      "[69,   400] loss: 0.002491\n",
      "[69,   600] loss: 0.002812\n",
      "[69,   800] loss: 0.003662\n",
      "[69,  1000] loss: 0.003999\n",
      "Got 3193 / 4000 correct (79.83)\n",
      "skip model saving\n",
      "[70,   200] loss: 0.002951\n",
      "[70,   400] loss: 0.001876\n",
      "[70,   600] loss: 0.003611\n",
      "[70,   800] loss: 0.003161\n",
      "[70,  1000] loss: 0.002223\n",
      "Got 3188 / 4000 correct (79.70)\n",
      "skip model saving\n",
      "[71,   200] loss: 0.002290\n",
      "[71,   400] loss: 0.003176\n",
      "[71,   600] loss: 0.001713\n",
      "[71,   800] loss: 0.000932\n",
      "[71,  1000] loss: 0.003558\n",
      "Got 3191 / 4000 correct (79.77)\n",
      "skip model saving\n",
      "[72,   200] loss: 0.001813\n",
      "[72,   400] loss: 0.001537\n",
      "[72,   600] loss: 0.002779\n",
      "[72,   800] loss: 0.001079\n",
      "[72,  1000] loss: 0.002029\n",
      "Got 3192 / 4000 correct (79.80)\n",
      "skip model saving\n",
      "[73,   200] loss: 0.004015\n",
      "[73,   400] loss: 0.001019\n",
      "[73,   600] loss: 0.004724\n",
      "[73,   800] loss: 0.003426\n",
      "[73,  1000] loss: 0.001489\n",
      "Got 3185 / 4000 correct (79.62)\n",
      "skip model saving\n",
      "[74,   200] loss: 0.002600\n",
      "[74,   400] loss: 0.001328\n",
      "[74,   600] loss: 0.002315\n",
      "[74,   800] loss: 0.002579\n",
      "[74,  1000] loss: 0.001735\n",
      "Got 3203 / 4000 correct (80.08)\n",
      "skip model saving\n",
      "[75,   200] loss: 0.000719\n",
      "[75,   400] loss: 0.001821\n",
      "[75,   600] loss: 0.002570\n",
      "[75,   800] loss: 0.002034\n",
      "[75,  1000] loss: 0.001959\n",
      "Got 3199 / 4000 correct (79.97)\n",
      "skip model saving\n",
      "[76,   200] loss: 0.001253\n",
      "[76,   400] loss: 0.003304\n",
      "[76,   600] loss: 0.000913\n",
      "[76,   800] loss: 0.001510\n",
      "[76,  1000] loss: 0.000912\n",
      "Got 3194 / 4000 correct (79.85)\n",
      "skip model saving\n",
      "[77,   200] loss: 0.002567\n",
      "[77,   400] loss: 0.002302\n",
      "[77,   600] loss: 0.001498\n",
      "[77,   800] loss: 0.002687\n",
      "[77,  1000] loss: 0.002548\n",
      "Got 3197 / 4000 correct (79.92)\n",
      "skip model saving\n",
      "[78,   200] loss: 0.002335\n",
      "[78,   400] loss: 0.000850\n",
      "[78,   600] loss: 0.004233\n",
      "[78,   800] loss: 0.002150\n",
      "[78,  1000] loss: 0.002577\n",
      "Got 3190 / 4000 correct (79.75)\n",
      "skip model saving\n",
      "[79,   200] loss: 0.002439\n",
      "[79,   400] loss: 0.002160\n",
      "[79,   600] loss: 0.001433\n",
      "[79,   800] loss: 0.002216\n",
      "[79,  1000] loss: 0.002094\n",
      "Got 3197 / 4000 correct (79.92)\n",
      "skip model saving\n",
      "[80,   200] loss: 0.002028\n",
      "[80,   400] loss: 0.000690\n",
      "[80,   600] loss: 0.001036\n",
      "[80,   800] loss: 0.000918\n",
      "[80,  1000] loss: 0.001136\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[81,   200] loss: 0.002014\n",
      "[81,   400] loss: 0.000936\n",
      "[81,   600] loss: 0.001284\n",
      "[81,   800] loss: 0.000503\n",
      "[81,  1000] loss: 0.002368\n",
      "Got 3194 / 4000 correct (79.85)\n",
      "skip model saving\n",
      "[82,   200] loss: 0.001274\n",
      "[82,   400] loss: 0.001100\n",
      "[82,   600] loss: 0.002513\n",
      "[82,   800] loss: 0.003141\n",
      "[82,  1000] loss: 0.001018\n",
      "Got 3201 / 4000 correct (80.03)\n",
      "skip model saving\n",
      "[83,   200] loss: 0.001355\n",
      "[83,   400] loss: 0.000801\n",
      "[83,   600] loss: 0.000575\n",
      "[83,   800] loss: 0.001512\n",
      "[83,  1000] loss: 0.001525\n",
      "Got 3184 / 4000 correct (79.60)\n",
      "skip model saving\n",
      "[84,   200] loss: 0.001069\n",
      "[84,   400] loss: 0.003035\n",
      "[84,   600] loss: 0.002925\n",
      "[84,   800] loss: 0.000404\n",
      "[84,  1000] loss: 0.002179\n",
      "Got 3184 / 4000 correct (79.60)\n",
      "skip model saving\n",
      "[85,   200] loss: 0.002466\n",
      "[85,   400] loss: 0.002481\n",
      "[85,   600] loss: 0.001677\n",
      "[85,   800] loss: 0.002171\n",
      "[85,  1000] loss: 0.003394\n",
      "Got 3220 / 4000 correct (80.50)\n",
      "saving model\n",
      "[86,   200] loss: 0.001618\n",
      "[86,   400] loss: 0.001141\n",
      "[86,   600] loss: 0.000589\n",
      "[86,   800] loss: 0.001973\n",
      "[86,  1000] loss: 0.002776\n",
      "Got 3170 / 4000 correct (79.25)\n",
      "skip model saving\n",
      "[87,   200] loss: 0.000909\n",
      "[87,   400] loss: 0.001571\n",
      "[87,   600] loss: 0.002134\n",
      "[87,   800] loss: 0.001023\n",
      "[87,  1000] loss: 0.001150\n",
      "Got 3194 / 4000 correct (79.85)\n",
      "skip model saving\n",
      "[88,   200] loss: 0.000705\n",
      "[88,   400] loss: 0.002589\n",
      "[88,   600] loss: 0.004162\n",
      "[88,   800] loss: 0.001174\n",
      "[88,  1000] loss: 0.001827\n",
      "Got 3189 / 4000 correct (79.72)\n",
      "skip model saving\n",
      "[89,   200] loss: 0.001376\n",
      "[89,   400] loss: 0.001789\n",
      "[89,   600] loss: 0.000720\n",
      "[89,   800] loss: 0.002583\n",
      "[89,  1000] loss: 0.001491\n",
      "Got 3190 / 4000 correct (79.75)\n",
      "skip model saving\n",
      "[90,   200] loss: 0.002935\n",
      "[90,   400] loss: 0.003180\n",
      "[90,   600] loss: 0.001859\n",
      "[90,   800] loss: 0.001689\n",
      "[90,  1000] loss: 0.000567\n",
      "Got 3190 / 4000 correct (79.75)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91,   200] loss: 0.000819\n",
      "[91,   400] loss: 0.001169\n",
      "[91,   600] loss: 0.000538\n",
      "[91,   800] loss: 0.000327\n",
      "[91,  1000] loss: 0.000550\n",
      "Got 3179 / 4000 correct (79.47)\n",
      "skip model saving\n",
      "[92,   200] loss: 0.000555\n",
      "[92,   400] loss: 0.000649\n",
      "[92,   600] loss: 0.002122\n",
      "[92,   800] loss: 0.001192\n",
      "[92,  1000] loss: 0.000587\n",
      "Got 3176 / 4000 correct (79.40)\n",
      "skip model saving\n",
      "[93,   200] loss: 0.001178\n",
      "[93,   400] loss: 0.001004\n",
      "[93,   600] loss: 0.002471\n",
      "[93,   800] loss: 0.001960\n",
      "[93,  1000] loss: 0.002076\n",
      "Got 3150 / 4000 correct (78.75)\n",
      "skip model saving\n",
      "[94,   200] loss: 0.000662\n",
      "[94,   400] loss: 0.001363\n",
      "[94,   600] loss: 0.001772\n",
      "[94,   800] loss: 0.001836\n",
      "[94,  1000] loss: 0.001880\n",
      "Got 3195 / 4000 correct (79.88)\n",
      "skip model saving\n",
      "[95,   200] loss: 0.000276\n",
      "[95,   400] loss: 0.001342\n",
      "[95,   600] loss: 0.000845\n",
      "[95,   800] loss: 0.000769\n",
      "[95,  1000] loss: 0.000960\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[96,   200] loss: 0.001245\n",
      "[96,   400] loss: 0.002646\n",
      "[96,   600] loss: 0.001107\n",
      "[96,   800] loss: 0.001251\n",
      "[96,  1000] loss: 0.000733\n",
      "Got 3182 / 4000 correct (79.55)\n",
      "skip model saving\n",
      "[97,   200] loss: 0.000430\n",
      "[97,   400] loss: 0.000505\n",
      "[97,   600] loss: 0.001526\n",
      "[97,   800] loss: 0.003147\n",
      "[97,  1000] loss: 0.003409\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[98,   200] loss: 0.000994\n",
      "[98,   400] loss: 0.002037\n",
      "[98,   600] loss: 0.001714\n",
      "[98,   800] loss: 0.001173\n",
      "[98,  1000] loss: 0.001560\n",
      "Got 3168 / 4000 correct (79.20)\n",
      "skip model saving\n",
      "[99,   200] loss: 0.002093\n",
      "[99,   400] loss: 0.001574\n",
      "[99,   600] loss: 0.000879\n",
      "[99,   800] loss: 0.000873\n",
      "[99,  1000] loss: 0.000825\n",
      "Got 3172 / 4000 correct (79.30)\n",
      "skip model saving\n",
      "[100,   200] loss: 0.000244\n",
      "[100,   400] loss: 0.003136\n",
      "[100,   600] loss: 0.000919\n",
      "[100,   800] loss: 0.000170\n",
      "[100,  1000] loss: 0.001544\n",
      "Got 3185 / 4000 correct (79.62)\n",
      "skip model saving\n",
      "[101,   200] loss: 0.000807\n",
      "[101,   400] loss: 0.000415\n",
      "[101,   600] loss: 0.000351\n",
      "[101,   800] loss: 0.000704\n",
      "[101,  1000] loss: 0.000567\n",
      "Got 3183 / 4000 correct (79.57)\n",
      "skip model saving\n",
      "[102,   200] loss: 0.000313\n",
      "[102,   400] loss: 0.001076\n",
      "[102,   600] loss: 0.000485\n",
      "[102,   800] loss: 0.001398\n",
      "[102,  1000] loss: 0.000476\n",
      "Got 3195 / 4000 correct (79.88)\n",
      "skip model saving\n",
      "[103,   200] loss: 0.000938\n",
      "[103,   400] loss: 0.000330\n",
      "[103,   600] loss: 0.000174\n",
      "[103,   800] loss: 0.000668\n",
      "[103,  1000] loss: 0.000120\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[104,   200] loss: 0.000491\n",
      "[104,   400] loss: 0.000765\n",
      "[104,   600] loss: 0.000114\n",
      "[104,   800] loss: 0.000280\n",
      "[104,  1000] loss: 0.001074\n",
      "Got 3214 / 4000 correct (80.35)\n",
      "skip model saving\n",
      "[105,   200] loss: 0.000197\n",
      "[105,   400] loss: 0.000439\n",
      "[105,   600] loss: 0.000710\n",
      "[105,   800] loss: 0.000441\n",
      "[105,  1000] loss: 0.000130\n",
      "Got 3195 / 4000 correct (79.88)\n",
      "skip model saving\n",
      "[106,   200] loss: 0.000467\n",
      "[106,   400] loss: 0.000906\n",
      "[106,   600] loss: 0.000308\n",
      "[106,   800] loss: 0.000527\n",
      "[106,  1000] loss: 0.000148\n",
      "Got 3204 / 4000 correct (80.10)\n",
      "skip model saving\n",
      "[107,   200] loss: 0.000292\n",
      "[107,   400] loss: 0.001450\n",
      "[107,   600] loss: 0.000244\n",
      "[107,   800] loss: 0.000316\n",
      "[107,  1000] loss: 0.000864\n",
      "Got 3198 / 4000 correct (79.95)\n",
      "skip model saving\n",
      "[108,   200] loss: 0.000142\n",
      "[108,   400] loss: 0.000283\n",
      "[108,   600] loss: 0.000555\n",
      "[108,   800] loss: 0.000297\n",
      "[108,  1000] loss: 0.000606\n",
      "Got 3202 / 4000 correct (80.05)\n",
      "skip model saving\n",
      "[109,   200] loss: 0.000368\n",
      "[109,   400] loss: 0.000141\n",
      "[109,   600] loss: 0.000426\n",
      "[109,   800] loss: 0.000331\n",
      "[109,  1000] loss: 0.000447\n",
      "Got 3204 / 4000 correct (80.10)\n",
      "skip model saving\n",
      "[110,   200] loss: 0.000725\n",
      "[110,   400] loss: 0.000148\n",
      "[110,   600] loss: 0.000212\n",
      "[110,   800] loss: 0.000292\n",
      "[110,  1000] loss: 0.001011\n",
      "Got 3202 / 4000 correct (80.05)\n",
      "skip model saving\n",
      "[111,   200] loss: 0.000280\n",
      "[111,   400] loss: 0.000264\n",
      "[111,   600] loss: 0.000390\n",
      "[111,   800] loss: 0.000491\n",
      "[111,  1000] loss: 0.000242\n",
      "Got 3202 / 4000 correct (80.05)\n",
      "skip model saving\n",
      "[112,   200] loss: 0.000163\n",
      "[112,   400] loss: 0.000368\n",
      "[112,   600] loss: 0.000319\n",
      "[112,   800] loss: 0.000257\n",
      "[112,  1000] loss: 0.000385\n",
      "Got 3203 / 4000 correct (80.08)\n",
      "skip model saving\n",
      "[113,   200] loss: 0.000437\n",
      "[113,   400] loss: 0.000160\n",
      "[113,   600] loss: 0.000110\n",
      "[113,   800] loss: 0.000587\n",
      "[113,  1000] loss: 0.000987\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[114,   200] loss: 0.000127\n",
      "[114,   400] loss: 0.000792\n",
      "[114,   600] loss: 0.000227\n",
      "[114,   800] loss: 0.000110\n",
      "[114,  1000] loss: 0.000325\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[115,   200] loss: 0.000345\n",
      "[115,   400] loss: 0.000981\n",
      "[115,   600] loss: 0.000258\n",
      "[115,   800] loss: 0.000475\n",
      "[115,  1000] loss: 0.000736\n",
      "Got 3216 / 4000 correct (80.40)\n",
      "skip model saving\n",
      "[116,   200] loss: 0.000460\n",
      "[116,   400] loss: 0.000172\n",
      "[116,   600] loss: 0.000078\n",
      "[116,   800] loss: 0.000325\n",
      "[116,  1000] loss: 0.000418\n",
      "Got 3212 / 4000 correct (80.30)\n",
      "skip model saving\n",
      "[117,   200] loss: 0.000230\n",
      "[117,   400] loss: 0.000142\n",
      "[117,   600] loss: 0.000291\n",
      "[117,   800] loss: 0.000101\n",
      "[117,  1000] loss: 0.000984\n",
      "Got 3209 / 4000 correct (80.23)\n",
      "skip model saving\n",
      "[118,   200] loss: 0.000479\n",
      "[118,   400] loss: 0.000114\n",
      "[118,   600] loss: 0.000309\n",
      "[118,   800] loss: 0.000123\n",
      "[118,  1000] loss: 0.000230\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[119,   200] loss: 0.000127\n",
      "[119,   400] loss: 0.000225\n",
      "[119,   600] loss: 0.000325\n",
      "[119,   800] loss: 0.000108\n",
      "[119,  1000] loss: 0.000122\n",
      "Got 3197 / 4000 correct (79.92)\n",
      "skip model saving\n",
      "[120,   200] loss: 0.000221\n",
      "[120,   400] loss: 0.000058\n",
      "[120,   600] loss: 0.000468\n",
      "[120,   800] loss: 0.000354\n",
      "[120,  1000] loss: 0.000056\n",
      "Got 3203 / 4000 correct (80.08)\n",
      "skip model saving\n",
      "[121,   200] loss: 0.000246\n",
      "[121,   400] loss: 0.000583\n",
      "[121,   600] loss: 0.000310\n",
      "[121,   800] loss: 0.000084\n",
      "[121,  1000] loss: 0.000145\n",
      "Got 3205 / 4000 correct (80.12)\n",
      "skip model saving\n",
      "[122,   200] loss: 0.000162\n",
      "[122,   400] loss: 0.000158\n",
      "[122,   600] loss: 0.000177\n",
      "[122,   800] loss: 0.000396\n",
      "[122,  1000] loss: 0.000396\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[123,   200] loss: 0.000697\n",
      "[123,   400] loss: 0.000235\n",
      "[123,   600] loss: 0.000208\n",
      "[123,   800] loss: 0.000129\n",
      "[123,  1000] loss: 0.000145\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[124,   200] loss: 0.000148\n",
      "[124,   400] loss: 0.000068\n",
      "[124,   600] loss: 0.000109\n",
      "[124,   800] loss: 0.000531\n",
      "[124,  1000] loss: 0.000286\n",
      "Got 3197 / 4000 correct (79.92)\n",
      "skip model saving\n",
      "[125,   200] loss: 0.000211\n",
      "[125,   400] loss: 0.000640\n",
      "[125,   600] loss: 0.000137\n",
      "[125,   800] loss: 0.000344\n",
      "[125,  1000] loss: 0.000148\n",
      "Got 3210 / 4000 correct (80.25)\n",
      "skip model saving\n",
      "[126,   200] loss: 0.000195\n",
      "[126,   400] loss: 0.000205\n",
      "[126,   600] loss: 0.000318\n",
      "[126,   800] loss: 0.000088\n",
      "[126,  1000] loss: 0.000121\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[127,   200] loss: 0.000217\n",
      "[127,   400] loss: 0.000303\n",
      "[127,   600] loss: 0.000379\n",
      "[127,   800] loss: 0.000053\n",
      "[127,  1000] loss: 0.000082\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[128,   200] loss: 0.000650\n",
      "[128,   400] loss: 0.000100\n",
      "[128,   600] loss: 0.000092\n",
      "[128,   800] loss: 0.000733\n",
      "[128,  1000] loss: 0.000298\n",
      "Got 3200 / 4000 correct (80.00)\n",
      "skip model saving\n",
      "[129,   200] loss: 0.000098\n",
      "[129,   400] loss: 0.000216\n",
      "[129,   600] loss: 0.000344\n",
      "[129,   800] loss: 0.000199\n",
      "[129,  1000] loss: 0.000190\n",
      "Got 3212 / 4000 correct (80.30)\n",
      "skip model saving\n",
      "[130,   200] loss: 0.000110\n",
      "[130,   400] loss: 0.000195\n",
      "[130,   600] loss: 0.000358\n",
      "[130,   800] loss: 0.000291\n",
      "[130,  1000] loss: 0.000063\n",
      "Got 3218 / 4000 correct (80.45)\n",
      "skip model saving\n",
      "[131,   200] loss: 0.000075\n",
      "[131,   400] loss: 0.000235\n",
      "[131,   600] loss: 0.000513\n",
      "[131,   800] loss: 0.000064\n",
      "[131,  1000] loss: 0.000100\n",
      "Got 3218 / 4000 correct (80.45)\n",
      "skip model saving\n",
      "[132,   200] loss: 0.000096\n",
      "[132,   400] loss: 0.000059\n",
      "[132,   600] loss: 0.000422\n",
      "[132,   800] loss: 0.000052\n",
      "[132,  1000] loss: 0.000162\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[133,   200] loss: 0.000192\n",
      "[133,   400] loss: 0.000658\n",
      "[133,   600] loss: 0.000053\n",
      "[133,   800] loss: 0.000083\n",
      "[133,  1000] loss: 0.000236\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[134,   200] loss: 0.000268\n",
      "[134,   400] loss: 0.000297\n",
      "[134,   600] loss: 0.000144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134,   800] loss: 0.000068\n",
      "[134,  1000] loss: 0.000032\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[135,   200] loss: 0.000097\n",
      "[135,   400] loss: 0.000382\n",
      "[135,   600] loss: 0.000134\n",
      "[135,   800] loss: 0.000087\n",
      "[135,  1000] loss: 0.000131\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[136,   200] loss: 0.000056\n",
      "[136,   400] loss: 0.000259\n",
      "[136,   600] loss: 0.000353\n",
      "[136,   800] loss: 0.000120\n",
      "[136,  1000] loss: 0.000157\n",
      "Got 3200 / 4000 correct (80.00)\n",
      "skip model saving\n",
      "[137,   200] loss: 0.000076\n",
      "[137,   400] loss: 0.000144\n",
      "[137,   600] loss: 0.000027\n",
      "[137,   800] loss: 0.000120\n",
      "[137,  1000] loss: 0.000561\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[138,   200] loss: 0.000130\n",
      "[138,   400] loss: 0.000048\n",
      "[138,   600] loss: 0.000260\n",
      "[138,   800] loss: 0.000283\n",
      "[138,  1000] loss: 0.000292\n",
      "Got 3186 / 4000 correct (79.65)\n",
      "skip model saving\n",
      "[139,   200] loss: 0.000056\n",
      "[139,   400] loss: 0.000057\n",
      "[139,   600] loss: 0.000129\n",
      "[139,   800] loss: 0.000250\n",
      "[139,  1000] loss: 0.000231\n",
      "Got 3197 / 4000 correct (79.92)\n",
      "skip model saving\n",
      "[140,   200] loss: 0.000061\n",
      "[140,   400] loss: 0.000113\n",
      "[140,   600] loss: 0.000084\n",
      "[140,   800] loss: 0.000242\n",
      "[140,  1000] loss: 0.000358\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[141,   200] loss: 0.000184\n",
      "[141,   400] loss: 0.000087\n",
      "[141,   600] loss: 0.000098\n",
      "[141,   800] loss: 0.000350\n",
      "[141,  1000] loss: 0.000302\n",
      "Got 3195 / 4000 correct (79.88)\n",
      "skip model saving\n",
      "[142,   200] loss: 0.000151\n",
      "[142,   400] loss: 0.000042\n",
      "[142,   600] loss: 0.000060\n",
      "[142,   800] loss: 0.000307\n",
      "[142,  1000] loss: 0.000093\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[143,   200] loss: 0.000261\n",
      "[143,   400] loss: 0.000076\n",
      "[143,   600] loss: 0.000077\n",
      "[143,   800] loss: 0.000265\n",
      "[143,  1000] loss: 0.000142\n",
      "Got 3199 / 4000 correct (79.97)\n",
      "skip model saving\n",
      "[144,   200] loss: 0.000294\n",
      "[144,   400] loss: 0.000045\n",
      "[144,   600] loss: 0.000062\n",
      "[144,   800] loss: 0.000128\n",
      "[144,  1000] loss: 0.000075\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[145,   200] loss: 0.000056\n",
      "[145,   400] loss: 0.000099\n",
      "[145,   600] loss: 0.000068\n",
      "[145,   800] loss: 0.000302\n",
      "[145,  1000] loss: 0.000142\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[146,   200] loss: 0.000162\n",
      "[146,   400] loss: 0.000283\n",
      "[146,   600] loss: 0.000066\n",
      "[146,   800] loss: 0.000246\n",
      "[146,  1000] loss: 0.000076\n",
      "Got 3198 / 4000 correct (79.95)\n",
      "skip model saving\n",
      "[147,   200] loss: 0.000050\n",
      "[147,   400] loss: 0.000264\n",
      "[147,   600] loss: 0.000150\n",
      "[147,   800] loss: 0.000159\n",
      "[147,  1000] loss: 0.000141\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[148,   200] loss: 0.000069\n",
      "[148,   400] loss: 0.000099\n",
      "[148,   600] loss: 0.000290\n",
      "[148,   800] loss: 0.000252\n",
      "[148,  1000] loss: 0.000169\n",
      "Got 3200 / 4000 correct (80.00)\n",
      "skip model saving\n",
      "[149,   200] loss: 0.000113\n",
      "[149,   400] loss: 0.000140\n",
      "[149,   600] loss: 0.000268\n",
      "[149,   800] loss: 0.000051\n",
      "[149,  1000] loss: 0.000087\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[150,   200] loss: 0.000241\n",
      "[150,   400] loss: 0.000068\n",
      "[150,   600] loss: 0.000092\n",
      "[150,   800] loss: 0.000065\n",
      "[150,  1000] loss: 0.000113\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[151,   200] loss: 0.000613\n",
      "[151,   400] loss: 0.000316\n",
      "[151,   600] loss: 0.000096\n",
      "[151,   800] loss: 0.000130\n",
      "[151,  1000] loss: 0.000045\n",
      "Got 3191 / 4000 correct (79.77)\n",
      "skip model saving\n",
      "[152,   200] loss: 0.000101\n",
      "[152,   400] loss: 0.000090\n",
      "[152,   600] loss: 0.000052\n",
      "[152,   800] loss: 0.000326\n",
      "[152,  1000] loss: 0.000048\n",
      "Got 3180 / 4000 correct (79.50)\n",
      "skip model saving\n",
      "[153,   200] loss: 0.000073\n",
      "[153,   400] loss: 0.000230\n",
      "[153,   600] loss: 0.000221\n",
      "[153,   800] loss: 0.000318\n",
      "[153,  1000] loss: 0.000066\n",
      "Got 3192 / 4000 correct (79.80)\n",
      "skip model saving\n",
      "[154,   200] loss: 0.000152\n",
      "[154,   400] loss: 0.000372\n",
      "[154,   600] loss: 0.000224\n",
      "[154,   800] loss: 0.000091\n",
      "[154,  1000] loss: 0.000066\n",
      "Got 3192 / 4000 correct (79.80)\n",
      "skip model saving\n",
      "[155,   200] loss: 0.000105\n",
      "[155,   400] loss: 0.000078\n",
      "[155,   600] loss: 0.000110\n",
      "[155,   800] loss: 0.000170\n",
      "[155,  1000] loss: 0.000215\n",
      "Got 3205 / 4000 correct (80.12)\n",
      "skip model saving\n",
      "[156,   200] loss: 0.000065\n",
      "[156,   400] loss: 0.000351\n",
      "[156,   600] loss: 0.000144\n",
      "[156,   800] loss: 0.000739\n",
      "[156,  1000] loss: 0.000091\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[157,   200] loss: 0.000085\n",
      "[157,   400] loss: 0.000083\n",
      "[157,   600] loss: 0.000141\n",
      "[157,   800] loss: 0.000097\n",
      "[157,  1000] loss: 0.000114\n",
      "Got 3203 / 4000 correct (80.08)\n",
      "skip model saving\n",
      "[158,   200] loss: 0.000104\n",
      "[158,   400] loss: 0.000046\n",
      "[158,   600] loss: 0.000222\n",
      "[158,   800] loss: 0.000096\n",
      "[158,  1000] loss: 0.000097\n",
      "Got 3204 / 4000 correct (80.10)\n",
      "skip model saving\n",
      "[159,   200] loss: 0.000099\n",
      "[159,   400] loss: 0.000207\n",
      "[159,   600] loss: 0.000265\n",
      "[159,   800] loss: 0.002017\n",
      "[159,  1000] loss: 0.000045\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[160,   200] loss: 0.000070\n",
      "[160,   400] loss: 0.000111\n",
      "[160,   600] loss: 0.000153\n",
      "[160,   800] loss: 0.000666\n",
      "[160,  1000] loss: 0.000299\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[161,   200] loss: 0.000068\n",
      "[161,   400] loss: 0.000057\n",
      "[161,   600] loss: 0.000259\n",
      "[161,   800] loss: 0.000114\n",
      "[161,  1000] loss: 0.000048\n",
      "Got 3215 / 4000 correct (80.38)\n",
      "skip model saving\n",
      "[162,   200] loss: 0.000070\n",
      "[162,   400] loss: 0.000064\n",
      "[162,   600] loss: 0.000260\n",
      "[162,   800] loss: 0.000046\n",
      "[162,  1000] loss: 0.000493\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[163,   200] loss: 0.000047\n",
      "[163,   400] loss: 0.000146\n",
      "[163,   600] loss: 0.000115\n",
      "[163,   800] loss: 0.000235\n",
      "[163,  1000] loss: 0.000101\n",
      "Got 3205 / 4000 correct (80.12)\n",
      "skip model saving\n",
      "[164,   200] loss: 0.000062\n",
      "[164,   400] loss: 0.000144\n",
      "[164,   600] loss: 0.000175\n",
      "[164,   800] loss: 0.000243\n",
      "[164,  1000] loss: 0.001075\n",
      "Got 3201 / 4000 correct (80.03)\n",
      "skip model saving\n",
      "[165,   200] loss: 0.000021\n",
      "[165,   400] loss: 0.000417\n",
      "[165,   600] loss: 0.000294\n",
      "[165,   800] loss: 0.000048\n",
      "[165,  1000] loss: 0.000042\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[166,   200] loss: 0.000067\n",
      "[166,   400] loss: 0.000066\n",
      "[166,   600] loss: 0.000029\n",
      "[166,   800] loss: 0.000062\n",
      "[166,  1000] loss: 0.000153\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[167,   200] loss: 0.000139\n",
      "[167,   400] loss: 0.000046\n",
      "[167,   600] loss: 0.000300\n",
      "[167,   800] loss: 0.000042\n",
      "[167,  1000] loss: 0.000057\n",
      "Got 3210 / 4000 correct (80.25)\n",
      "skip model saving\n",
      "[168,   200] loss: 0.000080\n",
      "[168,   400] loss: 0.000039\n",
      "[168,   600] loss: 0.000116\n",
      "[168,   800] loss: 0.000258\n",
      "[168,  1000] loss: 0.000060\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[169,   200] loss: 0.000074\n",
      "[169,   400] loss: 0.000025\n",
      "[169,   600] loss: 0.000105\n",
      "[169,   800] loss: 0.000476\n",
      "[169,  1000] loss: 0.000039\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[170,   200] loss: 0.000078\n",
      "[170,   400] loss: 0.000140\n",
      "[170,   600] loss: 0.000075\n",
      "[170,   800] loss: 0.000084\n",
      "[170,  1000] loss: 0.000301\n",
      "Got 3210 / 4000 correct (80.25)\n",
      "skip model saving\n",
      "[171,   200] loss: 0.000073\n",
      "[171,   400] loss: 0.000288\n",
      "[171,   600] loss: 0.000349\n",
      "[171,   800] loss: 0.000498\n",
      "[171,  1000] loss: 0.000092\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[172,   200] loss: 0.000084\n",
      "[172,   400] loss: 0.000217\n",
      "[172,   600] loss: 0.000055\n",
      "[172,   800] loss: 0.000047\n",
      "[172,  1000] loss: 0.000254\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[173,   200] loss: 0.000059\n",
      "[173,   400] loss: 0.000129\n",
      "[173,   600] loss: 0.000066\n",
      "[173,   800] loss: 0.000148\n",
      "[173,  1000] loss: 0.000035\n",
      "Got 3200 / 4000 correct (80.00)\n",
      "skip model saving\n",
      "[174,   200] loss: 0.000060\n",
      "[174,   400] loss: 0.000043\n",
      "[174,   600] loss: 0.000131\n",
      "[174,   800] loss: 0.000053\n",
      "[174,  1000] loss: 0.000274\n",
      "Got 3200 / 4000 correct (80.00)\n",
      "skip model saving\n",
      "[175,   200] loss: 0.000366\n",
      "[175,   400] loss: 0.000070\n",
      "[175,   600] loss: 0.000021\n",
      "[175,   800] loss: 0.000242\n",
      "[175,  1000] loss: 0.000092\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[176,   200] loss: 0.000246\n",
      "[176,   400] loss: 0.000043\n",
      "[176,   600] loss: 0.000029\n",
      "[176,   800] loss: 0.000086\n",
      "[176,  1000] loss: 0.000056\n",
      "Got 3193 / 4000 correct (79.83)\n",
      "skip model saving\n",
      "[177,   200] loss: 0.000085\n",
      "[177,   400] loss: 0.000037\n",
      "[177,   600] loss: 0.000087\n",
      "[177,   800] loss: 0.000121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[177,  1000] loss: 0.000052\n",
      "Got 3205 / 4000 correct (80.12)\n",
      "skip model saving\n",
      "[178,   200] loss: 0.000054\n",
      "[178,   400] loss: 0.000038\n",
      "[178,   600] loss: 0.000045\n",
      "[178,   800] loss: 0.000232\n",
      "[178,  1000] loss: 0.000110\n",
      "Got 3201 / 4000 correct (80.03)\n",
      "skip model saving\n",
      "[179,   200] loss: 0.000054\n",
      "[179,   400] loss: 0.000030\n",
      "[179,   600] loss: 0.000265\n",
      "[179,   800] loss: 0.000065\n",
      "[179,  1000] loss: 0.000261\n",
      "Got 3191 / 4000 correct (79.77)\n",
      "skip model saving\n",
      "[180,   200] loss: 0.000266\n",
      "[180,   400] loss: 0.000039\n",
      "[180,   600] loss: 0.000055\n",
      "[180,   800] loss: 0.000316\n",
      "[180,  1000] loss: 0.000071\n",
      "Got 3194 / 4000 correct (79.85)\n",
      "skip model saving\n",
      "[181,   200] loss: 0.000118\n",
      "[181,   400] loss: 0.000099\n",
      "[181,   600] loss: 0.000232\n",
      "[181,   800] loss: 0.000041\n",
      "[181,  1000] loss: 0.000039\n",
      "Got 3193 / 4000 correct (79.83)\n",
      "skip model saving\n",
      "[182,   200] loss: 0.000216\n",
      "[182,   400] loss: 0.000056\n",
      "[182,   600] loss: 0.000064\n",
      "[182,   800] loss: 0.000065\n",
      "[182,  1000] loss: 0.000040\n",
      "Got 3199 / 4000 correct (79.97)\n",
      "skip model saving\n",
      "[183,   200] loss: 0.000275\n",
      "[183,   400] loss: 0.000053\n",
      "[183,   600] loss: 0.000035\n",
      "[183,   800] loss: 0.000119\n",
      "[183,  1000] loss: 0.000116\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[184,   200] loss: 0.000051\n",
      "[184,   400] loss: 0.000227\n",
      "[184,   600] loss: 0.000120\n",
      "[184,   800] loss: 0.000043\n",
      "[184,  1000] loss: 0.000054\n",
      "Got 3200 / 4000 correct (80.00)\n",
      "skip model saving\n",
      "[185,   200] loss: 0.000096\n",
      "[185,   400] loss: 0.000081\n",
      "[185,   600] loss: 0.000369\n",
      "[185,   800] loss: 0.000241\n",
      "[185,  1000] loss: 0.000046\n",
      "Got 3207 / 4000 correct (80.17)\n",
      "skip model saving\n",
      "[186,   200] loss: 0.000147\n",
      "[186,   400] loss: 0.000069\n",
      "[186,   600] loss: 0.000081\n",
      "[186,   800] loss: 0.000032\n",
      "[186,  1000] loss: 0.000212\n",
      "Got 3201 / 4000 correct (80.03)\n",
      "skip model saving\n",
      "[187,   200] loss: 0.000035\n",
      "[187,   400] loss: 0.000280\n",
      "[187,   600] loss: 0.000074\n",
      "[187,   800] loss: 0.000190\n",
      "[187,  1000] loss: 0.000062\n",
      "Got 3197 / 4000 correct (79.92)\n",
      "skip model saving\n",
      "[188,   200] loss: 0.000521\n",
      "[188,   400] loss: 0.000060\n",
      "[188,   600] loss: 0.000052\n",
      "[188,   800] loss: 0.000119\n",
      "[188,  1000] loss: 0.000252\n",
      "Got 3199 / 4000 correct (79.97)\n",
      "skip model saving\n",
      "[189,   200] loss: 0.000069\n",
      "[189,   400] loss: 0.000065\n",
      "[189,   600] loss: 0.000062\n",
      "[189,   800] loss: 0.000086\n",
      "[189,  1000] loss: 0.000241\n",
      "Got 3213 / 4000 correct (80.33)\n",
      "skip model saving\n",
      "[190,   200] loss: 0.000062\n",
      "[190,   400] loss: 0.000073\n",
      "[190,   600] loss: 0.000082\n",
      "[190,   800] loss: 0.000066\n",
      "[190,  1000] loss: 0.000022\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[191,   200] loss: 0.000088\n",
      "[191,   400] loss: 0.000048\n",
      "[191,   600] loss: 0.000073\n",
      "[191,   800] loss: 0.000276\n",
      "[191,  1000] loss: 0.000121\n",
      "Got 3200 / 4000 correct (80.00)\n",
      "skip model saving\n",
      "[192,   200] loss: 0.000106\n",
      "[192,   400] loss: 0.000129\n",
      "[192,   600] loss: 0.000056\n",
      "[192,   800] loss: 0.000036\n",
      "[192,  1000] loss: 0.000396\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[193,   200] loss: 0.000084\n",
      "[193,   400] loss: 0.000059\n",
      "[193,   600] loss: 0.000041\n",
      "[193,   800] loss: 0.000241\n",
      "[193,  1000] loss: 0.000052\n",
      "Got 3214 / 4000 correct (80.35)\n",
      "skip model saving\n",
      "[194,   200] loss: 0.000337\n",
      "[194,   400] loss: 0.000102\n",
      "[194,   600] loss: 0.000034\n",
      "[194,   800] loss: 0.000119\n",
      "[194,  1000] loss: 0.000078\n",
      "Got 3206 / 4000 correct (80.15)\n",
      "skip model saving\n",
      "[195,   200] loss: 0.000123\n",
      "[195,   400] loss: 0.000051\n",
      "[195,   600] loss: 0.000601\n",
      "[195,   800] loss: 0.000192\n",
      "[195,  1000] loss: 0.000072\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[196,   200] loss: 0.000300\n",
      "[196,   400] loss: 0.000075\n",
      "[196,   600] loss: 0.000059\n",
      "[196,   800] loss: 0.000092\n",
      "[196,  1000] loss: 0.000247\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[197,   200] loss: 0.000032\n",
      "[197,   400] loss: 0.000084\n",
      "[197,   600] loss: 0.000401\n",
      "[197,   800] loss: 0.000288\n",
      "[197,  1000] loss: 0.000044\n",
      "Got 3192 / 4000 correct (79.80)\n",
      "skip model saving\n",
      "[198,   200] loss: 0.000083\n",
      "[198,   400] loss: 0.000205\n",
      "[198,   600] loss: 0.000426\n",
      "[198,   800] loss: 0.000041\n",
      "[198,  1000] loss: 0.000082\n",
      "Got 3208 / 4000 correct (80.20)\n",
      "skip model saving\n",
      "[199,   200] loss: 0.000032\n",
      "[199,   400] loss: 0.000507\n",
      "[199,   600] loss: 0.000240\n",
      "[199,   800] loss: 0.000092\n",
      "[199,  1000] loss: 0.000235\n",
      "Got 3204 / 4000 correct (80.10)\n",
      "skip model saving\n",
      "[200,   200] loss: 0.000038\n",
      "[200,   400] loss: 0.000063\n",
      "[200,   600] loss: 0.000085\n",
      "[200,   800] loss: 0.000196\n",
      "[200,  1000] loss: 0.000227\n",
      "Got 3195 / 4000 correct (79.88)\n",
      "skip model saving\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# starting from 70\n",
    "# define and train the network\n",
    "vanilla_model_path = './unstylised_cifar_new.pth'\n",
    "vanilla_model = ResNet50()\n",
    "lr=0.1\n",
    "vanilla_optimizer = optim.Adam(vanilla_model.parameters(), lr=lr)\n",
    "vanilla_lr_scheduler = optim.lr_scheduler.MultiStepLR(vanilla_optimizer, milestones=[60, 100, 140, 180])\n",
    "train_part(vanilla_model, data_loader_train, data_loader_val, vanilla_model_path, vanilla_optimizer, vanilla_lr_scheduler, epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Normal CIFAR10 on Vanilla ResNet50\n",
    "\n",
    "The below code tests the vanilla ResNet50 model on the normal CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 7953 / 10000 correct, accuracy of the dataset is: 79.530 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "model = ResNet50()\n",
    "model.load_state_dict(torch.load('./unstylised_cifar_new.pth'))\n",
    "model.to(device)\n",
    "check_accuracy(data_loader_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylised ResNet50 Training\n",
    "\n",
    "Stylised ResNet50 training has all the same settings as vanilla ResNet50, except the data being used here are the stylised CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# define and train the network\n",
    "stylised_model_path = './stylised_cifar.pth'\n",
    "stylised_model = ResNet50()\n",
    "lr=0.1\n",
    "stylised_optimizer = optim.Adam(stylised_model.parameters(), lr=lr)\n",
    "stylised_lr_scheduler = optim.lr_scheduler.MultiStepLR(stylised_optimizer, milestones=[80, 120, 160, 180])\n",
    "train_part(stylised_model, data_loader_stylised_train, data_loader_stylised_val, stylised_model_path, stylised_optimizer, stylised_lr_scheduler, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised CIFAR10 on Stylised ResNet50\n",
    "\n",
    "The below code tests the stylised ResNet50 model on the stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dataset is: 37.886111 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "model.load_state_dict(torch.load('./stylised_cifar.pth'))\n",
    "check_accuracy(data_loader_stylised_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised CIFAR10 on Vanilla ResNet50\n",
    "\n",
    "The below code tests the vanilla ResNet50 model on the stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1261 / 10000 correct, accuracy of the dataset is: 12.610 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "model = ResNet50()\n",
    "model.load_state_dict(torch.load('./unstylised_cifar_new.pth'))\n",
    "model.to(device)\n",
    "check_accuracy(data_loader_stylised_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vanilla CIFAR10 on Stylised ResNet50\n",
    "\n",
    "The below code tests the stylised ResNet50 model on the vanilla CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dataset is: 37.886111 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "model.load_state_dict(torch.load('./stylised_cifar.pth'))\n",
    "check_accuracy(data_loader_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
