{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Contextural Bias of ResNet50 on CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook trains and tests a vanilla ResNet50 model and a stylised ResNet50 model with the CIFAR10 dataset. It includes functions for loading the dataset, turning them into tensors, model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv2d, AvgPool2d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following cell provides a class that loads the CIFAR dataset given the relevant path, processes it into a dictionary format of class labels and content then processes the images into tensors. The class also has helper functions to extract information about the dataset needed for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        super(CifarDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        \n",
    "        classes_list = []\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            classes_list.append(class_name)\n",
    "        classes_list.sort()\n",
    "        self.classes = [dict(class_idx = k, class_name = v) for k, v in enumerate(classes_list)]\n",
    "        \n",
    "\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "        tr = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        img = tr(img)\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_datasets(data_path):\n",
    "    dataset = CifarDataset(data_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used for this experiment are normal CIFAR10 dataset and the stylised version of the CIFAR10 dataset created using AdaIN style transfer.\n",
    "\n",
    "The following cell calls the function created above to load the training, validation and testing datasets of both normal and stylised CIFAR10 and transforms them into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 36000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load normal CIFAR10\n",
    "data_path_train = \"../../CIFAR/cifar32/training\"\n",
    "dataset_train = get_cifar_datasets(data_path_train)\n",
    "\n",
    "data_path_val = \"../../CIFAR/cifar32/validation/\"\n",
    "dataset_val = get_cifar_datasets(data_path_val)\n",
    "\n",
    "data_path_test = \"../../CIFAR/cifar32/testing/\"\n",
    "dataset_test = get_cifar_datasets(data_path_test)\n",
    "\n",
    "print(f\"Number of train samples {dataset_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of val samples {dataset_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of test samples {dataset_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val = DataLoader(dataset_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with original kaggle images\n",
    "data_path_train_style = \"../../CIFAR/cifar32_style/training\"\n",
    "dataset_train_style = get_cifar_datasets(data_path_train_style)\n",
    "\n",
    "data_path_val_style = \"../../CIFAR/cifar32_style/validation/\"\n",
    "dataset_val_style = get_cifar_datasets(data_path_val_style)\n",
    "\n",
    "data_path_test_style = \"../../CIFAR/cifar32_style/testing/\"\n",
    "dataset_test_style = get_cifar_datasets(data_path_test_style)\n",
    "\n",
    "print(f\"Number of stylised train samples {dataset_train_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised val samples {dataset_val_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised test samples {dataset_test_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style = DataLoader(dataset_train_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style = DataLoader(dataset_val_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style = DataLoader(dataset_test_style, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with reduced kaggle images\n",
    "data_path_train_style_red = \"../../CIFAR/cifar32_style_red/training\"\n",
    "dataset_train_style_red = get_cifar_datasets(data_path_train_style_red)\n",
    "\n",
    "data_path_val_style_red = \"../../CIFAR/cifar32_style_red/validation/\"\n",
    "dataset_val_style_red = get_cifar_datasets(data_path_val_style_red)\n",
    "\n",
    "data_path_test_style_red = \"../../CIFAR/cifar32_style_red/testing/\"\n",
    "dataset_test_style_red = get_cifar_datasets(data_path_test_style_red)\n",
    "\n",
    "print(f\"Number of reduced stylised train samples {dataset_train_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised val samples {dataset_val_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised test samples {dataset_test_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style_red.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style_red = DataLoader(dataset_train_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style_red = DataLoader(dataset_val_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style_red = DataLoader(dataset_test_style_red, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch['image'] torch.Size([64, 3, 32, 32])\n",
      "Shape of batch['cls'] torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Delete before submission\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(8, 8))\n",
    "\n",
    "for batch in data_loader_train_style:\n",
    "\n",
    "    print(f\"Shape of batch['image'] {batch['image'].shape}\")\n",
    "    print(f\"Shape of batch['cls'] {batch['cls'].shape}\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        col = i % 4\n",
    "        row = i // 4\n",
    "\n",
    "        img = batch['image'][i].numpy()\n",
    "\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(batch['class_name'][i])\n",
    "        axes[row,col].imshow(np.transpose(img,(1,2,0)))\n",
    "                         \n",
    "        if i >= 15:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining ResNet50\n",
    "\n",
    "The following code defines Resnet50 architecture that will be used to train and test the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define resnet building blocks\n",
    "# class ResidualBlock(nn.Module): \n",
    "#     expansion = 4\n",
    "    \n",
    "#     def __init__(self, inchannel, outchannel, stride=1): \n",
    "        \n",
    "#         super(ResidualBlock, self).__init__() \n",
    "        \n",
    "#         self.left = nn.Sequential(\n",
    "#             Conv2d(inchannel, outchannel, kernel_size=1, bias=False),\n",
    "#             nn.BatchNorm2d(outchannel),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             Conv2d(outchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(outchannel),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             Conv2d(outchannel, self.expansion*outchannel, kernel_size=1, bias=False),\n",
    "#             nn.BatchNorm2d(self.expansion*outchannel)\n",
    "#         ) \n",
    "        \n",
    "#         self.shortcut = nn.Sequential()\n",
    "        \n",
    "#         if stride != 1 or inchannel != self.expansion*outchannel: \n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 Conv2d(inchannel, self.expansion*outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(self.expansion*outchannel)\n",
    "#             ) \n",
    "            \n",
    "#     def forward(self, x): \n",
    "#         out = self.left(x) \n",
    "#         out += self.shortcut(x) \n",
    "#         out = F.relu(out) \n",
    "#         return out\n",
    "\n",
    "    \n",
    "# # define resnet\n",
    "# class ResNet(nn.Module):\n",
    "    \n",
    "#     def __init__(self, ResidualBlock, num_classes = 10):\n",
    "        \n",
    "#         super(ResNet, self).__init__()\n",
    "        \n",
    "#         self.inchannel = 64\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             Conv2d(3, 64, kernel_size = 3, stride = 1,padding = 1, bias = False),\n",
    "#             nn.BatchNorm2d(64), \n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.layer1 = self.make_layer(ResidualBlock, 64, 3, stride = 1)\n",
    "#         self.layer2 = self.make_layer(ResidualBlock, 128, 4, stride = 2)\n",
    "#         self.layer3 = self.make_layer(ResidualBlock, 256, 6, stride = 2)\n",
    "#         self.layer4 = self.make_layer(ResidualBlock, 512, 3, stride = 2)\n",
    "#         self.avgpool = AvgPool2d(4)\n",
    "#         self.fc = nn.Linear(512*ResidualBlock.expansion, num_classes)\n",
    "        \n",
    "    \n",
    "#     def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \n",
    "#         strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.inchannel, channels, stride))\n",
    "#             self.inchannel = channels * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = self.conv1(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# def ResNet50():\n",
    "#     return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the training, validation and testing functions for the experiment. train_part() function trains and updates gradients on each batch of the training set and once that is done, it tests its accuracy on the validation set. Every time the validation test returns a better accuracy than the current maximum, the model is saved and carries on with the next epoch. Then it checks with the learning reate scheduler for any changes in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "print(device)\n",
    "    \n",
    "print_every = 100\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "            scores = model(inputs)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct, accuracy of the dataset is: %.3f %%' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_part(model, train_data, val_data, model_path, optimizer, lr_scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "    val_acc = 0\n",
    "    num_epoch = 2\n",
    "    # Main Loop\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        val_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for i, batch in enumerate(train_data, 0):\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "\n",
    "            # get outputs from the input data and calculate the cross entropy loss\n",
    "            scores = model(inputs)\n",
    "            loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "            # zero and update the gradients and optimise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i, batch in enumerate(val_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['cls'].to(device)\n",
    "\n",
    "                # get the outputs from the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # compute accuracy based on the outputs\n",
    "                _, preds = outputs.max(1)\n",
    "                num_correct += (preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "            if acc > val_acc:\n",
    "                print('saving model')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                val_acc = acc\n",
    "            else:\n",
    "                print('skip model saving')\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla ResNet50 Training\n",
    "\n",
    "The model used in this experiment is ResNet50 with Adam optimiser with learning rate scheduler and default settings. Learning rate changes every 80, 120, 160, 180th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 4.350472\n",
      "[1,   400] loss: 2.096447\n",
      "Got 905 / 4000 correct (22.62)\n",
      "saving model\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# define and train the network\n",
    "vanilla_model_path = './cifar32_model_torch.pth'\n",
    "vanilla_model = torchvision.models.resnet50().to(device)\n",
    "lr=0.1\n",
    "vanilla_optimizer = optim.Adam(vanilla_model.parameters(), lr=lr)\n",
    "vanilla_lr_scheduler = optim.lr_scheduler.MultiStepLR(vanilla_optimizer, milestones=[1])\n",
    "train_part(vanilla_model, data_loader_train, data_loader_val, vanilla_model_path, vanilla_optimizer, vanilla_lr_scheduler, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Normal CIFAR10 on Vanilla ResNet50\n",
    "\n",
    "The below code tests the vanilla ResNet50 model on the normal CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report test set accuracy\n",
    "vanilla_model = torchvision.models.resnet50().to(device)\n",
    "vanilla_model.load_state_dict(torch.load('./cifar32_model_torch.pth'))\n",
    "vanilla_model.to(device)\n",
    "check_accuracy(data_loader_test, vanilla_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylised ResNet50 Training\n",
    "\n",
    "Stylised ResNet50 training has all the same settings as vanilla ResNet50, except the data being used here are the stylised CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # define and train the network\n",
    "# stylised_model_path = './cifar32_style_model.pth'\n",
    "# stylised_model = ResNet50()\n",
    "# lr=0.1\n",
    "# stylised_optimizer = optim.Adam(stylised_model.parameters(), lr=lr)\n",
    "# stylised_lr_scheduler = optim.lr_scheduler.MultiStepLR(stylised_optimizer, milestones=[80, 120, 150, 180])\n",
    "# train_part(stylised_model, data_loader_train_style, data_loader_val_style, stylised_model_path, stylised_optimizer, stylised_lr_scheduler, epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised CIFAR10 on Stylised ResNet50\n",
    "\n",
    "The below code tests the stylised ResNet50 model on the stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dataset is: 37.886111 %\n"
     ]
    }
   ],
   "source": [
    "# # report test set accuracy\n",
    "# stylised_model = ResNet50()\n",
    "# stylised_model.load_state_dict(torch.load('./cifar32_style_model.pth'))\n",
    "# stylised_model.to(device)\n",
    "# check_accuracy(data_loader_test_style, stylised_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised CIFAR10 on Vanilla ResNet50\n",
    "\n",
    "The below code tests the vanilla ResNet50 model on the stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dataset is: 37.886111 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "vanilla_model = torchvision.models.resnet50().to(device)\n",
    "vanilla_model.load_state_dict(torch.load('./cifar32_model_torch.pth'))\n",
    "vanilla_model.to(device)\n",
    "check_accuracy(data_loader_test_style, vanilla_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vanilla CIFAR10 on Stylised ResNet50\n",
    "\n",
    "The below code tests the stylised ResNet50 model on the vanilla CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dataset is: 37.886111 %\n"
     ]
    }
   ],
   "source": [
    "# # report test set accuracy\n",
    "# stylised_model = ResNet50()\n",
    "# stylised_model.load_state_dict(torch.load('./cifar32_style_model.pth'))\n",
    "# stylised_model.to(device)\n",
    "# check_accuracy(data_loader_test, stylised_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report test set accuracy\n",
    "vanilla_model = torchvision.models.resnet50().to(device)\n",
    "vanilla_model.load_state_dict(torch.load('./cifar32_model_torch.pth'))\n",
    "vanilla_model.to(device)\n",
    "check_accuracy(data_loader_test_style_red, vanilla_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
