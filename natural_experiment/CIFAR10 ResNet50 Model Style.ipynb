{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Contextural Bias of ResNet50 on CIFAR10 Dataset - Stylised ResNet50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook trains and tests a stylised ResNet50 model with the CIFAR10 dataset. It includes functions for loading the dataset, turning them into tensors, model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv2d, AvgPool2d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following cell provides a class that loads the CIFAR dataset given the relevant path, processes it into a dictionary format of class labels and content then processes the images into tensors. The class also has helper functions to extract information about the dataset needed for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        super(CifarDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        \n",
    "        classes_list = []\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            classes_list.append(class_name)\n",
    "        classes_list.sort()\n",
    "        self.classes = [dict(class_idx = k, class_name = v) for k, v in enumerate(classes_list)]\n",
    "        \n",
    "\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "        tr = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        img = tr(img)\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_datasets(data_path):\n",
    "    dataset = CifarDataset(data_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used for this experiment are normal CIFAR10 dataset, stylised CIFAR10 dataset and stylised CIFAR10 dataset created by using reduced style images where stylisation was done by AdaIN style transfer.\n",
    "\n",
    "The following cells call the function created above to load the training, validation and testing datasets of the normal, stylised and reduced stylised CIFAR10 datasets and transform them into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 36000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load normal CIFAR10\n",
    "data_path_train = \"../../CIFAR/cifar32/training\"\n",
    "dataset_train = get_cifar_datasets(data_path_train)\n",
    "\n",
    "data_path_val = \"../../CIFAR/cifar32/validation/\"\n",
    "dataset_val = get_cifar_datasets(data_path_val)\n",
    "\n",
    "data_path_test = \"../../CIFAR/cifar32/testing/\"\n",
    "dataset_test = get_cifar_datasets(data_path_test)\n",
    "\n",
    "print(f\"Number of train samples {dataset_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of val samples {dataset_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of test samples {dataset_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val = DataLoader(dataset_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with original kaggle images\n",
    "data_path_train_style = \"../../CIFAR/cifar32_style/training\"\n",
    "dataset_train_style = get_cifar_datasets(data_path_train_style)\n",
    "\n",
    "data_path_val_style = \"../../CIFAR/cifar32_style/validation/\"\n",
    "dataset_val_style = get_cifar_datasets(data_path_val_style)\n",
    "\n",
    "data_path_test_style = \"../../CIFAR/cifar32_style/testing/\"\n",
    "dataset_test_style = get_cifar_datasets(data_path_test_style)\n",
    "\n",
    "print(f\"Number of stylised train samples {dataset_train_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised val samples {dataset_val_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised test samples {dataset_test_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style = DataLoader(dataset_train_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style = DataLoader(dataset_val_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style = DataLoader(dataset_test_style, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with reduced kaggle images\n",
    "data_path_train_style_red = \"../../CIFAR/cifar32_style_red/training\"\n",
    "dataset_train_style_red = get_cifar_datasets(data_path_train_style_red)\n",
    "\n",
    "data_path_val_style_red = \"../../CIFAR/cifar32_style_red/validation/\"\n",
    "dataset_val_style_red = get_cifar_datasets(data_path_val_style_red)\n",
    "\n",
    "data_path_test_style_red = \"../../CIFAR/cifar32_style_red/testing/\"\n",
    "dataset_test_style_red = get_cifar_datasets(data_path_test_style_red)\n",
    "\n",
    "print(f\"Number of reduced stylised train samples {dataset_train_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised val samples {dataset_val_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised test samples {dataset_test_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style_red.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style_red = DataLoader(dataset_train_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style_red = DataLoader(dataset_val_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style_red = DataLoader(dataset_test_style_red, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch['image'] torch.Size([64, 3, 32, 32])\n",
      "Shape of batch['cls'] torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the data was loaded correctly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(8, 8))\n",
    "\n",
    "for batch in data_loader_test_style:\n",
    "\n",
    "    print(f\"Shape of batch['image'] {batch['image'].shape}\")\n",
    "    print(f\"Shape of batch['cls'] {batch['cls'].shape}\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        col = i % 4\n",
    "        row = i // 4\n",
    "\n",
    "        img = batch['image'][i].numpy()\n",
    "\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(batch['class_name'][i])\n",
    "        axes[row,col].imshow(np.transpose(img,(1,2,0)))\n",
    "                         \n",
    "        if i >= 15:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining ResNet50\n",
    "\n",
    "The following code defines Resnet50 architecture that will be used to train and test the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resnet building blocks\n",
    "class ResidualBlock(nn.Module): \n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inchannel, outchannel, stride=1): \n",
    "        \n",
    "        super(ResidualBlock, self).__init__() \n",
    "        \n",
    "        self.left = nn.Sequential(\n",
    "            Conv2d(inchannel, outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, self.expansion*outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*outchannel)\n",
    "        ) \n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or inchannel != self.expansion*outchannel: \n",
    "            self.shortcut = nn.Sequential(\n",
    "                Conv2d(inchannel, self.expansion*outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*outchannel)\n",
    "            ) \n",
    "            \n",
    "    def forward(self, x): \n",
    "        out = self.left(x) \n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out) \n",
    "        return out\n",
    "\n",
    "    \n",
    "# define resnet\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ResidualBlock, num_classes = 10):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv2d(3, 64, kernel_size = 3, stride = 1,padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 3, stride = 1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 4, stride = 2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 6, stride = 2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 3, stride = 2)\n",
    "        self.avgpool = AvgPool2d(4)\n",
    "        self.fc = nn.Linear(512*ResidualBlock.expansion, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet50():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the training, validation and testing functions for the experiment. train_part() function trains and updates gradients on each batch of the training set and once that is done, it tests its accuracy on the validation set. Every time the validation test returns a better accuracy than the current maximum, the model is saved and carries on with the next epoch. Then it checks with the learning reate scheduler for any changes in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "    \n",
    "print_every = 100\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "            scores = model(inputs)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct, accuracy of the dataset is: %.3f %%' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_part(model, train_data, val_data, model_path, optimizer, lr_scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "    val_acc = 0\n",
    "    num_epoch = 2\n",
    "    # Main Loop\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        val_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for i, batch in enumerate(train_data, 0):\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "\n",
    "            # get outputs from the input data and calculate the cross entropy loss\n",
    "            scores = model(inputs)\n",
    "            loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "            # zero and update the gradients and optimise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i, batch in enumerate(val_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['cls'].to(device)\n",
    "\n",
    "                # get the outputs from the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # compute accuracy based on the outputs\n",
    "                _, preds = outputs.max(1)\n",
    "                num_correct += (preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "            if acc > val_acc:\n",
    "                print('saving model')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                val_acc = acc\n",
    "            else:\n",
    "                print('skip model saving')\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylised ResNet50 Training\n",
    "\n",
    "The model used in this experiment is ResNet50 trained for 350 epochs with Adam optimiser, learning rate scheduler and default settings. The dataset used to train the model is stylised CIFAR10. Learning rate starts at 0.1 and changes every 150, 250th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 3.957054\n",
      "[1,   400] loss: 2.309854\n",
      "[1,   600] loss: 2.310457\n",
      "[1,   800] loss: 2.312256\n",
      "[1,  1000] loss: 2.310472\n",
      "[1,  1200] loss: 2.312515\n",
      "[1,  1400] loss: 2.311104\n",
      "[1,  1600] loss: 2.311209\n",
      "[1,  1800] loss: 2.312208\n",
      "[1,  2000] loss: 2.311020\n",
      "[1,  2200] loss: 2.312641\n",
      "[1,  2400] loss: 2.312757\n",
      "[1,  2600] loss: 2.313660\n",
      "[1,  2800] loss: 2.314361\n",
      "[1,  3000] loss: 2.313005\n",
      "[1,  3200] loss: 2.312669\n",
      "Got 396 / 4000 correct (9.90)\n",
      "saving model\n",
      "[2,   200] loss: 2.309316\n",
      "[2,   400] loss: 2.312715\n",
      "[2,   600] loss: 2.309918\n",
      "[2,   800] loss: 2.311647\n",
      "[2,  1000] loss: 2.311525\n",
      "[2,  1200] loss: 2.311335\n",
      "[2,  1400] loss: 2.311724\n",
      "[2,  1600] loss: 2.311339\n",
      "[2,  1800] loss: 2.313641\n",
      "[2,  2000] loss: 2.313833\n",
      "[2,  2200] loss: 2.312947\n",
      "[2,  2400] loss: 2.310124\n",
      "[2,  2600] loss: 2.311779\n",
      "[2,  2800] loss: 2.311035\n",
      "[2,  3000] loss: 2.340954\n",
      "[2,  3200] loss: 2.385290\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[3,   200] loss: 2.312273\n",
      "[3,   400] loss: 2.311303\n",
      "[3,   600] loss: 2.312345\n",
      "[3,   800] loss: 2.312928\n",
      "[3,  1000] loss: 2.312571\n",
      "[3,  1200] loss: 2.311744\n",
      "[3,  1400] loss: 2.310506\n",
      "[3,  1600] loss: 2.310443\n",
      "[3,  1800] loss: 2.310379\n",
      "[3,  2000] loss: 2.311559\n",
      "[3,  2200] loss: 2.313062\n",
      "[3,  2400] loss: 2.311689\n",
      "[3,  2600] loss: 2.309695\n",
      "[3,  2800] loss: 2.311445\n",
      "[3,  3000] loss: 2.311469\n",
      "[3,  3200] loss: 2.313452\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[4,   200] loss: 2.311475\n",
      "[4,   400] loss: 2.312639\n",
      "[4,   600] loss: 2.311726\n",
      "[4,   800] loss: 2.312858\n",
      "[4,  1000] loss: 2.312094\n",
      "[4,  1200] loss: 2.309603\n",
      "[4,  1400] loss: 2.312028\n",
      "[4,  1600] loss: 2.310346\n",
      "[4,  1800] loss: 2.312553\n",
      "[4,  2000] loss: 2.311033\n",
      "[4,  2200] loss: 2.311189\n",
      "[4,  2400] loss: 2.311051\n",
      "[4,  2600] loss: 2.310666\n",
      "[4,  2800] loss: 2.311828\n",
      "[4,  3000] loss: 2.312558\n",
      "[4,  3200] loss: 2.310792\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[5,   200] loss: 2.311532\n",
      "[5,   400] loss: 2.311204\n",
      "[5,   600] loss: 2.309467\n",
      "[5,   800] loss: 2.310453\n",
      "[5,  1000] loss: 2.311153\n",
      "[5,  1200] loss: 2.309193\n",
      "[5,  1400] loss: 2.312872\n",
      "[5,  1600] loss: 2.309642\n",
      "[5,  1800] loss: 2.310269\n",
      "[5,  2000] loss: 2.312033\n",
      "[5,  2200] loss: 2.310912\n",
      "[5,  2400] loss: 2.311347\n",
      "[5,  2600] loss: 2.310227\n",
      "[5,  2800] loss: 2.308316\n",
      "[5,  3000] loss: 2.311370\n",
      "[5,  3200] loss: 2.311508\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[6,   200] loss: 2.310773\n",
      "[6,   400] loss: 2.310336\n",
      "[6,   600] loss: 2.312228\n",
      "[6,   800] loss: 2.312309\n",
      "[6,  1000] loss: 2.312019\n",
      "[6,  1200] loss: 2.311117\n",
      "[6,  1400] loss: 2.310953\n",
      "[6,  1600] loss: 2.311048\n",
      "[6,  1800] loss: 2.311492\n",
      "[6,  2000] loss: 2.313098\n",
      "[6,  2200] loss: 2.310620\n",
      "[6,  2400] loss: 2.311569\n",
      "[6,  2600] loss: 2.310082\n",
      "[6,  2800] loss: 2.313440\n",
      "[6,  3000] loss: 2.312028\n",
      "[6,  3200] loss: 2.312683\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[7,   200] loss: 2.313088\n",
      "[7,   400] loss: 2.310743\n",
      "[7,   600] loss: 2.311167\n",
      "[7,   800] loss: 2.311667\n",
      "[7,  1000] loss: 2.310775\n",
      "[7,  1200] loss: 2.310074\n",
      "[7,  1400] loss: 2.310872\n",
      "[7,  1600] loss: 2.308924\n",
      "[7,  1800] loss: 2.310077\n",
      "[7,  2000] loss: 2.311635\n",
      "[7,  2200] loss: 2.313161\n",
      "[7,  2400] loss: 2.311255\n",
      "[7,  2600] loss: 2.309128\n",
      "[7,  2800] loss: 2.310898\n",
      "[7,  3000] loss: 2.310951\n",
      "[7,  3200] loss: 2.311953\n",
      "Got 422 / 4000 correct (10.55)\n",
      "saving model\n",
      "[8,   200] loss: 2.311291\n",
      "[8,   400] loss: 2.309885\n",
      "[8,   600] loss: 2.310340\n",
      "[8,   800] loss: 2.310385\n",
      "[8,  1000] loss: 2.310630\n",
      "[8,  1200] loss: 2.310875\n",
      "[8,  1400] loss: 2.311827\n",
      "[8,  1600] loss: 2.311997\n",
      "[8,  1800] loss: 2.312790\n",
      "[8,  2000] loss: 2.312450\n",
      "[8,  2200] loss: 2.312003\n",
      "[8,  2400] loss: 2.310367\n",
      "[8,  2600] loss: 2.309839\n",
      "[8,  2800] loss: 2.311251\n",
      "[8,  3000] loss: 2.312037\n",
      "[8,  3200] loss: 2.310409\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[9,   200] loss: 2.312690\n",
      "[9,   400] loss: 2.310406\n",
      "[9,   600] loss: 2.311239\n",
      "[9,   800] loss: 2.311353\n",
      "[9,  1000] loss: 2.312231\n",
      "[9,  1200] loss: 2.311334\n",
      "[9,  1400] loss: 2.311513\n",
      "[9,  1600] loss: 2.311268\n",
      "[9,  1800] loss: 2.312022\n",
      "[9,  2000] loss: 2.313215\n",
      "[9,  2200] loss: 2.311208\n",
      "[9,  2400] loss: 2.311597\n",
      "[9,  2600] loss: 2.313090\n",
      "[9,  2800] loss: 2.311009\n",
      "[9,  3000] loss: 2.309706\n",
      "[9,  3200] loss: 2.313124\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[10,   200] loss: 2.311188\n",
      "[10,   400] loss: 2.312426\n",
      "[10,   600] loss: 2.311926\n",
      "[10,   800] loss: 2.310368\n",
      "[10,  1000] loss: 2.310801\n",
      "[10,  1200] loss: 2.314049\n",
      "[10,  1400] loss: 2.309289\n",
      "[10,  1600] loss: 2.311576\n",
      "[10,  1800] loss: 2.313142\n",
      "[10,  2000] loss: 2.313683\n",
      "[10,  2200] loss: 2.311688\n",
      "[10,  2400] loss: 2.312252\n",
      "[10,  2600] loss: 2.313776\n",
      "[10,  2800] loss: 2.311045\n",
      "[10,  3000] loss: 2.310696\n",
      "[10,  3200] loss: 2.311250\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[11,   200] loss: 2.311638\n",
      "[11,   400] loss: 2.313176\n",
      "[11,   600] loss: 2.311157\n",
      "[11,   800] loss: 2.311854\n",
      "[11,  1000] loss: 2.311038\n",
      "[11,  1200] loss: 2.312374\n",
      "[11,  1400] loss: 2.312162\n",
      "[11,  1600] loss: 2.310448\n",
      "[11,  1800] loss: 2.313868\n",
      "[11,  2000] loss: 2.312327\n",
      "[11,  2200] loss: 2.309990\n",
      "[11,  2400] loss: 2.311634\n",
      "[11,  2600] loss: 2.311467\n",
      "[11,  2800] loss: 2.312378\n",
      "[11,  3000] loss: 2.312280\n",
      "[11,  3200] loss: 2.310061\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[12,   200] loss: 2.312800\n",
      "[12,   400] loss: 2.310238\n",
      "[12,   600] loss: 2.310975\n",
      "[12,   800] loss: 2.310868\n",
      "[12,  1000] loss: 2.310473\n",
      "[12,  1200] loss: 2.310938\n",
      "[12,  1400] loss: 2.311842\n",
      "[12,  1600] loss: 2.310167\n",
      "[12,  1800] loss: 2.311981\n",
      "[12,  2000] loss: 2.309308\n",
      "[12,  2200] loss: 2.311066\n",
      "[12,  2400] loss: 2.311303\n",
      "[12,  2600] loss: 2.311534\n",
      "[12,  2800] loss: 2.312222\n",
      "[12,  3000] loss: 2.310511\n",
      "[12,  3200] loss: 2.310096\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[13,   200] loss: 2.312733\n",
      "[13,   400] loss: 2.311030\n",
      "[13,   600] loss: 2.310776\n",
      "[13,   800] loss: 2.312101\n",
      "[13,  1000] loss: 2.312443\n",
      "[13,  1200] loss: 2.310108\n",
      "[13,  1400] loss: 2.312406\n",
      "[13,  1600] loss: 2.311749\n",
      "[13,  1800] loss: 2.311078\n",
      "[13,  2000] loss: 2.309634\n",
      "[13,  2200] loss: 2.310277\n",
      "[13,  2400] loss: 2.311488\n",
      "[13,  2600] loss: 2.311264\n",
      "[13,  2800] loss: 2.310545\n",
      "[13,  3000] loss: 2.312496\n",
      "[13,  3200] loss: 2.312073\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[14,   200] loss: 2.311743\n",
      "[14,   400] loss: 2.310797\n",
      "[14,   600] loss: 2.310894\n",
      "[14,   800] loss: 2.310709\n",
      "[14,  1000] loss: 2.311599\n",
      "[14,  1200] loss: 2.310552\n",
      "[14,  1400] loss: 2.312717\n",
      "[14,  1600] loss: 2.310556\n",
      "[14,  1800] loss: 2.313828\n",
      "[14,  2000] loss: 2.310365\n",
      "[14,  2200] loss: 2.309800\n",
      "[14,  2400] loss: 2.313126\n",
      "[14,  2600] loss: 2.311658\n",
      "[14,  2800] loss: 2.311754\n",
      "[14,  3000] loss: 2.310399\n",
      "[14,  3200] loss: 2.308615\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[15,   200] loss: 2.309566\n",
      "[15,   400] loss: 2.310224\n",
      "[15,   600] loss: 2.313568\n",
      "[15,   800] loss: 2.309740\n",
      "[15,  1000] loss: 2.312280\n",
      "[15,  1200] loss: 2.311479\n",
      "[15,  1400] loss: 2.309049\n",
      "[15,  1600] loss: 2.312752\n",
      "[15,  1800] loss: 2.310416\n",
      "[15,  2000] loss: 2.311875\n",
      "[15,  2200] loss: 2.313206\n",
      "[15,  2400] loss: 2.310712\n",
      "[15,  2600] loss: 2.312708\n",
      "[15,  2800] loss: 2.309851\n",
      "[15,  3000] loss: 2.309293\n",
      "[15,  3200] loss: 2.314162\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[16,   200] loss: 2.309979\n",
      "[16,   400] loss: 2.312357\n",
      "[16,   600] loss: 2.312567\n",
      "[16,   800] loss: 2.311219\n",
      "[16,  1000] loss: 2.311931\n",
      "[16,  1200] loss: 2.312870\n",
      "[16,  1400] loss: 2.310976\n",
      "[16,  1600] loss: 2.312347\n",
      "[16,  1800] loss: 2.311700\n",
      "[16,  2000] loss: 2.311359\n",
      "[16,  2200] loss: 2.311119\n",
      "[16,  2400] loss: 2.313158\n",
      "[16,  2600] loss: 2.311091\n",
      "[16,  2800] loss: 2.310531\n",
      "[16,  3000] loss: 2.312366\n",
      "[16,  3200] loss: 2.311138\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[17,   200] loss: 2.312075\n",
      "[17,   400] loss: 2.311101\n",
      "[17,   600] loss: 2.310966\n",
      "[17,   800] loss: 2.311667\n",
      "[17,  1000] loss: 2.309613\n",
      "[17,  1200] loss: 2.311686\n",
      "[17,  1400] loss: 2.312919\n",
      "[17,  1600] loss: 2.312468\n",
      "[17,  1800] loss: 2.313566\n",
      "[17,  2000] loss: 2.311780\n",
      "[17,  2200] loss: 2.309399\n",
      "[17,  2400] loss: 2.309327\n",
      "[17,  2600] loss: 2.310137\n",
      "[17,  2800] loss: 2.311604\n",
      "[17,  3000] loss: 2.311279\n",
      "[17,  3200] loss: 2.311351\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[18,   200] loss: 2.312722\n",
      "[18,   400] loss: 2.310881\n",
      "[18,   600] loss: 2.312798\n",
      "[18,   800] loss: 2.311692\n",
      "[18,  1000] loss: 2.310415\n",
      "[18,  1200] loss: 2.311529\n",
      "[18,  1400] loss: 2.312263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,  1600] loss: 2.310255\n",
      "[18,  1800] loss: 2.310228\n",
      "[18,  2000] loss: 2.310925\n",
      "[18,  2200] loss: 2.311773\n",
      "[18,  2400] loss: 2.310134\n",
      "[18,  2600] loss: 2.312995\n",
      "[18,  2800] loss: 2.311641\n",
      "[18,  3000] loss: 2.310129\n",
      "[18,  3200] loss: 2.311383\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[19,   200] loss: 2.311464\n",
      "[19,   400] loss: 2.310137\n",
      "[19,   600] loss: 2.313895\n",
      "[19,   800] loss: 2.310169\n",
      "[19,  1000] loss: 2.311131\n",
      "[19,  1200] loss: 2.310398\n",
      "[19,  1400] loss: 2.312143\n",
      "[19,  1600] loss: 2.311520\n",
      "[19,  1800] loss: 2.311032\n",
      "[19,  2000] loss: 2.314157\n",
      "[19,  2200] loss: 2.310608\n",
      "[19,  2400] loss: 2.311236\n",
      "[19,  2600] loss: 2.312939\n",
      "[19,  2800] loss: 2.310826\n",
      "[19,  3000] loss: 2.311592\n",
      "[19,  3200] loss: 2.311235\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[20,   200] loss: 2.312753\n",
      "[20,   400] loss: 2.311493\n",
      "[20,   600] loss: 2.311619\n",
      "[20,   800] loss: 2.311684\n",
      "[20,  1000] loss: 2.313390\n",
      "[20,  1200] loss: 2.311181\n",
      "[20,  1400] loss: 2.311384\n",
      "[20,  1600] loss: 2.310285\n",
      "[20,  1800] loss: 2.311860\n",
      "[20,  2000] loss: 2.311185\n",
      "[20,  2200] loss: 2.310347\n",
      "[20,  2400] loss: 2.309299\n",
      "[20,  2600] loss: 2.310581\n",
      "[20,  2800] loss: 2.309981\n",
      "[20,  3000] loss: 2.310060\n",
      "[20,  3200] loss: 2.311845\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[21,   200] loss: 2.311368\n",
      "[21,   400] loss: 2.312665\n",
      "[21,   600] loss: 2.311946\n",
      "[21,   800] loss: 2.310215\n",
      "[21,  1000] loss: 2.312308\n",
      "[21,  1200] loss: 2.311035\n",
      "[21,  1400] loss: 2.312171\n",
      "[21,  1600] loss: 2.310225\n",
      "[21,  1800] loss: 2.311084\n",
      "[21,  2000] loss: 2.311932\n",
      "[21,  2200] loss: 2.311252\n",
      "[21,  2400] loss: 2.310666\n",
      "[21,  2600] loss: 2.311635\n",
      "[21,  2800] loss: 2.311071\n",
      "[21,  3000] loss: 2.310298\n",
      "[21,  3200] loss: 2.311701\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[22,   200] loss: 2.311398\n",
      "[22,   400] loss: 2.311590\n",
      "[22,   600] loss: 2.311635\n",
      "[22,   800] loss: 2.313317\n",
      "[22,  1000] loss: 2.310451\n",
      "[22,  1200] loss: 2.312439\n",
      "[22,  1400] loss: 2.310140\n",
      "[22,  1600] loss: 2.310702\n",
      "[22,  1800] loss: 2.311776\n",
      "[22,  2000] loss: 2.310919\n",
      "[22,  2200] loss: 2.311986\n",
      "[22,  2400] loss: 2.310916\n",
      "[22,  2600] loss: 2.311916\n",
      "[22,  2800] loss: 2.311748\n",
      "[22,  3000] loss: 2.311728\n",
      "[22,  3200] loss: 2.311026\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[23,   200] loss: 2.310772\n",
      "[23,   400] loss: 2.314632\n",
      "[23,   600] loss: 2.311256\n",
      "[23,   800] loss: 2.310893\n",
      "[23,  1000] loss: 2.313488\n",
      "[23,  1200] loss: 2.310868\n",
      "[23,  1400] loss: 2.310602\n",
      "[23,  1600] loss: 2.310631\n",
      "[23,  1800] loss: 2.311228\n",
      "[23,  2000] loss: 2.312584\n",
      "[23,  2200] loss: 2.311773\n",
      "[23,  2400] loss: 2.312524\n",
      "[23,  2600] loss: 2.310843\n",
      "[23,  2800] loss: 2.310637\n",
      "[23,  3000] loss: 2.309172\n",
      "[23,  3200] loss: 2.311359\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[24,   200] loss: 2.313111\n",
      "[24,   400] loss: 2.312309\n",
      "[24,   600] loss: 2.314259\n",
      "[24,   800] loss: 2.311562\n",
      "[24,  1000] loss: 2.312799\n",
      "[24,  1200] loss: 2.312546\n",
      "[24,  1400] loss: 2.311878\n",
      "[24,  1600] loss: 2.310381\n",
      "[24,  1800] loss: 2.310866\n",
      "[24,  2000] loss: 2.311511\n",
      "[24,  2200] loss: 2.311437\n",
      "[24,  2400] loss: 2.310596\n",
      "[24,  2600] loss: 2.311744\n",
      "[24,  2800] loss: 2.312982\n",
      "[24,  3000] loss: 2.313325\n",
      "[24,  3200] loss: 2.310886\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[25,   200] loss: 2.310636\n",
      "[25,   400] loss: 2.310473\n",
      "[25,   600] loss: 2.310580\n",
      "[25,   800] loss: 3.411336\n",
      "[25,  1000] loss: 3.265776\n",
      "[25,  1200] loss: 2.309788\n",
      "[25,  1400] loss: 2.310542\n",
      "[25,  1600] loss: 2.307972\n",
      "[25,  1800] loss: 2.310761\n",
      "[25,  2000] loss: 2.311774\n",
      "[25,  2200] loss: 2.312437\n",
      "[25,  2400] loss: 2.313279\n",
      "[25,  2600] loss: 2.310156\n",
      "[25,  2800] loss: 2.310261\n",
      "[25,  3000] loss: 2.311166\n",
      "[25,  3200] loss: 2.311487\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[26,   200] loss: 2.311428\n",
      "[26,   400] loss: 2.311227\n",
      "[26,   600] loss: 2.312741\n",
      "[26,   800] loss: 2.310513\n",
      "[26,  1000] loss: 2.311992\n",
      "[26,  1200] loss: 2.312250\n",
      "[26,  1400] loss: 2.311003\n",
      "[26,  1600] loss: 2.311304\n",
      "[26,  1800] loss: 2.311634\n",
      "[26,  2000] loss: 2.311342\n",
      "[26,  2200] loss: 2.311320\n",
      "[26,  2400] loss: 2.312374\n",
      "[26,  2600] loss: 2.311583\n",
      "[26,  2800] loss: 2.311876\n",
      "[26,  3000] loss: 2.309912\n",
      "[26,  3200] loss: 2.313185\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[27,   200] loss: 2.312444\n",
      "[27,   400] loss: 2.309799\n",
      "[27,   600] loss: 2.311535\n",
      "[27,   800] loss: 2.311191\n",
      "[27,  1000] loss: 2.310321\n",
      "[27,  1200] loss: 2.312079\n",
      "[27,  1400] loss: 2.310418\n",
      "[27,  1600] loss: 2.311916\n",
      "[27,  1800] loss: 2.312419\n",
      "[27,  2000] loss: 2.309775\n",
      "[27,  2200] loss: 2.311638\n",
      "[27,  2400] loss: 2.313278\n",
      "[27,  2600] loss: 2.311143\n",
      "[27,  2800] loss: 2.311774\n",
      "[27,  3000] loss: 2.311085\n",
      "[27,  3200] loss: 2.309501\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[28,   200] loss: 2.311536\n",
      "[28,   400] loss: 2.312784\n",
      "[28,   600] loss: 2.312979\n",
      "[28,   800] loss: 2.311271\n",
      "[28,  1000] loss: 2.311059\n",
      "[28,  1200] loss: 2.311128\n",
      "[28,  1400] loss: 2.309738\n",
      "[28,  1600] loss: 2.311345\n",
      "[28,  1800] loss: 2.310596\n",
      "[28,  2000] loss: 2.310853\n",
      "[28,  2200] loss: 2.310125\n",
      "[28,  2400] loss: 2.312157\n",
      "[28,  2600] loss: 2.311398\n",
      "[28,  2800] loss: 2.310733\n",
      "[28,  3000] loss: 2.309195\n",
      "[28,  3200] loss: 2.311212\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[29,   200] loss: 2.312821\n",
      "[29,   400] loss: 2.311709\n",
      "[29,   600] loss: 2.312558\n",
      "[29,   800] loss: 2.308254\n",
      "[29,  1000] loss: 2.311464\n",
      "[29,  1200] loss: 2.309881\n",
      "[29,  1400] loss: 2.312477\n",
      "[29,  1600] loss: 2.311203\n",
      "[29,  1800] loss: 2.310755\n",
      "[29,  2000] loss: 2.312588\n",
      "[29,  2200] loss: 2.311773\n",
      "[29,  2400] loss: 2.309070\n",
      "[29,  2600] loss: 2.312675\n",
      "[29,  2800] loss: 2.310533\n",
      "[29,  3000] loss: 2.309938\n",
      "[29,  3200] loss: 2.313367\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[30,   200] loss: 2.311987\n",
      "[30,   400] loss: 2.311649\n",
      "[30,   600] loss: 2.309903\n",
      "[30,   800] loss: 2.311571\n",
      "[30,  1000] loss: 2.311859\n",
      "[30,  1200] loss: 2.310465\n",
      "[30,  1400] loss: 2.311729\n",
      "[30,  1600] loss: 2.311735\n",
      "[30,  1800] loss: 2.312145\n",
      "[30,  2000] loss: 2.312855\n",
      "[30,  2200] loss: 2.311696\n",
      "[30,  2400] loss: 2.312042\n",
      "[30,  2600] loss: 2.312935\n",
      "[30,  2800] loss: 2.310427\n",
      "[30,  3000] loss: 2.311539\n",
      "[30,  3200] loss: 2.311245\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[31,   200] loss: 2.310727\n",
      "[31,   400] loss: 2.312142\n",
      "[31,   600] loss: 2.311783\n",
      "[31,   800] loss: 2.312572\n",
      "[31,  1000] loss: 2.312403\n",
      "[31,  1200] loss: 2.310213\n",
      "[31,  1400] loss: 2.312721\n",
      "[31,  1600] loss: 2.312588\n",
      "[31,  1800] loss: 2.311623\n",
      "[31,  2000] loss: 2.310980\n",
      "[31,  2200] loss: 2.310380\n",
      "[31,  2400] loss: 2.312924\n",
      "[31,  2600] loss: 2.311746\n",
      "[31,  2800] loss: 2.311019\n",
      "[31,  3000] loss: 2.310531\n",
      "[31,  3200] loss: 2.311417\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[32,   200] loss: 2.312526\n",
      "[32,   400] loss: 2.310984\n",
      "[32,   600] loss: 2.311582\n",
      "[32,   800] loss: 2.490372\n",
      "[32,  1000] loss: 2.311455\n",
      "[32,  1200] loss: 2.310901\n",
      "[32,  1400] loss: 2.310009\n",
      "[32,  1600] loss: 2.311802\n",
      "[32,  1800] loss: 2.311404\n",
      "[32,  2000] loss: 2.311310\n",
      "[32,  2200] loss: 2.310394\n",
      "[32,  2400] loss: 2.312473\n",
      "[32,  2600] loss: 2.314788\n",
      "[32,  2800] loss: 2.311789\n",
      "[32,  3000] loss: 2.311790\n",
      "[32,  3200] loss: 2.312224\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[33,   200] loss: 2.313063\n",
      "[33,   400] loss: 2.312868\n",
      "[33,   600] loss: 2.311830\n",
      "[33,   800] loss: 2.310779\n",
      "[33,  1000] loss: 2.311508\n",
      "[33,  1200] loss: 2.310881\n",
      "[33,  1400] loss: 2.310789\n",
      "[33,  1600] loss: 2.308683\n",
      "[33,  1800] loss: 2.313320\n",
      "[33,  2000] loss: 2.313299\n",
      "[33,  2200] loss: 2.311193\n",
      "[33,  2400] loss: 2.312338\n",
      "[33,  2600] loss: 2.311331\n",
      "[33,  2800] loss: 2.310770\n",
      "[33,  3000] loss: 2.309499\n",
      "[33,  3200] loss: 2.310366\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[34,   200] loss: 2.309195\n",
      "[34,   400] loss: 2.312220\n",
      "[34,   600] loss: 2.311733\n",
      "[34,   800] loss: 2.312835\n",
      "[34,  1000] loss: 2.312889\n",
      "[34,  1200] loss: 2.309790\n",
      "[34,  1400] loss: 2.309529\n",
      "[34,  1600] loss: 2.309415\n",
      "[34,  1800] loss: 2.313931\n",
      "[34,  2000] loss: 2.312065\n",
      "[34,  2200] loss: 2.312086\n",
      "[34,  2400] loss: 2.310860\n",
      "[34,  2600] loss: 2.312677\n",
      "[34,  2800] loss: 2.309103\n",
      "[34,  3000] loss: 2.312220\n",
      "[34,  3200] loss: 2.312608\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[35,   200] loss: 2.311367\n",
      "[35,   400] loss: 2.311174\n",
      "[35,   600] loss: 2.312413\n",
      "[35,   800] loss: 2.311085\n",
      "[35,  1000] loss: 2.312235\n",
      "[35,  1200] loss: 2.310517\n",
      "[35,  1400] loss: 2.312143\n",
      "[35,  1600] loss: 2.310614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,  1800] loss: 2.311486\n",
      "[35,  2000] loss: 2.312290\n",
      "[35,  2200] loss: 2.311744\n",
      "[35,  2400] loss: 2.311786\n",
      "[35,  2600] loss: 2.311458\n",
      "[35,  2800] loss: 2.310631\n",
      "[35,  3000] loss: 2.310690\n",
      "[35,  3200] loss: 2.312959\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[36,   200] loss: 2.312196\n",
      "[36,   400] loss: 2.310258\n",
      "[36,   600] loss: 2.311254\n",
      "[36,   800] loss: 2.310758\n",
      "[36,  1000] loss: 2.310600\n",
      "[36,  1200] loss: 2.310181\n",
      "[36,  1400] loss: 2.313831\n",
      "[36,  1600] loss: 2.311746\n",
      "[36,  1800] loss: 2.309990\n",
      "[36,  2000] loss: 2.309896\n",
      "[36,  2200] loss: 2.310459\n",
      "[36,  2400] loss: 2.312920\n",
      "[36,  2600] loss: 2.312827\n",
      "[36,  2800] loss: 2.311617\n",
      "[36,  3000] loss: 2.312100\n",
      "[36,  3200] loss: 2.309221\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[37,   200] loss: 2.311801\n",
      "[37,   400] loss: 2.311306\n",
      "[37,   600] loss: 2.309986\n",
      "[37,   800] loss: 2.312132\n",
      "[37,  1000] loss: 2.312793\n",
      "[37,  1200] loss: 2.313595\n",
      "[37,  1400] loss: 2.314409\n",
      "[37,  1600] loss: 2.310985\n",
      "[37,  1800] loss: 2.311101\n",
      "[37,  2000] loss: 2.313317\n",
      "[37,  2200] loss: 2.309904\n",
      "[37,  2400] loss: 2.309186\n",
      "[37,  2600] loss: 2.311809\n",
      "[37,  2800] loss: 2.311261\n",
      "[37,  3000] loss: 2.311997\n",
      "[37,  3200] loss: 2.312049\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[38,   200] loss: 2.312086\n",
      "[38,   400] loss: 2.311747\n",
      "[38,   600] loss: 2.312296\n",
      "[38,   800] loss: 2.314173\n",
      "[38,  1000] loss: 2.309254\n",
      "[38,  1200] loss: 2.311255\n",
      "[38,  1400] loss: 2.311399\n",
      "[38,  1600] loss: 2.312437\n",
      "[38,  1800] loss: 2.312047\n",
      "[38,  2000] loss: 2.311437\n",
      "[38,  2200] loss: 2.310452\n",
      "[38,  2400] loss: 2.312976\n",
      "[38,  2600] loss: 2.310574\n",
      "[38,  2800] loss: 2.312492\n",
      "[38,  3000] loss: 2.310650\n",
      "[38,  3200] loss: 2.312588\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[39,   200] loss: 2.311221\n",
      "[39,   400] loss: 2.311514\n",
      "[39,   600] loss: 2.310353\n",
      "[39,   800] loss: 2.312655\n",
      "[39,  1000] loss: 2.310330\n",
      "[39,  1200] loss: 2.312545\n",
      "[39,  1400] loss: 2.313033\n",
      "[39,  1600] loss: 2.311976\n",
      "[39,  1800] loss: 2.310203\n",
      "[39,  2000] loss: 2.309471\n",
      "[39,  2200] loss: 2.313103\n",
      "[39,  2400] loss: 2.311406\n",
      "[39,  2600] loss: 2.312788\n",
      "[39,  2800] loss: 2.312419\n",
      "[39,  3000] loss: 2.311417\n",
      "[39,  3200] loss: 2.309837\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[40,   200] loss: 2.312520\n",
      "[40,   400] loss: 2.311083\n",
      "[40,   600] loss: 2.311370\n",
      "[40,   800] loss: 2.312765\n",
      "[40,  1000] loss: 2.312443\n",
      "[40,  1200] loss: 2.311174\n",
      "[40,  1400] loss: 2.310865\n",
      "[40,  1600] loss: 2.312046\n",
      "[40,  1800] loss: 2.311811\n",
      "[40,  2000] loss: 2.311437\n",
      "[40,  2200] loss: 2.311783\n",
      "[40,  2400] loss: 2.310165\n",
      "[40,  2600] loss: 2.309795\n",
      "[40,  2800] loss: 2.312263\n",
      "[40,  3000] loss: 2.312607\n",
      "[40,  3200] loss: 2.310489\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[41,   200] loss: 2.311913\n",
      "[41,   400] loss: 2.310950\n",
      "[41,   600] loss: 2.312023\n",
      "[41,   800] loss: 2.310614\n",
      "[41,  1000] loss: 2.311814\n",
      "[41,  1200] loss: 2.311834\n",
      "[41,  1400] loss: 2.312915\n",
      "[41,  1600] loss: 2.310742\n",
      "[41,  1800] loss: 2.311947\n",
      "[41,  2000] loss: 2.311091\n",
      "[41,  2200] loss: 2.312313\n",
      "[41,  2400] loss: 2.310916\n",
      "[41,  2600] loss: 2.312880\n",
      "[41,  2800] loss: 2.312685\n",
      "[41,  3000] loss: 2.312556\n",
      "[41,  3200] loss: 2.311062\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[42,   200] loss: 2.313943\n",
      "[42,   400] loss: 2.310950\n",
      "[42,   600] loss: 2.312647\n",
      "[42,   800] loss: 2.311934\n",
      "[42,  1000] loss: 2.311270\n",
      "[42,  1200] loss: 2.312273\n",
      "[42,  1400] loss: 2.311725\n",
      "[42,  1600] loss: 2.311562\n",
      "[42,  1800] loss: 2.314271\n",
      "[42,  2000] loss: 2.311043\n",
      "[42,  2200] loss: 2.311122\n",
      "[42,  2400] loss: 2.310639\n",
      "[42,  2600] loss: 2.312268\n",
      "[42,  2800] loss: 2.313376\n",
      "[42,  3000] loss: 2.310764\n",
      "[42,  3200] loss: 2.311087\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[43,   200] loss: 2.310885\n",
      "[43,   400] loss: 2.310315\n",
      "[43,   600] loss: 2.311135\n",
      "[43,   800] loss: 2.308790\n",
      "[43,  1000] loss: 2.309435\n",
      "[43,  1200] loss: 2.312872\n",
      "[43,  1400] loss: 2.312570\n",
      "[43,  1600] loss: 2.312290\n",
      "[43,  1800] loss: 2.311942\n",
      "[43,  2000] loss: 2.308893\n",
      "[43,  2200] loss: 2.311727\n",
      "[43,  2400] loss: 2.309525\n",
      "[43,  2600] loss: 2.312533\n",
      "[43,  2800] loss: 2.310722\n",
      "[43,  3000] loss: 2.311746\n",
      "[43,  3200] loss: 2.312284\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[44,   200] loss: 2.309774\n",
      "[44,   400] loss: 2.310354\n",
      "[44,   600] loss: 2.310659\n",
      "[44,   800] loss: 2.311095\n",
      "[44,  1000] loss: 2.313015\n",
      "[44,  1200] loss: 2.310104\n",
      "[44,  1400] loss: 2.311367\n",
      "[44,  1600] loss: 2.312320\n",
      "[44,  1800] loss: 2.309290\n",
      "[44,  2000] loss: 2.311284\n",
      "[44,  2200] loss: 2.313420\n",
      "[44,  2400] loss: 2.313046\n",
      "[44,  2600] loss: 2.310877\n",
      "[44,  2800] loss: 2.311897\n",
      "[44,  3000] loss: 2.309279\n",
      "[44,  3200] loss: 2.310986\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[45,   200] loss: 2.311364\n",
      "[45,   400] loss: 2.311060\n",
      "[45,   600] loss: 2.311664\n",
      "[45,   800] loss: 2.312225\n",
      "[45,  1000] loss: 2.312961\n",
      "[45,  1200] loss: 2.313182\n",
      "[45,  1400] loss: 2.313426\n",
      "[45,  1600] loss: 2.310194\n",
      "[45,  1800] loss: 2.311183\n",
      "[45,  2000] loss: 2.310601\n",
      "[45,  2200] loss: 2.312303\n",
      "[45,  2400] loss: 2.312279\n",
      "[45,  2600] loss: 2.313265\n",
      "[45,  2800] loss: 2.312642\n",
      "[45,  3000] loss: 2.310930\n",
      "[45,  3200] loss: 2.311340\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[46,   200] loss: 2.312077\n",
      "[46,   400] loss: 2.312427\n",
      "[46,   600] loss: 2.312740\n",
      "[46,   800] loss: 2.311538\n",
      "[46,  1000] loss: 2.312673\n",
      "[46,  1200] loss: 2.311631\n",
      "[46,  1400] loss: 2.310739\n",
      "[46,  1600] loss: 2.310314\n",
      "[46,  1800] loss: 2.310534\n",
      "[46,  2000] loss: 2.311406\n",
      "[46,  2200] loss: 2.313082\n",
      "[46,  2400] loss: 2.311724\n",
      "[46,  2600] loss: 2.310959\n",
      "[46,  2800] loss: 2.311043\n",
      "[46,  3000] loss: 2.312212\n",
      "[46,  3200] loss: 2.311775\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[47,   200] loss: 2.311667\n",
      "[47,   400] loss: 2.310974\n",
      "[47,   600] loss: 2.311978\n",
      "[47,   800] loss: 2.310171\n",
      "[47,  1000] loss: 2.311235\n",
      "[47,  1200] loss: 2.310903\n",
      "[47,  1400] loss: 2.312666\n",
      "[47,  1600] loss: 2.310655\n",
      "[47,  1800] loss: 2.311396\n",
      "[47,  2000] loss: 2.311807\n",
      "[47,  2200] loss: 2.313178\n",
      "[47,  2400] loss: 2.311444\n",
      "[47,  2600] loss: 2.312380\n",
      "[47,  2800] loss: 2.312068\n",
      "[47,  3000] loss: 2.311248\n",
      "[47,  3200] loss: 2.311069\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[48,   200] loss: 2.310226\n",
      "[48,   400] loss: 2.313634\n",
      "[48,   600] loss: 2.311110\n",
      "[48,   800] loss: 2.311198\n",
      "[48,  1000] loss: 2.313170\n",
      "[48,  1200] loss: 2.311063\n",
      "[48,  1400] loss: 2.309954\n",
      "[48,  1600] loss: 2.310153\n",
      "[48,  1800] loss: 2.314466\n",
      "[48,  2000] loss: 2.312112\n",
      "[48,  2200] loss: 2.312037\n",
      "[48,  2400] loss: 2.311564\n",
      "[48,  2600] loss: 2.312252\n",
      "[48,  2800] loss: 2.309496\n",
      "[48,  3000] loss: 2.310245\n",
      "[48,  3200] loss: 2.312831\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[49,   200] loss: 2.311253\n",
      "[49,   400] loss: 2.311408\n",
      "[49,   600] loss: 2.311830\n",
      "[49,   800] loss: 2.311922\n",
      "[49,  1000] loss: 2.311172\n",
      "[49,  1200] loss: 2.312146\n",
      "[49,  1400] loss: 2.311459\n",
      "[49,  1600] loss: 2.311391\n",
      "[49,  1800] loss: 2.311826\n",
      "[49,  2000] loss: 2.311836\n",
      "[49,  2200] loss: 2.312085\n",
      "[49,  2400] loss: 2.310952\n",
      "[49,  2600] loss: 2.310152\n",
      "[49,  2800] loss: 2.309767\n",
      "[49,  3000] loss: 2.310417\n",
      "[49,  3200] loss: 2.309351\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[50,   200] loss: 2.311588\n",
      "[50,   400] loss: 2.311293\n",
      "[50,   600] loss: 2.312392\n",
      "[50,   800] loss: 2.315160\n",
      "[50,  1000] loss: 2.311769\n",
      "[50,  1200] loss: 2.313204\n",
      "[50,  1400] loss: 2.310404\n",
      "[50,  1600] loss: 2.311960\n",
      "[50,  1800] loss: 2.310682\n",
      "[50,  2000] loss: 2.310645\n",
      "[50,  2200] loss: 2.311439\n",
      "[50,  2400] loss: 2.309959\n",
      "[50,  2600] loss: 2.312141\n",
      "[50,  2800] loss: 2.310297\n",
      "[50,  3000] loss: 2.311040\n",
      "[50,  3200] loss: 2.312865\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[51,   200] loss: 2.312387\n",
      "[51,   400] loss: 2.310846\n",
      "[51,   600] loss: 2.309450\n",
      "[51,   800] loss: 2.309878\n",
      "[51,  1000] loss: 2.311483\n",
      "[51,  1200] loss: 2.310914\n",
      "[51,  1400] loss: 2.313557\n",
      "[51,  1600] loss: 2.311650\n",
      "[51,  1800] loss: 2.311854\n",
      "[51,  2000] loss: 2.312918\n",
      "[51,  2200] loss: 2.312216\n",
      "[51,  2400] loss: 2.309535\n",
      "[51,  2600] loss: 2.310871\n",
      "[51,  2800] loss: 2.311405\n",
      "[51,  3000] loss: 2.312946\n",
      "[51,  3200] loss: 2.313651\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[52,   200] loss: 2.311146\n",
      "[52,   400] loss: 2.312568\n",
      "[52,   600] loss: 2.314237\n",
      "[52,   800] loss: 2.311067\n",
      "[52,  1000] loss: 2.311290\n",
      "[52,  1200] loss: 2.310497\n",
      "[52,  1400] loss: 2.314062\n",
      "[52,  1600] loss: 2.311535\n",
      "[52,  1800] loss: 2.310551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52,  2000] loss: 2.312041\n",
      "[52,  2200] loss: 2.311237\n",
      "[52,  2400] loss: 2.311553\n",
      "[52,  2600] loss: 2.312147\n",
      "[52,  2800] loss: 2.309171\n",
      "[52,  3000] loss: 2.312638\n",
      "[52,  3200] loss: 2.312055\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[53,   200] loss: 2.310076\n",
      "[53,   400] loss: 2.311470\n",
      "[53,   600] loss: 2.309980\n",
      "[53,   800] loss: 2.312093\n",
      "[53,  1000] loss: 2.311072\n",
      "[53,  1200] loss: 2.311404\n",
      "[53,  1400] loss: 2.311075\n",
      "[53,  1600] loss: 2.309230\n",
      "[53,  1800] loss: 2.310322\n",
      "[53,  2000] loss: 2.313308\n",
      "[53,  2200] loss: 2.311995\n",
      "[53,  2400] loss: 2.311121\n",
      "[53,  2600] loss: 2.311593\n",
      "[53,  2800] loss: 2.310726\n",
      "[53,  3000] loss: 2.309841\n",
      "[53,  3200] loss: 2.313098\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[54,   200] loss: 2.311714\n",
      "[54,   400] loss: 2.310141\n",
      "[54,   600] loss: 2.311661\n",
      "[54,   800] loss: 2.310556\n",
      "[54,  1000] loss: 2.311555\n",
      "[54,  1200] loss: 2.311055\n",
      "[54,  1400] loss: 2.312875\n",
      "[54,  1600] loss: 2.312013\n",
      "[54,  1800] loss: 2.311897\n",
      "[54,  2000] loss: 2.309591\n",
      "[54,  2200] loss: 2.311150\n",
      "[54,  2400] loss: 2.310503\n",
      "[54,  2600] loss: 2.311835\n",
      "[54,  2800] loss: 2.311043\n",
      "[54,  3000] loss: 2.311838\n",
      "[54,  3200] loss: 2.310037\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[55,   200] loss: 2.312320\n",
      "[55,   400] loss: 2.310732\n",
      "[55,   600] loss: 2.309247\n",
      "[55,   800] loss: 2.312517\n",
      "[55,  1000] loss: 2.311559\n",
      "[55,  1200] loss: 2.310360\n",
      "[55,  1400] loss: 2.310946\n",
      "[55,  1600] loss: 2.310342\n",
      "[55,  1800] loss: 2.311022\n",
      "[55,  2000] loss: 2.312636\n",
      "[55,  2200] loss: 2.313084\n",
      "[55,  2400] loss: 2.311170\n",
      "[55,  2600] loss: 2.312556\n",
      "[55,  2800] loss: 2.310908\n",
      "[55,  3000] loss: 2.310553\n",
      "[55,  3200] loss: 2.312222\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[56,   200] loss: 2.311687\n",
      "[56,   400] loss: 2.311738\n",
      "[56,   600] loss: 2.311714\n",
      "[56,   800] loss: 2.312835\n",
      "[56,  1000] loss: 2.312014\n",
      "[56,  1200] loss: 2.313833\n",
      "[56,  1400] loss: 2.311782\n",
      "[56,  1600] loss: 2.310321\n",
      "[56,  1800] loss: 2.309800\n",
      "[56,  2000] loss: 2.311001\n",
      "[56,  2200] loss: 2.310141\n",
      "[56,  2400] loss: 2.311723\n",
      "[56,  2600] loss: 2.311493\n",
      "[56,  2800] loss: 2.313104\n",
      "[56,  3000] loss: 2.311606\n",
      "[56,  3200] loss: 2.312693\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[57,   200] loss: 2.310251\n",
      "[57,   400] loss: 2.311575\n",
      "[57,   600] loss: 2.312673\n",
      "[57,   800] loss: 2.311680\n",
      "[57,  1000] loss: 2.313509\n",
      "[57,  1200] loss: 2.310810\n",
      "[57,  1400] loss: 2.311895\n",
      "[57,  1600] loss: 2.310951\n",
      "[57,  1800] loss: 2.310843\n",
      "[57,  2000] loss: 2.310170\n",
      "[57,  2200] loss: 2.312695\n",
      "[57,  2400] loss: 2.311545\n",
      "[57,  2600] loss: 2.309894\n",
      "[57,  2800] loss: 2.310445\n",
      "[57,  3000] loss: 2.312732\n",
      "[57,  3200] loss: 2.312966\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[58,   200] loss: 2.312302\n",
      "[58,   400] loss: 2.311168\n",
      "[58,   600] loss: 2.312334\n",
      "[58,   800] loss: 2.311148\n",
      "[58,  1000] loss: 2.310582\n",
      "[58,  1200] loss: 2.311037\n",
      "[58,  1400] loss: 2.311459\n",
      "[58,  1600] loss: 2.311011\n",
      "[58,  1800] loss: 2.311550\n",
      "[58,  2000] loss: 2.312321\n",
      "[58,  2200] loss: 2.312166\n",
      "[58,  2400] loss: 2.311852\n",
      "[58,  2600] loss: 2.310740\n",
      "[58,  2800] loss: 2.309105\n",
      "[58,  3000] loss: 2.310831\n",
      "[58,  3200] loss: 2.311290\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[59,   200] loss: 2.312171\n",
      "[59,   400] loss: 2.311033\n",
      "[59,   600] loss: 2.310297\n",
      "[59,   800] loss: 2.312276\n",
      "[59,  1000] loss: 2.310815\n",
      "[59,  1200] loss: 2.310224\n",
      "[59,  1400] loss: 2.313593\n",
      "[59,  1600] loss: 2.309893\n",
      "[59,  1800] loss: 2.309158\n",
      "[59,  2000] loss: 2.311256\n",
      "[59,  2200] loss: 2.311463\n",
      "[59,  2400] loss: 2.312895\n",
      "[59,  2600] loss: 2.310859\n",
      "[59,  2800] loss: 2.311210\n",
      "[59,  3000] loss: 2.311174\n",
      "[59,  3200] loss: 2.312782\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[60,   200] loss: 2.312326\n",
      "[60,   400] loss: 2.310073\n",
      "[60,   600] loss: 2.309401\n",
      "[60,   800] loss: 2.310606\n",
      "[60,  1000] loss: 2.311187\n",
      "[60,  1200] loss: 2.312752\n",
      "[60,  1400] loss: 2.311976\n",
      "[60,  1600] loss: 2.310749\n",
      "[60,  1800] loss: 2.311090\n",
      "[60,  2000] loss: 2.312318\n",
      "[60,  2200] loss: 2.311826\n",
      "[60,  2400] loss: 2.312591\n",
      "[60,  2600] loss: 2.313109\n",
      "[60,  2800] loss: 2.311710\n",
      "[60,  3000] loss: 2.311474\n",
      "[60,  3200] loss: 2.311349\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[61,   200] loss: 2.311235\n",
      "[61,   400] loss: 2.311406\n",
      "[61,   600] loss: 2.312088\n",
      "[61,   800] loss: 2.310067\n",
      "[61,  1000] loss: 2.311205\n",
      "[61,  1200] loss: 2.311075\n",
      "[61,  1400] loss: 2.310820\n",
      "[61,  1600] loss: 2.312644\n",
      "[61,  1800] loss: 2.312264\n",
      "[61,  2000] loss: 2.312976\n",
      "[61,  2200] loss: 2.312674\n",
      "[61,  2400] loss: 2.310115\n",
      "[61,  2600] loss: 2.313227\n",
      "[61,  2800] loss: 2.312455\n",
      "[61,  3000] loss: 2.311743\n",
      "[61,  3200] loss: 2.312823\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[62,   200] loss: 2.311236\n",
      "[62,   400] loss: 2.312664\n",
      "[62,   600] loss: 2.310791\n",
      "[62,   800] loss: 2.311345\n",
      "[62,  1000] loss: 2.312158\n",
      "[62,  1200] loss: 2.310330\n",
      "[62,  1400] loss: 2.310775\n",
      "[62,  1600] loss: 2.309897\n",
      "[62,  1800] loss: 2.310292\n",
      "[62,  2000] loss: 2.310878\n",
      "[62,  2200] loss: 2.309980\n",
      "[62,  2400] loss: 2.310620\n",
      "[62,  2600] loss: 2.310825\n",
      "[62,  2800] loss: 2.312729\n",
      "[62,  3000] loss: 2.312184\n",
      "[62,  3200] loss: 2.311355\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[63,   200] loss: 2.309979\n",
      "[63,   400] loss: 2.310972\n",
      "[63,   600] loss: 2.310451\n",
      "[63,   800] loss: 2.310196\n",
      "[63,  1000] loss: 2.313255\n",
      "[63,  1200] loss: 2.310104\n",
      "[63,  1400] loss: 2.310612\n",
      "[63,  1600] loss: 2.314171\n",
      "[63,  1800] loss: 2.311253\n",
      "[63,  2000] loss: 2.313831\n",
      "[63,  2200] loss: 2.310556\n",
      "[63,  2400] loss: 2.311410\n",
      "[63,  2600] loss: 2.310802\n",
      "[63,  2800] loss: 2.310371\n",
      "[63,  3000] loss: 2.310341\n",
      "[63,  3200] loss: 2.313490\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[64,   200] loss: 2.310691\n",
      "[64,   400] loss: 2.312518\n",
      "[64,   600] loss: 2.310450\n",
      "[64,   800] loss: 2.310259\n",
      "[64,  1000] loss: 2.310849\n",
      "[64,  1200] loss: 2.313186\n",
      "[64,  1400] loss: 2.311538\n",
      "[64,  1600] loss: 2.313100\n",
      "[64,  1800] loss: 2.310344\n",
      "[64,  2000] loss: 2.311044\n",
      "[64,  2200] loss: 2.312684\n",
      "[64,  2400] loss: 2.315673\n",
      "[64,  2600] loss: 2.311624\n",
      "[64,  2800] loss: 2.311423\n",
      "[64,  3000] loss: 2.312470\n",
      "[64,  3200] loss: 2.311483\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[65,   200] loss: 2.310914\n",
      "[65,   400] loss: 2.310642\n",
      "[65,   600] loss: 2.309375\n",
      "[65,   800] loss: 2.311684\n",
      "[65,  1000] loss: 2.310990\n",
      "[65,  1200] loss: 2.310319\n",
      "[65,  1400] loss: 2.311493\n",
      "[65,  1600] loss: 2.311413\n",
      "[65,  1800] loss: 2.311168\n",
      "[65,  2000] loss: 2.311212\n",
      "[65,  2200] loss: 2.310986\n",
      "[65,  2400] loss: 2.312661\n",
      "[65,  2600] loss: 2.310934\n",
      "[65,  2800] loss: 2.311151\n",
      "[65,  3000] loss: 2.312394\n",
      "[65,  3200] loss: 2.311408\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[66,   200] loss: 2.311318\n",
      "[66,   400] loss: 2.311098\n",
      "[66,   600] loss: 2.312245\n",
      "[66,   800] loss: 2.310580\n",
      "[66,  1000] loss: 2.311918\n",
      "[66,  1200] loss: 2.312059\n",
      "[66,  1400] loss: 2.309882\n",
      "[66,  1600] loss: 2.308266\n",
      "[66,  1800] loss: 2.313106\n",
      "[66,  2000] loss: 2.310147\n",
      "[66,  2200] loss: 2.310655\n",
      "[66,  2400] loss: 2.312393\n",
      "[66,  2600] loss: 2.311748\n",
      "[66,  2800] loss: 2.313298\n",
      "[66,  3000] loss: 2.314273\n",
      "[66,  3200] loss: 2.310479\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[67,   200] loss: 2.311241\n",
      "[67,   400] loss: 2.311586\n",
      "[67,   600] loss: 2.310440\n",
      "[67,   800] loss: 2.311013\n",
      "[67,  1000] loss: 2.312144\n",
      "[67,  1200] loss: 2.312485\n",
      "[67,  1400] loss: 2.312975\n",
      "[67,  1600] loss: 2.312628\n",
      "[67,  1800] loss: 2.311624\n",
      "[67,  2000] loss: 2.312352\n",
      "[67,  2200] loss: 2.309916\n",
      "[67,  2400] loss: 2.311231\n",
      "[67,  2600] loss: 2.312565\n",
      "[67,  2800] loss: 2.312843\n",
      "[67,  3000] loss: 2.312567\n",
      "[67,  3200] loss: 2.310578\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[68,   200] loss: 2.313218\n",
      "[68,   400] loss: 2.314727\n",
      "[68,   600] loss: 2.311880\n",
      "[68,   800] loss: 2.313918\n",
      "[68,  1000] loss: 2.311315\n",
      "[68,  1200] loss: 2.313508\n",
      "[68,  1400] loss: 2.311220\n",
      "[68,  1600] loss: 2.312142\n",
      "[68,  1800] loss: 2.312402\n",
      "[68,  2000] loss: 2.312810\n",
      "[68,  2200] loss: 2.310197\n",
      "[68,  2400] loss: 2.310478\n",
      "[68,  2600] loss: 2.310874\n",
      "[68,  2800] loss: 2.309657\n",
      "[68,  3000] loss: 2.312004\n",
      "[68,  3200] loss: 2.311788\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[69,   200] loss: 2.310877\n",
      "[69,   400] loss: 2.311989\n",
      "[69,   600] loss: 2.312115\n",
      "[69,   800] loss: 2.311445\n",
      "[69,  1000] loss: 2.313188\n",
      "[69,  1200] loss: 2.310708\n",
      "[69,  1400] loss: 2.310896\n",
      "[69,  1600] loss: 2.310838\n",
      "[69,  1800] loss: 2.311361\n",
      "[69,  2000] loss: 2.313814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69,  2200] loss: 2.312381\n",
      "[69,  2400] loss: 2.311164\n",
      "[69,  2600] loss: 2.310594\n",
      "[69,  2800] loss: 2.312039\n",
      "[69,  3000] loss: 2.311549\n",
      "[69,  3200] loss: 2.310969\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[70,   200] loss: 2.310534\n",
      "[70,   400] loss: 2.310536\n",
      "[70,   600] loss: 2.311156\n",
      "[70,   800] loss: 2.311038\n",
      "[70,  1000] loss: 2.312354\n",
      "[70,  1200] loss: 2.309186\n",
      "[70,  1400] loss: 2.311313\n",
      "[70,  1600] loss: 2.312143\n",
      "[70,  1800] loss: 2.312208\n",
      "[70,  2000] loss: 2.314847\n",
      "[70,  2200] loss: 2.311601\n",
      "[70,  2400] loss: 2.310692\n",
      "[70,  2600] loss: 2.311048\n",
      "[70,  2800] loss: 2.311377\n",
      "[70,  3000] loss: 2.311994\n",
      "[70,  3200] loss: 2.309557\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[71,   200] loss: 2.310543\n",
      "[71,   400] loss: 2.312409\n",
      "[71,   600] loss: 2.310148\n",
      "[71,   800] loss: 2.311468\n",
      "[71,  1000] loss: 2.309864\n",
      "[71,  1200] loss: 2.311442\n",
      "[71,  1400] loss: 2.311382\n",
      "[71,  1600] loss: 2.311138\n",
      "[71,  1800] loss: 2.311642\n",
      "[71,  2000] loss: 2.311513\n",
      "[71,  2200] loss: 2.310507\n",
      "[71,  2400] loss: 2.311717\n",
      "[71,  2600] loss: 2.311083\n",
      "[71,  2800] loss: 2.310105\n",
      "[71,  3000] loss: 2.311467\n",
      "[71,  3200] loss: 2.312258\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[72,   200] loss: 2.310453\n",
      "[72,   400] loss: 2.312125\n",
      "[72,   600] loss: 2.315095\n",
      "[72,   800] loss: 2.312631\n",
      "[72,  1000] loss: 2.311669\n",
      "[72,  1200] loss: 2.311000\n",
      "[72,  1400] loss: 2.310722\n",
      "[72,  1600] loss: 2.311414\n",
      "[72,  1800] loss: 2.311593\n",
      "[72,  2000] loss: 2.310694\n",
      "[72,  2200] loss: 2.311852\n",
      "[72,  2400] loss: 2.311662\n",
      "[72,  2600] loss: 2.314351\n",
      "[72,  2800] loss: 2.312166\n",
      "[72,  3000] loss: 2.311861\n",
      "[72,  3200] loss: 2.312172\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[73,   200] loss: 2.310606\n",
      "[73,   400] loss: 2.310477\n",
      "[73,   600] loss: 2.311277\n",
      "[73,   800] loss: 2.311554\n",
      "[73,  1000] loss: 2.312694\n",
      "[73,  1200] loss: 2.310475\n",
      "[73,  1400] loss: 2.310461\n",
      "[73,  1600] loss: 2.311392\n",
      "[73,  1800] loss: 2.310979\n",
      "[73,  2000] loss: 2.309921\n",
      "[73,  2200] loss: 2.314543\n",
      "[73,  2400] loss: 2.311990\n",
      "[73,  2600] loss: 2.310984\n",
      "[73,  2800] loss: 2.312491\n",
      "[73,  3000] loss: 2.310362\n",
      "[73,  3200] loss: 2.309751\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[74,   200] loss: 2.312387\n",
      "[74,   400] loss: 2.311000\n",
      "[74,   600] loss: 2.311272\n",
      "[74,   800] loss: 2.311171\n",
      "[74,  1000] loss: 2.312105\n",
      "[74,  1200] loss: 2.311001\n",
      "[74,  1400] loss: 2.309868\n",
      "[74,  1600] loss: 2.312145\n",
      "[74,  1800] loss: 2.312909\n",
      "[74,  2000] loss: 2.309978\n",
      "[74,  2200] loss: 2.310179\n",
      "[74,  2400] loss: 2.312727\n",
      "[74,  2600] loss: 2.311685\n",
      "[74,  2800] loss: 2.310338\n",
      "[74,  3000] loss: 2.310384\n",
      "[74,  3200] loss: 2.313111\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[75,   200] loss: 2.311238\n",
      "[75,   400] loss: 2.311190\n",
      "[75,   600] loss: 2.312315\n",
      "[75,   800] loss: 2.312687\n",
      "[75,  1000] loss: 2.311132\n",
      "[75,  1200] loss: 2.311416\n",
      "[75,  1400] loss: 2.309737\n",
      "[75,  1600] loss: 2.310974\n",
      "[75,  1800] loss: 2.310625\n",
      "[75,  2000] loss: 2.310409\n",
      "[75,  2200] loss: 2.310428\n",
      "[75,  2400] loss: 2.312435\n",
      "[75,  2600] loss: 2.311650\n",
      "[75,  2800] loss: 2.312681\n",
      "[75,  3000] loss: 2.309811\n",
      "[75,  3200] loss: 2.310112\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[76,   200] loss: 2.312210\n",
      "[76,   400] loss: 2.311452\n",
      "[76,   600] loss: 2.311145\n",
      "[76,   800] loss: 2.310797\n",
      "[76,  1000] loss: 2.310027\n",
      "[76,  1200] loss: 2.310878\n",
      "[76,  1400] loss: 2.312003\n",
      "[76,  1600] loss: 2.312536\n",
      "[76,  1800] loss: 2.310588\n",
      "[76,  2000] loss: 2.311988\n",
      "[76,  2200] loss: 2.311823\n",
      "[76,  2400] loss: 2.310650\n",
      "[76,  2600] loss: 2.310506\n",
      "[76,  2800] loss: 2.310476\n",
      "[76,  3000] loss: 2.312502\n",
      "[76,  3200] loss: 2.313263\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[77,   200] loss: 2.309901\n",
      "[77,   400] loss: 2.311880\n",
      "[77,   600] loss: 2.311724\n",
      "[77,   800] loss: 2.311945\n",
      "[77,  1000] loss: 2.312636\n",
      "[77,  1200] loss: 2.310466\n",
      "[77,  1400] loss: 2.310628\n",
      "[77,  1600] loss: 2.310848\n",
      "[77,  1800] loss: 2.311707\n",
      "[77,  2000] loss: 2.311680\n",
      "[77,  2200] loss: 2.312276\n",
      "[77,  2400] loss: 2.312412\n",
      "[77,  2600] loss: 2.311531\n",
      "[77,  2800] loss: 2.310282\n",
      "[77,  3000] loss: 2.311114\n",
      "[77,  3200] loss: 2.311245\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[78,   200] loss: 2.312519\n",
      "[78,   400] loss: 2.311544\n",
      "[78,   600] loss: 2.312546\n",
      "[78,   800] loss: 2.310291\n",
      "[78,  1000] loss: 2.312476\n",
      "[78,  1200] loss: 2.310712\n",
      "[78,  1400] loss: 2.310460\n",
      "[78,  1600] loss: 2.309901\n",
      "[78,  1800] loss: 2.311547\n",
      "[78,  2000] loss: 2.313762\n",
      "[78,  2200] loss: 2.309654\n",
      "[78,  2400] loss: 2.312189\n",
      "[78,  2600] loss: 2.311273\n",
      "[78,  2800] loss: 2.310438\n",
      "[78,  3000] loss: 2.310873\n",
      "[78,  3200] loss: 2.309565\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[79,   200] loss: 2.311328\n",
      "[79,   400] loss: 2.312880\n",
      "[79,   600] loss: 2.310149\n",
      "[79,   800] loss: 2.310045\n",
      "[79,  1000] loss: 2.311959\n",
      "[79,  1200] loss: 2.310528\n",
      "[79,  1400] loss: 2.313666\n",
      "[79,  1600] loss: 2.312809\n",
      "[79,  1800] loss: 2.311879\n",
      "[79,  2000] loss: 2.311904\n",
      "[79,  2200] loss: 2.314421\n",
      "[79,  2400] loss: 2.311044\n",
      "[79,  2600] loss: 2.311909\n",
      "[79,  2800] loss: 2.311484\n",
      "[79,  3000] loss: 2.309933\n",
      "[79,  3200] loss: 2.310607\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[80,   200] loss: 2.312605\n",
      "[80,   400] loss: 2.310881\n",
      "[80,   600] loss: 2.310096\n",
      "[80,   800] loss: 2.311749\n",
      "[80,  1000] loss: 2.311467\n",
      "[80,  1200] loss: 2.310813\n",
      "[80,  1400] loss: 2.314445\n",
      "[80,  1600] loss: 2.310807\n",
      "[80,  1800] loss: 2.310301\n",
      "[80,  2000] loss: 2.311655\n",
      "[80,  2200] loss: 2.310956\n",
      "[80,  2400] loss: 2.311734\n",
      "[80,  2600] loss: 2.310330\n",
      "[80,  2800] loss: 2.312895\n",
      "[80,  3000] loss: 2.313055\n",
      "[80,  3200] loss: 2.310705\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[81,   200] loss: 2.309904\n",
      "[81,   400] loss: 2.311688\n",
      "[81,   600] loss: 2.310981\n",
      "[81,   800] loss: 2.311305\n",
      "[81,  1000] loss: 2.311972\n",
      "[81,  1200] loss: 2.312422\n",
      "[81,  1400] loss: 2.313422\n",
      "[81,  1600] loss: 2.311802\n",
      "[81,  1800] loss: 2.311426\n",
      "[81,  2000] loss: 2.312353\n",
      "[81,  2200] loss: 2.311110\n",
      "[81,  2400] loss: 2.311847\n",
      "[81,  2600] loss: 2.310196\n",
      "[81,  2800] loss: 2.313751\n",
      "[81,  3000] loss: 2.312144\n",
      "[81,  3200] loss: 2.310300\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[82,   200] loss: 2.310080\n",
      "[82,   400] loss: 2.310603\n",
      "[82,   600] loss: 2.313372\n",
      "[82,   800] loss: 2.312372\n",
      "[82,  1000] loss: 2.313105\n",
      "[82,  1200] loss: 2.310931\n",
      "[82,  1400] loss: 2.312500\n",
      "[82,  1600] loss: 2.313787\n",
      "[82,  1800] loss: 2.310260\n",
      "[82,  2000] loss: 2.312122\n",
      "[82,  2200] loss: 2.311963\n",
      "[82,  2400] loss: 2.310408\n",
      "[82,  2600] loss: 2.310949\n",
      "[82,  2800] loss: 2.312889\n",
      "[82,  3000] loss: 2.311467\n",
      "[82,  3200] loss: 2.313314\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[83,   200] loss: 2.314121\n",
      "[83,   400] loss: 2.311663\n",
      "[83,   600] loss: 2.312185\n",
      "[83,   800] loss: 2.310049\n",
      "[83,  1000] loss: 2.312063\n",
      "[83,  1200] loss: 2.310928\n",
      "[83,  1400] loss: 2.315100\n",
      "[83,  1600] loss: 2.311239\n",
      "[83,  1800] loss: 2.309541\n",
      "[83,  2000] loss: 2.312339\n",
      "[83,  2200] loss: 2.311565\n",
      "[83,  2400] loss: 2.311016\n",
      "[83,  2600] loss: 2.311007\n",
      "[83,  2800] loss: 2.311281\n",
      "[83,  3000] loss: 2.309341\n",
      "[83,  3200] loss: 2.311703\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[84,   200] loss: 2.309885\n",
      "[84,   400] loss: 2.310419\n",
      "[84,   600] loss: 2.312345\n",
      "[84,   800] loss: 2.311422\n",
      "[84,  1000] loss: 2.311689\n",
      "[84,  1200] loss: 2.312162\n",
      "[84,  1400] loss: 2.309940\n",
      "[84,  1600] loss: 2.309307\n",
      "[84,  1800] loss: 2.310454\n",
      "[84,  2000] loss: 2.313097\n",
      "[84,  2200] loss: 2.311468\n",
      "[84,  2400] loss: 2.312796\n",
      "[84,  2600] loss: 2.312888\n",
      "[84,  2800] loss: 2.311961\n",
      "[84,  3000] loss: 2.311382\n",
      "[84,  3200] loss: 2.310306\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[85,   200] loss: 2.311120\n",
      "[85,   400] loss: 2.309547\n",
      "[85,   600] loss: 2.310090\n",
      "[85,   800] loss: 2.311863\n",
      "[85,  1000] loss: 2.311592\n",
      "[85,  1200] loss: 2.310185\n",
      "[85,  1400] loss: 2.310559\n",
      "[85,  1600] loss: 2.311934\n",
      "[85,  1800] loss: 2.311698\n",
      "[85,  2000] loss: 2.312229\n",
      "[85,  2200] loss: 2.313140\n",
      "[85,  2400] loss: 2.312436\n",
      "[85,  2600] loss: 2.311000\n",
      "[85,  2800] loss: 2.312521\n",
      "[85,  3000] loss: 2.312521\n",
      "[85,  3200] loss: 2.312597\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[86,   200] loss: 2.309343\n",
      "[86,   400] loss: 2.310440\n",
      "[86,   600] loss: 2.312785\n",
      "[86,   800] loss: 2.310911\n",
      "[86,  1000] loss: 2.312312\n",
      "[86,  1200] loss: 2.311709\n",
      "[86,  1400] loss: 2.312575\n",
      "[86,  1600] loss: 2.313119\n",
      "[86,  1800] loss: 2.312187\n",
      "[86,  2000] loss: 2.311559\n",
      "[86,  2200] loss: 2.311582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86,  2400] loss: 2.310767\n",
      "[86,  2600] loss: 2.311680\n",
      "[86,  2800] loss: 2.311485\n",
      "[86,  3000] loss: 2.310957\n",
      "[86,  3200] loss: 2.309894\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[87,   200] loss: 2.310309\n",
      "[87,   400] loss: 2.312289\n",
      "[87,   600] loss: 2.312061\n",
      "[87,   800] loss: 2.313916\n",
      "[87,  1000] loss: 2.310641\n",
      "[87,  1200] loss: 2.312160\n",
      "[87,  1400] loss: 2.310377\n",
      "[87,  1600] loss: 2.311097\n",
      "[87,  1800] loss: 2.312209\n",
      "[87,  2000] loss: 2.313206\n",
      "[87,  2200] loss: 2.310786\n",
      "[87,  2400] loss: 2.312487\n",
      "[87,  2600] loss: 2.312291\n",
      "[87,  2800] loss: 2.311878\n",
      "[87,  3000] loss: 2.310559\n",
      "[87,  3200] loss: 2.310667\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[88,   200] loss: 2.312171\n",
      "[88,   400] loss: 2.310588\n",
      "[88,   600] loss: 2.312237\n",
      "[88,   800] loss: 2.311273\n",
      "[88,  1000] loss: 2.312629\n",
      "[88,  1200] loss: 2.311749\n",
      "[88,  1400] loss: 2.310372\n",
      "[88,  1600] loss: 2.310875\n",
      "[88,  1800] loss: 2.311331\n",
      "[88,  2000] loss: 2.310688\n",
      "[88,  2200] loss: 2.311720\n",
      "[88,  2400] loss: 2.311013\n",
      "[88,  2600] loss: 2.310178\n",
      "[88,  2800] loss: 2.311406\n",
      "[88,  3000] loss: 2.311869\n",
      "[88,  3200] loss: 2.309907\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[89,   200] loss: 2.311667\n",
      "[89,   400] loss: 2.311094\n",
      "[89,   600] loss: 2.313411\n",
      "[89,   800] loss: 2.311422\n",
      "[89,  1000] loss: 2.310067\n",
      "[89,  1200] loss: 2.312660\n",
      "[89,  1400] loss: 2.310671\n",
      "[89,  1600] loss: 2.309384\n",
      "[89,  1800] loss: 2.311936\n",
      "[89,  2000] loss: 2.310308\n",
      "[89,  2200] loss: 2.311017\n",
      "[89,  2400] loss: 2.311568\n",
      "[89,  2600] loss: 2.309266\n",
      "[89,  2800] loss: 2.311347\n",
      "[89,  3000] loss: 2.313519\n",
      "[89,  3200] loss: 2.311376\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[90,   200] loss: 2.311814\n",
      "[90,   400] loss: 2.313161\n",
      "[90,   600] loss: 2.311019\n",
      "[90,   800] loss: 2.313329\n",
      "[90,  1000] loss: 2.311196\n",
      "[90,  1200] loss: 2.311510\n",
      "[90,  1400] loss: 2.310464\n",
      "[90,  1600] loss: 2.311752\n",
      "[90,  1800] loss: 2.309154\n",
      "[90,  2000] loss: 2.310127\n",
      "[90,  2200] loss: 2.311948\n",
      "[90,  2400] loss: 2.310961\n",
      "[90,  2600] loss: 2.312178\n",
      "[90,  2800] loss: 2.311585\n",
      "[90,  3000] loss: 2.311158\n",
      "[90,  3200] loss: 2.311932\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[91,   200] loss: 2.310858\n",
      "[91,   400] loss: 2.310649\n",
      "[91,   600] loss: 2.310710\n",
      "[91,   800] loss: 2.311804\n",
      "[91,  1000] loss: 2.311158\n",
      "[91,  1200] loss: 2.310255\n",
      "[91,  1400] loss: 2.310420\n",
      "[91,  1600] loss: 2.310610\n",
      "[91,  1800] loss: 2.311781\n",
      "[91,  2000] loss: 2.309516\n",
      "[91,  2200] loss: 2.312940\n",
      "[91,  2400] loss: 2.311041\n",
      "[91,  2600] loss: 2.311031\n",
      "[91,  2800] loss: 2.313492\n",
      "[91,  3000] loss: 2.312452\n",
      "[91,  3200] loss: 2.314218\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[92,   200] loss: 2.310149\n",
      "[92,   400] loss: 2.312343\n",
      "[92,   600] loss: 2.311234\n",
      "[92,   800] loss: 2.312007\n",
      "[92,  1000] loss: 2.313301\n",
      "[92,  1200] loss: 2.310852\n",
      "[92,  1400] loss: 2.312945\n",
      "[92,  1600] loss: 2.310871\n",
      "[92,  1800] loss: 2.311335\n",
      "[92,  2000] loss: 2.310689\n",
      "[92,  2200] loss: 2.311116\n",
      "[92,  2400] loss: 2.312333\n",
      "[92,  2600] loss: 2.313155\n",
      "[92,  2800] loss: 2.311173\n",
      "[92,  3000] loss: 2.312019\n",
      "[92,  3200] loss: 2.311000\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[93,   200] loss: 2.310676\n",
      "[93,   400] loss: 2.310724\n",
      "[93,   600] loss: 2.310387\n",
      "[93,   800] loss: 2.312754\n",
      "[93,  1000] loss: 2.309986\n",
      "[93,  1200] loss: 2.311270\n",
      "[93,  1400] loss: 2.310979\n",
      "[93,  1600] loss: 2.311075\n",
      "[93,  1800] loss: 2.311050\n",
      "[93,  2000] loss: 2.311031\n",
      "[93,  2200] loss: 2.312258\n",
      "[93,  2400] loss: 2.313132\n",
      "[93,  2600] loss: 2.310544\n",
      "[93,  2800] loss: 2.311747\n",
      "[93,  3000] loss: 2.310741\n",
      "[93,  3200] loss: 2.310798\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[94,   200] loss: 2.310665\n",
      "[94,   400] loss: 2.312717\n",
      "[94,   600] loss: 2.312689\n",
      "[94,   800] loss: 2.311393\n",
      "[94,  1000] loss: 2.311900\n",
      "[94,  1200] loss: 2.311514\n",
      "[94,  1400] loss: 2.311512\n",
      "[94,  1600] loss: 2.310685\n",
      "[94,  1800] loss: 2.311090\n",
      "[94,  2000] loss: 2.312470\n",
      "[94,  2200] loss: 2.310912\n",
      "[94,  2400] loss: 2.310069\n",
      "[94,  2600] loss: 2.312661\n",
      "[94,  2800] loss: 2.311575\n",
      "[94,  3000] loss: 2.311468\n",
      "[94,  3200] loss: 2.311089\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[95,   200] loss: 2.311815\n",
      "[95,   400] loss: 2.311919\n",
      "[95,   600] loss: 2.312475\n",
      "[95,   800] loss: 2.310600\n",
      "[95,  1000] loss: 2.313738\n",
      "[95,  1200] loss: 2.309864\n",
      "[95,  1400] loss: 2.309438\n",
      "[95,  1600] loss: 2.311860\n",
      "[95,  1800] loss: 2.311163\n",
      "[95,  2000] loss: 2.311619\n",
      "[95,  2200] loss: 2.311909\n",
      "[95,  2400] loss: 2.311576\n",
      "[95,  2600] loss: 2.313468\n",
      "[95,  2800] loss: 2.313496\n",
      "[95,  3000] loss: 2.309495\n",
      "[95,  3200] loss: 2.310778\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[96,   200] loss: 2.310015\n",
      "[96,   400] loss: 2.311533\n",
      "[96,   600] loss: 2.310872\n",
      "[96,   800] loss: 2.312525\n",
      "[96,  1000] loss: 2.310764\n",
      "[96,  1200] loss: 2.312225\n",
      "[96,  1400] loss: 2.311065\n",
      "[96,  1600] loss: 2.310633\n",
      "[96,  1800] loss: 2.310961\n",
      "[96,  2000] loss: 2.312584\n",
      "[96,  2200] loss: 2.314018\n",
      "[96,  2400] loss: 2.311667\n",
      "[96,  2600] loss: 2.312477\n",
      "[96,  2800] loss: 2.310966\n",
      "[96,  3000] loss: 2.311689\n",
      "[96,  3200] loss: 2.310641\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[97,   200] loss: 2.310308\n",
      "[97,   400] loss: 2.311163\n",
      "[97,   600] loss: 2.311896\n",
      "[97,   800] loss: 2.311515\n",
      "[97,  1000] loss: 2.309884\n",
      "[97,  1200] loss: 2.310679\n",
      "[97,  1400] loss: 2.312592\n",
      "[97,  1600] loss: 2.312084\n",
      "[97,  1800] loss: 2.310736\n",
      "[97,  2000] loss: 2.311348\n",
      "[97,  2200] loss: 2.310356\n",
      "[97,  2400] loss: 2.310894\n",
      "[97,  2600] loss: 2.311469\n",
      "[97,  2800] loss: 2.310438\n",
      "[97,  3000] loss: 2.312122\n",
      "[97,  3200] loss: 2.311317\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[98,   200] loss: 2.312247\n",
      "[98,   400] loss: 2.311671\n",
      "[98,   600] loss: 2.310286\n",
      "[98,   800] loss: 2.311267\n",
      "[98,  1000] loss: 2.311454\n",
      "[98,  1200] loss: 2.311912\n",
      "[98,  1400] loss: 2.311655\n",
      "[98,  1600] loss: 2.311841\n",
      "[98,  1800] loss: 2.312351\n",
      "[98,  2000] loss: 2.310826\n",
      "[98,  2200] loss: 2.309709\n",
      "[98,  2400] loss: 2.311869\n",
      "[98,  2600] loss: 2.311917\n",
      "[98,  2800] loss: 2.312767\n",
      "[98,  3000] loss: 2.310851\n",
      "[98,  3200] loss: 2.312806\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[99,   200] loss: 2.310434\n",
      "[99,   400] loss: 2.311869\n",
      "[99,   600] loss: 2.312390\n",
      "[99,   800] loss: 2.311559\n",
      "[99,  1000] loss: 2.311109\n",
      "[99,  1200] loss: 2.310341\n",
      "[99,  1400] loss: 2.311446\n",
      "[99,  1600] loss: 2.312574\n",
      "[99,  1800] loss: 2.311701\n",
      "[99,  2000] loss: 2.310224\n",
      "[99,  2200] loss: 2.313725\n",
      "[99,  2400] loss: 2.312430\n",
      "[99,  2600] loss: 2.311027\n",
      "[99,  2800] loss: 2.310374\n",
      "[99,  3000] loss: 2.311690\n",
      "[99,  3200] loss: 2.311247\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[100,   200] loss: 2.311367\n",
      "[100,   400] loss: 2.311262\n",
      "[100,   600] loss: 2.310673\n",
      "[100,   800] loss: 2.310559\n",
      "[100,  1000] loss: 2.310890\n",
      "[100,  1200] loss: 2.311504\n",
      "[100,  1400] loss: 2.311975\n",
      "[100,  1600] loss: 2.310105\n",
      "[100,  1800] loss: 2.312910\n",
      "[100,  2000] loss: 2.313724\n",
      "[100,  2200] loss: 2.310285\n",
      "[100,  2400] loss: 2.311344\n",
      "[100,  2600] loss: 2.310098\n",
      "[100,  2800] loss: 2.311422\n",
      "[100,  3000] loss: 2.313010\n",
      "[100,  3200] loss: 2.310816\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[101,   200] loss: 2.312084\n",
      "[101,   400] loss: 2.312172\n",
      "[101,   600] loss: 2.313970\n",
      "[101,   800] loss: 2.311053\n",
      "[101,  1000] loss: 2.312572\n",
      "[101,  1200] loss: 2.311262\n",
      "[101,  1400] loss: 2.311255\n",
      "[101,  1600] loss: 2.311913\n",
      "[101,  1800] loss: 2.310446\n",
      "[101,  2000] loss: 2.311586\n",
      "[101,  2200] loss: 2.310246\n",
      "[101,  2400] loss: 2.311681\n",
      "[101,  2600] loss: 2.309497\n",
      "[101,  2800] loss: 2.310404\n",
      "[101,  3000] loss: 2.310906\n",
      "[101,  3200] loss: 2.312607\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[102,   200] loss: 2.312386\n",
      "[102,   400] loss: 2.311527\n",
      "[102,   600] loss: 2.310415\n",
      "[102,   800] loss: 2.311615\n",
      "[102,  1000] loss: 2.312236\n",
      "[102,  1200] loss: 2.311232\n",
      "[102,  1400] loss: 2.311665\n",
      "[102,  1600] loss: 2.311566\n",
      "[102,  1800] loss: 2.312270\n",
      "[102,  2000] loss: 2.311835\n",
      "[102,  2200] loss: 2.310996\n",
      "[102,  2400] loss: 2.314178\n",
      "[102,  2600] loss: 2.311530\n",
      "[102,  2800] loss: 2.311488\n",
      "[102,  3000] loss: 2.311591\n",
      "[102,  3200] loss: 2.310373\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[103,   200] loss: 2.311359\n",
      "[103,   400] loss: 2.310757\n",
      "[103,   600] loss: 2.311112\n",
      "[103,   800] loss: 2.312458\n",
      "[103,  1000] loss: 2.311956\n",
      "[103,  1200] loss: 2.312235\n",
      "[103,  1400] loss: 2.310986\n",
      "[103,  1600] loss: 2.311229\n",
      "[103,  1800] loss: 2.313283\n",
      "[103,  2000] loss: 2.310905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103,  2200] loss: 2.310886\n",
      "[103,  2400] loss: 2.309730\n",
      "[103,  2600] loss: 2.312535\n",
      "[103,  2800] loss: 2.311579\n",
      "[103,  3000] loss: 2.311000\n",
      "[103,  3200] loss: 2.311423\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[104,   200] loss: 2.312983\n",
      "[104,   400] loss: 2.311408\n",
      "[104,   600] loss: 2.312453\n",
      "[104,   800] loss: 2.310704\n",
      "[104,  1000] loss: 2.310688\n",
      "[104,  1200] loss: 2.311407\n",
      "[104,  1400] loss: 2.310856\n",
      "[104,  1600] loss: 2.310932\n",
      "[104,  1800] loss: 2.311425\n",
      "[104,  2000] loss: 2.309511\n",
      "[104,  2200] loss: 2.311691\n",
      "[104,  2400] loss: 2.311955\n",
      "[104,  2600] loss: 2.311188\n",
      "[104,  2800] loss: 2.312780\n",
      "[104,  3000] loss: 2.311755\n",
      "[104,  3200] loss: 2.311402\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[105,   200] loss: 2.312728\n",
      "[105,   400] loss: 2.310948\n",
      "[105,   600] loss: 2.310344\n",
      "[105,   800] loss: 2.311388\n",
      "[105,  1000] loss: 2.311089\n",
      "[105,  1200] loss: 2.312076\n",
      "[105,  1400] loss: 2.312277\n",
      "[105,  1600] loss: 2.313991\n",
      "[105,  1800] loss: 2.311442\n",
      "[105,  2000] loss: 2.314081\n",
      "[105,  2200] loss: 2.312043\n",
      "[105,  2400] loss: 2.310868\n",
      "[105,  2600] loss: 2.309931\n",
      "[105,  2800] loss: 2.311941\n",
      "[105,  3000] loss: 2.310641\n",
      "[105,  3200] loss: 2.310758\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[106,   200] loss: 2.313095\n",
      "[106,   400] loss: 2.310866\n",
      "[106,   600] loss: 2.310171\n",
      "[106,   800] loss: 2.310764\n",
      "[106,  1000] loss: 2.312195\n",
      "[106,  1200] loss: 2.311656\n",
      "[106,  1400] loss: 2.312019\n",
      "[106,  1600] loss: 2.311097\n",
      "[106,  1800] loss: 2.310503\n",
      "[106,  2000] loss: 2.311149\n",
      "[106,  2200] loss: 2.313358\n",
      "[106,  2400] loss: 2.313421\n",
      "[106,  2600] loss: 2.311184\n",
      "[106,  2800] loss: 2.309548\n",
      "[106,  3000] loss: 2.310754\n",
      "[106,  3200] loss: 2.310905\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[107,   200] loss: 2.310157\n",
      "[107,   400] loss: 2.310397\n",
      "[107,   600] loss: 2.310641\n",
      "[107,   800] loss: 2.312452\n",
      "[107,  1000] loss: 2.311895\n",
      "[107,  1200] loss: 2.311547\n",
      "[107,  1400] loss: 2.311889\n",
      "[107,  1600] loss: 2.310659\n",
      "[107,  1800] loss: 2.310744\n",
      "[107,  2000] loss: 2.310364\n",
      "[107,  2200] loss: 2.310095\n",
      "[107,  2400] loss: 2.311115\n",
      "[107,  2600] loss: 2.309716\n",
      "[107,  2800] loss: 2.312297\n",
      "[107,  3000] loss: 2.310334\n",
      "[107,  3200] loss: 2.311009\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[108,   200] loss: 2.311500\n",
      "[108,   400] loss: 2.310814\n",
      "[108,   600] loss: 2.310737\n",
      "[108,   800] loss: 2.310293\n",
      "[108,  1000] loss: 2.312263\n",
      "[108,  1200] loss: 2.312975\n",
      "[108,  1400] loss: 2.309501\n",
      "[108,  1600] loss: 2.309620\n",
      "[108,  1800] loss: 2.311326\n",
      "[108,  2000] loss: 2.313369\n",
      "[108,  2200] loss: 2.313072\n",
      "[108,  2400] loss: 2.312484\n",
      "[108,  2600] loss: 2.311197\n",
      "[108,  2800] loss: 2.311477\n",
      "[108,  3000] loss: 2.308997\n",
      "[108,  3200] loss: 2.311013\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[109,   200] loss: 2.311334\n",
      "[109,   400] loss: 2.312493\n",
      "[109,   600] loss: 2.311173\n",
      "[109,   800] loss: 2.312715\n",
      "[109,  1000] loss: 2.312633\n",
      "[109,  1200] loss: 2.312759\n",
      "[109,  1400] loss: 2.310971\n",
      "[109,  1600] loss: 2.311190\n",
      "[109,  1800] loss: 2.311154\n",
      "[109,  2000] loss: 2.309499\n",
      "[109,  2200] loss: 2.309835\n",
      "[109,  2400] loss: 2.311379\n",
      "[109,  2600] loss: 2.311021\n",
      "[109,  2800] loss: 2.309787\n",
      "[109,  3000] loss: 2.311786\n",
      "[109,  3200] loss: 2.311817\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[110,   200] loss: 2.310530\n",
      "[110,   400] loss: 2.311682\n",
      "[110,   600] loss: 2.310726\n",
      "[110,   800] loss: 2.312916\n",
      "[110,  1000] loss: 2.309680\n",
      "[110,  1200] loss: 2.309911\n",
      "[110,  1400] loss: 2.310802\n",
      "[110,  1600] loss: 2.312084\n",
      "[110,  1800] loss: 2.311863\n",
      "[110,  2000] loss: 2.311893\n",
      "[110,  2200] loss: 2.309953\n",
      "[110,  2400] loss: 2.310734\n",
      "[110,  2600] loss: 2.312088\n",
      "[110,  2800] loss: 2.311007\n",
      "[110,  3000] loss: 2.310813\n",
      "[110,  3200] loss: 2.311676\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[111,   200] loss: 2.310313\n",
      "[111,   400] loss: 2.312269\n",
      "[111,   600] loss: 2.311066\n",
      "[111,   800] loss: 2.308759\n",
      "[111,  1000] loss: 2.311085\n",
      "[111,  1200] loss: 2.312351\n",
      "[111,  1400] loss: 2.312491\n",
      "[111,  1600] loss: 2.312089\n",
      "[111,  1800] loss: 2.310272\n",
      "[111,  2000] loss: 2.311116\n",
      "[111,  2200] loss: 2.311323\n",
      "[111,  2400] loss: 2.311255\n",
      "[111,  2600] loss: 2.310784\n",
      "[111,  2800] loss: 2.312738\n",
      "[111,  3000] loss: 2.310262\n",
      "[111,  3200] loss: 2.312088\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[112,   200] loss: 2.311786\n",
      "[112,   400] loss: 2.311785\n",
      "[112,   600] loss: 2.313466\n",
      "[112,   800] loss: 2.311703\n",
      "[112,  1000] loss: 2.311154\n",
      "[112,  1200] loss: 2.312081\n",
      "[112,  1400] loss: 2.311354\n",
      "[112,  1600] loss: 2.312328\n",
      "[112,  1800] loss: 2.311923\n",
      "[112,  2000] loss: 2.309328\n",
      "[112,  2200] loss: 2.311927\n",
      "[112,  2400] loss: 2.312589\n",
      "[112,  2600] loss: 2.310817\n",
      "[112,  2800] loss: 2.310707\n",
      "[112,  3000] loss: 2.309740\n",
      "[112,  3200] loss: 2.311193\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[113,   200] loss: 2.310507\n",
      "[113,   400] loss: 2.311338\n",
      "[113,   600] loss: 2.312081\n",
      "[113,   800] loss: 2.309965\n",
      "[113,  1000] loss: 2.311768\n",
      "[113,  1200] loss: 2.312314\n",
      "[113,  1400] loss: 2.312556\n",
      "[113,  1600] loss: 2.311280\n",
      "[113,  1800] loss: 2.311427\n",
      "[113,  2000] loss: 2.312520\n",
      "[113,  2200] loss: 2.313045\n",
      "[113,  2400] loss: 2.311275\n",
      "[113,  2600] loss: 2.311131\n",
      "[113,  2800] loss: 2.310511\n",
      "[113,  3000] loss: 2.313475\n",
      "[113,  3200] loss: 2.311504\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[114,   200] loss: 2.311076\n",
      "[114,   400] loss: 2.312841\n",
      "[114,   600] loss: 2.311142\n",
      "[114,   800] loss: 2.311253\n",
      "[114,  1000] loss: 2.312437\n",
      "[114,  1200] loss: 2.310386\n",
      "[114,  1400] loss: 2.310100\n",
      "[114,  1600] loss: 2.312381\n",
      "[114,  1800] loss: 2.309324\n",
      "[114,  2000] loss: 2.312353\n",
      "[114,  2200] loss: 2.311238\n",
      "[114,  2400] loss: 2.312566\n",
      "[114,  2600] loss: 2.311624\n",
      "[114,  2800] loss: 2.310755\n",
      "[114,  3000] loss: 2.311588\n",
      "[114,  3200] loss: 2.312093\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[115,   200] loss: 2.311519\n",
      "[115,   400] loss: 2.310406\n",
      "[115,   600] loss: 2.309194\n",
      "[115,   800] loss: 2.311753\n",
      "[115,  1000] loss: 2.311246\n",
      "[115,  1200] loss: 2.311046\n",
      "[115,  1400] loss: 2.311329\n",
      "[115,  1600] loss: 2.313391\n",
      "[115,  1800] loss: 2.311339\n",
      "[115,  2000] loss: 2.312406\n",
      "[115,  2200] loss: 2.311401\n",
      "[115,  2400] loss: 2.313527\n",
      "[115,  2600] loss: 2.309937\n",
      "[115,  2800] loss: 2.311110\n",
      "[115,  3000] loss: 2.311437\n",
      "[115,  3200] loss: 2.310693\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[116,   200] loss: 2.311112\n",
      "[116,   400] loss: 2.311290\n",
      "[116,   600] loss: 2.309371\n",
      "[116,   800] loss: 2.310430\n",
      "[116,  1000] loss: 2.311042\n",
      "[116,  1200] loss: 2.310064\n",
      "[116,  1400] loss: 2.309720\n",
      "[116,  1600] loss: 2.313210\n",
      "[116,  1800] loss: 2.310948\n",
      "[116,  2000] loss: 2.313035\n",
      "[116,  2200] loss: 2.311043\n",
      "[116,  2400] loss: 2.310746\n",
      "[116,  2600] loss: 2.311869\n",
      "[116,  2800] loss: 2.312630\n",
      "[116,  3000] loss: 2.310599\n",
      "[116,  3200] loss: 2.311638\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[117,   200] loss: 2.311107\n",
      "[117,   400] loss: 2.311276\n",
      "[117,   600] loss: 2.310863\n",
      "[117,   800] loss: 2.311158\n",
      "[117,  1000] loss: 2.310481\n",
      "[117,  1200] loss: 2.311189\n",
      "[117,  1400] loss: 2.311846\n",
      "[117,  1600] loss: 2.312627\n",
      "[117,  1800] loss: 2.311712\n",
      "[117,  2000] loss: 2.311031\n",
      "[117,  2200] loss: 2.312056\n",
      "[117,  2400] loss: 2.310636\n",
      "[117,  2600] loss: 2.311438\n",
      "[117,  2800] loss: 2.310969\n",
      "[117,  3000] loss: 2.311560\n",
      "[117,  3200] loss: 2.310506\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[118,   200] loss: 2.309993\n",
      "[118,   400] loss: 2.312400\n",
      "[118,   600] loss: 2.311640\n",
      "[118,   800] loss: 2.311013\n",
      "[118,  1000] loss: 2.312925\n",
      "[118,  1200] loss: 2.311480\n",
      "[118,  1400] loss: 2.311109\n",
      "[118,  1600] loss: 2.311647\n",
      "[118,  1800] loss: 2.311327\n",
      "[118,  2000] loss: 2.313633\n",
      "[118,  2200] loss: 2.312678\n",
      "[118,  2400] loss: 2.310333\n",
      "[118,  2600] loss: 2.310411\n",
      "[118,  2800] loss: 2.310939\n",
      "[118,  3000] loss: 2.309600\n",
      "[118,  3200] loss: 2.310292\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[119,   200] loss: 2.313254\n",
      "[119,   400] loss: 2.311347\n",
      "[119,   600] loss: 2.311129\n",
      "[119,   800] loss: 2.311656\n",
      "[119,  1000] loss: 2.309888\n",
      "[119,  1200] loss: 2.313712\n",
      "[119,  1400] loss: 2.313061\n",
      "[119,  1600] loss: 2.311431\n",
      "[119,  1800] loss: 2.310968\n",
      "[119,  2000] loss: 2.310683\n",
      "[119,  2200] loss: 2.312331\n",
      "[119,  2400] loss: 2.310205\n",
      "[119,  2600] loss: 2.310095\n",
      "[119,  2800] loss: 2.312451\n",
      "[119,  3000] loss: 2.310083\n",
      "[119,  3200] loss: 2.310794\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[120,   200] loss: 2.311355\n",
      "[120,   400] loss: 2.310300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120,   600] loss: 2.310340\n",
      "[120,   800] loss: 2.313110\n",
      "[120,  1000] loss: 2.310680\n",
      "[120,  1200] loss: 2.311395\n",
      "[120,  1400] loss: 2.313756\n",
      "[120,  1600] loss: 2.311434\n",
      "[120,  1800] loss: 2.312557\n",
      "[120,  2000] loss: 2.312496\n",
      "[120,  2200] loss: 2.311953\n",
      "[120,  2400] loss: 2.312566\n",
      "[120,  2600] loss: 2.310836\n",
      "[120,  2800] loss: 2.310933\n",
      "[120,  3000] loss: 2.310166\n",
      "[120,  3200] loss: 2.310138\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[121,   200] loss: 2.311669\n",
      "[121,   400] loss: 2.312088\n",
      "[121,   600] loss: 2.311570\n",
      "[121,   800] loss: 2.311385\n",
      "[121,  1000] loss: 2.311498\n",
      "[121,  1200] loss: 2.311869\n",
      "[121,  1400] loss: 2.311091\n",
      "[121,  1600] loss: 2.312294\n",
      "[121,  1800] loss: 2.309047\n",
      "[121,  2000] loss: 2.311762\n",
      "[121,  2200] loss: 2.311589\n",
      "[121,  2400] loss: 2.310906\n",
      "[121,  2600] loss: 2.313209\n",
      "[121,  2800] loss: 2.310953\n",
      "[121,  3000] loss: 2.312239\n",
      "[121,  3200] loss: 2.311445\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[122,   200] loss: 2.312379\n",
      "[122,   400] loss: 2.310421\n",
      "[122,   600] loss: 2.313039\n",
      "[122,   800] loss: 2.311666\n",
      "[122,  1000] loss: 2.311592\n",
      "[122,  1200] loss: 2.311259\n",
      "[122,  1400] loss: 2.312387\n",
      "[122,  1600] loss: 2.314400\n",
      "[122,  1800] loss: 2.309283\n",
      "[122,  2000] loss: 2.308867\n",
      "[122,  2200] loss: 2.313064\n",
      "[122,  2400] loss: 2.311312\n",
      "[122,  2600] loss: 2.312656\n",
      "[122,  2800] loss: 2.310608\n",
      "[122,  3000] loss: 2.310951\n",
      "[122,  3200] loss: 2.311584\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[123,   200] loss: 2.312326\n",
      "[123,   400] loss: 2.309125\n",
      "[123,   600] loss: 2.311343\n",
      "[123,   800] loss: 2.311647\n",
      "[123,  1000] loss: 2.310420\n",
      "[123,  1200] loss: 2.311722\n",
      "[123,  1400] loss: 2.311146\n",
      "[123,  1600] loss: 2.310797\n",
      "[123,  1800] loss: 2.312034\n",
      "[123,  2000] loss: 2.309962\n",
      "[123,  2200] loss: 2.311867\n",
      "[123,  2400] loss: 2.311892\n",
      "[123,  2600] loss: 2.310605\n",
      "[123,  2800] loss: 2.310992\n",
      "[123,  3000] loss: 2.311074\n",
      "[123,  3200] loss: 2.309447\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[124,   200] loss: 2.311014\n",
      "[124,   400] loss: 2.312593\n",
      "[124,   600] loss: 2.311223\n",
      "[124,   800] loss: 2.311099\n",
      "[124,  1000] loss: 2.311076\n",
      "[124,  1200] loss: 2.311867\n",
      "[124,  1400] loss: 2.310765\n",
      "[124,  1600] loss: 2.309143\n",
      "[124,  1800] loss: 2.311201\n",
      "[124,  2000] loss: 2.312314\n",
      "[124,  2200] loss: 2.311540\n",
      "[124,  2400] loss: 2.311630\n",
      "[124,  2600] loss: 2.310798\n",
      "[124,  2800] loss: 2.310314\n",
      "[124,  3000] loss: 2.315503\n",
      "[124,  3200] loss: 2.310620\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[125,   200] loss: 2.309103\n",
      "[125,   400] loss: 2.311675\n",
      "[125,   600] loss: 2.312578\n",
      "[125,   800] loss: 2.310255\n",
      "[125,  1000] loss: 2.311388\n",
      "[125,  1200] loss: 2.311841\n",
      "[125,  1400] loss: 2.311034\n",
      "[125,  1600] loss: 2.314236\n",
      "[125,  1800] loss: 2.311251\n",
      "[125,  2000] loss: 2.314640\n",
      "[125,  2200] loss: 2.311733\n",
      "[125,  2400] loss: 2.309900\n",
      "[125,  2600] loss: 2.312748\n",
      "[125,  2800] loss: 2.312029\n",
      "[125,  3000] loss: 2.308989\n",
      "[125,  3200] loss: 2.309079\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[126,   200] loss: 2.310557\n",
      "[126,   400] loss: 2.311656\n",
      "[126,   600] loss: 2.311674\n",
      "[126,   800] loss: 2.310504\n",
      "[126,  1000] loss: 2.311025\n",
      "[126,  1200] loss: 2.310901\n",
      "[126,  1400] loss: 2.309629\n",
      "[126,  1600] loss: 2.314134\n",
      "[126,  1800] loss: 2.311692\n",
      "[126,  2000] loss: 2.308806\n",
      "[126,  2200] loss: 2.310330\n",
      "[126,  2400] loss: 2.310624\n",
      "[126,  2600] loss: 2.311164\n",
      "[126,  2800] loss: 2.310838\n",
      "[126,  3000] loss: 2.312349\n",
      "[126,  3200] loss: 2.309955\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[127,   200] loss: 2.309423\n",
      "[127,   400] loss: 2.312234\n",
      "[127,   600] loss: 2.309858\n",
      "[127,   800] loss: 2.311595\n",
      "[127,  1000] loss: 2.310624\n",
      "[127,  1200] loss: 2.311147\n",
      "[127,  1400] loss: 2.311394\n",
      "[127,  1600] loss: 2.309863\n",
      "[127,  1800] loss: 2.312916\n",
      "[127,  2000] loss: 2.312701\n",
      "[127,  2200] loss: 2.311062\n",
      "[127,  2400] loss: 2.311072\n",
      "[127,  2600] loss: 2.311167\n",
      "[127,  2800] loss: 2.311100\n",
      "[127,  3000] loss: 2.311399\n",
      "[127,  3200] loss: 2.310553\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[128,   200] loss: 2.311955\n",
      "[128,   400] loss: 2.312408\n",
      "[128,   600] loss: 2.314963\n",
      "[128,   800] loss: 2.310780\n",
      "[128,  1000] loss: 2.310688\n",
      "[128,  1200] loss: 2.310614\n",
      "[128,  1400] loss: 2.311235\n",
      "[128,  1600] loss: 2.311624\n",
      "[128,  1800] loss: 2.310206\n",
      "[128,  2000] loss: 2.311714\n",
      "[128,  2200] loss: 2.310511\n",
      "[128,  2400] loss: 2.311107\n",
      "[128,  2600] loss: 2.313388\n",
      "[128,  2800] loss: 2.311546\n",
      "[128,  3000] loss: 2.311990\n",
      "[128,  3200] loss: 2.311504\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[129,   200] loss: 2.311861\n",
      "[129,   400] loss: 2.310594\n",
      "[129,   600] loss: 2.312016\n",
      "[129,   800] loss: 2.311468\n",
      "[129,  1000] loss: 2.312450\n",
      "[129,  1200] loss: 2.310682\n",
      "[129,  1400] loss: 2.312497\n",
      "[129,  1600] loss: 2.311490\n",
      "[129,  1800] loss: 2.311644\n",
      "[129,  2000] loss: 2.311174\n",
      "[129,  2200] loss: 2.311171\n",
      "[129,  2400] loss: 2.310691\n",
      "[129,  2600] loss: 2.312646\n",
      "[129,  2800] loss: 2.309478\n",
      "[129,  3000] loss: 2.311833\n",
      "[129,  3200] loss: 2.310577\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[130,   200] loss: 2.312015\n",
      "[130,   400] loss: 2.309763\n",
      "[130,   600] loss: 2.312133\n",
      "[130,   800] loss: 2.311182\n",
      "[130,  1000] loss: 2.311736\n",
      "[130,  1200] loss: 2.312241\n",
      "[130,  1400] loss: 2.310210\n",
      "[130,  1600] loss: 2.310968\n",
      "[130,  1800] loss: 2.310379\n",
      "[130,  2000] loss: 2.312300\n",
      "[130,  2200] loss: 2.313050\n",
      "[130,  2400] loss: 2.311866\n",
      "[130,  2600] loss: 2.313428\n",
      "[130,  2800] loss: 2.311331\n",
      "[130,  3000] loss: 2.312678\n",
      "[130,  3200] loss: 2.311870\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[131,   200] loss: 2.311818\n",
      "[131,   400] loss: 2.312036\n",
      "[131,   600] loss: 2.312405\n",
      "[131,   800] loss: 2.311484\n",
      "[131,  1000] loss: 2.311752\n",
      "[131,  1200] loss: 2.311636\n",
      "[131,  1400] loss: 2.311632\n",
      "[131,  1600] loss: 2.312197\n",
      "[131,  1800] loss: 2.310612\n",
      "[131,  2000] loss: 2.313109\n",
      "[131,  2200] loss: 2.310718\n",
      "[131,  2400] loss: 2.312156\n",
      "[131,  2600] loss: 2.312071\n",
      "[131,  2800] loss: 2.312323\n",
      "[131,  3000] loss: 2.309636\n",
      "[131,  3200] loss: 2.310840\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[132,   200] loss: 2.311856\n",
      "[132,   400] loss: 2.312397\n",
      "[132,   600] loss: 2.311076\n",
      "[132,   800] loss: 2.311436\n",
      "[132,  1000] loss: 2.309818\n",
      "[132,  1200] loss: 2.312012\n",
      "[132,  1400] loss: 2.310475\n",
      "[132,  1600] loss: 2.312514\n",
      "[132,  1800] loss: 2.312076\n",
      "[132,  2000] loss: 2.311716\n",
      "[132,  2200] loss: 2.310947\n",
      "[132,  2400] loss: 2.312266\n",
      "[132,  2600] loss: 2.311047\n",
      "[132,  2800] loss: 2.313012\n",
      "[132,  3000] loss: 2.313125\n",
      "[132,  3200] loss: 2.310521\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[133,   200] loss: 2.312424\n",
      "[133,   400] loss: 2.310961\n",
      "[133,   600] loss: 2.310543\n",
      "[133,   800] loss: 2.311236\n",
      "[133,  1000] loss: 2.310832\n",
      "[133,  1200] loss: 2.310050\n",
      "[133,  1400] loss: 2.311231\n",
      "[133,  1600] loss: 2.309274\n",
      "[133,  1800] loss: 2.309721\n",
      "[133,  2000] loss: 2.312621\n",
      "[133,  2200] loss: 2.312269\n",
      "[133,  2400] loss: 2.310615\n",
      "[133,  2600] loss: 2.310656\n",
      "[133,  2800] loss: 2.312683\n",
      "[133,  3000] loss: 2.312568\n",
      "[133,  3200] loss: 2.311319\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[134,   200] loss: 2.312131\n",
      "[134,   400] loss: 2.312727\n",
      "[134,   600] loss: 2.313657\n",
      "[134,   800] loss: 2.311461\n",
      "[134,  1000] loss: 2.311056\n",
      "[134,  1200] loss: 2.311068\n",
      "[134,  1400] loss: 2.309653\n",
      "[134,  1600] loss: 2.311732\n",
      "[134,  1800] loss: 2.310854\n",
      "[134,  2000] loss: 2.312120\n",
      "[134,  2200] loss: 2.310719\n",
      "[134,  2400] loss: 2.310696\n",
      "[134,  2600] loss: 2.314073\n",
      "[134,  2800] loss: 2.311303\n",
      "[134,  3000] loss: 2.312150\n",
      "[134,  3200] loss: 2.311127\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[135,   200] loss: 2.311416\n",
      "[135,   400] loss: 2.310967\n",
      "[135,   600] loss: 2.311336\n",
      "[135,   800] loss: 2.313130\n",
      "[135,  1000] loss: 2.311946\n",
      "[135,  1200] loss: 2.312110\n",
      "[135,  1400] loss: 2.309595\n",
      "[135,  1600] loss: 2.309971\n",
      "[135,  1800] loss: 2.309385\n",
      "[135,  2000] loss: 2.309210\n",
      "[135,  2200] loss: 2.310729\n",
      "[135,  2400] loss: 2.310704\n",
      "[135,  2600] loss: 2.314752\n",
      "[135,  2800] loss: 2.312204\n",
      "[135,  3000] loss: 2.311182\n",
      "[135,  3200] loss: 2.311470\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[136,   200] loss: 2.312707\n",
      "[136,   400] loss: 2.312216\n",
      "[136,   600] loss: 2.311848\n",
      "[136,   800] loss: 2.311606\n",
      "[136,  1000] loss: 2.310509\n",
      "[136,  1200] loss: 2.310264\n",
      "[136,  1400] loss: 2.312205\n",
      "[136,  1600] loss: 2.310863\n",
      "[136,  1800] loss: 2.311564\n",
      "[136,  2000] loss: 2.312687\n",
      "[136,  2200] loss: 2.312409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136,  2400] loss: 2.309977\n",
      "[136,  2600] loss: 2.311743\n",
      "[136,  2800] loss: 2.310999\n",
      "[136,  3000] loss: 2.311986\n",
      "[136,  3200] loss: 2.311704\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[137,   200] loss: 2.310295\n",
      "[137,   400] loss: 2.310470\n",
      "[137,   600] loss: 2.311754\n",
      "[137,   800] loss: 2.310989\n",
      "[137,  1000] loss: 2.311053\n",
      "[137,  1200] loss: 2.311028\n",
      "[137,  1400] loss: 2.311823\n",
      "[137,  1600] loss: 2.310964\n",
      "[137,  1800] loss: 2.312857\n",
      "[137,  2000] loss: 2.313871\n",
      "[137,  2200] loss: 2.308275\n",
      "[137,  2400] loss: 2.310962\n",
      "[137,  2600] loss: 2.310148\n",
      "[137,  2800] loss: 2.311931\n",
      "[137,  3000] loss: 2.311281\n",
      "[137,  3200] loss: 2.312135\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[138,   200] loss: 2.309987\n",
      "[138,   400] loss: 2.310830\n",
      "[138,   600] loss: 2.310986\n",
      "[138,   800] loss: 2.311038\n",
      "[138,  1000] loss: 2.310532\n",
      "[138,  1200] loss: 2.312183\n",
      "[138,  1400] loss: 2.311087\n",
      "[138,  1600] loss: 2.311134\n",
      "[138,  1800] loss: 2.313109\n",
      "[138,  2000] loss: 2.311052\n",
      "[138,  2200] loss: 2.312236\n",
      "[138,  2400] loss: 2.310432\n",
      "[138,  2600] loss: 2.310674\n",
      "[138,  2800] loss: 2.312838\n",
      "[138,  3000] loss: 2.310128\n",
      "[138,  3200] loss: 2.314392\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[139,   200] loss: 2.312140\n",
      "[139,   400] loss: 2.312239\n",
      "[139,   600] loss: 2.312810\n",
      "[139,   800] loss: 2.310999\n",
      "[139,  1000] loss: 2.313382\n",
      "[139,  1200] loss: 2.312708\n",
      "[139,  1400] loss: 2.311060\n",
      "[139,  1600] loss: 2.310965\n",
      "[139,  1800] loss: 2.312788\n",
      "[139,  2000] loss: 2.312604\n",
      "[139,  2200] loss: 2.310269\n",
      "[139,  2400] loss: 2.310874\n",
      "[139,  2600] loss: 2.311635\n",
      "[139,  2800] loss: 2.313926\n",
      "[139,  3000] loss: 2.311985\n",
      "[139,  3200] loss: 2.311443\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[140,   200] loss: 2.312131\n",
      "[140,   400] loss: 2.312184\n",
      "[140,   600] loss: 2.311128\n",
      "[140,   800] loss: 2.311035\n",
      "[140,  1000] loss: 2.314404\n",
      "[140,  1200] loss: 2.309246\n",
      "[140,  1400] loss: 2.312294\n",
      "[140,  1600] loss: 2.311332\n",
      "[140,  1800] loss: 2.310760\n",
      "[140,  2000] loss: 2.309808\n",
      "[140,  2200] loss: 2.310368\n",
      "[140,  2400] loss: 2.309533\n",
      "[140,  2600] loss: 2.312138\n",
      "[140,  2800] loss: 2.312553\n",
      "[140,  3000] loss: 2.311915\n",
      "[140,  3200] loss: 2.310297\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[141,   200] loss: 2.311117\n",
      "[141,   400] loss: 2.311272\n",
      "[141,   600] loss: 2.311645\n",
      "[141,   800] loss: 2.312309\n",
      "[141,  1000] loss: 2.310931\n",
      "[141,  1200] loss: 2.311338\n",
      "[141,  1400] loss: 2.312745\n",
      "[141,  1600] loss: 2.311042\n",
      "[141,  1800] loss: 2.310704\n",
      "[141,  2000] loss: 2.309594\n",
      "[141,  2200] loss: 2.311649\n",
      "[141,  2400] loss: 2.311356\n",
      "[141,  2600] loss: 2.311269\n",
      "[141,  2800] loss: 2.311125\n",
      "[141,  3000] loss: 2.313069\n",
      "[141,  3200] loss: 2.310623\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[142,   200] loss: 2.311415\n",
      "[142,   400] loss: 2.311300\n",
      "[142,   600] loss: 2.313551\n",
      "[142,   800] loss: 2.310834\n",
      "[142,  1000] loss: 2.312100\n",
      "[142,  1200] loss: 2.310248\n",
      "[142,  1400] loss: 2.311635\n",
      "[142,  1600] loss: 2.309629\n",
      "[142,  1800] loss: 2.310395\n",
      "[142,  2000] loss: 2.311269\n",
      "[142,  2200] loss: 2.310583\n",
      "[142,  2400] loss: 2.311622\n",
      "[142,  2600] loss: 2.313714\n",
      "[142,  2800] loss: 2.310753\n",
      "[142,  3000] loss: 2.310588\n",
      "[142,  3200] loss: 2.311006\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[143,   200] loss: 2.312179\n",
      "[143,   400] loss: 2.312263\n",
      "[143,   600] loss: 2.311605\n",
      "[143,   800] loss: 2.312609\n",
      "[143,  1000] loss: 2.313814\n",
      "[143,  1200] loss: 2.311537\n",
      "[143,  1400] loss: 2.313939\n",
      "[143,  1600] loss: 2.311488\n",
      "[143,  1800] loss: 2.313920\n",
      "[143,  2000] loss: 2.309447\n",
      "[143,  2200] loss: 2.311087\n",
      "[143,  2400] loss: 2.311461\n",
      "[143,  2600] loss: 2.310719\n",
      "[143,  2800] loss: 2.312567\n",
      "[143,  3000] loss: 2.311002\n",
      "[143,  3200] loss: 2.312357\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[144,   200] loss: 2.311777\n",
      "[144,   400] loss: 2.313449\n",
      "[144,   600] loss: 2.312191\n",
      "[144,   800] loss: 2.311076\n",
      "[144,  1000] loss: 2.310193\n",
      "[144,  1200] loss: 2.310532\n",
      "[144,  1400] loss: 2.310896\n",
      "[144,  1600] loss: 2.313163\n",
      "[144,  1800] loss: 2.311153\n",
      "[144,  2000] loss: 2.311628\n",
      "[144,  2200] loss: 2.309987\n",
      "[144,  2400] loss: 2.310426\n",
      "[144,  2600] loss: 2.311865\n",
      "[144,  2800] loss: 2.311703\n",
      "[144,  3000] loss: 2.308202\n",
      "[144,  3200] loss: 2.312565\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[145,   200] loss: 2.311656\n",
      "[145,   400] loss: 2.310850\n",
      "[145,   600] loss: 2.311958\n",
      "[145,   800] loss: 2.311170\n",
      "[145,  1000] loss: 2.312763\n",
      "[145,  1200] loss: 2.313778\n",
      "[145,  1400] loss: 2.311864\n",
      "[145,  1600] loss: 2.311547\n",
      "[145,  1800] loss: 2.311670\n",
      "[145,  2000] loss: 2.313027\n",
      "[145,  2200] loss: 2.311946\n",
      "[145,  2400] loss: 2.312273\n",
      "[145,  2600] loss: 2.309315\n",
      "[145,  2800] loss: 2.313780\n",
      "[145,  3000] loss: 2.311215\n",
      "[145,  3200] loss: 2.309960\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[146,   200] loss: 2.312592\n",
      "[146,   400] loss: 2.310945\n",
      "[146,   600] loss: 2.312027\n",
      "[146,   800] loss: 2.310379\n",
      "[146,  1000] loss: 2.309488\n",
      "[146,  1200] loss: 2.309888\n",
      "[146,  1400] loss: 2.310254\n",
      "[146,  1600] loss: 2.310617\n",
      "[146,  1800] loss: 2.312210\n",
      "[146,  2000] loss: 2.310654\n",
      "[146,  2200] loss: 2.311096\n",
      "[146,  2400] loss: 2.312373\n",
      "[146,  2600] loss: 2.312501\n",
      "[146,  2800] loss: 2.310881\n",
      "[146,  3000] loss: 2.311168\n",
      "[146,  3200] loss: 2.310907\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[147,   200] loss: 2.311358\n",
      "[147,   400] loss: 2.311250\n",
      "[147,   600] loss: 2.310621\n",
      "[147,   800] loss: 2.309181\n",
      "[147,  1000] loss: 2.310051\n",
      "[147,  1200] loss: 2.311623\n",
      "[147,  1400] loss: 2.311802\n",
      "[147,  1600] loss: 2.310351\n",
      "[147,  1800] loss: 2.312149\n",
      "[147,  2000] loss: 2.312706\n",
      "[147,  2200] loss: 2.312282\n",
      "[147,  2400] loss: 2.311897\n",
      "[147,  2600] loss: 2.312440\n",
      "[147,  2800] loss: 2.310900\n",
      "[147,  3000] loss: 2.312699\n",
      "[147,  3200] loss: 2.311767\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[148,   200] loss: 2.312102\n",
      "[148,   400] loss: 2.310900\n",
      "[148,   600] loss: 2.310855\n",
      "[148,   800] loss: 2.310048\n",
      "[148,  1000] loss: 2.313157\n",
      "[148,  1200] loss: 2.312138\n",
      "[148,  1400] loss: 2.312062\n",
      "[148,  1600] loss: 2.312842\n",
      "[148,  1800] loss: 2.310728\n",
      "[148,  2000] loss: 2.312310\n",
      "[148,  2200] loss: 2.311326\n",
      "[148,  2400] loss: 2.310541\n",
      "[148,  2600] loss: 2.310481\n",
      "[148,  2800] loss: 2.312294\n",
      "[148,  3000] loss: 2.309946\n",
      "[148,  3200] loss: 2.312014\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[149,   200] loss: 2.312861\n",
      "[149,   400] loss: 2.311976\n",
      "[149,   600] loss: 2.312896\n",
      "[149,   800] loss: 2.311210\n",
      "[149,  1000] loss: 2.311518\n",
      "[149,  1200] loss: 2.312365\n",
      "[149,  1400] loss: 2.310303\n",
      "[149,  1600] loss: 2.310667\n",
      "[149,  1800] loss: 2.310503\n",
      "[149,  2000] loss: 2.313460\n",
      "[149,  2200] loss: 2.312493\n",
      "[149,  2400] loss: 2.309905\n",
      "[149,  2600] loss: 2.310572\n",
      "[149,  2800] loss: 2.309268\n",
      "[149,  3000] loss: 2.311576\n",
      "[149,  3200] loss: 2.312849\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[150,   200] loss: 2.311621\n",
      "[150,   400] loss: 2.312670\n",
      "[150,   600] loss: 2.311371\n",
      "[150,   800] loss: 2.311652\n",
      "[150,  1000] loss: 2.311100\n",
      "[150,  1200] loss: 2.311136\n",
      "[150,  1400] loss: 2.311016\n",
      "[150,  1600] loss: 2.312619\n",
      "[150,  1800] loss: 2.310627\n",
      "[150,  2000] loss: 2.312215\n",
      "[150,  2200] loss: 2.312039\n",
      "[150,  2400] loss: 2.311963\n",
      "[150,  2600] loss: 2.310933\n",
      "[150,  2800] loss: 2.309167\n",
      "[150,  3000] loss: 2.309603\n",
      "[150,  3200] loss: 2.311711\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[151,   200] loss: 2.305294\n",
      "[151,   400] loss: 2.303371\n",
      "[151,   600] loss: 2.303311\n",
      "[151,   800] loss: 2.303199\n",
      "[151,  1000] loss: 2.303265\n",
      "[151,  1200] loss: 2.303602\n",
      "[151,  1400] loss: 2.303482\n",
      "[151,  1600] loss: 2.303866\n",
      "[151,  1800] loss: 2.303523\n",
      "[151,  2000] loss: 2.303319\n",
      "[151,  2200] loss: 2.303062\n",
      "[151,  2400] loss: 2.303492\n",
      "[151,  2600] loss: 2.303630\n",
      "[151,  2800] loss: 2.303412\n",
      "[151,  3000] loss: 2.303616\n",
      "[151,  3200] loss: 2.303791\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[152,   200] loss: 2.303223\n",
      "[152,   400] loss: 2.303227\n",
      "[152,   600] loss: 2.303662\n",
      "[152,   800] loss: 2.303747\n",
      "[152,  1000] loss: 2.303553\n",
      "[152,  1200] loss: 2.303655\n",
      "[152,  1400] loss: 2.303811\n",
      "[152,  1600] loss: 2.303183\n",
      "[152,  1800] loss: 2.303785\n",
      "[152,  2000] loss: 2.303818\n",
      "[152,  2200] loss: 2.303637\n",
      "[152,  2400] loss: 2.303113\n",
      "[152,  2600] loss: 2.303611\n",
      "[152,  2800] loss: 2.303604\n",
      "[152,  3000] loss: 2.303765\n",
      "[152,  3200] loss: 2.303513\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[153,   200] loss: 2.303624\n",
      "[153,   400] loss: 2.303599\n",
      "[153,   600] loss: 2.303100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153,   800] loss: 2.303954\n",
      "[153,  1000] loss: 2.303860\n",
      "[153,  1200] loss: 2.303278\n",
      "[153,  1400] loss: 2.303771\n",
      "[153,  1600] loss: 2.303444\n",
      "[153,  1800] loss: 2.303881\n",
      "[153,  2000] loss: 2.303806\n",
      "[153,  2200] loss: 2.303514\n",
      "[153,  2400] loss: 2.304038\n",
      "[153,  2600] loss: 2.303633\n",
      "[153,  2800] loss: 2.303434\n",
      "[153,  3000] loss: 2.303035\n",
      "[153,  3200] loss: 2.303666\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[154,   200] loss: 2.303720\n",
      "[154,   400] loss: 2.303409\n",
      "[154,   600] loss: 2.303370\n",
      "[154,   800] loss: 2.303538\n",
      "[154,  1000] loss: 2.303356\n",
      "[154,  1200] loss: 2.303457\n",
      "[154,  1400] loss: 2.303452\n",
      "[154,  1600] loss: 2.303575\n",
      "[154,  1800] loss: 2.303304\n",
      "[154,  2000] loss: 2.303480\n",
      "[154,  2200] loss: 2.303605\n",
      "[154,  2400] loss: 2.303033\n",
      "[154,  2600] loss: 2.303670\n",
      "[154,  2800] loss: 2.303489\n",
      "[154,  3000] loss: 2.303478\n",
      "[154,  3200] loss: 2.303447\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[155,   200] loss: 2.303179\n",
      "[155,   400] loss: 2.303142\n",
      "[155,   600] loss: 2.303528\n",
      "[155,   800] loss: 2.303021\n",
      "[155,  1000] loss: 2.303470\n",
      "[155,  1200] loss: 2.303454\n",
      "[155,  1400] loss: 2.303422\n",
      "[155,  1600] loss: 2.303678\n",
      "[155,  1800] loss: 2.303572\n",
      "[155,  2000] loss: 2.303570\n",
      "[155,  2200] loss: 2.303329\n",
      "[155,  2400] loss: 2.303761\n",
      "[155,  2600] loss: 2.303748\n",
      "[155,  2800] loss: 2.303295\n",
      "[155,  3000] loss: 2.303730\n",
      "[155,  3200] loss: 2.303352\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[156,   200] loss: 2.303608\n",
      "[156,   400] loss: 2.303106\n",
      "[156,   600] loss: 2.303205\n",
      "[156,   800] loss: 2.303032\n",
      "[156,  1000] loss: 2.303040\n",
      "[156,  1200] loss: 2.303650\n",
      "[156,  1400] loss: 2.303183\n",
      "[156,  1600] loss: 2.303691\n",
      "[156,  1800] loss: 2.302792\n",
      "[156,  2000] loss: 2.304046\n",
      "[156,  2200] loss: 2.303683\n",
      "[156,  2400] loss: 2.303075\n",
      "[156,  2600] loss: 2.303453\n",
      "[156,  2800] loss: 2.303255\n",
      "[156,  3000] loss: 2.303232\n",
      "[156,  3200] loss: 2.304187\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[157,   200] loss: 2.303392\n",
      "[157,   400] loss: 2.303667\n",
      "[157,   600] loss: 2.303254\n",
      "[157,   800] loss: 2.303362\n",
      "[157,  1000] loss: 2.303059\n",
      "[157,  1200] loss: 2.303447\n",
      "[157,  1400] loss: 2.303319\n",
      "[157,  1600] loss: 2.303363\n",
      "[157,  1800] loss: 2.302934\n",
      "[157,  2000] loss: 2.303930\n",
      "[157,  2200] loss: 2.303746\n",
      "[157,  2400] loss: 2.303533\n",
      "[157,  2600] loss: 2.303395\n",
      "[157,  2800] loss: 2.303485\n",
      "[157,  3000] loss: 2.303178\n",
      "[157,  3200] loss: 2.303521\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[158,   200] loss: 2.303687\n",
      "[158,   400] loss: 2.303543\n",
      "[158,   600] loss: 2.303291\n",
      "[158,   800] loss: 2.303781\n",
      "[158,  1000] loss: 2.303265\n",
      "[158,  1200] loss: 2.303549\n",
      "[158,  1400] loss: 2.303472\n",
      "[158,  1600] loss: 2.303612\n",
      "[158,  1800] loss: 2.303457\n",
      "[158,  2000] loss: 2.303578\n",
      "[158,  2200] loss: 2.303767\n",
      "[158,  2400] loss: 2.303352\n",
      "[158,  2600] loss: 2.303852\n",
      "[158,  2800] loss: 2.303304\n",
      "[158,  3000] loss: 2.303478\n",
      "[158,  3200] loss: 2.303473\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[159,   200] loss: 2.303549\n",
      "[159,   400] loss: 2.303233\n",
      "[159,   600] loss: 2.303211\n",
      "[159,   800] loss: 2.303575\n",
      "[159,  1000] loss: 2.303702\n",
      "[159,  1200] loss: 2.303678\n",
      "[159,  1400] loss: 2.303510\n",
      "[159,  1600] loss: 2.303720\n",
      "[159,  1800] loss: 2.303432\n",
      "[159,  2000] loss: 2.303510\n",
      "[159,  2200] loss: 2.303246\n",
      "[159,  2400] loss: 2.303797\n",
      "[159,  2600] loss: 2.303562\n",
      "[159,  2800] loss: 2.303793\n",
      "[159,  3000] loss: 2.303301\n",
      "[159,  3200] loss: 2.303540\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[160,   200] loss: 2.303568\n",
      "[160,   400] loss: 2.303178\n",
      "[160,   600] loss: 2.303659\n",
      "[160,   800] loss: 2.303799\n",
      "[160,  1000] loss: 2.303593\n",
      "[160,  1200] loss: 2.303162\n",
      "[160,  1400] loss: 2.303753\n",
      "[160,  1600] loss: 2.303300\n",
      "[160,  1800] loss: 2.303308\n",
      "[160,  2000] loss: 2.303635\n",
      "[160,  2200] loss: 2.303866\n",
      "[160,  2400] loss: 2.303697\n",
      "[160,  2600] loss: 2.303318\n",
      "[160,  2800] loss: 2.303433\n",
      "[160,  3000] loss: 2.303732\n",
      "[160,  3200] loss: 2.303798\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[161,   200] loss: 2.303591\n",
      "[161,   400] loss: 2.303243\n",
      "[161,   600] loss: 2.303491\n",
      "[161,   800] loss: 2.303381\n",
      "[161,  1000] loss: 2.303635\n",
      "[161,  1200] loss: 2.303427\n",
      "[161,  1400] loss: 2.303757\n",
      "[161,  1600] loss: 2.303645\n",
      "[161,  1800] loss: 2.303311\n",
      "[161,  2000] loss: 2.303838\n",
      "[161,  2200] loss: 2.303260\n",
      "[161,  2400] loss: 2.303371\n",
      "[161,  2600] loss: 2.303371\n",
      "[161,  2800] loss: 2.303667\n",
      "[161,  3000] loss: 2.303787\n",
      "[161,  3200] loss: 2.303465\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[162,   200] loss: 2.303332\n",
      "[162,   400] loss: 2.303329\n",
      "[162,   600] loss: 2.303393\n",
      "[162,   800] loss: 2.303800\n",
      "[162,  1000] loss: 2.303569\n",
      "[162,  1200] loss: 2.303205\n",
      "[162,  1400] loss: 2.303628\n",
      "[162,  1600] loss: 2.303655\n",
      "[162,  1800] loss: 2.302907\n",
      "[162,  2000] loss: 2.303458\n",
      "[162,  2200] loss: 2.303380\n",
      "[162,  2400] loss: 2.303407\n",
      "[162,  2600] loss: 2.303971\n",
      "[162,  2800] loss: 2.303763\n",
      "[162,  3000] loss: 2.303763\n",
      "[162,  3200] loss: 2.303544\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[163,   200] loss: 2.303396\n",
      "[163,   400] loss: 2.303479\n",
      "[163,   600] loss: 2.303723\n",
      "[163,   800] loss: 2.303589\n",
      "[163,  1000] loss: 2.303759\n",
      "[163,  1200] loss: 2.303696\n",
      "[163,  1400] loss: 2.303438\n",
      "[163,  1600] loss: 2.303666\n",
      "[163,  1800] loss: 2.303136\n",
      "[163,  2000] loss: 2.303446\n",
      "[163,  2200] loss: 2.303465\n",
      "[163,  2400] loss: 2.303255\n",
      "[163,  2600] loss: 2.303813\n",
      "[163,  2800] loss: 2.303203\n",
      "[163,  3000] loss: 2.303759\n",
      "[163,  3200] loss: 2.303239\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[164,   200] loss: 2.303503\n",
      "[164,   400] loss: 2.303736\n",
      "[164,   600] loss: 2.303335\n",
      "[164,   800] loss: 2.303423\n",
      "[164,  1000] loss: 2.303496\n",
      "[164,  1200] loss: 2.303301\n",
      "[164,  1400] loss: 2.303671\n",
      "[164,  1600] loss: 2.303836\n",
      "[164,  1800] loss: 2.303893\n",
      "[164,  2000] loss: 2.303319\n",
      "[164,  2200] loss: 2.303756\n",
      "[164,  2400] loss: 2.303691\n",
      "[164,  2600] loss: 2.303665\n",
      "[164,  2800] loss: 2.303714\n",
      "[164,  3000] loss: 2.303367\n",
      "[164,  3200] loss: 2.303590\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[165,   200] loss: 2.303723\n",
      "[165,   400] loss: 2.303328\n",
      "[165,   600] loss: 2.303469\n",
      "[165,   800] loss: 2.303763\n",
      "[165,  1000] loss: 2.303812\n",
      "[165,  1200] loss: 2.303698\n",
      "[165,  1400] loss: 2.303665\n",
      "[165,  1600] loss: 2.303137\n",
      "[165,  1800] loss: 2.303446\n",
      "[165,  2000] loss: 2.303376\n",
      "[165,  2200] loss: 2.303480\n",
      "[165,  2400] loss: 2.303421\n",
      "[165,  2600] loss: 2.303236\n",
      "[165,  2800] loss: 2.303207\n",
      "[165,  3000] loss: 2.303474\n",
      "[165,  3200] loss: 2.303878\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[166,   200] loss: 2.303567\n",
      "[166,   400] loss: 2.303772\n",
      "[166,   600] loss: 2.303603\n",
      "[166,   800] loss: 2.302611\n",
      "[166,  1000] loss: 2.303805\n",
      "[166,  1200] loss: 2.303339\n",
      "[166,  1400] loss: 2.303390\n",
      "[166,  1600] loss: 2.303033\n",
      "[166,  1800] loss: 2.303557\n",
      "[166,  2000] loss: 2.303501\n",
      "[166,  2200] loss: 2.303611\n",
      "[166,  2400] loss: 2.303652\n",
      "[166,  2600] loss: 2.302872\n",
      "[166,  2800] loss: 2.303798\n",
      "[166,  3000] loss: 2.303472\n",
      "[166,  3200] loss: 2.303736\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[167,   200] loss: 2.303645\n",
      "[167,   400] loss: 2.303351\n",
      "[167,   600] loss: 2.303406\n",
      "[167,   800] loss: 2.303369\n",
      "[167,  1000] loss: 2.303511\n",
      "[167,  1200] loss: 2.303492\n",
      "[167,  1400] loss: 2.303290\n",
      "[167,  1600] loss: 2.303480\n",
      "[167,  1800] loss: 2.303056\n",
      "[167,  2000] loss: 2.303708\n",
      "[167,  2200] loss: 2.303541\n",
      "[167,  2400] loss: 2.303759\n",
      "[167,  2600] loss: 2.303610\n",
      "[167,  2800] loss: 2.303375\n",
      "[167,  3000] loss: 2.303609\n",
      "[167,  3200] loss: 2.303022\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[168,   200] loss: 2.303600\n",
      "[168,   400] loss: 2.303371\n",
      "[168,   600] loss: 2.303659\n",
      "[168,   800] loss: 2.303690\n",
      "[168,  1000] loss: 2.302940\n",
      "[168,  1200] loss: 2.303564\n",
      "[168,  1400] loss: 2.303272\n",
      "[168,  1600] loss: 2.303621\n",
      "[168,  1800] loss: 2.303574\n",
      "[168,  2000] loss: 2.303603\n",
      "[168,  2200] loss: 2.303474\n",
      "[168,  2400] loss: 2.303801\n",
      "[168,  2600] loss: 2.303383\n",
      "[168,  2800] loss: 2.303924\n",
      "[168,  3000] loss: 2.303487\n",
      "[168,  3200] loss: 2.303640\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[169,   200] loss: 2.303501\n",
      "[169,   400] loss: 2.303272\n",
      "[169,   600] loss: 2.303304\n",
      "[169,   800] loss: 2.303799\n",
      "[169,  1000] loss: 2.303478\n",
      "[169,  1200] loss: 2.303431\n",
      "[169,  1400] loss: 2.303328\n",
      "[169,  1600] loss: 2.303610\n",
      "[169,  1800] loss: 2.303400\n",
      "[169,  2000] loss: 2.303478\n",
      "[169,  2200] loss: 2.303726\n",
      "[169,  2400] loss: 2.303579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169,  2600] loss: 2.303552\n",
      "[169,  2800] loss: 2.303090\n",
      "[169,  3000] loss: 2.303641\n",
      "[169,  3200] loss: 2.303726\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[170,   200] loss: 2.303422\n",
      "[170,   400] loss: 2.303562\n",
      "[170,   600] loss: 2.303808\n",
      "[170,   800] loss: 2.303827\n",
      "[170,  1000] loss: 2.303450\n",
      "[170,  1200] loss: 2.303466\n",
      "[170,  1400] loss: 2.303185\n",
      "[170,  1600] loss: 2.303598\n",
      "[170,  1800] loss: 2.303554\n",
      "[170,  2000] loss: 2.303390\n",
      "[170,  2200] loss: 2.303344\n",
      "[170,  2400] loss: 2.302886\n",
      "[170,  2600] loss: 2.303859\n",
      "[170,  2800] loss: 2.303217\n",
      "[170,  3000] loss: 2.303286\n",
      "[170,  3200] loss: 2.303547\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[171,   200] loss: 2.303143\n",
      "[171,   400] loss: 2.303241\n",
      "[171,   600] loss: 2.303563\n",
      "[171,   800] loss: 2.303610\n",
      "[171,  1000] loss: 2.303096\n",
      "[171,  1200] loss: 2.303847\n",
      "[171,  1400] loss: 2.303427\n",
      "[171,  1600] loss: 2.303683\n",
      "[171,  1800] loss: 2.302494\n",
      "[171,  2000] loss: 2.303569\n",
      "[171,  2200] loss: 2.303945\n",
      "[171,  2400] loss: 2.303616\n",
      "[171,  2600] loss: 2.303061\n",
      "[171,  2800] loss: 2.303863\n",
      "[171,  3000] loss: 2.303518\n",
      "[171,  3200] loss: 2.303536\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[172,   200] loss: 2.303469\n",
      "[172,   400] loss: 2.302752\n",
      "[172,   600] loss: 2.303363\n",
      "[172,   800] loss: 2.303400\n",
      "[172,  1000] loss: 2.303703\n",
      "[172,  1200] loss: 2.303578\n",
      "[172,  1400] loss: 2.303277\n",
      "[172,  1600] loss: 2.303270\n",
      "[172,  1800] loss: 2.303006\n",
      "[172,  2000] loss: 2.303421\n",
      "[172,  2200] loss: 2.302989\n",
      "[172,  2400] loss: 2.303458\n",
      "[172,  2600] loss: 2.303984\n",
      "[172,  2800] loss: 2.303437\n",
      "[172,  3000] loss: 2.303229\n",
      "[172,  3200] loss: 2.303566\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[173,   200] loss: 2.303555\n",
      "[173,   400] loss: 2.303383\n",
      "[173,   600] loss: 2.303291\n",
      "[173,   800] loss: 2.303847\n",
      "[173,  1000] loss: 2.303595\n",
      "[173,  1200] loss: 2.303472\n",
      "[173,  1400] loss: 2.303329\n",
      "[173,  1600] loss: 2.303988\n",
      "[173,  1800] loss: 2.303496\n",
      "[173,  2000] loss: 2.303451\n",
      "[173,  2200] loss: 2.303411\n",
      "[173,  2400] loss: 2.302895\n",
      "[173,  2600] loss: 2.303142\n",
      "[173,  2800] loss: 2.303429\n",
      "[173,  3000] loss: 2.303371\n",
      "[173,  3200] loss: 2.303606\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[174,   200] loss: 2.303322\n",
      "[174,   400] loss: 2.303310\n",
      "[174,   600] loss: 2.303901\n",
      "[174,   800] loss: 2.303415\n",
      "[174,  1000] loss: 2.303827\n",
      "[174,  1200] loss: 2.303249\n",
      "[174,  1400] loss: 2.303306\n",
      "[174,  1600] loss: 2.303461\n",
      "[174,  1800] loss: 2.303616\n",
      "[174,  2000] loss: 2.303481\n",
      "[174,  2200] loss: 2.303147\n",
      "[174,  2400] loss: 2.303537\n",
      "[174,  2600] loss: 2.303583\n",
      "[174,  2800] loss: 2.303428\n",
      "[174,  3000] loss: 2.303083\n",
      "[174,  3200] loss: 2.303719\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[175,   200] loss: 2.303796\n",
      "[175,   400] loss: 2.303427\n",
      "[175,   600] loss: 2.303308\n",
      "[175,   800] loss: 2.303841\n",
      "[175,  1000] loss: 2.303242\n",
      "[175,  1200] loss: 2.303406\n",
      "[175,  1400] loss: 2.303317\n",
      "[175,  1600] loss: 2.303293\n",
      "[175,  1800] loss: 2.303394\n",
      "[175,  2000] loss: 2.303757\n",
      "[175,  2200] loss: 2.303700\n",
      "[175,  2400] loss: 2.303647\n",
      "[175,  2600] loss: 2.303624\n",
      "[175,  2800] loss: 2.303456\n",
      "[175,  3000] loss: 2.303508\n",
      "[175,  3200] loss: 2.303700\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[176,   200] loss: 2.303443\n",
      "[176,   400] loss: 2.303295\n",
      "[176,   600] loss: 2.303601\n",
      "[176,   800] loss: 2.303335\n",
      "[176,  1000] loss: 2.303521\n",
      "[176,  1200] loss: 2.303258\n",
      "[176,  1400] loss: 2.303546\n",
      "[176,  1600] loss: 2.303316\n",
      "[176,  1800] loss: 2.303660\n",
      "[176,  2000] loss: 2.303555\n",
      "[176,  2200] loss: 2.303172\n",
      "[176,  2400] loss: 2.303740\n",
      "[176,  2600] loss: 2.303600\n",
      "[176,  2800] loss: 2.303345\n",
      "[176,  3000] loss: 2.303627\n",
      "[176,  3200] loss: 2.303318\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[177,   200] loss: 2.303210\n",
      "[177,   400] loss: 2.303278\n",
      "[177,   600] loss: 2.303287\n",
      "[177,   800] loss: 2.303501\n",
      "[177,  1000] loss: 2.303263\n",
      "[177,  1200] loss: 2.303210\n",
      "[177,  1400] loss: 2.303703\n",
      "[177,  1600] loss: 2.303030\n",
      "[177,  1800] loss: 2.303329\n",
      "[177,  2000] loss: 2.303493\n",
      "[177,  2200] loss: 2.303848\n",
      "[177,  2400] loss: 2.303561\n",
      "[177,  2600] loss: 2.303797\n",
      "[177,  2800] loss: 2.303318\n",
      "[177,  3000] loss: 2.303849\n",
      "[177,  3200] loss: 2.303235\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[178,   200] loss: 2.302918\n",
      "[178,   400] loss: 2.303423\n",
      "[178,   600] loss: 2.303347\n",
      "[178,   800] loss: 2.303353\n",
      "[178,  1000] loss: 2.303783\n",
      "[178,  1200] loss: 2.303431\n",
      "[178,  1400] loss: 2.303610\n",
      "[178,  1600] loss: 2.303482\n",
      "[178,  1800] loss: 2.303723\n",
      "[178,  2000] loss: 2.303578\n",
      "[178,  2200] loss: 2.303525\n",
      "[178,  2400] loss: 2.302955\n",
      "[178,  2600] loss: 2.303448\n",
      "[178,  2800] loss: 2.303453\n",
      "[178,  3000] loss: 2.303401\n",
      "[178,  3200] loss: 2.303269\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[179,   200] loss: 2.303366\n",
      "[179,   400] loss: 2.303335\n",
      "[179,   600] loss: 2.303568\n",
      "[179,   800] loss: 2.303348\n",
      "[179,  1000] loss: 2.303703\n",
      "[179,  1200] loss: 2.303472\n",
      "[179,  1400] loss: 2.303453\n",
      "[179,  1600] loss: 2.303856\n",
      "[179,  1800] loss: 2.302950\n",
      "[179,  2000] loss: 2.303195\n",
      "[179,  2200] loss: 2.303369\n",
      "[179,  2400] loss: 2.303286\n",
      "[179,  2600] loss: 2.303686\n",
      "[179,  2800] loss: 2.303490\n",
      "[179,  3000] loss: 2.303259\n",
      "[179,  3200] loss: 2.303671\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[180,   200] loss: 2.303871\n",
      "[180,   400] loss: 2.303905\n",
      "[180,   600] loss: 2.303488\n",
      "[180,   800] loss: 2.302971\n",
      "[180,  1000] loss: 2.303220\n",
      "[180,  1200] loss: 2.303166\n",
      "[180,  1400] loss: 2.303307\n",
      "[180,  1600] loss: 2.303545\n",
      "[180,  1800] loss: 2.303623\n",
      "[180,  2000] loss: 2.303408\n",
      "[180,  2200] loss: 2.303632\n",
      "[180,  2400] loss: 2.303566\n",
      "[180,  2600] loss: 2.303697\n",
      "[180,  2800] loss: 2.303820\n",
      "[180,  3000] loss: 2.303646\n",
      "[180,  3200] loss: 2.303129\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[181,   200] loss: 2.303446\n",
      "[181,   400] loss: 2.303602\n",
      "[181,   600] loss: 2.303261\n",
      "[181,   800] loss: 2.303403\n",
      "[181,  1000] loss: 2.303093\n",
      "[181,  1200] loss: 2.303891\n",
      "[181,  1400] loss: 2.303396\n",
      "[181,  1600] loss: 2.303449\n",
      "[181,  1800] loss: 2.303529\n",
      "[181,  2000] loss: 2.303230\n",
      "[181,  2200] loss: 2.304051\n",
      "[181,  2400] loss: 2.303077\n",
      "[181,  2600] loss: 2.303403\n",
      "[181,  2800] loss: 2.303578\n",
      "[181,  3000] loss: 2.303668\n",
      "[181,  3200] loss: 2.303656\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[182,   200] loss: 2.303083\n",
      "[182,   400] loss: 2.303407\n",
      "[182,   600] loss: 2.303504\n",
      "[182,   800] loss: 2.303947\n",
      "[182,  1000] loss: 2.303640\n",
      "[182,  1200] loss: 2.303604\n",
      "[182,  1400] loss: 2.303260\n",
      "[182,  1600] loss: 2.303675\n",
      "[182,  1800] loss: 2.303469\n",
      "[182,  2000] loss: 2.303583\n",
      "[182,  2200] loss: 2.304060\n",
      "[182,  2400] loss: 2.303252\n",
      "[182,  2600] loss: 2.303854\n",
      "[182,  2800] loss: 2.303294\n",
      "[182,  3000] loss: 2.303642\n",
      "[182,  3200] loss: 2.303446\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[183,   200] loss: 2.303945\n",
      "[183,   400] loss: 2.303630\n",
      "[183,   600] loss: 2.303247\n",
      "[183,   800] loss: 2.303694\n",
      "[183,  1000] loss: 2.303558\n",
      "[183,  1200] loss: 2.303512\n",
      "[183,  1400] loss: 2.303423\n",
      "[183,  1600] loss: 2.303343\n",
      "[183,  1800] loss: 2.302949\n",
      "[183,  2000] loss: 2.303023\n",
      "[183,  2200] loss: 2.303432\n",
      "[183,  2400] loss: 2.303521\n",
      "[183,  2600] loss: 2.303565\n",
      "[183,  2800] loss: 2.303397\n",
      "[183,  3000] loss: 2.303383\n",
      "[183,  3200] loss: 2.303231\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[184,   200] loss: 2.303367\n",
      "[184,   400] loss: 2.303162\n",
      "[184,   600] loss: 2.303399\n",
      "[184,   800] loss: 2.303825\n",
      "[184,  1000] loss: 2.303100\n",
      "[184,  1200] loss: 2.303600\n",
      "[184,  1400] loss: 2.303506\n",
      "[184,  1600] loss: 2.303452\n",
      "[184,  1800] loss: 2.303575\n",
      "[184,  2000] loss: 2.303606\n",
      "[184,  2200] loss: 2.303699\n",
      "[184,  2400] loss: 2.303788\n",
      "[184,  2600] loss: 2.303419\n",
      "[184,  2800] loss: 2.302910\n",
      "[184,  3000] loss: 2.303470\n",
      "[184,  3200] loss: 2.303488\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[185,   200] loss: 2.303755\n",
      "[185,   400] loss: 2.303229\n",
      "[185,   600] loss: 2.303607\n",
      "[185,   800] loss: 2.303235\n",
      "[185,  1000] loss: 2.303545\n",
      "[185,  1200] loss: 2.303011\n",
      "[185,  1400] loss: 2.303215\n",
      "[185,  1600] loss: 2.303765\n",
      "[185,  1800] loss: 2.303673\n",
      "[185,  2000] loss: 2.304013\n",
      "[185,  2200] loss: 2.303407\n",
      "[185,  2400] loss: 2.303446\n",
      "[185,  2600] loss: 2.303727\n",
      "[185,  2800] loss: 2.303054\n",
      "[185,  3000] loss: 2.303252\n",
      "[185,  3200] loss: 2.303120\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[186,   200] loss: 2.303939\n",
      "[186,   400] loss: 2.303623\n",
      "[186,   600] loss: 2.302948\n",
      "[186,   800] loss: 2.303791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186,  1000] loss: 2.303408\n",
      "[186,  1200] loss: 2.303780\n",
      "[186,  1400] loss: 2.303439\n",
      "[186,  1600] loss: 2.303763\n",
      "[186,  1800] loss: 2.303433\n",
      "[186,  2000] loss: 2.303100\n",
      "[186,  2200] loss: 2.303838\n",
      "[186,  2400] loss: 2.303150\n",
      "[186,  2600] loss: 2.303187\n",
      "[186,  2800] loss: 2.303591\n",
      "[186,  3000] loss: 2.303872\n",
      "[186,  3200] loss: 2.303122\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[187,   200] loss: 2.303612\n",
      "[187,   400] loss: 2.303546\n",
      "[187,   600] loss: 2.303753\n",
      "[187,   800] loss: 2.303533\n",
      "[187,  1000] loss: 2.303421\n",
      "[187,  1200] loss: 2.303549\n",
      "[187,  1400] loss: 2.303375\n",
      "[187,  1600] loss: 2.303097\n",
      "[187,  1800] loss: 2.303593\n",
      "[187,  2000] loss: 2.304478\n",
      "[187,  2200] loss: 2.303271\n",
      "[187,  2400] loss: 2.303537\n",
      "[187,  2600] loss: 2.303141\n",
      "[187,  2800] loss: 2.303642\n",
      "[187,  3000] loss: 2.303478\n",
      "[187,  3200] loss: 2.303470\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[188,   200] loss: 2.303801\n",
      "[188,   400] loss: 2.303740\n",
      "[188,   600] loss: 2.303747\n",
      "[188,   800] loss: 2.303435\n",
      "[188,  1000] loss: 2.303378\n",
      "[188,  1200] loss: 2.303612\n",
      "[188,  1400] loss: 2.303896\n",
      "[188,  1600] loss: 2.303714\n",
      "[188,  1800] loss: 2.303323\n",
      "[188,  2000] loss: 2.303187\n",
      "[188,  2200] loss: 2.304108\n",
      "[188,  2400] loss: 2.303252\n",
      "[188,  2600] loss: 2.303148\n",
      "[188,  2800] loss: 2.303835\n",
      "[188,  3000] loss: 2.303824\n",
      "[188,  3200] loss: 2.303598\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[189,   200] loss: 2.303511\n",
      "[189,   400] loss: 2.302988\n",
      "[189,   600] loss: 2.303720\n",
      "[189,   800] loss: 2.303583\n",
      "[189,  1000] loss: 2.303234\n",
      "[189,  1200] loss: 2.303690\n",
      "[189,  1400] loss: 2.303398\n",
      "[189,  1600] loss: 2.303363\n",
      "[189,  1800] loss: 2.303575\n",
      "[189,  2000] loss: 2.303442\n",
      "[189,  2200] loss: 2.303569\n",
      "[189,  2400] loss: 2.303617\n",
      "[189,  2600] loss: 2.303582\n",
      "[189,  2800] loss: 2.303103\n",
      "[189,  3000] loss: 2.303578\n",
      "[189,  3200] loss: 2.303200\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[190,   200] loss: 2.303493\n",
      "[190,   400] loss: 2.303633\n",
      "[190,   600] loss: 2.303457\n",
      "[190,   800] loss: 2.303495\n",
      "[190,  1000] loss: 2.303365\n",
      "[190,  1200] loss: 2.304074\n",
      "[190,  1400] loss: 2.303237\n",
      "[190,  1600] loss: 2.303390\n",
      "[190,  1800] loss: 2.303259\n",
      "[190,  2000] loss: 2.303883\n",
      "[190,  2200] loss: 2.303432\n",
      "[190,  2400] loss: 2.303501\n",
      "[190,  2600] loss: 2.303320\n",
      "[190,  2800] loss: 2.303321\n",
      "[190,  3000] loss: 2.303222\n",
      "[190,  3200] loss: 2.303938\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[191,   200] loss: 2.303520\n",
      "[191,   400] loss: 2.303640\n",
      "[191,   600] loss: 2.303293\n",
      "[191,   800] loss: 2.303500\n",
      "[191,  1000] loss: 2.303242\n",
      "[191,  1200] loss: 2.303440\n",
      "[191,  1400] loss: 2.303510\n",
      "[191,  1600] loss: 2.303308\n",
      "[191,  1800] loss: 2.303130\n",
      "[191,  2000] loss: 2.303538\n",
      "[191,  2200] loss: 2.303228\n",
      "[191,  2400] loss: 2.302875\n",
      "[191,  2600] loss: 2.303540\n",
      "[191,  2800] loss: 2.303747\n",
      "[191,  3000] loss: 2.303581\n",
      "[191,  3200] loss: 2.303759\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[192,   200] loss: 2.303619\n",
      "[192,   400] loss: 2.303263\n",
      "[192,   600] loss: 2.303681\n",
      "[192,   800] loss: 2.303730\n",
      "[192,  1000] loss: 2.303608\n",
      "[192,  1200] loss: 2.303188\n",
      "[192,  1400] loss: 2.303185\n",
      "[192,  1600] loss: 2.303549\n",
      "[192,  1800] loss: 2.303442\n",
      "[192,  2000] loss: 2.303358\n",
      "[192,  2200] loss: 2.303222\n",
      "[192,  2400] loss: 2.303715\n",
      "[192,  2600] loss: 2.303444\n",
      "[192,  2800] loss: 2.303643\n",
      "[192,  3000] loss: 2.303730\n",
      "[192,  3200] loss: 2.303869\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[193,   200] loss: 2.303110\n",
      "[193,   400] loss: 2.303336\n",
      "[193,   600] loss: 2.303727\n",
      "[193,   800] loss: 2.303879\n",
      "[193,  1000] loss: 2.303520\n",
      "[193,  1200] loss: 2.303405\n",
      "[193,  1400] loss: 2.303609\n",
      "[193,  1600] loss: 2.302815\n",
      "[193,  1800] loss: 2.303584\n",
      "[193,  2000] loss: 2.302982\n",
      "[193,  2200] loss: 2.303877\n",
      "[193,  2400] loss: 2.303335\n",
      "[193,  2600] loss: 2.303458\n",
      "[193,  2800] loss: 2.303215\n",
      "[193,  3000] loss: 2.303656\n",
      "[193,  3200] loss: 2.303650\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[194,   200] loss: 2.303180\n",
      "[194,   400] loss: 2.303907\n",
      "[194,   600] loss: 2.303486\n",
      "[194,   800] loss: 2.303612\n",
      "[194,  1000] loss: 2.303727\n",
      "[194,  1200] loss: 2.303536\n",
      "[194,  1400] loss: 2.303729\n",
      "[194,  1600] loss: 2.303240\n",
      "[194,  1800] loss: 2.304052\n",
      "[194,  2000] loss: 2.303647\n",
      "[194,  2200] loss: 2.303253\n",
      "[194,  2400] loss: 2.303504\n",
      "[194,  2600] loss: 2.303556\n",
      "[194,  2800] loss: 2.302889\n",
      "[194,  3000] loss: 2.303417\n",
      "[194,  3200] loss: 2.303046\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[195,   200] loss: 2.303368\n",
      "[195,   400] loss: 2.303566\n",
      "[195,   600] loss: 2.303422\n",
      "[195,   800] loss: 2.303663\n",
      "[195,  1000] loss: 2.303640\n",
      "[195,  1200] loss: 2.303478\n",
      "[195,  1400] loss: 2.303056\n",
      "[195,  1600] loss: 2.303115\n",
      "[195,  1800] loss: 2.303408\n",
      "[195,  2000] loss: 2.303915\n",
      "[195,  2200] loss: 2.303321\n",
      "[195,  2400] loss: 2.303374\n",
      "[195,  2600] loss: 2.303980\n",
      "[195,  2800] loss: 2.303156\n",
      "[195,  3000] loss: 2.303657\n",
      "[195,  3200] loss: 2.303257\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[196,   200] loss: 2.303310\n",
      "[196,   400] loss: 2.303740\n",
      "[196,   600] loss: 2.303832\n",
      "[196,   800] loss: 2.303364\n",
      "[196,  1000] loss: 2.303514\n",
      "[196,  1200] loss: 2.304000\n",
      "[196,  1400] loss: 2.303056\n",
      "[196,  1600] loss: 2.303633\n",
      "[196,  1800] loss: 2.303401\n",
      "[196,  2000] loss: 2.303305\n",
      "[196,  2200] loss: 2.303409\n",
      "[196,  2400] loss: 2.302936\n",
      "[196,  2600] loss: 2.303512\n",
      "[196,  2800] loss: 2.303091\n",
      "[196,  3000] loss: 2.303281\n",
      "[196,  3200] loss: 2.303113\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[197,   200] loss: 2.303578\n",
      "[197,   400] loss: 2.303593\n",
      "[197,   600] loss: 2.303783\n",
      "[197,   800] loss: 2.303887\n",
      "[197,  1000] loss: 2.303664\n",
      "[197,  1200] loss: 2.303961\n",
      "[197,  1400] loss: 2.303803\n",
      "[197,  1600] loss: 2.303403\n",
      "[197,  1800] loss: 2.303591\n",
      "[197,  2000] loss: 2.303324\n",
      "[197,  2200] loss: 2.303458\n",
      "[197,  2400] loss: 2.303055\n",
      "[197,  2600] loss: 2.303357\n",
      "[197,  2800] loss: 2.303798\n",
      "[197,  3000] loss: 2.303774\n",
      "[197,  3200] loss: 2.303335\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[198,   200] loss: 2.303550\n",
      "[198,   400] loss: 2.303429\n",
      "[198,   600] loss: 2.303126\n",
      "[198,   800] loss: 2.303842\n",
      "[198,  1000] loss: 2.303462\n",
      "[198,  1200] loss: 2.303413\n",
      "[198,  1400] loss: 2.303584\n",
      "[198,  1600] loss: 2.303408\n",
      "[198,  1800] loss: 2.303442\n",
      "[198,  2000] loss: 2.303439\n",
      "[198,  2200] loss: 2.303406\n",
      "[198,  2400] loss: 2.303734\n",
      "[198,  2600] loss: 2.303381\n",
      "[198,  2800] loss: 2.303248\n",
      "[198,  3000] loss: 2.303586\n",
      "[198,  3200] loss: 2.303405\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[199,   200] loss: 2.303502\n",
      "[199,   400] loss: 2.303257\n",
      "[199,   600] loss: 2.303537\n",
      "[199,   800] loss: 2.303417\n",
      "[199,  1000] loss: 2.303520\n",
      "[199,  1200] loss: 2.303529\n",
      "[199,  1400] loss: 2.303377\n",
      "[199,  1600] loss: 2.303733\n",
      "[199,  1800] loss: 2.303384\n",
      "[199,  2000] loss: 2.303948\n",
      "[199,  2200] loss: 2.303810\n",
      "[199,  2400] loss: 2.303407\n",
      "[199,  2600] loss: 2.303220\n",
      "[199,  2800] loss: 2.304010\n",
      "[199,  3000] loss: 2.303447\n",
      "[199,  3200] loss: 2.303546\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[200,   200] loss: 2.303431\n",
      "[200,   400] loss: 2.303256\n",
      "[200,   600] loss: 2.303594\n",
      "[200,   800] loss: 2.303425\n",
      "[200,  1000] loss: 2.303476\n",
      "[200,  1200] loss: 2.303154\n",
      "[200,  1400] loss: 2.303726\n",
      "[200,  1600] loss: 2.303266\n",
      "[200,  1800] loss: 2.303364\n",
      "[200,  2000] loss: 2.303583\n",
      "[200,  2200] loss: 2.303431\n",
      "[200,  2400] loss: 2.304093\n",
      "[200,  2600] loss: 2.303355\n",
      "[200,  2800] loss: 2.303369\n",
      "[200,  3000] loss: 2.303807\n",
      "[200,  3200] loss: 2.303184\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[201,   200] loss: 2.303304\n",
      "[201,   400] loss: 2.303849\n",
      "[201,   600] loss: 2.303229\n",
      "[201,   800] loss: 2.303507\n",
      "[201,  1000] loss: 2.303528\n",
      "[201,  1200] loss: 2.303334\n",
      "[201,  1400] loss: 2.303725\n",
      "[201,  1600] loss: 2.303347\n",
      "[201,  1800] loss: 2.303499\n",
      "[201,  2000] loss: 2.303644\n",
      "[201,  2200] loss: 2.303553\n",
      "[201,  2400] loss: 2.303249\n",
      "[201,  2600] loss: 2.303649\n",
      "[201,  2800] loss: 2.303449\n",
      "[201,  3000] loss: 2.303373\n",
      "[201,  3200] loss: 2.303340\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[202,   200] loss: 2.303371\n",
      "[202,   400] loss: 2.303527\n",
      "[202,   600] loss: 2.303570\n",
      "[202,   800] loss: 2.303338\n",
      "[202,  1000] loss: 2.303828\n",
      "[202,  1200] loss: 2.303401\n",
      "[202,  1400] loss: 2.303780\n",
      "[202,  1600] loss: 2.303583\n",
      "[202,  1800] loss: 2.303247\n",
      "[202,  2000] loss: 2.303557\n",
      "[202,  2200] loss: 2.303820\n",
      "[202,  2400] loss: 2.303587\n",
      "[202,  2600] loss: 2.303479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202,  2800] loss: 2.303588\n",
      "[202,  3000] loss: 2.303741\n",
      "[202,  3200] loss: 2.303009\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[203,   200] loss: 2.303152\n",
      "[203,   400] loss: 2.303864\n",
      "[203,   600] loss: 2.303449\n",
      "[203,   800] loss: 2.303096\n",
      "[203,  1000] loss: 2.303751\n",
      "[203,  1200] loss: 2.303484\n",
      "[203,  1400] loss: 2.303272\n",
      "[203,  1600] loss: 2.303360\n",
      "[203,  1800] loss: 2.303616\n",
      "[203,  2000] loss: 2.303676\n",
      "[203,  2200] loss: 2.303553\n",
      "[203,  2400] loss: 2.303594\n",
      "[203,  2600] loss: 2.303540\n",
      "[203,  2800] loss: 2.303503\n",
      "[203,  3000] loss: 2.303589\n",
      "[203,  3200] loss: 2.303483\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[204,   200] loss: 2.303504\n",
      "[204,   400] loss: 2.303924\n",
      "[204,   600] loss: 2.303301\n",
      "[204,   800] loss: 2.303875\n",
      "[204,  1000] loss: 2.303498\n",
      "[204,  1200] loss: 2.302745\n",
      "[204,  1400] loss: 2.303251\n",
      "[204,  1600] loss: 2.303685\n",
      "[204,  1800] loss: 2.303474\n",
      "[204,  2000] loss: 2.303535\n",
      "[204,  2200] loss: 2.303740\n",
      "[204,  2400] loss: 2.303551\n",
      "[204,  2600] loss: 2.303529\n",
      "[204,  2800] loss: 2.303854\n",
      "[204,  3000] loss: 2.303249\n",
      "[204,  3200] loss: 2.303345\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[205,   200] loss: 2.303726\n",
      "[205,   400] loss: 2.303273\n",
      "[205,   600] loss: 2.303291\n",
      "[205,   800] loss: 2.303619\n",
      "[205,  1000] loss: 2.303588\n",
      "[205,  1200] loss: 2.303246\n",
      "[205,  1400] loss: 2.303949\n",
      "[205,  1600] loss: 2.303260\n",
      "[205,  1800] loss: 2.303996\n",
      "[205,  2000] loss: 2.303393\n",
      "[205,  2200] loss: 2.303419\n",
      "[205,  2400] loss: 2.303003\n",
      "[205,  2600] loss: 2.303206\n",
      "[205,  2800] loss: 2.303808\n",
      "[205,  3000] loss: 2.303270\n",
      "[205,  3200] loss: 2.303603\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[206,   200] loss: 2.303462\n",
      "[206,   400] loss: 2.303473\n",
      "[206,   600] loss: 2.303664\n",
      "[206,   800] loss: 2.303232\n",
      "[206,  1000] loss: 2.303221\n",
      "[206,  1200] loss: 2.303186\n",
      "[206,  1400] loss: 2.303739\n",
      "[206,  1600] loss: 2.303555\n",
      "[206,  1800] loss: 2.303591\n",
      "[206,  2000] loss: 2.303787\n",
      "[206,  2200] loss: 2.303214\n",
      "[206,  2400] loss: 2.303419\n",
      "[206,  2600] loss: 2.303098\n",
      "[206,  2800] loss: 2.303485\n",
      "[206,  3000] loss: 2.303614\n",
      "[206,  3200] loss: 2.303089\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[207,   200] loss: 2.303304\n",
      "[207,   400] loss: 2.303937\n",
      "[207,   600] loss: 2.303193\n",
      "[207,   800] loss: 2.303418\n",
      "[207,  1000] loss: 2.303662\n",
      "[207,  1200] loss: 2.303475\n",
      "[207,  1400] loss: 2.303262\n",
      "[207,  1600] loss: 2.303807\n",
      "[207,  1800] loss: 2.303641\n",
      "[207,  2000] loss: 2.303348\n",
      "[207,  2200] loss: 2.303527\n",
      "[207,  2400] loss: 2.303428\n",
      "[207,  2600] loss: 2.303632\n",
      "[207,  2800] loss: 2.303859\n",
      "[207,  3000] loss: 2.302949\n",
      "[207,  3200] loss: 2.303887\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[208,   200] loss: 2.303728\n",
      "[208,   400] loss: 2.303597\n",
      "[208,   600] loss: 2.303150\n",
      "[208,   800] loss: 2.303246\n",
      "[208,  1000] loss: 2.303523\n",
      "[208,  1200] loss: 2.303931\n",
      "[208,  1400] loss: 2.303620\n",
      "[208,  1600] loss: 2.303586\n",
      "[208,  1800] loss: 2.303391\n",
      "[208,  2000] loss: 2.303484\n",
      "[208,  2200] loss: 2.303202\n",
      "[208,  2400] loss: 2.303556\n",
      "[208,  2600] loss: 2.303826\n",
      "[208,  2800] loss: 2.303384\n",
      "[208,  3000] loss: 2.303426\n",
      "[208,  3200] loss: 2.303588\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[209,   200] loss: 2.303288\n",
      "[209,   400] loss: 2.303004\n",
      "[209,   600] loss: 2.303699\n",
      "[209,   800] loss: 2.303346\n",
      "[209,  1000] loss: 2.303617\n",
      "[209,  1200] loss: 2.303737\n",
      "[209,  1400] loss: 2.303649\n",
      "[209,  1600] loss: 2.303771\n",
      "[209,  1800] loss: 2.303474\n",
      "[209,  2000] loss: 2.303546\n",
      "[209,  2200] loss: 2.303079\n",
      "[209,  2400] loss: 2.303389\n",
      "[209,  2600] loss: 2.303184\n",
      "[209,  2800] loss: 2.303890\n",
      "[209,  3000] loss: 2.303733\n",
      "[209,  3200] loss: 2.303368\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[210,   200] loss: 2.303634\n",
      "[210,   400] loss: 2.303743\n",
      "[210,   600] loss: 2.303694\n",
      "[210,   800] loss: 2.303484\n",
      "[210,  1000] loss: 2.303418\n",
      "[210,  1200] loss: 2.303716\n",
      "[210,  1400] loss: 2.303617\n",
      "[210,  1600] loss: 2.303316\n",
      "[210,  1800] loss: 2.303838\n",
      "[210,  2000] loss: 2.303850\n",
      "[210,  2200] loss: 2.303328\n",
      "[210,  2400] loss: 2.303316\n",
      "[210,  2600] loss: 2.303974\n",
      "[210,  2800] loss: 2.303257\n",
      "[210,  3000] loss: 2.303518\n",
      "[210,  3200] loss: 2.303498\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[211,   200] loss: 2.303611\n",
      "[211,   400] loss: 2.302937\n",
      "[211,   600] loss: 2.303655\n",
      "[211,   800] loss: 2.303368\n",
      "[211,  1000] loss: 2.303903\n",
      "[211,  1200] loss: 2.303190\n",
      "[211,  1400] loss: 2.303127\n",
      "[211,  1600] loss: 2.303730\n",
      "[211,  1800] loss: 2.303507\n",
      "[211,  2000] loss: 2.303401\n",
      "[211,  2200] loss: 2.303429\n",
      "[211,  2400] loss: 2.303482\n",
      "[211,  2600] loss: 2.303632\n",
      "[211,  2800] loss: 2.303337\n",
      "[211,  3000] loss: 2.303002\n",
      "[211,  3200] loss: 2.303616\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[212,   200] loss: 2.303193\n",
      "[212,   400] loss: 2.303657\n",
      "[212,   600] loss: 2.303217\n",
      "[212,   800] loss: 2.303555\n",
      "[212,  1000] loss: 2.303555\n",
      "[212,  1200] loss: 2.302799\n",
      "[212,  1400] loss: 2.303508\n",
      "[212,  1600] loss: 2.303305\n",
      "[212,  1800] loss: 2.304001\n",
      "[212,  2000] loss: 2.303479\n",
      "[212,  2200] loss: 2.303321\n",
      "[212,  2400] loss: 2.303631\n",
      "[212,  2600] loss: 2.303229\n",
      "[212,  2800] loss: 2.303157\n",
      "[212,  3000] loss: 2.303316\n",
      "[212,  3200] loss: 2.303247\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[213,   200] loss: 2.303534\n",
      "[213,   400] loss: 2.303712\n",
      "[213,   600] loss: 2.303584\n",
      "[213,   800] loss: 2.303443\n",
      "[213,  1000] loss: 2.303301\n",
      "[213,  1200] loss: 2.303655\n",
      "[213,  1400] loss: 2.303830\n",
      "[213,  1600] loss: 2.303743\n",
      "[213,  1800] loss: 2.303361\n",
      "[213,  2000] loss: 2.303713\n",
      "[213,  2200] loss: 2.303578\n",
      "[213,  2400] loss: 2.302968\n",
      "[213,  2600] loss: 2.303253\n",
      "[213,  2800] loss: 2.303587\n",
      "[213,  3000] loss: 2.303681\n",
      "[213,  3200] loss: 2.303090\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[214,   200] loss: 2.303208\n",
      "[214,   400] loss: 2.303250\n",
      "[214,   600] loss: 2.303009\n",
      "[214,   800] loss: 2.303762\n",
      "[214,  1000] loss: 2.303669\n",
      "[214,  1200] loss: 2.303615\n",
      "[214,  1400] loss: 2.303699\n",
      "[214,  1600] loss: 2.303462\n",
      "[214,  1800] loss: 2.302946\n",
      "[214,  2000] loss: 2.303678\n",
      "[214,  2200] loss: 2.303698\n",
      "[214,  2400] loss: 2.303098\n",
      "[214,  2600] loss: 2.303397\n",
      "[214,  2800] loss: 2.303581\n",
      "[214,  3000] loss: 2.303241\n",
      "[214,  3200] loss: 2.303689\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[215,   200] loss: 2.303723\n",
      "[215,   400] loss: 2.303600\n",
      "[215,   600] loss: 2.303298\n",
      "[215,   800] loss: 2.303653\n",
      "[215,  1000] loss: 2.303101\n",
      "[215,  1200] loss: 2.303534\n",
      "[215,  1400] loss: 2.303699\n",
      "[215,  1600] loss: 2.303381\n",
      "[215,  1800] loss: 2.303761\n",
      "[215,  2000] loss: 2.303399\n",
      "[215,  2200] loss: 2.303592\n",
      "[215,  2400] loss: 2.303356\n",
      "[215,  2600] loss: 2.303991\n",
      "[215,  2800] loss: 2.303648\n",
      "[215,  3000] loss: 2.303244\n",
      "[215,  3200] loss: 2.303503\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[216,   200] loss: 2.303474\n",
      "[216,   400] loss: 2.303851\n",
      "[216,   600] loss: 2.303560\n",
      "[216,   800] loss: 2.303808\n",
      "[216,  1000] loss: 2.303016\n",
      "[216,  1200] loss: 2.303130\n",
      "[216,  1400] loss: 2.303661\n",
      "[216,  1600] loss: 2.303740\n",
      "[216,  1800] loss: 2.303356\n",
      "[216,  2000] loss: 2.303565\n",
      "[216,  2200] loss: 2.303277\n",
      "[216,  2400] loss: 2.303261\n",
      "[216,  2600] loss: 2.303544\n",
      "[216,  2800] loss: 2.303352\n",
      "[216,  3000] loss: 2.303318\n",
      "[216,  3200] loss: 2.304028\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[217,   200] loss: 2.303522\n",
      "[217,   400] loss: 2.303393\n",
      "[217,   600] loss: 2.303493\n",
      "[217,   800] loss: 2.303401\n",
      "[217,  1000] loss: 2.303389\n",
      "[217,  1200] loss: 2.303947\n",
      "[217,  1400] loss: 2.303340\n",
      "[217,  1600] loss: 2.303311\n",
      "[217,  1800] loss: 2.303737\n",
      "[217,  2000] loss: 2.303192\n",
      "[217,  2200] loss: 2.303722\n",
      "[217,  2400] loss: 2.303449\n",
      "[217,  2600] loss: 2.303603\n",
      "[217,  2800] loss: 2.303128\n",
      "[217,  3000] loss: 2.303437\n",
      "[217,  3200] loss: 2.303297\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[218,   200] loss: 2.303246\n",
      "[218,   400] loss: 2.303035\n",
      "[218,   600] loss: 2.303789\n",
      "[218,   800] loss: 2.303423\n",
      "[218,  1000] loss: 2.303523\n",
      "[218,  1200] loss: 2.303365\n",
      "[218,  1400] loss: 2.303660\n",
      "[218,  1600] loss: 2.303960\n",
      "[218,  1800] loss: 2.303166\n",
      "[218,  2000] loss: 2.303804\n",
      "[218,  2200] loss: 2.303614\n",
      "[218,  2400] loss: 2.303669\n",
      "[218,  2600] loss: 2.303234\n",
      "[218,  2800] loss: 2.302953\n",
      "[218,  3000] loss: 2.303297\n",
      "[218,  3200] loss: 2.302996\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[219,   200] loss: 2.303093\n",
      "[219,   400] loss: 2.303786\n",
      "[219,   600] loss: 2.303279\n",
      "[219,   800] loss: 2.303171\n",
      "[219,  1000] loss: 2.303917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[219,  1200] loss: 2.303093\n",
      "[219,  1400] loss: 2.303748\n",
      "[219,  1600] loss: 2.303695\n",
      "[219,  1800] loss: 2.303593\n",
      "[219,  2000] loss: 2.303593\n",
      "[219,  2200] loss: 2.303311\n",
      "[219,  2400] loss: 2.303571\n",
      "[219,  2600] loss: 2.303564\n",
      "[219,  2800] loss: 2.303704\n",
      "[219,  3000] loss: 2.303269\n",
      "[219,  3200] loss: 2.303091\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[220,   200] loss: 2.303503\n",
      "[220,   400] loss: 2.303119\n",
      "[220,   600] loss: 2.303615\n",
      "[220,   800] loss: 2.303816\n",
      "[220,  1000] loss: 2.303302\n",
      "[220,  1200] loss: 2.303200\n",
      "[220,  1400] loss: 2.303448\n",
      "[220,  1600] loss: 2.302978\n",
      "[220,  1800] loss: 2.303574\n",
      "[220,  2000] loss: 2.303281\n",
      "[220,  2200] loss: 2.303401\n",
      "[220,  2400] loss: 2.303416\n",
      "[220,  2600] loss: 2.303691\n",
      "[220,  2800] loss: 2.303559\n",
      "[220,  3000] loss: 2.303611\n",
      "[220,  3200] loss: 2.303373\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[221,   200] loss: 2.303255\n",
      "[221,   400] loss: 2.303465\n",
      "[221,   600] loss: 2.303298\n",
      "[221,   800] loss: 2.303421\n",
      "[221,  1000] loss: 2.303237\n",
      "[221,  1200] loss: 2.303525\n",
      "[221,  1400] loss: 2.303013\n",
      "[221,  1600] loss: 2.303324\n",
      "[221,  1800] loss: 2.303074\n",
      "[221,  2000] loss: 2.304073\n",
      "[221,  2200] loss: 2.303330\n",
      "[221,  2400] loss: 2.303997\n",
      "[221,  2600] loss: 2.303240\n",
      "[221,  2800] loss: 2.303953\n",
      "[221,  3000] loss: 2.303770\n",
      "[221,  3200] loss: 2.303652\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[222,   200] loss: 2.302967\n",
      "[222,   400] loss: 2.303259\n",
      "[222,   600] loss: 2.303229\n",
      "[222,   800] loss: 2.303106\n",
      "[222,  1000] loss: 2.303822\n",
      "[222,  1200] loss: 2.303214\n",
      "[222,  1400] loss: 2.303595\n",
      "[222,  1600] loss: 2.303639\n",
      "[222,  1800] loss: 2.303570\n",
      "[222,  2000] loss: 2.303257\n",
      "[222,  2200] loss: 2.303711\n",
      "[222,  2400] loss: 2.303805\n",
      "[222,  2600] loss: 2.303275\n",
      "[222,  2800] loss: 2.303521\n",
      "[222,  3000] loss: 2.303317\n",
      "[222,  3200] loss: 2.303482\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[223,   200] loss: 2.303320\n",
      "[223,   400] loss: 2.303550\n",
      "[223,   600] loss: 2.303887\n",
      "[223,   800] loss: 2.303625\n",
      "[223,  1000] loss: 2.302791\n",
      "[223,  1200] loss: 2.303017\n",
      "[223,  1400] loss: 2.304040\n",
      "[223,  1600] loss: 2.303431\n",
      "[223,  1800] loss: 2.303562\n",
      "[223,  2000] loss: 2.303477\n",
      "[223,  2200] loss: 2.303595\n",
      "[223,  2400] loss: 2.303385\n",
      "[223,  2600] loss: 2.303822\n",
      "[223,  2800] loss: 2.303564\n",
      "[223,  3000] loss: 2.303493\n",
      "[223,  3200] loss: 2.303377\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[224,   200] loss: 2.303083\n",
      "[224,   400] loss: 2.303867\n",
      "[224,   600] loss: 2.303713\n",
      "[224,   800] loss: 2.303453\n",
      "[224,  1000] loss: 2.303913\n",
      "[224,  1200] loss: 2.303600\n",
      "[224,  1400] loss: 2.303377\n",
      "[224,  1600] loss: 2.303298\n",
      "[224,  1800] loss: 2.303131\n",
      "[224,  2000] loss: 2.303548\n",
      "[224,  2200] loss: 2.303711\n",
      "[224,  2400] loss: 2.303741\n",
      "[224,  2600] loss: 2.303433\n",
      "[224,  2800] loss: 2.303344\n",
      "[224,  3000] loss: 2.303423\n",
      "[224,  3200] loss: 2.303551\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[225,   200] loss: 2.303254\n",
      "[225,   400] loss: 2.303594\n",
      "[225,   600] loss: 2.303662\n",
      "[225,   800] loss: 2.303812\n",
      "[225,  1000] loss: 2.303878\n",
      "[225,  1200] loss: 2.302924\n",
      "[225,  1400] loss: 2.303246\n",
      "[225,  1600] loss: 2.303621\n",
      "[225,  1800] loss: 2.303546\n",
      "[225,  2000] loss: 2.303990\n",
      "[225,  2200] loss: 2.303363\n",
      "[225,  2400] loss: 2.304055\n",
      "[225,  2600] loss: 2.303133\n",
      "[225,  2800] loss: 2.303502\n",
      "[225,  3000] loss: 2.303534\n",
      "[225,  3200] loss: 2.303646\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[226,   200] loss: 2.303257\n",
      "[226,   400] loss: 2.303823\n",
      "[226,   600] loss: 2.303309\n",
      "[226,   800] loss: 2.303700\n",
      "[226,  1000] loss: 2.303353\n",
      "[226,  1200] loss: 2.303237\n",
      "[226,  1400] loss: 2.303520\n",
      "[226,  1600] loss: 2.303430\n",
      "[226,  1800] loss: 2.303342\n",
      "[226,  2000] loss: 2.303438\n",
      "[226,  2200] loss: 2.303313\n",
      "[226,  2400] loss: 2.303961\n",
      "[226,  2600] loss: 2.303562\n",
      "[226,  2800] loss: 2.303349\n",
      "[226,  3000] loss: 2.303450\n",
      "[226,  3200] loss: 2.303187\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[227,   200] loss: 2.303561\n",
      "[227,   400] loss: 2.304052\n",
      "[227,   600] loss: 2.303515\n",
      "[227,   800] loss: 2.303210\n",
      "[227,  1000] loss: 2.303579\n",
      "[227,  1200] loss: 2.303198\n",
      "[227,  1400] loss: 2.304012\n",
      "[227,  1600] loss: 2.302831\n",
      "[227,  1800] loss: 2.303824\n",
      "[227,  2000] loss: 2.303215\n",
      "[227,  2200] loss: 2.303579\n",
      "[227,  2400] loss: 2.303216\n",
      "[227,  2600] loss: 2.303655\n",
      "[227,  2800] loss: 2.303396\n",
      "[227,  3000] loss: 2.302989\n",
      "[227,  3200] loss: 2.303869\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[228,   200] loss: 2.303463\n",
      "[228,   400] loss: 2.303690\n",
      "[228,   600] loss: 2.303623\n",
      "[228,   800] loss: 2.303312\n",
      "[228,  1000] loss: 2.303761\n",
      "[228,  1200] loss: 2.303674\n",
      "[228,  1400] loss: 2.303274\n",
      "[228,  1600] loss: 2.304212\n",
      "[228,  1800] loss: 2.302842\n",
      "[228,  2000] loss: 2.303344\n",
      "[228,  2200] loss: 2.303775\n",
      "[228,  2400] loss: 2.303157\n",
      "[228,  2600] loss: 2.302989\n",
      "[228,  2800] loss: 2.304157\n",
      "[228,  3000] loss: 2.303593\n",
      "[228,  3200] loss: 2.303174\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[229,   200] loss: 2.303200\n",
      "[229,   400] loss: 2.303419\n",
      "[229,   600] loss: 2.303508\n",
      "[229,   800] loss: 2.303544\n",
      "[229,  1000] loss: 2.303302\n",
      "[229,  1200] loss: 2.303363\n",
      "[229,  1400] loss: 2.303235\n",
      "[229,  1600] loss: 2.303487\n",
      "[229,  1800] loss: 2.303946\n",
      "[229,  2000] loss: 2.303565\n",
      "[229,  2200] loss: 2.303917\n",
      "[229,  2400] loss: 2.303676\n",
      "[229,  2600] loss: 2.303758\n",
      "[229,  2800] loss: 2.303598\n",
      "[229,  3000] loss: 2.303577\n",
      "[229,  3200] loss: 2.303370\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[230,   200] loss: 2.303214\n",
      "[230,   400] loss: 2.303362\n",
      "[230,   600] loss: 2.303644\n",
      "[230,   800] loss: 2.303691\n",
      "[230,  1000] loss: 2.303496\n",
      "[230,  1200] loss: 2.303521\n",
      "[230,  1400] loss: 2.303299\n",
      "[230,  1600] loss: 2.303460\n",
      "[230,  1800] loss: 2.303415\n",
      "[230,  2000] loss: 2.303462\n",
      "[230,  2200] loss: 2.303318\n",
      "[230,  2400] loss: 2.303408\n",
      "[230,  2600] loss: 2.303599\n",
      "[230,  2800] loss: 2.303032\n",
      "[230,  3000] loss: 2.303681\n",
      "[230,  3200] loss: 2.303497\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[231,   200] loss: 2.303778\n",
      "[231,   400] loss: 2.303522\n",
      "[231,   600] loss: 2.303590\n",
      "[231,   800] loss: 2.303424\n",
      "[231,  1000] loss: 2.303222\n",
      "[231,  1200] loss: 2.303189\n",
      "[231,  1400] loss: 2.303541\n",
      "[231,  1600] loss: 2.303442\n",
      "[231,  1800] loss: 2.303742\n",
      "[231,  2000] loss: 2.303109\n",
      "[231,  2200] loss: 2.303815\n",
      "[231,  2400] loss: 2.302904\n",
      "[231,  2600] loss: 2.303505\n",
      "[231,  2800] loss: 2.303301\n",
      "[231,  3000] loss: 2.303811\n",
      "[231,  3200] loss: 2.303234\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[232,   200] loss: 2.303219\n",
      "[232,   400] loss: 2.303430\n",
      "[232,   600] loss: 2.303299\n",
      "[232,   800] loss: 2.303270\n",
      "[232,  1000] loss: 2.303323\n",
      "[232,  1200] loss: 2.303595\n",
      "[232,  1400] loss: 2.303176\n",
      "[232,  1600] loss: 2.303634\n",
      "[232,  1800] loss: 2.303697\n",
      "[232,  2000] loss: 2.303474\n",
      "[232,  2200] loss: 2.303779\n",
      "[232,  2400] loss: 2.303273\n",
      "[232,  2600] loss: 2.303602\n",
      "[232,  2800] loss: 2.303451\n",
      "[232,  3000] loss: 2.303567\n",
      "[232,  3200] loss: 2.303698\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[233,   200] loss: 2.303707\n",
      "[233,   400] loss: 2.303691\n",
      "[233,   600] loss: 2.302697\n",
      "[233,   800] loss: 2.303720\n",
      "[233,  1000] loss: 2.303555\n",
      "[233,  1200] loss: 2.303415\n",
      "[233,  1400] loss: 2.303427\n",
      "[233,  1600] loss: 2.303376\n",
      "[233,  1800] loss: 2.303750\n",
      "[233,  2000] loss: 2.303213\n",
      "[233,  2200] loss: 2.303646\n",
      "[233,  2400] loss: 2.303580\n",
      "[233,  2600] loss: 2.303276\n",
      "[233,  2800] loss: 2.303437\n",
      "[233,  3000] loss: 2.303747\n",
      "[233,  3200] loss: 2.303315\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[234,   200] loss: 2.303663\n",
      "[234,   400] loss: 2.303183\n",
      "[234,   600] loss: 2.303840\n",
      "[234,   800] loss: 2.303723\n",
      "[234,  1000] loss: 2.303662\n",
      "[234,  1200] loss: 2.303334\n",
      "[234,  1400] loss: 2.303778\n",
      "[234,  1600] loss: 2.303494\n",
      "[234,  1800] loss: 2.302580\n",
      "[234,  2000] loss: 2.303878\n",
      "[234,  2200] loss: 2.303368\n",
      "[234,  2400] loss: 2.303522\n",
      "[234,  2600] loss: 2.303384\n",
      "[234,  2800] loss: 2.303445\n",
      "[234,  3000] loss: 2.303435\n",
      "[234,  3200] loss: 2.303739\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[235,   200] loss: 2.303322\n",
      "[235,   400] loss: 2.303487\n",
      "[235,   600] loss: 2.303541\n",
      "[235,   800] loss: 2.303846\n",
      "[235,  1000] loss: 2.303325\n",
      "[235,  1200] loss: 2.303691\n",
      "[235,  1400] loss: 2.303664\n",
      "[235,  1600] loss: 2.303482\n",
      "[235,  1800] loss: 2.303396\n",
      "[235,  2000] loss: 2.303726\n",
      "[235,  2200] loss: 2.303183\n",
      "[235,  2400] loss: 2.303724\n",
      "[235,  2600] loss: 2.303286\n",
      "[235,  2800] loss: 2.303863\n",
      "[235,  3000] loss: 2.303430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235,  3200] loss: 2.303665\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[236,   200] loss: 2.303610\n",
      "[236,   400] loss: 2.303677\n",
      "[236,   600] loss: 2.303553\n",
      "[236,   800] loss: 2.303294\n",
      "[236,  1000] loss: 2.303540\n",
      "[236,  1200] loss: 2.303766\n",
      "[236,  1400] loss: 2.303876\n",
      "[236,  1600] loss: 2.303406\n",
      "[236,  1800] loss: 2.303993\n",
      "[236,  2000] loss: 2.303169\n",
      "[236,  2200] loss: 2.303304\n",
      "[236,  2400] loss: 2.303048\n",
      "[236,  2600] loss: 2.303580\n",
      "[236,  2800] loss: 2.303354\n",
      "[236,  3000] loss: 2.303561\n",
      "[236,  3200] loss: 2.303526\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[237,   200] loss: 2.303739\n",
      "[237,   400] loss: 2.303442\n",
      "[237,   600] loss: 2.303404\n",
      "[237,   800] loss: 2.303483\n",
      "[237,  1000] loss: 2.303492\n",
      "[237,  1200] loss: 2.303853\n",
      "[237,  1400] loss: 2.303709\n",
      "[237,  1600] loss: 2.303859\n",
      "[237,  1800] loss: 2.303361\n",
      "[237,  2000] loss: 2.303384\n",
      "[237,  2200] loss: 2.303549\n",
      "[237,  2400] loss: 2.303555\n",
      "[237,  2600] loss: 2.303782\n",
      "[237,  2800] loss: 2.303485\n",
      "[237,  3000] loss: 2.303651\n",
      "[237,  3200] loss: 2.303481\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[238,   200] loss: 2.303492\n",
      "[238,   400] loss: 2.303970\n",
      "[238,   600] loss: 2.303618\n",
      "[238,   800] loss: 2.303392\n",
      "[238,  1000] loss: 2.303264\n",
      "[238,  1200] loss: 2.303645\n",
      "[238,  1400] loss: 2.303406\n",
      "[238,  1600] loss: 2.303527\n",
      "[238,  1800] loss: 2.303742\n",
      "[238,  2000] loss: 2.303386\n",
      "[238,  2200] loss: 2.303359\n",
      "[238,  2400] loss: 2.303295\n",
      "[238,  2600] loss: 2.303725\n",
      "[238,  2800] loss: 2.303433\n",
      "[238,  3000] loss: 2.303639\n",
      "[238,  3200] loss: 2.303153\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[239,   200] loss: 2.303561\n",
      "[239,   400] loss: 2.303759\n",
      "[239,   600] loss: 2.303397\n",
      "[239,   800] loss: 2.303316\n",
      "[239,  1000] loss: 2.303612\n",
      "[239,  1200] loss: 2.303855\n",
      "[239,  1400] loss: 2.303221\n",
      "[239,  1600] loss: 2.303985\n",
      "[239,  1800] loss: 2.303163\n",
      "[239,  2000] loss: 2.303307\n",
      "[239,  2200] loss: 2.303486\n",
      "[239,  2400] loss: 2.303729\n",
      "[239,  2600] loss: 2.303760\n",
      "[239,  2800] loss: 2.303661\n",
      "[239,  3000] loss: 2.302857\n",
      "[239,  3200] loss: 2.303442\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[240,   200] loss: 2.303184\n",
      "[240,   400] loss: 2.303150\n",
      "[240,   600] loss: 2.303123\n",
      "[240,   800] loss: 2.303078\n",
      "[240,  1000] loss: 2.303348\n",
      "[240,  1200] loss: 2.303019\n",
      "[240,  1400] loss: 2.303615\n",
      "[240,  1600] loss: 2.303475\n",
      "[240,  1800] loss: 2.303968\n",
      "[240,  2000] loss: 2.303355\n",
      "[240,  2200] loss: 2.303161\n",
      "[240,  2400] loss: 2.303406\n",
      "[240,  2600] loss: 2.303063\n",
      "[240,  2800] loss: 2.303258\n",
      "[240,  3000] loss: 2.302974\n",
      "[240,  3200] loss: 2.303065\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[241,   200] loss: 2.303546\n",
      "[241,   400] loss: 2.303405\n",
      "[241,   600] loss: 2.303516\n",
      "[241,   800] loss: 2.303706\n",
      "[241,  1000] loss: 2.303243\n",
      "[241,  1200] loss: 2.303479\n",
      "[241,  1400] loss: 2.303618\n",
      "[241,  1600] loss: 2.303629\n",
      "[241,  1800] loss: 2.303313\n",
      "[241,  2000] loss: 2.303443\n",
      "[241,  2200] loss: 2.303939\n",
      "[241,  2400] loss: 2.303421\n",
      "[241,  2600] loss: 2.303536\n",
      "[241,  2800] loss: 2.303688\n",
      "[241,  3000] loss: 2.303223\n",
      "[241,  3200] loss: 2.303537\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[242,   200] loss: 2.303617\n",
      "[242,   400] loss: 2.303802\n",
      "[242,   600] loss: 2.304095\n",
      "[242,   800] loss: 2.303525\n",
      "[242,  1000] loss: 2.303638\n",
      "[242,  1200] loss: 2.303549\n",
      "[242,  1400] loss: 2.303372\n",
      "[242,  1600] loss: 2.303672\n",
      "[242,  1800] loss: 2.303808\n",
      "[242,  2000] loss: 2.303875\n",
      "[242,  2200] loss: 2.303695\n",
      "[242,  2400] loss: 2.302954\n",
      "[242,  2600] loss: 2.303181\n",
      "[242,  2800] loss: 2.303532\n",
      "[242,  3000] loss: 2.303682\n",
      "[242,  3200] loss: 2.303399\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[243,   200] loss: 2.303377\n",
      "[243,   400] loss: 2.303490\n",
      "[243,   600] loss: 2.303293\n",
      "[243,   800] loss: 2.303777\n",
      "[243,  1000] loss: 2.303335\n",
      "[243,  1200] loss: 2.303769\n",
      "[243,  1400] loss: 2.303828\n",
      "[243,  1600] loss: 2.303280\n",
      "[243,  1800] loss: 2.303563\n",
      "[243,  2000] loss: 2.303349\n",
      "[243,  2200] loss: 2.303296\n",
      "[243,  2400] loss: 2.303615\n",
      "[243,  2600] loss: 2.303748\n",
      "[243,  2800] loss: 2.303381\n",
      "[243,  3000] loss: 2.303595\n",
      "[243,  3200] loss: 2.303629\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[244,   200] loss: 2.303670\n",
      "[244,   400] loss: 2.303825\n",
      "[244,   600] loss: 2.303064\n",
      "[244,   800] loss: 2.303796\n",
      "[244,  1000] loss: 2.303787\n",
      "[244,  1200] loss: 2.303912\n",
      "[244,  1400] loss: 2.303477\n",
      "[244,  1600] loss: 2.303996\n",
      "[244,  1800] loss: 2.303066\n",
      "[244,  2000] loss: 2.303609\n",
      "[244,  2200] loss: 2.303494\n",
      "[244,  2400] loss: 2.303549\n",
      "[244,  2600] loss: 2.303045\n",
      "[244,  2800] loss: 2.303577\n",
      "[244,  3000] loss: 2.303868\n",
      "[244,  3200] loss: 2.303256\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[245,   200] loss: 2.303441\n",
      "[245,   400] loss: 2.303432\n",
      "[245,   600] loss: 2.303748\n",
      "[245,   800] loss: 2.303459\n",
      "[245,  1000] loss: 2.303586\n",
      "[245,  1200] loss: 2.303683\n",
      "[245,  1400] loss: 2.303505\n",
      "[245,  1600] loss: 2.303589\n",
      "[245,  1800] loss: 2.303697\n",
      "[245,  2000] loss: 2.303446\n",
      "[245,  2200] loss: 2.303307\n",
      "[245,  2400] loss: 2.303240\n",
      "[245,  2600] loss: 2.303397\n",
      "[245,  2800] loss: 2.303195\n",
      "[245,  3000] loss: 2.303318\n",
      "[245,  3200] loss: 2.303174\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[246,   200] loss: 2.303098\n",
      "[246,   400] loss: 2.303777\n",
      "[246,   600] loss: 2.303580\n",
      "[246,   800] loss: 2.303810\n",
      "[246,  1000] loss: 2.303660\n",
      "[246,  1200] loss: 2.303568\n",
      "[246,  1400] loss: 2.303866\n",
      "[246,  1600] loss: 2.303653\n",
      "[246,  1800] loss: 2.303375\n",
      "[246,  2000] loss: 2.303453\n",
      "[246,  2200] loss: 2.304097\n",
      "[246,  2400] loss: 2.303551\n",
      "[246,  2600] loss: 2.303294\n",
      "[246,  2800] loss: 2.303664\n",
      "[246,  3000] loss: 2.303466\n",
      "[246,  3200] loss: 2.303426\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[247,   200] loss: 2.303343\n",
      "[247,   400] loss: 2.303645\n",
      "[247,   600] loss: 2.303393\n",
      "[247,   800] loss: 2.303393\n",
      "[247,  1000] loss: 2.303846\n",
      "[247,  1200] loss: 2.303117\n",
      "[247,  1400] loss: 2.303536\n",
      "[247,  1600] loss: 2.303388\n",
      "[247,  1800] loss: 2.303473\n",
      "[247,  2000] loss: 2.303430\n",
      "[247,  2200] loss: 2.303299\n",
      "[247,  2400] loss: 2.303086\n",
      "[247,  2600] loss: 2.303790\n",
      "[247,  2800] loss: 2.303099\n",
      "[247,  3000] loss: 2.304095\n",
      "[247,  3200] loss: 2.303586\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[248,   200] loss: 2.303481\n",
      "[248,   400] loss: 2.303256\n",
      "[248,   600] loss: 2.303408\n",
      "[248,   800] loss: 2.303441\n",
      "[248,  1000] loss: 2.303613\n",
      "[248,  1200] loss: 2.303845\n",
      "[248,  1400] loss: 2.303769\n",
      "[248,  1600] loss: 2.303748\n",
      "[248,  1800] loss: 2.302913\n",
      "[248,  2000] loss: 2.303568\n",
      "[248,  2200] loss: 2.303859\n",
      "[248,  2400] loss: 2.303261\n",
      "[248,  2600] loss: 2.303481\n",
      "[248,  2800] loss: 2.303726\n",
      "[248,  3000] loss: 2.303723\n",
      "[248,  3200] loss: 2.303894\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[249,   200] loss: 2.303993\n",
      "[249,   400] loss: 2.303581\n",
      "[249,   600] loss: 2.303333\n",
      "[249,   800] loss: 2.303346\n",
      "[249,  1000] loss: 2.303631\n",
      "[249,  1200] loss: 2.303478\n",
      "[249,  1400] loss: 2.304078\n",
      "[249,  1600] loss: 2.303069\n",
      "[249,  1800] loss: 2.303892\n",
      "[249,  2000] loss: 2.303551\n",
      "[249,  2200] loss: 2.303345\n",
      "[249,  2400] loss: 2.302968\n",
      "[249,  2600] loss: 2.303820\n",
      "[249,  2800] loss: 2.303736\n",
      "[249,  3000] loss: 2.303409\n",
      "[249,  3200] loss: 2.303625\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[250,   200] loss: 2.303485\n",
      "[250,   400] loss: 2.303343\n",
      "[250,   600] loss: 2.303195\n",
      "[250,   800] loss: 2.303619\n",
      "[250,  1000] loss: 2.303661\n",
      "[250,  1200] loss: 2.303455\n",
      "[250,  1400] loss: 2.303792\n",
      "[250,  1600] loss: 2.303592\n",
      "[250,  1800] loss: 2.303296\n",
      "[250,  2000] loss: 2.303428\n",
      "[250,  2200] loss: 2.303442\n",
      "[250,  2400] loss: 2.303728\n",
      "[250,  2600] loss: 2.303493\n",
      "[250,  2800] loss: 2.302713\n",
      "[250,  3000] loss: 2.303827\n",
      "[250,  3200] loss: 2.303491\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[251,   200] loss: 2.302874\n",
      "[251,   400] loss: 2.302445\n",
      "[251,   600] loss: 2.302520\n",
      "[251,   800] loss: 2.302769\n",
      "[251,  1000] loss: 2.302676\n",
      "[251,  1200] loss: 2.302534\n",
      "[251,  1400] loss: 2.302850\n",
      "[251,  1600] loss: 2.302732\n",
      "[251,  1800] loss: 2.302721\n",
      "[251,  2000] loss: 2.302630\n",
      "[251,  2200] loss: 2.302703\n",
      "[251,  2400] loss: 2.302543\n",
      "[251,  2600] loss: 2.302671\n",
      "[251,  2800] loss: 2.302515\n",
      "[251,  3000] loss: 2.302686\n",
      "[251,  3200] loss: 2.302767\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[252,   200] loss: 2.302515\n",
      "[252,   400] loss: 2.302395\n",
      "[252,   600] loss: 2.302730\n",
      "[252,   800] loss: 2.302850\n",
      "[252,  1000] loss: 2.302638\n",
      "[252,  1200] loss: 2.302686\n",
      "[252,  1400] loss: 2.302649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[252,  1600] loss: 2.302558\n",
      "[252,  1800] loss: 2.302673\n",
      "[252,  2000] loss: 2.302635\n",
      "[252,  2200] loss: 2.302582\n",
      "[252,  2400] loss: 2.302517\n",
      "[252,  2600] loss: 2.302699\n",
      "[252,  2800] loss: 2.302650\n",
      "[252,  3000] loss: 2.302756\n",
      "[252,  3200] loss: 2.302750\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[253,   200] loss: 2.302645\n",
      "[253,   400] loss: 2.302658\n",
      "[253,   600] loss: 2.302506\n",
      "[253,   800] loss: 2.302707\n",
      "[253,  1000] loss: 2.302427\n",
      "[253,  1200] loss: 2.302838\n",
      "[253,  1400] loss: 2.302528\n",
      "[253,  1600] loss: 2.302330\n",
      "[253,  1800] loss: 2.302607\n",
      "[253,  2000] loss: 2.302722\n",
      "[253,  2200] loss: 2.302550\n",
      "[253,  2400] loss: 2.302869\n",
      "[253,  2600] loss: 2.302738\n",
      "[253,  2800] loss: 2.302743\n",
      "[253,  3000] loss: 2.302593\n",
      "[253,  3200] loss: 2.302587\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[254,   200] loss: 2.302704\n",
      "[254,   400] loss: 2.302669\n",
      "[254,   600] loss: 2.302800\n",
      "[254,   800] loss: 2.302791\n",
      "[254,  1000] loss: 2.302450\n",
      "[254,  1200] loss: 2.302766\n",
      "[254,  1400] loss: 2.302525\n",
      "[254,  1600] loss: 2.302653\n",
      "[254,  1800] loss: 2.302740\n",
      "[254,  2000] loss: 2.302784\n",
      "[254,  2200] loss: 2.302685\n",
      "[254,  2400] loss: 2.302675\n",
      "[254,  2600] loss: 2.302489\n",
      "[254,  2800] loss: 2.302647\n",
      "[254,  3000] loss: 2.302347\n",
      "[254,  3200] loss: 2.302433\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[255,   200] loss: 2.302604\n",
      "[255,   400] loss: 2.302648\n",
      "[255,   600] loss: 2.302753\n",
      "[255,   800] loss: 2.302657\n",
      "[255,  1000] loss: 2.302645\n",
      "[255,  1200] loss: 2.302733\n",
      "[255,  1400] loss: 2.302755\n",
      "[255,  1600] loss: 2.302492\n",
      "[255,  1800] loss: 2.302488\n",
      "[255,  2000] loss: 2.302500\n",
      "[255,  2200] loss: 2.302596\n",
      "[255,  2400] loss: 2.302807\n",
      "[255,  2600] loss: 2.302579\n",
      "[255,  2800] loss: 2.302769\n",
      "[255,  3000] loss: 2.302672\n",
      "[255,  3200] loss: 2.302478\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[256,   200] loss: 2.302650\n",
      "[256,   400] loss: 2.302728\n",
      "[256,   600] loss: 2.302385\n",
      "[256,   800] loss: 2.302919\n",
      "[256,  1000] loss: 2.302674\n",
      "[256,  1200] loss: 2.302671\n",
      "[256,  1400] loss: 2.302527\n",
      "[256,  1600] loss: 2.302612\n",
      "[256,  1800] loss: 2.302595\n",
      "[256,  2000] loss: 2.302883\n",
      "[256,  2200] loss: 2.302707\n",
      "[256,  2400] loss: 2.302592\n",
      "[256,  2600] loss: 2.302768\n",
      "[256,  2800] loss: 2.302639\n",
      "[256,  3000] loss: 2.302340\n",
      "[256,  3200] loss: 2.302352\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[257,   200] loss: 2.302787\n",
      "[257,   400] loss: 2.302634\n",
      "[257,   600] loss: 2.302530\n",
      "[257,   800] loss: 2.302676\n",
      "[257,  1000] loss: 2.302576\n",
      "[257,  1200] loss: 2.302671\n",
      "[257,  1400] loss: 2.302537\n",
      "[257,  1600] loss: 2.302838\n",
      "[257,  1800] loss: 2.302550\n",
      "[257,  2000] loss: 2.302493\n",
      "[257,  2200] loss: 2.302876\n",
      "[257,  2400] loss: 2.302624\n",
      "[257,  2600] loss: 2.302602\n",
      "[257,  2800] loss: 2.302669\n",
      "[257,  3000] loss: 2.302474\n",
      "[257,  3200] loss: 2.302806\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[258,   200] loss: 2.302629\n",
      "[258,   400] loss: 2.302488\n",
      "[258,   600] loss: 2.302577\n",
      "[258,   800] loss: 2.302715\n",
      "[258,  1000] loss: 2.302776\n",
      "[258,  1200] loss: 2.302679\n",
      "[258,  1400] loss: 2.302559\n",
      "[258,  1600] loss: 2.302493\n",
      "[258,  1800] loss: 2.302800\n",
      "[258,  2000] loss: 2.302662\n",
      "[258,  2200] loss: 2.302763\n",
      "[258,  2400] loss: 2.302616\n",
      "[258,  2600] loss: 2.302802\n",
      "[258,  2800] loss: 2.302730\n",
      "[258,  3000] loss: 2.302688\n",
      "[258,  3200] loss: 2.302543\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[259,   200] loss: 2.302791\n",
      "[259,   400] loss: 2.302786\n",
      "[259,   600] loss: 2.302606\n",
      "[259,   800] loss: 2.302375\n",
      "[259,  1000] loss: 2.302591\n",
      "[259,  1200] loss: 2.302603\n",
      "[259,  1400] loss: 2.302697\n",
      "[259,  1600] loss: 2.302672\n",
      "[259,  1800] loss: 2.302675\n",
      "[259,  2000] loss: 2.302838\n",
      "[259,  2200] loss: 2.302471\n",
      "[259,  2400] loss: 2.302621\n",
      "[259,  2600] loss: 2.302501\n",
      "[259,  2800] loss: 2.302720\n",
      "[259,  3000] loss: 2.302599\n",
      "[259,  3200] loss: 2.302475\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[260,   200] loss: 2.302703\n",
      "[260,   400] loss: 2.302752\n",
      "[260,   600] loss: 2.302727\n",
      "[260,   800] loss: 2.302560\n",
      "[260,  1000] loss: 2.302646\n",
      "[260,  1200] loss: 2.302725\n",
      "[260,  1400] loss: 2.302739\n",
      "[260,  1600] loss: 2.302806\n",
      "[260,  1800] loss: 2.302681\n",
      "[260,  2000] loss: 2.302661\n",
      "[260,  2200] loss: 2.302610\n",
      "[260,  2400] loss: 2.302591\n",
      "[260,  2600] loss: 2.302511\n",
      "[260,  2800] loss: 2.302715\n",
      "[260,  3000] loss: 2.302446\n",
      "[260,  3200] loss: 2.302452\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[261,   200] loss: 2.302526\n",
      "[261,   400] loss: 2.302511\n",
      "[261,   600] loss: 2.302753\n",
      "[261,   800] loss: 2.302218\n",
      "[261,  1000] loss: 2.302678\n",
      "[261,  1200] loss: 2.302877\n",
      "[261,  1400] loss: 2.302523\n",
      "[261,  1600] loss: 2.302837\n",
      "[261,  1800] loss: 2.302652\n",
      "[261,  2000] loss: 2.302667\n",
      "[261,  2200] loss: 2.302683\n",
      "[261,  2400] loss: 2.302687\n",
      "[261,  2600] loss: 2.302627\n",
      "[261,  2800] loss: 2.302526\n",
      "[261,  3000] loss: 2.302675\n",
      "[261,  3200] loss: 2.302705\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[262,   200] loss: 2.302445\n",
      "[262,   400] loss: 2.302882\n",
      "[262,   600] loss: 2.302416\n",
      "[262,   800] loss: 2.302536\n",
      "[262,  1000] loss: 2.302779\n",
      "[262,  1200] loss: 2.302788\n",
      "[262,  1400] loss: 2.302730\n",
      "[262,  1600] loss: 2.302696\n",
      "[262,  1800] loss: 2.302631\n",
      "[262,  2000] loss: 2.302671\n",
      "[262,  2200] loss: 2.302569\n",
      "[262,  2400] loss: 2.302755\n",
      "[262,  2600] loss: 2.302590\n",
      "[262,  2800] loss: 2.302524\n",
      "[262,  3000] loss: 2.302714\n",
      "[262,  3200] loss: 2.302583\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[263,   200] loss: 2.302581\n",
      "[263,   400] loss: 2.302619\n",
      "[263,   600] loss: 2.302474\n",
      "[263,   800] loss: 2.302553\n",
      "[263,  1000] loss: 2.302830\n",
      "[263,  1200] loss: 2.302822\n",
      "[263,  1400] loss: 2.302654\n",
      "[263,  1600] loss: 2.302359\n",
      "[263,  1800] loss: 2.302554\n",
      "[263,  2000] loss: 2.302819\n",
      "[263,  2200] loss: 2.302694\n",
      "[263,  2400] loss: 2.302663\n",
      "[263,  2600] loss: 2.302720\n",
      "[263,  2800] loss: 2.302632\n",
      "[263,  3000] loss: 2.302626\n",
      "[263,  3200] loss: 2.302642\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[264,   200] loss: 2.302664\n",
      "[264,   400] loss: 2.302734\n",
      "[264,   600] loss: 2.302808\n",
      "[264,   800] loss: 2.302697\n",
      "[264,  1000] loss: 2.302620\n",
      "[264,  1200] loss: 2.302732\n",
      "[264,  1400] loss: 2.302805\n",
      "[264,  1600] loss: 2.302472\n",
      "[264,  1800] loss: 2.302434\n",
      "[264,  2000] loss: 2.302443\n",
      "[264,  2200] loss: 2.303068\n",
      "[264,  2400] loss: 2.302285\n",
      "[264,  2600] loss: 2.302600\n",
      "[264,  2800] loss: 2.302792\n",
      "[264,  3000] loss: 2.302608\n",
      "[264,  3200] loss: 2.302716\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[265,   200] loss: 2.302431\n",
      "[265,   400] loss: 2.302661\n",
      "[265,   600] loss: 2.302877\n",
      "[265,   800] loss: 2.302636\n",
      "[265,  1000] loss: 2.302700\n",
      "[265,  1200] loss: 2.302690\n",
      "[265,  1400] loss: 2.302789\n",
      "[265,  1600] loss: 2.302620\n",
      "[265,  1800] loss: 2.302684\n",
      "[265,  2000] loss: 2.302519\n",
      "[265,  2200] loss: 2.302695\n",
      "[265,  2400] loss: 2.302672\n",
      "[265,  2600] loss: 2.302464\n",
      "[265,  2800] loss: 2.302573\n",
      "[265,  3000] loss: 2.302734\n",
      "[265,  3200] loss: 2.302320\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[266,   200] loss: 2.302570\n",
      "[266,   400] loss: 2.302498\n",
      "[266,   600] loss: 2.302849\n",
      "[266,   800] loss: 2.302683\n",
      "[266,  1000] loss: 2.302673\n",
      "[266,  1200] loss: 2.302526\n",
      "[266,  1400] loss: 2.302615\n",
      "[266,  1600] loss: 2.302685\n",
      "[266,  1800] loss: 2.302731\n",
      "[266,  2000] loss: 2.302620\n",
      "[266,  2200] loss: 2.302678\n",
      "[266,  2400] loss: 2.302670\n",
      "[266,  2600] loss: 2.302696\n",
      "[266,  2800] loss: 2.302691\n",
      "[266,  3000] loss: 2.302605\n",
      "[266,  3200] loss: 2.302556\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[267,   200] loss: 2.302563\n",
      "[267,   400] loss: 2.302882\n",
      "[267,   600] loss: 2.302730\n",
      "[267,   800] loss: 2.302571\n",
      "[267,  1000] loss: 2.302799\n",
      "[267,  1200] loss: 2.302410\n",
      "[267,  1400] loss: 2.302659\n",
      "[267,  1600] loss: 2.302253\n",
      "[267,  1800] loss: 2.302756\n",
      "[267,  2000] loss: 2.302708\n",
      "[267,  2200] loss: 2.302818\n",
      "[267,  2400] loss: 2.302530\n",
      "[267,  2600] loss: 2.302648\n",
      "[267,  2800] loss: 2.302686\n",
      "[267,  3000] loss: 2.302597\n",
      "[267,  3200] loss: 2.302758\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[268,   200] loss: 2.302400\n",
      "[268,   400] loss: 2.302772\n",
      "[268,   600] loss: 2.302708\n",
      "[268,   800] loss: 2.302618\n",
      "[268,  1000] loss: 2.302588\n",
      "[268,  1200] loss: 2.302788\n",
      "[268,  1400] loss: 2.302674\n",
      "[268,  1600] loss: 2.302591\n",
      "[268,  1800] loss: 2.302702\n",
      "[268,  2000] loss: 2.302424\n",
      "[268,  2200] loss: 2.302804\n",
      "[268,  2400] loss: 2.302845\n",
      "[268,  2600] loss: 2.302659\n",
      "[268,  2800] loss: 2.302584\n",
      "[268,  3000] loss: 2.302505\n",
      "[268,  3200] loss: 2.302674\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269,   200] loss: 2.302710\n",
      "[269,   400] loss: 2.302813\n",
      "[269,   600] loss: 2.302677\n",
      "[269,   800] loss: 2.302589\n",
      "[269,  1000] loss: 2.302636\n",
      "[269,  1200] loss: 2.302648\n",
      "[269,  1400] loss: 2.302456\n",
      "[269,  1600] loss: 2.302586\n",
      "[269,  1800] loss: 2.302444\n",
      "[269,  2000] loss: 2.302809\n",
      "[269,  2200] loss: 2.302826\n",
      "[269,  2400] loss: 2.302694\n",
      "[269,  2600] loss: 2.302373\n",
      "[269,  2800] loss: 2.302838\n",
      "[269,  3000] loss: 2.302638\n",
      "[269,  3200] loss: 2.302712\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[270,   200] loss: 2.302776\n",
      "[270,   400] loss: 2.302769\n",
      "[270,   600] loss: 2.302506\n",
      "[270,   800] loss: 2.302519\n",
      "[270,  1000] loss: 2.302699\n",
      "[270,  1200] loss: 2.302493\n",
      "[270,  1400] loss: 2.302596\n",
      "[270,  1600] loss: 2.302741\n",
      "[270,  1800] loss: 2.302773\n",
      "[270,  2000] loss: 2.302500\n",
      "[270,  2200] loss: 2.302842\n",
      "[270,  2400] loss: 2.302692\n",
      "[270,  2600] loss: 2.302507\n",
      "[270,  2800] loss: 2.302620\n",
      "[270,  3000] loss: 2.302725\n",
      "[270,  3200] loss: 2.302780\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[271,   200] loss: 2.302678\n",
      "[271,   400] loss: 2.302518\n",
      "[271,   600] loss: 2.302534\n",
      "[271,   800] loss: 2.302621\n",
      "[271,  1000] loss: 2.302513\n",
      "[271,  1200] loss: 2.302839\n",
      "[271,  1400] loss: 2.302626\n",
      "[271,  1600] loss: 2.302540\n",
      "[271,  1800] loss: 2.302762\n",
      "[271,  2000] loss: 2.302668\n",
      "[271,  2200] loss: 2.302447\n",
      "[271,  2400] loss: 2.302910\n",
      "[271,  2600] loss: 2.302710\n",
      "[271,  2800] loss: 2.302645\n",
      "[271,  3000] loss: 2.302813\n",
      "[271,  3200] loss: 2.302698\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[272,   200] loss: 2.302740\n",
      "[272,   400] loss: 2.302635\n",
      "[272,   600] loss: 2.302629\n",
      "[272,   800] loss: 2.302812\n",
      "[272,  1000] loss: 2.302609\n",
      "[272,  1200] loss: 2.302637\n",
      "[272,  1400] loss: 2.302746\n",
      "[272,  1600] loss: 2.302681\n",
      "[272,  1800] loss: 2.302285\n",
      "[272,  2000] loss: 2.302625\n",
      "[272,  2200] loss: 2.302828\n",
      "[272,  2400] loss: 2.302737\n",
      "[272,  2600] loss: 2.302655\n",
      "[272,  2800] loss: 2.302485\n",
      "[272,  3000] loss: 2.302648\n",
      "[272,  3200] loss: 2.302624\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[273,   200] loss: 2.302662\n",
      "[273,   400] loss: 2.302637\n",
      "[273,   600] loss: 2.302621\n",
      "[273,   800] loss: 2.302663\n",
      "[273,  1000] loss: 2.302668\n",
      "[273,  1200] loss: 2.302608\n",
      "[273,  1400] loss: 2.302846\n",
      "[273,  1600] loss: 2.302743\n",
      "[273,  1800] loss: 2.302677\n",
      "[273,  2000] loss: 2.302605\n",
      "[273,  2200] loss: 2.302440\n",
      "[273,  2400] loss: 2.302671\n",
      "[273,  2600] loss: 2.302755\n",
      "[273,  2800] loss: 2.302310\n",
      "[273,  3000] loss: 2.302757\n",
      "[273,  3200] loss: 2.302656\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[274,   200] loss: 2.302690\n",
      "[274,   400] loss: 2.302737\n",
      "[274,   600] loss: 2.302616\n",
      "[274,   800] loss: 2.302647\n",
      "[274,  1000] loss: 2.302620\n",
      "[274,  1200] loss: 2.302591\n",
      "[274,  1400] loss: 2.302828\n",
      "[274,  1600] loss: 2.302505\n",
      "[274,  1800] loss: 2.302691\n",
      "[274,  2000] loss: 2.302766\n",
      "[274,  2200] loss: 2.302768\n",
      "[274,  2400] loss: 2.302611\n",
      "[274,  2600] loss: 2.302379\n",
      "[274,  2800] loss: 2.302808\n",
      "[274,  3000] loss: 2.302583\n",
      "[274,  3200] loss: 2.302751\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[275,   200] loss: 2.302339\n",
      "[275,   400] loss: 2.302463\n",
      "[275,   600] loss: 2.302685\n",
      "[275,   800] loss: 2.302969\n",
      "[275,  1000] loss: 2.302598\n",
      "[275,  1200] loss: 2.302452\n",
      "[275,  1400] loss: 2.302653\n",
      "[275,  1600] loss: 2.302470\n",
      "[275,  1800] loss: 2.302624\n",
      "[275,  2000] loss: 2.302944\n",
      "[275,  2200] loss: 2.302796\n",
      "[275,  2400] loss: 2.302569\n",
      "[275,  2600] loss: 2.302520\n",
      "[275,  2800] loss: 2.302763\n",
      "[275,  3000] loss: 2.302711\n",
      "[275,  3200] loss: 2.302615\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[276,   200] loss: 2.302387\n",
      "[276,   400] loss: 2.302870\n",
      "[276,   600] loss: 2.302695\n",
      "[276,   800] loss: 2.302691\n",
      "[276,  1000] loss: 2.302541\n",
      "[276,  1200] loss: 2.302690\n",
      "[276,  1400] loss: 2.302532\n",
      "[276,  1600] loss: 2.302679\n",
      "[276,  1800] loss: 2.302743\n",
      "[276,  2000] loss: 2.302693\n",
      "[276,  2200] loss: 2.302525\n",
      "[276,  2400] loss: 2.302401\n",
      "[276,  2600] loss: 2.302504\n",
      "[276,  2800] loss: 2.302806\n",
      "[276,  3000] loss: 2.302767\n",
      "[276,  3200] loss: 2.302520\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[277,   200] loss: 2.302688\n",
      "[277,   400] loss: 2.302610\n",
      "[277,   600] loss: 2.302668\n",
      "[277,   800] loss: 2.302689\n",
      "[277,  1000] loss: 2.302552\n",
      "[277,  1200] loss: 2.302468\n",
      "[277,  1400] loss: 2.302808\n",
      "[277,  1600] loss: 2.302698\n",
      "[277,  1800] loss: 2.302539\n",
      "[277,  2000] loss: 2.302727\n",
      "[277,  2200] loss: 2.302508\n",
      "[277,  2400] loss: 2.302687\n",
      "[277,  2600] loss: 2.302454\n",
      "[277,  2800] loss: 2.302692\n",
      "[277,  3000] loss: 2.302723\n",
      "[277,  3200] loss: 2.302577\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[278,   200] loss: 2.302685\n",
      "[278,   400] loss: 2.302622\n",
      "[278,   600] loss: 2.302597\n",
      "[278,   800] loss: 2.302600\n",
      "[278,  1000] loss: 2.302621\n",
      "[278,  1200] loss: 2.302684\n",
      "[278,  1400] loss: 2.302436\n",
      "[278,  1600] loss: 2.302671\n",
      "[278,  1800] loss: 2.302678\n",
      "[278,  2000] loss: 2.302664\n",
      "[278,  2200] loss: 2.302685\n",
      "[278,  2400] loss: 2.302594\n",
      "[278,  2600] loss: 2.302579\n",
      "[278,  2800] loss: 2.302809\n",
      "[278,  3000] loss: 2.302694\n",
      "[278,  3200] loss: 2.302750\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[279,   200] loss: 2.302590\n",
      "[279,   400] loss: 2.302656\n",
      "[279,   600] loss: 2.302626\n",
      "[279,   800] loss: 2.302607\n",
      "[279,  1000] loss: 2.302456\n",
      "[279,  1200] loss: 2.302945\n",
      "[279,  1400] loss: 2.302614\n",
      "[279,  1600] loss: 2.302617\n",
      "[279,  1800] loss: 2.302797\n",
      "[279,  2000] loss: 2.302557\n",
      "[279,  2200] loss: 2.302735\n",
      "[279,  2400] loss: 2.302678\n",
      "[279,  2600] loss: 2.302565\n",
      "[279,  2800] loss: 2.302845\n",
      "[279,  3000] loss: 2.302586\n",
      "[279,  3200] loss: 2.302598\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[280,   200] loss: 2.302653\n",
      "[280,   400] loss: 2.302713\n",
      "[280,   600] loss: 2.302883\n",
      "[280,   800] loss: 2.302529\n",
      "[280,  1000] loss: 2.302548\n",
      "[280,  1200] loss: 2.302721\n",
      "[280,  1400] loss: 2.302770\n",
      "[280,  1600] loss: 2.302781\n",
      "[280,  1800] loss: 2.302386\n",
      "[280,  2000] loss: 2.302623\n",
      "[280,  2200] loss: 2.302685\n",
      "[280,  2400] loss: 2.302682\n",
      "[280,  2600] loss: 2.302517\n",
      "[280,  2800] loss: 2.302768\n",
      "[280,  3000] loss: 2.302530\n",
      "[280,  3200] loss: 2.302725\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[281,   200] loss: 2.302819\n",
      "[281,   400] loss: 2.302757\n",
      "[281,   600] loss: 2.302357\n",
      "[281,   800] loss: 2.302606\n",
      "[281,  1000] loss: 2.302740\n",
      "[281,  1200] loss: 2.302675\n",
      "[281,  1400] loss: 2.302614\n",
      "[281,  1600] loss: 2.302724\n",
      "[281,  1800] loss: 2.302623\n",
      "[281,  2000] loss: 2.302645\n",
      "[281,  2200] loss: 2.302624\n",
      "[281,  2400] loss: 2.302516\n",
      "[281,  2600] loss: 2.302754\n",
      "[281,  2800] loss: 2.302405\n",
      "[281,  3000] loss: 2.302768\n",
      "[281,  3200] loss: 2.302520\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[282,   200] loss: 2.302693\n",
      "[282,   400] loss: 2.302590\n",
      "[282,   600] loss: 2.302564\n",
      "[282,   800] loss: 2.302646\n",
      "[282,  1000] loss: 2.302652\n",
      "[282,  1200] loss: 2.302758\n",
      "[282,  1400] loss: 2.302661\n",
      "[282,  1600] loss: 2.302540\n",
      "[282,  1800] loss: 2.302763\n",
      "[282,  2000] loss: 2.302669\n",
      "[282,  2200] loss: 2.302409\n",
      "[282,  2400] loss: 2.302946\n",
      "[282,  2600] loss: 2.302583\n",
      "[282,  2800] loss: 2.302628\n",
      "[282,  3000] loss: 2.302798\n",
      "[282,  3200] loss: 2.302657\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[283,   200] loss: 2.302582\n",
      "[283,   400] loss: 2.302826\n",
      "[283,   600] loss: 2.302620\n",
      "[283,   800] loss: 2.302468\n",
      "[283,  1000] loss: 2.302832\n",
      "[283,  1200] loss: 2.302618\n",
      "[283,  1400] loss: 2.302529\n",
      "[283,  1600] loss: 2.302703\n",
      "[283,  1800] loss: 2.302474\n",
      "[283,  2000] loss: 2.302535\n",
      "[283,  2200] loss: 2.302790\n",
      "[283,  2400] loss: 2.302867\n",
      "[283,  2600] loss: 2.302372\n",
      "[283,  2800] loss: 2.302980\n",
      "[283,  3000] loss: 2.302681\n",
      "[283,  3200] loss: 2.302530\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[284,   200] loss: 2.302615\n",
      "[284,   400] loss: 2.302533\n",
      "[284,   600] loss: 2.302651\n",
      "[284,   800] loss: 2.302634\n",
      "[284,  1000] loss: 2.302691\n",
      "[284,  1200] loss: 2.302630\n",
      "[284,  1400] loss: 2.302487\n",
      "[284,  1600] loss: 2.302777\n",
      "[284,  1800] loss: 2.302524\n",
      "[284,  2000] loss: 2.302744\n",
      "[284,  2200] loss: 2.302656\n",
      "[284,  2400] loss: 2.302754\n",
      "[284,  2600] loss: 2.302696\n",
      "[284,  2800] loss: 2.302572\n",
      "[284,  3000] loss: 2.302727\n",
      "[284,  3200] loss: 2.302791\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[285,   200] loss: 2.302576\n",
      "[285,   400] loss: 2.302892\n",
      "[285,   600] loss: 2.302661\n",
      "[285,   800] loss: 2.302467\n",
      "[285,  1000] loss: 2.302605\n",
      "[285,  1200] loss: 2.302651\n",
      "[285,  1400] loss: 2.302767\n",
      "[285,  1600] loss: 2.302658\n",
      "[285,  1800] loss: 2.302581\n",
      "[285,  2000] loss: 2.302436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285,  2200] loss: 2.302631\n",
      "[285,  2400] loss: 2.302496\n",
      "[285,  2600] loss: 2.302658\n",
      "[285,  2800] loss: 2.302539\n",
      "[285,  3000] loss: 2.302724\n",
      "[285,  3200] loss: 2.302861\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[286,   200] loss: 2.302612\n",
      "[286,   400] loss: 2.302790\n",
      "[286,   600] loss: 2.302717\n",
      "[286,   800] loss: 2.302738\n",
      "[286,  1000] loss: 2.302692\n",
      "[286,  1200] loss: 2.302498\n",
      "[286,  1400] loss: 2.302737\n",
      "[286,  1600] loss: 2.302480\n",
      "[286,  1800] loss: 2.302631\n",
      "[286,  2000] loss: 2.302450\n",
      "[286,  2200] loss: 2.302867\n",
      "[286,  2400] loss: 2.302756\n",
      "[286,  2600] loss: 2.302683\n",
      "[286,  2800] loss: 2.302489\n",
      "[286,  3000] loss: 2.302768\n",
      "[286,  3200] loss: 2.302600\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[287,   200] loss: 2.302603\n",
      "[287,   400] loss: 2.302600\n",
      "[287,   600] loss: 2.302700\n",
      "[287,   800] loss: 2.302670\n",
      "[287,  1000] loss: 2.302672\n",
      "[287,  1200] loss: 2.302519\n",
      "[287,  1400] loss: 2.302653\n",
      "[287,  1600] loss: 2.302816\n",
      "[287,  1800] loss: 2.302614\n",
      "[287,  2000] loss: 2.302532\n",
      "[287,  2200] loss: 2.302764\n",
      "[287,  2400] loss: 2.302561\n",
      "[287,  2600] loss: 2.302662\n",
      "[287,  2800] loss: 2.302750\n",
      "[287,  3000] loss: 2.302738\n",
      "[287,  3200] loss: 2.302482\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[288,   200] loss: 2.302401\n",
      "[288,   400] loss: 2.302681\n",
      "[288,   600] loss: 2.302528\n",
      "[288,   800] loss: 2.302710\n",
      "[288,  1000] loss: 2.302805\n",
      "[288,  1200] loss: 2.302536\n",
      "[288,  1400] loss: 2.302663\n",
      "[288,  1600] loss: 2.302630\n",
      "[288,  1800] loss: 2.302507\n",
      "[288,  2000] loss: 2.302705\n",
      "[288,  2200] loss: 2.302593\n",
      "[288,  2400] loss: 2.302571\n",
      "[288,  2600] loss: 2.302488\n",
      "[288,  2800] loss: 2.302788\n",
      "[288,  3000] loss: 2.302576\n",
      "[288,  3200] loss: 2.302906\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[289,   200] loss: 2.302748\n",
      "[289,   400] loss: 2.302625\n",
      "[289,   600] loss: 2.302534\n",
      "[289,   800] loss: 2.302670\n",
      "[289,  1000] loss: 2.302555\n",
      "[289,  1200] loss: 2.302649\n",
      "[289,  1400] loss: 2.302702\n",
      "[289,  1600] loss: 2.302797\n",
      "[289,  1800] loss: 2.302694\n",
      "[289,  2000] loss: 2.302460\n",
      "[289,  2200] loss: 2.302856\n",
      "[289,  2400] loss: 2.302624\n",
      "[289,  2600] loss: 2.302499\n",
      "[289,  2800] loss: 2.302566\n",
      "[289,  3000] loss: 2.302680\n",
      "[289,  3200] loss: 2.302714\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[290,   200] loss: 2.302749\n",
      "[290,   400] loss: 2.302637\n",
      "[290,   600] loss: 2.302649\n",
      "[290,   800] loss: 2.302534\n",
      "[290,  1000] loss: 2.302834\n",
      "[290,  1200] loss: 2.302598\n",
      "[290,  1400] loss: 2.302570\n",
      "[290,  1600] loss: 2.302626\n",
      "[290,  1800] loss: 2.302486\n",
      "[290,  2000] loss: 2.302613\n",
      "[290,  2200] loss: 2.302689\n",
      "[290,  2400] loss: 2.302695\n",
      "[290,  2600] loss: 2.302684\n",
      "[290,  2800] loss: 2.302550\n",
      "[290,  3000] loss: 2.302706\n",
      "[290,  3200] loss: 2.302573\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[291,   200] loss: 2.302210\n",
      "[291,   400] loss: 2.302553\n",
      "[291,   600] loss: 2.303026\n",
      "[291,   800] loss: 2.302664\n",
      "[291,  1000] loss: 2.302611\n",
      "[291,  1200] loss: 2.302731\n",
      "[291,  1400] loss: 2.302645\n",
      "[291,  1600] loss: 2.302713\n",
      "[291,  1800] loss: 2.302727\n",
      "[291,  2000] loss: 2.302653\n",
      "[291,  2200] loss: 2.302480\n",
      "[291,  2400] loss: 2.302246\n",
      "[291,  2600] loss: 2.302854\n",
      "[291,  2800] loss: 2.302584\n",
      "[291,  3000] loss: 2.302725\n",
      "[291,  3200] loss: 2.302769\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[292,   200] loss: 2.302625\n",
      "[292,   400] loss: 2.302757\n",
      "[292,   600] loss: 2.302576\n",
      "[292,   800] loss: 2.302839\n",
      "[292,  1000] loss: 2.302730\n",
      "[292,  1200] loss: 2.302595\n",
      "[292,  1400] loss: 2.302569\n",
      "[292,  1600] loss: 2.302710\n",
      "[292,  1800] loss: 2.302598\n",
      "[292,  2000] loss: 2.302660\n",
      "[292,  2200] loss: 2.302584\n",
      "[292,  2400] loss: 2.302689\n",
      "[292,  2600] loss: 2.302605\n",
      "[292,  2800] loss: 2.302561\n",
      "[292,  3000] loss: 2.302372\n",
      "[292,  3200] loss: 2.302952\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[293,   200] loss: 2.302749\n",
      "[293,   400] loss: 2.302610\n",
      "[293,   600] loss: 2.302345\n",
      "[293,   800] loss: 2.302285\n",
      "[293,  1000] loss: 2.302790\n",
      "[293,  1200] loss: 2.302477\n",
      "[293,  1400] loss: 2.302596\n",
      "[293,  1600] loss: 2.302678\n",
      "[293,  1800] loss: 2.302744\n",
      "[293,  2000] loss: 2.302747\n",
      "[293,  2200] loss: 2.302612\n",
      "[293,  2400] loss: 2.302495\n",
      "[293,  2600] loss: 2.302778\n",
      "[293,  2800] loss: 2.302756\n",
      "[293,  3000] loss: 2.302604\n",
      "[293,  3200] loss: 2.302646\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[294,   200] loss: 2.302699\n",
      "[294,   400] loss: 2.302507\n",
      "[294,   600] loss: 2.302766\n",
      "[294,   800] loss: 2.302431\n",
      "[294,  1000] loss: 2.302925\n",
      "[294,  1200] loss: 2.302671\n",
      "[294,  1400] loss: 2.302558\n",
      "[294,  1600] loss: 2.302860\n",
      "[294,  1800] loss: 2.302756\n",
      "[294,  2000] loss: 2.302522\n",
      "[294,  2200] loss: 2.302524\n",
      "[294,  2400] loss: 2.302575\n",
      "[294,  2600] loss: 2.302666\n",
      "[294,  2800] loss: 2.302697\n",
      "[294,  3000] loss: 2.302561\n",
      "[294,  3200] loss: 2.302565\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[295,   200] loss: 2.302622\n",
      "[295,   400] loss: 2.302700\n",
      "[295,   600] loss: 2.302836\n",
      "[295,   800] loss: 2.302721\n",
      "[295,  1000] loss: 2.302678\n",
      "[295,  1200] loss: 2.302324\n",
      "[295,  1400] loss: 2.302624\n",
      "[295,  1600] loss: 2.302711\n",
      "[295,  1800] loss: 2.302679\n",
      "[295,  2000] loss: 2.302754\n",
      "[295,  2200] loss: 2.302455\n",
      "[295,  2400] loss: 2.302865\n",
      "[295,  2600] loss: 2.302608\n",
      "[295,  2800] loss: 2.302632\n",
      "[295,  3000] loss: 2.302462\n",
      "[295,  3200] loss: 2.302849\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[296,   200] loss: 2.302333\n",
      "[296,   400] loss: 2.302674\n",
      "[296,   600] loss: 2.302685\n",
      "[296,   800] loss: 2.302891\n",
      "[296,  1000] loss: 2.302655\n",
      "[296,  1200] loss: 2.302654\n",
      "[296,  1400] loss: 2.302438\n",
      "[296,  1600] loss: 2.302423\n",
      "[296,  1800] loss: 2.302523\n",
      "[296,  2000] loss: 2.302631\n",
      "[296,  2200] loss: 2.302858\n",
      "[296,  2400] loss: 2.302736\n",
      "[296,  2600] loss: 2.302631\n",
      "[296,  2800] loss: 2.302535\n",
      "[296,  3000] loss: 2.302697\n",
      "[296,  3200] loss: 2.302762\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[297,   200] loss: 2.302632\n",
      "[297,   400] loss: 2.302700\n",
      "[297,   600] loss: 2.302533\n",
      "[297,   800] loss: 2.302685\n",
      "[297,  1000] loss: 2.302715\n",
      "[297,  1200] loss: 2.302788\n",
      "[297,  1400] loss: 2.302685\n",
      "[297,  1600] loss: 2.302618\n",
      "[297,  1800] loss: 2.302643\n",
      "[297,  2000] loss: 2.302562\n",
      "[297,  2200] loss: 2.302847\n",
      "[297,  2400] loss: 2.302711\n",
      "[297,  2600] loss: 2.302559\n",
      "[297,  2800] loss: 2.302541\n",
      "[297,  3000] loss: 2.302764\n",
      "[297,  3200] loss: 2.302627\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[298,   200] loss: 2.302634\n",
      "[298,   400] loss: 2.302370\n",
      "[298,   600] loss: 2.302737\n",
      "[298,   800] loss: 2.302518\n",
      "[298,  1000] loss: 2.302532\n",
      "[298,  1200] loss: 2.302577\n",
      "[298,  1400] loss: 2.302687\n",
      "[298,  1600] loss: 2.302721\n",
      "[298,  1800] loss: 2.302570\n",
      "[298,  2000] loss: 2.302634\n",
      "[298,  2200] loss: 2.302732\n",
      "[298,  2400] loss: 2.302652\n",
      "[298,  2600] loss: 2.302669\n",
      "[298,  2800] loss: 2.302592\n",
      "[298,  3000] loss: 2.302845\n",
      "[298,  3200] loss: 2.302646\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[299,   200] loss: 2.302675\n",
      "[299,   400] loss: 2.302427\n",
      "[299,   600] loss: 2.302711\n",
      "[299,   800] loss: 2.302612\n",
      "[299,  1000] loss: 2.302789\n",
      "[299,  1200] loss: 2.302757\n",
      "[299,  1400] loss: 2.302659\n",
      "[299,  1600] loss: 2.302520\n",
      "[299,  1800] loss: 2.302607\n",
      "[299,  2000] loss: 2.302487\n",
      "[299,  2200] loss: 2.302785\n",
      "[299,  2400] loss: 2.302671\n",
      "[299,  2600] loss: 2.302607\n",
      "[299,  2800] loss: 2.302551\n",
      "[299,  3000] loss: 2.302686\n",
      "[299,  3200] loss: 2.302675\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[300,   200] loss: 2.302603\n",
      "[300,   400] loss: 2.302614\n",
      "[300,   600] loss: 2.302724\n",
      "[300,   800] loss: 2.302539\n",
      "[300,  1000] loss: 2.302771\n",
      "[300,  1200] loss: 2.302764\n",
      "[300,  1400] loss: 2.302544\n",
      "[300,  1600] loss: 2.302560\n",
      "[300,  1800] loss: 2.302646\n",
      "[300,  2000] loss: 2.302756\n",
      "[300,  2200] loss: 2.302692\n",
      "[300,  2400] loss: 2.302614\n",
      "[300,  2600] loss: 2.302607\n",
      "[300,  2800] loss: 2.302586\n",
      "[300,  3000] loss: 2.302481\n",
      "[300,  3200] loss: 2.302453\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[301,   200] loss: 2.302754\n",
      "[301,   400] loss: 2.302623\n",
      "[301,   600] loss: 2.302444\n",
      "[301,   800] loss: 2.302679\n",
      "[301,  1000] loss: 2.302719\n",
      "[301,  1200] loss: 2.302724\n",
      "[301,  1400] loss: 2.302652\n",
      "[301,  1600] loss: 2.302736\n",
      "[301,  1800] loss: 2.302529\n",
      "[301,  2000] loss: 2.302600\n",
      "[301,  2200] loss: 2.302661\n",
      "[301,  2400] loss: 2.302726\n",
      "[301,  2600] loss: 2.302367\n",
      "[301,  2800] loss: 2.302777\n",
      "[301,  3000] loss: 2.302839\n",
      "[301,  3200] loss: 2.302639\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[302,   200] loss: 2.302817\n",
      "[302,   400] loss: 2.302672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302,   600] loss: 2.302546\n",
      "[302,   800] loss: 2.302678\n",
      "[302,  1000] loss: 2.302387\n",
      "[302,  1200] loss: 2.302740\n",
      "[302,  1400] loss: 2.302817\n",
      "[302,  1600] loss: 2.302650\n",
      "[302,  1800] loss: 2.302737\n",
      "[302,  2000] loss: 2.302543\n",
      "[302,  2200] loss: 2.302580\n",
      "[302,  2400] loss: 2.302349\n",
      "[302,  2600] loss: 2.302535\n",
      "[302,  2800] loss: 2.302601\n",
      "[302,  3000] loss: 2.302779\n",
      "[302,  3200] loss: 2.302773\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[303,   200] loss: 2.302570\n",
      "[303,   400] loss: 2.302567\n",
      "[303,   600] loss: 2.302761\n",
      "[303,   800] loss: 2.302647\n",
      "[303,  1000] loss: 2.302792\n",
      "[303,  1200] loss: 2.302753\n",
      "[303,  1400] loss: 2.302624\n",
      "[303,  1600] loss: 2.302476\n",
      "[303,  1800] loss: 2.302583\n",
      "[303,  2000] loss: 2.302441\n",
      "[303,  2200] loss: 2.302954\n",
      "[303,  2400] loss: 2.302468\n",
      "[303,  2600] loss: 2.302593\n",
      "[303,  2800] loss: 2.302759\n",
      "[303,  3000] loss: 2.302704\n",
      "[303,  3200] loss: 2.302679\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[304,   200] loss: 2.302540\n",
      "[304,   400] loss: 2.302829\n",
      "[304,   600] loss: 2.302589\n",
      "[304,   800] loss: 2.302772\n",
      "[304,  1000] loss: 2.302765\n",
      "[304,  1200] loss: 2.302682\n",
      "[304,  1400] loss: 2.302598\n",
      "[304,  1600] loss: 2.302715\n",
      "[304,  1800] loss: 2.302608\n",
      "[304,  2000] loss: 2.302719\n",
      "[304,  2200] loss: 2.302645\n",
      "[304,  2400] loss: 2.302522\n",
      "[304,  2600] loss: 2.302684\n",
      "[304,  2800] loss: 2.302748\n",
      "[304,  3000] loss: 2.302544\n",
      "[304,  3200] loss: 2.302641\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[305,   200] loss: 2.302854\n",
      "[305,   400] loss: 2.302730\n",
      "[305,   600] loss: 2.302704\n",
      "[305,   800] loss: 2.302655\n",
      "[305,  1000] loss: 2.302685\n",
      "[305,  1200] loss: 2.302446\n",
      "[305,  1400] loss: 2.302615\n",
      "[305,  1600] loss: 2.302543\n",
      "[305,  1800] loss: 2.302672\n",
      "[305,  2000] loss: 2.302812\n",
      "[305,  2200] loss: 2.302579\n",
      "[305,  2400] loss: 2.302718\n",
      "[305,  2600] loss: 2.302716\n",
      "[305,  2800] loss: 2.302474\n",
      "[305,  3000] loss: 2.302788\n",
      "[305,  3200] loss: 2.302435\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[306,   200] loss: 2.302616\n",
      "[306,   400] loss: 2.302570\n",
      "[306,   600] loss: 2.302676\n",
      "[306,   800] loss: 2.302781\n",
      "[306,  1000] loss: 2.302656\n",
      "[306,  1200] loss: 2.302516\n",
      "[306,  1400] loss: 2.302624\n",
      "[306,  1600] loss: 2.302502\n",
      "[306,  1800] loss: 2.302530\n",
      "[306,  2000] loss: 2.302795\n",
      "[306,  2200] loss: 2.302662\n",
      "[306,  2400] loss: 2.302679\n",
      "[306,  2600] loss: 2.302734\n",
      "[306,  2800] loss: 2.302686\n",
      "[306,  3000] loss: 2.302671\n",
      "[306,  3200] loss: 2.302687\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[307,   200] loss: 2.302609\n",
      "[307,   400] loss: 2.302669\n",
      "[307,   600] loss: 2.302718\n",
      "[307,   800] loss: 2.302517\n",
      "[307,  1000] loss: 2.302583\n",
      "[307,  1200] loss: 2.302760\n",
      "[307,  1400] loss: 2.302646\n",
      "[307,  1600] loss: 2.302431\n",
      "[307,  1800] loss: 2.302247\n",
      "[307,  2000] loss: 2.303014\n",
      "[307,  2200] loss: 2.302592\n",
      "[307,  2400] loss: 2.302642\n",
      "[307,  2600] loss: 2.302768\n",
      "[307,  2800] loss: 2.302580\n",
      "[307,  3000] loss: 2.302545\n",
      "[307,  3200] loss: 2.302822\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[308,   200] loss: 2.302628\n",
      "[308,   400] loss: 2.302663\n",
      "[308,   600] loss: 2.302611\n",
      "[308,   800] loss: 2.302690\n",
      "[308,  1000] loss: 2.302599\n",
      "[308,  1200] loss: 2.302713\n",
      "[308,  1400] loss: 2.302680\n",
      "[308,  1600] loss: 2.302532\n",
      "[308,  1800] loss: 2.302735\n",
      "[308,  2000] loss: 2.302421\n",
      "[308,  2200] loss: 2.302802\n",
      "[308,  2400] loss: 2.302664\n",
      "[308,  2600] loss: 2.302802\n",
      "[308,  2800] loss: 2.302706\n",
      "[308,  3000] loss: 2.302619\n",
      "[308,  3200] loss: 2.302662\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[309,   200] loss: 2.302467\n",
      "[309,   400] loss: 2.302685\n",
      "[309,   600] loss: 2.302619\n",
      "[309,   800] loss: 2.302682\n",
      "[309,  1000] loss: 2.302574\n",
      "[309,  1200] loss: 2.302717\n",
      "[309,  1400] loss: 2.302790\n",
      "[309,  1600] loss: 2.302613\n",
      "[309,  1800] loss: 2.302756\n",
      "[309,  2000] loss: 2.302353\n",
      "[309,  2200] loss: 2.302381\n",
      "[309,  2400] loss: 2.302732\n",
      "[309,  2600] loss: 2.302767\n",
      "[309,  2800] loss: 2.302547\n",
      "[309,  3000] loss: 2.302611\n",
      "[309,  3200] loss: 2.302653\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[310,   200] loss: 2.302685\n",
      "[310,   400] loss: 2.302700\n",
      "[310,   600] loss: 2.302740\n",
      "[310,   800] loss: 2.302787\n",
      "[310,  1000] loss: 2.302798\n",
      "[310,  1200] loss: 2.302653\n",
      "[310,  1400] loss: 2.302610\n",
      "[310,  1600] loss: 2.302615\n",
      "[310,  1800] loss: 2.302644\n",
      "[310,  2000] loss: 2.302616\n",
      "[310,  2200] loss: 2.302449\n",
      "[310,  2400] loss: 2.302630\n",
      "[310,  2600] loss: 2.302562\n",
      "[310,  2800] loss: 2.302590\n",
      "[310,  3000] loss: 2.302609\n",
      "[310,  3200] loss: 2.302736\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[311,   200] loss: 2.302756\n",
      "[311,   400] loss: 2.302638\n",
      "[311,   600] loss: 2.302608\n",
      "[311,   800] loss: 2.302586\n",
      "[311,  1000] loss: 2.302641\n",
      "[311,  1200] loss: 2.302653\n",
      "[311,  1400] loss: 2.302693\n",
      "[311,  1600] loss: 2.302777\n",
      "[311,  1800] loss: 2.302413\n",
      "[311,  2000] loss: 2.302693\n",
      "[311,  2200] loss: 2.302760\n",
      "[311,  2400] loss: 2.302650\n",
      "[311,  2600] loss: 2.302520\n",
      "[311,  2800] loss: 2.302874\n",
      "[311,  3000] loss: 2.302682\n",
      "[311,  3200] loss: 2.302574\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[312,   200] loss: 2.302358\n",
      "[312,   400] loss: 2.302864\n",
      "[312,   600] loss: 2.302744\n",
      "[312,   800] loss: 2.302593\n",
      "[312,  1000] loss: 2.302854\n",
      "[312,  1200] loss: 2.302630\n",
      "[312,  1400] loss: 2.302468\n",
      "[312,  1600] loss: 2.302662\n",
      "[312,  1800] loss: 2.302645\n",
      "[312,  2000] loss: 2.302573\n",
      "[312,  2200] loss: 2.302678\n",
      "[312,  2400] loss: 2.302578\n",
      "[312,  2600] loss: 2.302812\n",
      "[312,  2800] loss: 2.302622\n",
      "[312,  3000] loss: 2.302678\n",
      "[312,  3200] loss: 2.302671\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[313,   200] loss: 2.302579\n",
      "[313,   400] loss: 2.302578\n",
      "[313,   600] loss: 2.302727\n",
      "[313,   800] loss: 2.302800\n",
      "[313,  1000] loss: 2.302718\n",
      "[313,  1200] loss: 2.302613\n",
      "[313,  1400] loss: 2.302620\n",
      "[313,  1600] loss: 2.302620\n",
      "[313,  1800] loss: 2.302543\n",
      "[313,  2000] loss: 2.302637\n",
      "[313,  2200] loss: 2.302809\n",
      "[313,  2400] loss: 2.302634\n",
      "[313,  2600] loss: 2.302691\n",
      "[313,  2800] loss: 2.302684\n",
      "[313,  3000] loss: 2.302241\n",
      "[313,  3200] loss: 2.302864\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[314,   200] loss: 2.302373\n",
      "[314,   400] loss: 2.302795\n",
      "[314,   600] loss: 2.302860\n",
      "[314,   800] loss: 2.302361\n",
      "[314,  1000] loss: 2.302877\n",
      "[314,  1200] loss: 2.302702\n",
      "[314,  1400] loss: 2.302512\n",
      "[314,  1600] loss: 2.302601\n",
      "[314,  1800] loss: 2.302819\n",
      "[314,  2000] loss: 2.302537\n",
      "[314,  2200] loss: 2.302805\n",
      "[314,  2400] loss: 2.302751\n",
      "[314,  2600] loss: 2.302616\n",
      "[314,  2800] loss: 2.302473\n",
      "[314,  3000] loss: 2.302693\n",
      "[314,  3200] loss: 2.302651\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[315,   200] loss: 2.302235\n",
      "[315,   400] loss: 2.302441\n",
      "[315,   600] loss: 2.302510\n",
      "[315,   800] loss: 2.302883\n",
      "[315,  1000] loss: 2.302668\n",
      "[315,  1200] loss: 2.302409\n",
      "[315,  1400] loss: 2.302658\n",
      "[315,  1600] loss: 2.302676\n",
      "[315,  1800] loss: 2.302664\n",
      "[315,  2000] loss: 2.302750\n",
      "[315,  2200] loss: 2.302697\n",
      "[315,  2400] loss: 2.302658\n",
      "[315,  2600] loss: 2.302376\n",
      "[315,  2800] loss: 2.302903\n",
      "[315,  3000] loss: 2.302738\n",
      "[315,  3200] loss: 2.302572\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[316,   200] loss: 2.302683\n",
      "[316,   400] loss: 2.302724\n",
      "[316,   600] loss: 2.302629\n",
      "[316,   800] loss: 2.302484\n",
      "[316,  1000] loss: 2.302455\n",
      "[316,  1200] loss: 2.302852\n",
      "[316,  1400] loss: 2.302732\n",
      "[316,  1600] loss: 2.302485\n",
      "[316,  1800] loss: 2.302533\n",
      "[316,  2000] loss: 2.302809\n",
      "[316,  2200] loss: 2.302814\n",
      "[316,  2400] loss: 2.302753\n",
      "[316,  2600] loss: 2.302563\n",
      "[316,  2800] loss: 2.302662\n",
      "[316,  3000] loss: 2.302656\n",
      "[316,  3200] loss: 2.302519\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[317,   200] loss: 2.302623\n",
      "[317,   400] loss: 2.302760\n",
      "[317,   600] loss: 2.302765\n",
      "[317,   800] loss: 2.302753\n",
      "[317,  1000] loss: 2.302672\n",
      "[317,  1200] loss: 2.302536\n",
      "[317,  1400] loss: 2.302624\n",
      "[317,  1600] loss: 2.302494\n",
      "[317,  1800] loss: 2.302762\n",
      "[317,  2000] loss: 2.302518\n",
      "[317,  2200] loss: 2.302717\n",
      "[317,  2400] loss: 2.302546\n",
      "[317,  2600] loss: 2.302528\n",
      "[317,  2800] loss: 2.302662\n",
      "[317,  3000] loss: 2.302678\n",
      "[317,  3200] loss: 2.302719\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[318,   200] loss: 2.302666\n",
      "[318,   400] loss: 2.302717\n",
      "[318,   600] loss: 2.302471\n",
      "[318,   800] loss: 2.302725\n",
      "[318,  1000] loss: 2.302646\n",
      "[318,  1200] loss: 2.302376\n",
      "[318,  1400] loss: 2.302568\n",
      "[318,  1600] loss: 2.302866\n",
      "[318,  1800] loss: 2.302752\n",
      "[318,  2000] loss: 2.302555\n",
      "[318,  2200] loss: 2.302699\n",
      "[318,  2400] loss: 2.302674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318,  2600] loss: 2.302574\n",
      "[318,  2800] loss: 2.302755\n",
      "[318,  3000] loss: 2.302556\n",
      "[318,  3200] loss: 2.302753\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[319,   200] loss: 2.302344\n",
      "[319,   400] loss: 2.302737\n",
      "[319,   600] loss: 2.302603\n",
      "[319,   800] loss: 2.302768\n",
      "[319,  1000] loss: 2.302466\n",
      "[319,  1200] loss: 2.302650\n",
      "[319,  1400] loss: 2.302707\n",
      "[319,  1600] loss: 2.302721\n",
      "[319,  1800] loss: 2.302680\n",
      "[319,  2000] loss: 2.302456\n",
      "[319,  2200] loss: 2.302573\n",
      "[319,  2400] loss: 2.302789\n",
      "[319,  2600] loss: 2.302655\n",
      "[319,  2800] loss: 2.302765\n",
      "[319,  3000] loss: 2.302576\n",
      "[319,  3200] loss: 2.302670\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[320,   200] loss: 2.302658\n",
      "[320,   400] loss: 2.302660\n",
      "[320,   600] loss: 2.302699\n",
      "[320,   800] loss: 2.302571\n",
      "[320,  1000] loss: 2.302744\n",
      "[320,  1200] loss: 2.302695\n",
      "[320,  1400] loss: 2.302704\n",
      "[320,  1600] loss: 2.302728\n",
      "[320,  1800] loss: 2.302576\n",
      "[320,  2000] loss: 2.302637\n",
      "[320,  2200] loss: 2.302537\n",
      "[320,  2400] loss: 2.302475\n",
      "[320,  2600] loss: 2.302422\n",
      "[320,  2800] loss: 2.302411\n",
      "[320,  3000] loss: 2.302525\n",
      "[320,  3200] loss: 2.302922\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[321,   200] loss: 2.302529\n",
      "[321,   400] loss: 2.302597\n",
      "[321,   600] loss: 2.302545\n",
      "[321,   800] loss: 2.302628\n",
      "[321,  1000] loss: 2.302672\n",
      "[321,  1200] loss: 2.302136\n",
      "[321,  1400] loss: 2.302996\n",
      "[321,  1600] loss: 2.302626\n",
      "[321,  1800] loss: 2.302742\n",
      "[321,  2000] loss: 2.302771\n",
      "[321,  2200] loss: 2.302597\n",
      "[321,  2400] loss: 2.302641\n",
      "[321,  2600] loss: 2.302687\n",
      "[321,  2800] loss: 2.302762\n",
      "[321,  3000] loss: 2.302449\n",
      "[321,  3200] loss: 2.302389\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[322,   200] loss: 2.302519\n",
      "[322,   400] loss: 2.302652\n",
      "[322,   600] loss: 2.302715\n",
      "[322,   800] loss: 2.302652\n",
      "[322,  1000] loss: 2.302670\n",
      "[322,  1200] loss: 2.302634\n",
      "[322,  1400] loss: 2.302584\n",
      "[322,  1600] loss: 2.302722\n",
      "[322,  1800] loss: 2.302741\n",
      "[322,  2000] loss: 2.302553\n",
      "[322,  2200] loss: 2.302750\n",
      "[322,  2400] loss: 2.302691\n",
      "[322,  2600] loss: 2.302526\n",
      "[322,  2800] loss: 2.302757\n",
      "[322,  3000] loss: 2.302508\n",
      "[322,  3200] loss: 2.302662\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[323,   200] loss: 2.302578\n",
      "[323,   400] loss: 2.302805\n",
      "[323,   600] loss: 2.302394\n",
      "[323,   800] loss: 2.302795\n",
      "[323,  1000] loss: 2.302637\n",
      "[323,  1200] loss: 2.302457\n",
      "[323,  1400] loss: 2.302728\n",
      "[323,  1600] loss: 2.302703\n",
      "[323,  1800] loss: 2.302581\n",
      "[323,  2000] loss: 2.302486\n",
      "[323,  2200] loss: 2.302860\n",
      "[323,  2400] loss: 2.302802\n",
      "[323,  2600] loss: 2.302606\n",
      "[323,  2800] loss: 2.302696\n",
      "[323,  3000] loss: 2.302808\n",
      "[323,  3200] loss: 2.302656\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[324,   200] loss: 2.302480\n",
      "[324,   400] loss: 2.302475\n",
      "[324,   600] loss: 2.302935\n",
      "[324,   800] loss: 2.302470\n",
      "[324,  1000] loss: 2.302625\n",
      "[324,  1200] loss: 2.302820\n",
      "[324,  1400] loss: 2.302728\n",
      "[324,  1600] loss: 2.302453\n",
      "[324,  1800] loss: 2.302569\n",
      "[324,  2000] loss: 2.302661\n",
      "[324,  2200] loss: 2.302781\n",
      "[324,  2400] loss: 2.302530\n",
      "[324,  2600] loss: 2.302523\n",
      "[324,  2800] loss: 2.302700\n",
      "[324,  3000] loss: 2.302755\n",
      "[324,  3200] loss: 2.302655\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[325,   200] loss: 2.302500\n",
      "[325,   400] loss: 2.302608\n",
      "[325,   600] loss: 2.302545\n",
      "[325,   800] loss: 2.302674\n",
      "[325,  1000] loss: 2.302854\n",
      "[325,  1200] loss: 2.302595\n",
      "[325,  1400] loss: 2.302378\n",
      "[325,  1600] loss: 2.302456\n",
      "[325,  1800] loss: 2.302904\n",
      "[325,  2000] loss: 2.302631\n",
      "[325,  2200] loss: 2.302689\n",
      "[325,  2400] loss: 2.302774\n",
      "[325,  2600] loss: 2.302718\n",
      "[325,  2800] loss: 2.302550\n",
      "[325,  3000] loss: 2.302785\n",
      "[325,  3200] loss: 2.302732\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[326,   200] loss: 2.302661\n",
      "[326,   400] loss: 2.302584\n",
      "[326,   600] loss: 2.302637\n",
      "[326,   800] loss: 2.302742\n",
      "[326,  1000] loss: 2.302653\n",
      "[326,  1200] loss: 2.302724\n",
      "[326,  1400] loss: 2.302481\n",
      "[326,  1600] loss: 2.302804\n",
      "[326,  1800] loss: 2.302648\n",
      "[326,  2000] loss: 2.302369\n",
      "[326,  2200] loss: 2.302660\n",
      "[326,  2400] loss: 2.302533\n",
      "[326,  2600] loss: 2.302869\n",
      "[326,  2800] loss: 2.302671\n",
      "[326,  3000] loss: 2.302688\n",
      "[326,  3200] loss: 2.302702\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[327,   200] loss: 2.302561\n",
      "[327,   400] loss: 2.302664\n",
      "[327,   600] loss: 2.302672\n",
      "[327,   800] loss: 2.302754\n",
      "[327,  1000] loss: 2.302850\n",
      "[327,  1200] loss: 2.302479\n",
      "[327,  1400] loss: 2.302757\n",
      "[327,  1600] loss: 2.302548\n",
      "[327,  1800] loss: 2.302505\n",
      "[327,  2000] loss: 2.302763\n",
      "[327,  2200] loss: 2.302437\n",
      "[327,  2400] loss: 2.302745\n",
      "[327,  2600] loss: 2.302749\n",
      "[327,  2800] loss: 2.302617\n",
      "[327,  3000] loss: 2.302775\n",
      "[327,  3200] loss: 2.302668\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[328,   200] loss: 2.302501\n",
      "[328,   400] loss: 2.302813\n",
      "[328,   600] loss: 2.302700\n",
      "[328,   800] loss: 2.302555\n",
      "[328,  1000] loss: 2.302733\n",
      "[328,  1200] loss: 2.302612\n",
      "[328,  1400] loss: 2.302667\n",
      "[328,  1600] loss: 2.302625\n",
      "[328,  1800] loss: 2.302601\n",
      "[328,  2000] loss: 2.302708\n",
      "[328,  2200] loss: 2.302610\n",
      "[328,  2400] loss: 2.302487\n",
      "[328,  2600] loss: 2.302746\n",
      "[328,  2800] loss: 2.302706\n",
      "[328,  3000] loss: 2.302493\n",
      "[328,  3200] loss: 2.302639\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[329,   200] loss: 2.302549\n",
      "[329,   400] loss: 2.302711\n",
      "[329,   600] loss: 2.302561\n",
      "[329,   800] loss: 2.302767\n",
      "[329,  1000] loss: 2.302354\n",
      "[329,  1200] loss: 2.302779\n",
      "[329,  1400] loss: 2.302769\n",
      "[329,  1600] loss: 2.302687\n",
      "[329,  1800] loss: 2.302562\n",
      "[329,  2000] loss: 2.302715\n",
      "[329,  2200] loss: 2.302685\n",
      "[329,  2400] loss: 2.302728\n",
      "[329,  2600] loss: 2.302303\n",
      "[329,  2800] loss: 2.302391\n",
      "[329,  3000] loss: 2.302642\n",
      "[329,  3200] loss: 2.302791\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[330,   200] loss: 2.302737\n",
      "[330,   400] loss: 2.302723\n",
      "[330,   600] loss: 2.302387\n",
      "[330,   800] loss: 2.302624\n",
      "[330,  1000] loss: 2.302703\n",
      "[330,  1200] loss: 2.302706\n",
      "[330,  1400] loss: 2.302655\n",
      "[330,  1600] loss: 2.302648\n",
      "[330,  1800] loss: 2.302535\n",
      "[330,  2000] loss: 2.302529\n",
      "[330,  2200] loss: 2.302620\n",
      "[330,  2400] loss: 2.302839\n",
      "[330,  2600] loss: 2.302554\n",
      "[330,  2800] loss: 2.302695\n",
      "[330,  3000] loss: 2.302698\n",
      "[330,  3200] loss: 2.302544\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[331,   200] loss: 2.302534\n",
      "[331,   400] loss: 2.302803\n",
      "[331,   600] loss: 2.302742\n",
      "[331,   800] loss: 2.302757\n",
      "[331,  1000] loss: 2.302739\n",
      "[331,  1200] loss: 2.302680\n",
      "[331,  1400] loss: 2.302717\n",
      "[331,  1600] loss: 2.302633\n",
      "[331,  1800] loss: 2.302709\n",
      "[331,  2000] loss: 2.302657\n",
      "[331,  2200] loss: 2.302648\n",
      "[331,  2400] loss: 2.302619\n",
      "[331,  2600] loss: 2.302657\n",
      "[331,  2800] loss: 2.302520\n",
      "[331,  3000] loss: 2.302618\n",
      "[331,  3200] loss: 2.302545\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[332,   200] loss: 2.302698\n",
      "[332,   400] loss: 2.302436\n",
      "[332,   600] loss: 2.302307\n",
      "[332,   800] loss: 2.303026\n",
      "[332,  1000] loss: 2.302687\n",
      "[332,  1200] loss: 2.302352\n",
      "[332,  1400] loss: 2.302788\n",
      "[332,  1600] loss: 2.302653\n",
      "[332,  1800] loss: 2.302393\n",
      "[332,  2000] loss: 2.302627\n",
      "[332,  2200] loss: 2.302632\n",
      "[332,  2400] loss: 2.302632\n",
      "[332,  2600] loss: 2.302487\n",
      "[332,  2800] loss: 2.302787\n",
      "[332,  3000] loss: 2.302596\n",
      "[332,  3200] loss: 2.302634\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[333,   200] loss: 2.302701\n",
      "[333,   400] loss: 2.302573\n",
      "[333,   600] loss: 2.302631\n",
      "[333,   800] loss: 2.302695\n",
      "[333,  1000] loss: 2.302470\n",
      "[333,  1200] loss: 2.302738\n",
      "[333,  1400] loss: 2.302857\n",
      "[333,  1600] loss: 2.302662\n",
      "[333,  1800] loss: 2.302555\n",
      "[333,  2000] loss: 2.302704\n",
      "[333,  2200] loss: 2.302800\n",
      "[333,  2400] loss: 2.302580\n",
      "[333,  2600] loss: 2.302588\n",
      "[333,  2800] loss: 2.302427\n",
      "[333,  3000] loss: 2.302691\n",
      "[333,  3200] loss: 2.302499\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[334,   200] loss: 2.302541\n",
      "[334,   400] loss: 2.302435\n",
      "[334,   600] loss: 2.302880\n",
      "[334,   800] loss: 2.302580\n",
      "[334,  1000] loss: 2.302762\n",
      "[334,  1200] loss: 2.302633\n",
      "[334,  1400] loss: 2.302663\n",
      "[334,  1600] loss: 2.302627\n",
      "[334,  1800] loss: 2.302417\n",
      "[334,  2000] loss: 2.302712\n",
      "[334,  2200] loss: 2.302745\n",
      "[334,  2400] loss: 2.302567\n",
      "[334,  2600] loss: 2.302432\n",
      "[334,  2800] loss: 2.302757\n",
      "[334,  3000] loss: 2.302682\n",
      "[334,  3200] loss: 2.302597\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[335,   200] loss: 2.302581\n",
      "[335,   400] loss: 2.302687\n",
      "[335,   600] loss: 2.302633\n",
      "[335,   800] loss: 2.302503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335,  1000] loss: 2.302899\n",
      "[335,  1200] loss: 2.302787\n",
      "[335,  1400] loss: 2.302636\n",
      "[335,  1600] loss: 2.302595\n",
      "[335,  1800] loss: 2.302502\n",
      "[335,  2000] loss: 2.302645\n",
      "[335,  2200] loss: 2.302663\n",
      "[335,  2400] loss: 2.302775\n",
      "[335,  2600] loss: 2.302530\n",
      "[335,  2800] loss: 2.302672\n",
      "[335,  3000] loss: 2.302565\n",
      "[335,  3200] loss: 2.302434\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[336,   200] loss: 2.302526\n",
      "[336,   400] loss: 2.302684\n",
      "[336,   600] loss: 2.302777\n",
      "[336,   800] loss: 2.302466\n",
      "[336,  1000] loss: 2.302450\n",
      "[336,  1200] loss: 2.302390\n",
      "[336,  1400] loss: 2.302564\n",
      "[336,  1600] loss: 2.302418\n",
      "[336,  1800] loss: 2.302661\n",
      "[336,  2000] loss: 2.302748\n",
      "[336,  2200] loss: 2.302837\n",
      "[336,  2400] loss: 2.302741\n",
      "[336,  2600] loss: 2.302547\n",
      "[336,  2800] loss: 2.302764\n",
      "[336,  3000] loss: 2.302751\n",
      "[336,  3200] loss: 2.302760\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[337,   200] loss: 2.302550\n",
      "[337,   400] loss: 2.302388\n",
      "[337,   600] loss: 2.302481\n",
      "[337,   800] loss: 2.302794\n",
      "[337,  1000] loss: 2.302671\n",
      "[337,  1200] loss: 2.302784\n",
      "[337,  1400] loss: 2.302716\n",
      "[337,  1600] loss: 2.302550\n",
      "[337,  1800] loss: 2.302647\n",
      "[337,  2000] loss: 2.302644\n",
      "[337,  2200] loss: 2.302695\n",
      "[337,  2400] loss: 2.302681\n",
      "[337,  2600] loss: 2.302515\n",
      "[337,  2800] loss: 2.302541\n",
      "[337,  3000] loss: 2.302861\n",
      "[337,  3200] loss: 2.302745\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[338,   200] loss: 2.302639\n",
      "[338,   400] loss: 2.302577\n",
      "[338,   600] loss: 2.302669\n",
      "[338,   800] loss: 2.302647\n",
      "[338,  1000] loss: 2.302585\n",
      "[338,  1200] loss: 2.302725\n",
      "[338,  1400] loss: 2.302605\n",
      "[338,  1600] loss: 2.302548\n",
      "[338,  1800] loss: 2.302794\n",
      "[338,  2000] loss: 2.302680\n",
      "[338,  2200] loss: 2.302535\n",
      "[338,  2400] loss: 2.302381\n",
      "[338,  2600] loss: 2.302699\n",
      "[338,  2800] loss: 2.302662\n",
      "[338,  3000] loss: 2.302821\n",
      "[338,  3200] loss: 2.302679\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[339,   200] loss: 2.302687\n",
      "[339,   400] loss: 2.302304\n",
      "[339,   600] loss: 2.302968\n",
      "[339,   800] loss: 2.302661\n",
      "[339,  1000] loss: 2.302679\n",
      "[339,  1200] loss: 2.302612\n",
      "[339,  1400] loss: 2.302712\n",
      "[339,  1600] loss: 2.302642\n",
      "[339,  1800] loss: 2.302593\n",
      "[339,  2000] loss: 2.302606\n",
      "[339,  2200] loss: 2.302621\n",
      "[339,  2400] loss: 2.302832\n",
      "[339,  2600] loss: 2.302577\n",
      "[339,  2800] loss: 2.302847\n",
      "[339,  3000] loss: 2.302641\n",
      "[339,  3200] loss: 2.302649\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[340,   200] loss: 2.302208\n",
      "[340,   400] loss: 2.302819\n",
      "[340,   600] loss: 2.302631\n",
      "[340,   800] loss: 2.302462\n",
      "[340,  1000] loss: 2.302807\n",
      "[340,  1200] loss: 2.302746\n",
      "[340,  1400] loss: 2.302643\n",
      "[340,  1600] loss: 2.302628\n",
      "[340,  1800] loss: 2.302720\n",
      "[340,  2000] loss: 2.302776\n",
      "[340,  2200] loss: 2.302649\n",
      "[340,  2400] loss: 2.302673\n",
      "[340,  2600] loss: 2.302630\n",
      "[340,  2800] loss: 2.302721\n",
      "[340,  3000] loss: 2.302774\n",
      "[340,  3200] loss: 2.302665\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[341,   200] loss: 2.302479\n",
      "[341,   400] loss: 2.302732\n",
      "[341,   600] loss: 2.302532\n",
      "[341,   800] loss: 2.302677\n",
      "[341,  1000] loss: 2.302788\n",
      "[341,  1200] loss: 2.302662\n",
      "[341,  1400] loss: 2.302751\n",
      "[341,  1600] loss: 2.302570\n",
      "[341,  1800] loss: 2.302560\n",
      "[341,  2000] loss: 2.302614\n",
      "[341,  2200] loss: 2.302629\n",
      "[341,  2400] loss: 2.302622\n",
      "[341,  2600] loss: 2.302597\n",
      "[341,  2800] loss: 2.302714\n",
      "[341,  3000] loss: 2.302554\n",
      "[341,  3200] loss: 2.302664\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[342,   200] loss: 2.302636\n",
      "[342,   400] loss: 2.302611\n",
      "[342,   600] loss: 2.302750\n",
      "[342,   800] loss: 2.302566\n",
      "[342,  1000] loss: 2.302755\n",
      "[342,  1200] loss: 2.302650\n",
      "[342,  1400] loss: 2.302657\n",
      "[342,  1600] loss: 2.302750\n",
      "[342,  1800] loss: 2.302514\n",
      "[342,  2000] loss: 2.302612\n",
      "[342,  2200] loss: 2.302908\n",
      "[342,  2400] loss: 2.302634\n",
      "[342,  2600] loss: 2.302670\n",
      "[342,  2800] loss: 2.302656\n",
      "[342,  3000] loss: 2.302613\n",
      "[342,  3200] loss: 2.302662\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[343,   200] loss: 2.302483\n",
      "[343,   400] loss: 2.302860\n",
      "[343,   600] loss: 2.302634\n",
      "[343,   800] loss: 2.302780\n",
      "[343,  1000] loss: 2.302623\n",
      "[343,  1200] loss: 2.302439\n",
      "[343,  1400] loss: 2.302842\n",
      "[343,  1600] loss: 2.302477\n",
      "[343,  1800] loss: 2.302829\n",
      "[343,  2000] loss: 2.302485\n",
      "[343,  2200] loss: 2.302626\n",
      "[343,  2400] loss: 2.302268\n",
      "[343,  2600] loss: 2.303082\n",
      "[343,  2800] loss: 2.302478\n",
      "[343,  3000] loss: 2.302757\n",
      "[343,  3200] loss: 2.302594\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[344,   200] loss: 2.302596\n",
      "[344,   400] loss: 2.302421\n",
      "[344,   600] loss: 2.302782\n",
      "[344,   800] loss: 2.302607\n",
      "[344,  1000] loss: 2.302797\n",
      "[344,  1200] loss: 2.302663\n",
      "[344,  1400] loss: 2.302567\n",
      "[344,  1600] loss: 2.302670\n",
      "[344,  1800] loss: 2.302416\n",
      "[344,  2000] loss: 2.302880\n",
      "[344,  2200] loss: 2.302490\n",
      "[344,  2400] loss: 2.302607\n",
      "[344,  2600] loss: 2.302765\n",
      "[344,  2800] loss: 2.302636\n",
      "[344,  3000] loss: 2.302699\n",
      "[344,  3200] loss: 2.302496\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[345,   200] loss: 2.302746\n",
      "[345,   400] loss: 2.302561\n",
      "[345,   600] loss: 2.302635\n",
      "[345,   800] loss: 2.302613\n",
      "[345,  1000] loss: 2.302793\n",
      "[345,  1200] loss: 2.302733\n",
      "[345,  1400] loss: 2.302578\n",
      "[345,  1600] loss: 2.302674\n",
      "[345,  1800] loss: 2.302779\n",
      "[345,  2000] loss: 2.302473\n",
      "[345,  2200] loss: 2.302565\n",
      "[345,  2400] loss: 2.302584\n",
      "[345,  2600] loss: 2.302527\n",
      "[345,  2800] loss: 2.302701\n",
      "[345,  3000] loss: 2.302545\n",
      "[345,  3200] loss: 2.302871\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[346,   200] loss: 2.302614\n",
      "[346,   400] loss: 2.302585\n",
      "[346,   600] loss: 2.302438\n",
      "[346,   800] loss: 2.302776\n",
      "[346,  1000] loss: 2.302834\n",
      "[346,  1200] loss: 2.302664\n",
      "[346,  1400] loss: 2.302411\n",
      "[346,  1600] loss: 2.302556\n",
      "[346,  1800] loss: 2.302675\n",
      "[346,  2000] loss: 2.302858\n",
      "[346,  2200] loss: 2.302714\n",
      "[346,  2400] loss: 2.302744\n",
      "[346,  2600] loss: 2.302703\n",
      "[346,  2800] loss: 2.302608\n",
      "[346,  3000] loss: 2.302641\n",
      "[346,  3200] loss: 2.302738\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[347,   200] loss: 2.302732\n",
      "[347,   400] loss: 2.302701\n",
      "[347,   600] loss: 2.302467\n",
      "[347,   800] loss: 2.302724\n",
      "[347,  1000] loss: 2.302840\n",
      "[347,  1200] loss: 2.302691\n",
      "[347,  1400] loss: 2.302552\n",
      "[347,  1600] loss: 2.302494\n",
      "[347,  1800] loss: 2.302550\n",
      "[347,  2000] loss: 2.302681\n",
      "[347,  2200] loss: 2.302578\n",
      "[347,  2400] loss: 2.302984\n",
      "[347,  2600] loss: 2.302625\n",
      "[347,  2800] loss: 2.302648\n",
      "[347,  3000] loss: 2.302778\n",
      "[347,  3200] loss: 2.302515\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[348,   200] loss: 2.302745\n",
      "[348,   400] loss: 2.302814\n",
      "[348,   600] loss: 2.302573\n",
      "[348,   800] loss: 2.302550\n",
      "[348,  1000] loss: 2.302499\n",
      "[348,  1200] loss: 2.302761\n",
      "[348,  1400] loss: 2.302817\n",
      "[348,  1600] loss: 2.302612\n",
      "[348,  1800] loss: 2.302685\n",
      "[348,  2000] loss: 2.302784\n",
      "[348,  2200] loss: 2.302520\n",
      "[348,  2400] loss: 2.302654\n",
      "[348,  2600] loss: 2.302552\n",
      "[348,  2800] loss: 2.302815\n",
      "[348,  3000] loss: 2.302600\n",
      "[348,  3200] loss: 2.302386\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[349,   200] loss: 2.302652\n",
      "[349,   400] loss: 2.302544\n",
      "[349,   600] loss: 2.302764\n",
      "[349,   800] loss: 2.302749\n",
      "[349,  1000] loss: 2.302732\n",
      "[349,  1200] loss: 2.302759\n",
      "[349,  1400] loss: 2.302576\n",
      "[349,  1600] loss: 2.302669\n",
      "[349,  1800] loss: 2.302547\n",
      "[349,  2000] loss: 2.302675\n",
      "[349,  2200] loss: 2.302497\n",
      "[349,  2400] loss: 2.302692\n",
      "[349,  2600] loss: 2.302681\n",
      "[349,  2800] loss: 2.302605\n",
      "[349,  3000] loss: 2.302651\n",
      "[349,  3200] loss: 2.302475\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[350,   200] loss: 2.302609\n",
      "[350,   400] loss: 2.302736\n",
      "[350,   600] loss: 2.302710\n",
      "[350,   800] loss: 2.302761\n",
      "[350,  1000] loss: 2.302741\n",
      "[350,  1200] loss: 2.302749\n",
      "[350,  1400] loss: 2.302612\n",
      "[350,  1600] loss: 2.302569\n",
      "[350,  1800] loss: 2.302519\n",
      "[350,  2000] loss: 2.302883\n",
      "[350,  2200] loss: 2.302510\n",
      "[350,  2400] loss: 2.302813\n",
      "[350,  2600] loss: 2.302585\n",
      "[350,  2800] loss: 2.302435\n",
      "[350,  3000] loss: 2.302483\n",
      "[350,  3200] loss: 2.302854\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# define and train the network\n",
    "stylised_model_path = './cifar32_style_model.pth'\n",
    "stylised_model = ResNet50()\n",
    "lr=0.1\n",
    "stylised_optimizer = optim.Adam(stylised_model.parameters(), lr=lr)\n",
    "stylised_lr_scheduler = optim.lr_scheduler.MultiStepLR(stylised_optimizer, milestones=[150, 250])\n",
    "train_part(stylised_model, data_loader_train_style, data_loader_val_style, stylised_model_path, stylised_optimizer, stylised_lr_scheduler, epochs = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised ResNet50 on Normal CIFAR10 Dataset\n",
    "\n",
    "The below code tests the stylised ResNet50 model on the vanilla CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1006 / 10000 correct, accuracy of the dataset is: 10.060 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "stylised_model = ResNet50()\n",
    "stylised_model.load_state_dict(torch.load('./cifar32_style_model.pth'))\n",
    "stylised_model.to(device)\n",
    "check_accuracy(data_loader_test, stylised_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised ResNet50 on Stylised CIFAR10 Dataset\n",
    "\n",
    "The below code tests the stylised ResNet50 model on the stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1000 / 10000 correct, accuracy of the dataset is: 10.000 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "stylised_model = ResNet50()\n",
    "stylised_model.load_state_dict(torch.load('./cifar32_style_model.pth'))\n",
    "stylised_model.to(device)\n",
    "check_accuracy(data_loader_test_style, stylised_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
