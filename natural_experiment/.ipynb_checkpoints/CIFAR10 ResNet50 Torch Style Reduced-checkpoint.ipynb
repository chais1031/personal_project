{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Contextural Bias of ResNet50 on CIFAR10 Dataset - Reduced Stylised ResNet50 Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook trains and tests a reduced stylised ResNet50 torch model with the CIFAR10 dataset. It includes functions for loading the dataset, turning them into tensors, model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv2d, AvgPool2d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following cell provides a class that loads the CIFAR dataset given the relevant path, processes it into a dictionary format of class labels and content then processes the images into tensors. The class also has helper functions to extract information about the dataset needed for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        super(CifarDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        \n",
    "        classes_list = []\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            classes_list.append(class_name)\n",
    "        classes_list.sort()\n",
    "        self.classes = [dict(class_idx = k, class_name = v) for k, v in enumerate(classes_list)]\n",
    "        \n",
    "\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "        tr = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        img = tr(img)\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_datasets(data_path):\n",
    "    dataset = CifarDataset(data_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used for this experiment are normal CIFAR10 dataset, stylised CIFAR10 dataset and stylised CIFAR10 dataset created by using reduced style images where stylisation was done by AdaIN style transfer.\n",
    "\n",
    "The following cells call the function created above to load the training, validation and testing datasets of the normal, stylised and reduced stylised CIFAR10 datasets and transform them into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 36000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load normal CIFAR10\n",
    "data_path_train = \"../../CIFAR/cifar32/training\"\n",
    "dataset_train = get_cifar_datasets(data_path_train)\n",
    "\n",
    "data_path_val = \"../../CIFAR/cifar32/validation/\"\n",
    "dataset_val = get_cifar_datasets(data_path_val)\n",
    "\n",
    "data_path_test = \"../../CIFAR/cifar32/testing/\"\n",
    "dataset_test = get_cifar_datasets(data_path_test)\n",
    "\n",
    "print(f\"Number of train samples {dataset_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of val samples {dataset_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of test samples {dataset_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val = DataLoader(dataset_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with original kaggle images\n",
    "data_path_train_style = \"../../CIFAR/cifar32_style/training\"\n",
    "dataset_train_style = get_cifar_datasets(data_path_train_style)\n",
    "\n",
    "data_path_val_style = \"../../CIFAR/cifar32_style/validation/\"\n",
    "dataset_val_style = get_cifar_datasets(data_path_val_style)\n",
    "\n",
    "data_path_test_style = \"../../CIFAR/cifar32_style/testing/\"\n",
    "dataset_test_style = get_cifar_datasets(data_path_test_style)\n",
    "\n",
    "print(f\"Number of stylised train samples {dataset_train_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised val samples {dataset_val_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised test samples {dataset_test_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style = DataLoader(dataset_train_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style = DataLoader(dataset_val_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style = DataLoader(dataset_test_style, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with reduced kaggle images\n",
    "data_path_train_style_red = \"../../CIFAR/cifar32_style_red/training\"\n",
    "dataset_train_style_red = get_cifar_datasets(data_path_train_style_red)\n",
    "\n",
    "data_path_val_style_red = \"../../CIFAR/cifar32_style_red/validation/\"\n",
    "dataset_val_style_red = get_cifar_datasets(data_path_val_style_red)\n",
    "\n",
    "data_path_test_style_red = \"../../CIFAR/cifar32_style_red/testing/\"\n",
    "dataset_test_style_red = get_cifar_datasets(data_path_test_style_red)\n",
    "\n",
    "print(f\"Number of reduced stylised train samples {dataset_train_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised val samples {dataset_val_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised test samples {dataset_test_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style_red.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style_red = DataLoader(dataset_train_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style_red = DataLoader(dataset_val_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style_red = DataLoader(dataset_test_style_red, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch['image'] torch.Size([64, 3, 32, 32])\n",
      "Shape of batch['cls'] torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the data was loaded correctly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(8, 8))\n",
    "\n",
    "for batch in data_loader_test_style:\n",
    "\n",
    "    print(f\"Shape of batch['image'] {batch['image'].shape}\")\n",
    "    print(f\"Shape of batch['cls'] {batch['cls'].shape}\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        col = i % 4\n",
    "        row = i // 4\n",
    "\n",
    "        img = batch['image'][i].numpy()\n",
    "\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(batch['class_name'][i])\n",
    "        axes[row,col].imshow(np.transpose(img,(1,2,0)))\n",
    "                         \n",
    "        if i >= 15:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the training, validation and testing functions for the experiment. train_part() function trains and updates gradients on each batch of the training set and once that is done, it tests its accuracy on the validation set. Every time the validation test returns a better accuracy than the current maximum, the model is saved and carries on with the next epoch. Then it checks with the learning reate scheduler for any changes in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "    \n",
    "print_every = 100\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "            scores = model(inputs)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct, accuracy of the dataset is: %.3f %%' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_part(model, train_data, val_data, model_path, optimizer, lr_scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "    val_acc = 0\n",
    "    num_epoch = 2\n",
    "    # Main Loop\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        val_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for i, batch in enumerate(train_data, 0):\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "\n",
    "            # get outputs from the input data and calculate the cross entropy loss\n",
    "            scores = model(inputs)\n",
    "            loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "            # zero and update the gradients and optimise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i, batch in enumerate(val_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['cls'].to(device)\n",
    "\n",
    "                # get the outputs from the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # compute accuracy based on the outputs\n",
    "                _, preds = outputs.max(1)\n",
    "                num_correct += (preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "            if acc > val_acc:\n",
    "                print('saving model')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                val_acc = acc\n",
    "            else:\n",
    "                print('skip model saving')\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Stylised ResNet50 Training\n",
    "\n",
    "The model used in this experiment is ResNet50 from torchvision library trained for 350 epochs with Adam optimiser, learning rate scheduler and default settings. The dataset used to train the model is reduced stylised CIFAR10. Learning rate starts at 0.1 and changes every 150, 250th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 4.941302\n",
      "[1,   400] loss: 2.334083\n",
      "[1,   600] loss: 2.325524\n",
      "[1,   800] loss: 2.319075\n",
      "[1,  1000] loss: 2.325960\n",
      "[1,  1200] loss: 2.317284\n",
      "[1,  1400] loss: 2.313965\n",
      "[1,  1600] loss: 2.314175\n",
      "[1,  1800] loss: 2.312116\n",
      "[1,  2000] loss: 2.311848\n",
      "[1,  2200] loss: 2.312420\n",
      "[1,  2400] loss: 2.312505\n",
      "[1,  2600] loss: 2.311522\n",
      "[1,  2800] loss: 2.311491\n",
      "[1,  3000] loss: 2.310222\n",
      "[1,  3200] loss: 2.312211\n",
      "Got 415 / 4000 correct (10.38)\n",
      "saving model\n",
      "[2,   200] loss: 2.311938\n",
      "[2,   400] loss: 2.312847\n",
      "[2,   600] loss: 2.310343\n",
      "[2,   800] loss: 2.311158\n",
      "[2,  1000] loss: 2.311969\n",
      "[2,  1200] loss: 2.310462\n",
      "[2,  1400] loss: 2.312093\n",
      "[2,  1600] loss: 2.310948\n",
      "[2,  1800] loss: 2.312190\n",
      "[2,  2000] loss: 2.312671\n",
      "[2,  2200] loss: 2.313061\n",
      "[2,  2400] loss: 2.311260\n",
      "[2,  2600] loss: 2.310213\n",
      "[2,  2800] loss: 2.310212\n",
      "[2,  3000] loss: 2.311886\n",
      "[2,  3200] loss: 2.312802\n",
      "Got 422 / 4000 correct (10.55)\n",
      "saving model\n",
      "[3,   200] loss: 2.311154\n",
      "[3,   400] loss: 2.311189\n",
      "[3,   600] loss: 2.311545\n",
      "[3,   800] loss: 2.310665\n",
      "[3,  1000] loss: 2.311995\n",
      "[3,  1200] loss: 2.313552\n",
      "[3,  1400] loss: 2.310420\n",
      "[3,  1600] loss: 2.311987\n",
      "[3,  1800] loss: 2.310024\n",
      "[3,  2000] loss: 2.312652\n",
      "[3,  2200] loss: 2.311828\n",
      "[3,  2400] loss: 2.312577\n",
      "[3,  2600] loss: 2.313067\n",
      "[3,  2800] loss: 2.310427\n",
      "[3,  3000] loss: 2.312764\n",
      "[3,  3200] loss: 2.314730\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[4,   200] loss: 2.311450\n",
      "[4,   400] loss: 2.310818\n",
      "[4,   600] loss: 2.314575\n",
      "[4,   800] loss: 2.312989\n",
      "[4,  1000] loss: 2.310305\n",
      "[4,  1200] loss: 2.311160\n",
      "[4,  1400] loss: 2.310918\n",
      "[4,  1600] loss: 2.313516\n",
      "[4,  1800] loss: 2.311755\n",
      "[4,  2000] loss: 2.310877\n",
      "[4,  2200] loss: 2.311581\n",
      "[4,  2400] loss: 2.313434\n",
      "[4,  2600] loss: 2.309093\n",
      "[4,  2800] loss: 2.311023\n",
      "[4,  3000] loss: 2.310707\n",
      "[4,  3200] loss: 2.310465\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[5,   200] loss: 2.310912\n",
      "[5,   400] loss: 2.311365\n",
      "[5,   600] loss: 2.312343\n",
      "[5,   800] loss: 2.312215\n",
      "[5,  1000] loss: 2.310841\n",
      "[5,  1200] loss: 2.310688\n",
      "[5,  1400] loss: 2.309964\n",
      "[5,  1600] loss: 2.312015\n",
      "[5,  1800] loss: 2.312293\n",
      "[5,  2000] loss: 2.310479\n",
      "[5,  2200] loss: 2.312066\n",
      "[5,  2400] loss: 2.311485\n",
      "[5,  2600] loss: 2.311581\n",
      "[5,  2800] loss: 2.312075\n",
      "[5,  3000] loss: 2.310980\n",
      "[5,  3200] loss: 2.310877\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[6,   200] loss: 2.310648\n",
      "[6,   400] loss: 2.312145\n",
      "[6,   600] loss: 2.310629\n",
      "[6,   800] loss: 2.311528\n",
      "[6,  1000] loss: 11.040587\n",
      "[6,  1200] loss: 6.357061\n",
      "[6,  1400] loss: 2.324235\n",
      "[6,  1600] loss: 2.316924\n",
      "[6,  1800] loss: 2.308794\n",
      "[6,  2000] loss: 2.527862\n",
      "[6,  2200] loss: 2.317012\n",
      "[6,  2400] loss: 2.309347\n",
      "[6,  2600] loss: 2.310773\n",
      "[6,  2800] loss: 2.311731\n",
      "[6,  3000] loss: 2.311589\n",
      "[6,  3200] loss: 2.314028\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[7,   200] loss: 2.309695\n",
      "[7,   400] loss: 2.310916\n",
      "[7,   600] loss: 2.311510\n",
      "[7,   800] loss: 2.311326\n",
      "[7,  1000] loss: 2.311995\n",
      "[7,  1200] loss: 2.310697\n",
      "[7,  1400] loss: 2.327175\n",
      "[7,  1600] loss: 2.315761\n",
      "[7,  1800] loss: 2.311097\n",
      "[7,  2000] loss: 2.312248\n",
      "[7,  2200] loss: 2.312484\n",
      "[7,  2400] loss: 2.413031\n",
      "[7,  2600] loss: 2.347141\n",
      "[7,  2800] loss: 2.313570\n",
      "[7,  3000] loss: 2.311631\n",
      "[7,  3200] loss: 2.311894\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[8,   200] loss: 2.356855\n",
      "[8,   400] loss: 2.312298\n",
      "[8,   600] loss: 2.311394\n",
      "[8,   800] loss: 2.311091\n",
      "[8,  1000] loss: 2.324190\n",
      "[8,  1200] loss: 2.910884\n",
      "[8,  1400] loss: 2.405767\n",
      "[8,  1600] loss: 2.312706\n",
      "[8,  1800] loss: 2.337823\n",
      "[8,  2000] loss: 2.311012\n",
      "[8,  2200] loss: 2.310553\n",
      "[8,  2400] loss: 2.313952\n",
      "[8,  2600] loss: 2.312148\n",
      "[8,  2800] loss: 2.311352\n",
      "[8,  3000] loss: 2.309814\n",
      "[8,  3200] loss: 2.311399\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[9,   200] loss: 2.377890\n",
      "[9,   400] loss: 2.320082\n",
      "[9,   600] loss: 2.312194\n",
      "[9,   800] loss: 2.311946\n",
      "[9,  1000] loss: 2.311174\n",
      "[9,  1200] loss: 2.312910\n",
      "[9,  1400] loss: 2.309890\n",
      "[9,  1600] loss: 2.311836\n",
      "[9,  1800] loss: 2.313013\n",
      "[9,  2000] loss: 2.312544\n",
      "[9,  2200] loss: 2.311254\n",
      "[9,  2400] loss: 2.311085\n",
      "[9,  2600] loss: 2.311668\n",
      "[9,  2800] loss: 2.312521\n",
      "[9,  3000] loss: 2.312489\n",
      "[9,  3200] loss: 2.311048\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[10,   200] loss: 2.311306\n",
      "[10,   400] loss: 2.311524\n",
      "[10,   600] loss: 2.311531\n",
      "[10,   800] loss: 2.311039\n",
      "[10,  1000] loss: 2.313622\n",
      "[10,  1200] loss: 2.313147\n",
      "[10,  1400] loss: 2.311292\n",
      "[10,  1600] loss: 2.312492\n",
      "[10,  1800] loss: 2.310609\n",
      "[10,  2000] loss: 2.310414\n",
      "[10,  2200] loss: 2.311364\n",
      "[10,  2400] loss: 2.310956\n",
      "[10,  2600] loss: 2.312480\n",
      "[10,  2800] loss: 2.312422\n",
      "[10,  3000] loss: 2.514615\n",
      "[10,  3200] loss: 2.464100\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[11,   200] loss: 2.310180\n",
      "[11,   400] loss: 2.310676\n",
      "[11,   600] loss: 2.310374\n",
      "[11,   800] loss: 2.310174\n",
      "[11,  1000] loss: 2.312724\n",
      "[11,  1200] loss: 2.311352\n",
      "[11,  1400] loss: 2.328580\n",
      "[11,  1600] loss: 2.318615\n",
      "[11,  1800] loss: 2.312692\n",
      "[11,  2000] loss: 2.311847\n",
      "[11,  2200] loss: 2.311212\n",
      "[11,  2400] loss: 2.312853\n",
      "[11,  2600] loss: 2.311274\n",
      "[11,  2800] loss: 2.310498\n",
      "[11,  3000] loss: 2.313548\n",
      "[11,  3200] loss: 2.358444\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[12,   200] loss: 2.311295\n",
      "[12,   400] loss: 2.314074\n",
      "[12,   600] loss: 2.313738\n",
      "[12,   800] loss: 2.309955\n",
      "[12,  1000] loss: 2.311057\n",
      "[12,  1200] loss: 2.313524\n",
      "[12,  1400] loss: 2.311373\n",
      "[12,  1600] loss: 2.312050\n",
      "[12,  1800] loss: 2.311400\n",
      "[12,  2000] loss: 2.311303\n",
      "[12,  2200] loss: 2.313124\n",
      "[12,  2400] loss: 2.310062\n",
      "[12,  2600] loss: 2.311128\n",
      "[12,  2800] loss: 2.310774\n",
      "[12,  3000] loss: 2.312561\n",
      "[12,  3200] loss: 2.312677\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[13,   200] loss: 2.309954\n",
      "[13,   400] loss: 2.311001\n",
      "[13,   600] loss: 2.312812\n",
      "[13,   800] loss: 2.311523\n",
      "[13,  1000] loss: 2.311068\n",
      "[13,  1200] loss: 2.311620\n",
      "[13,  1400] loss: 2.312040\n",
      "[13,  1600] loss: 2.311340\n",
      "[13,  1800] loss: 2.312011\n",
      "[13,  2000] loss: 2.310884\n",
      "[13,  2200] loss: 2.312188\n",
      "[13,  2400] loss: 2.312816\n",
      "[13,  2600] loss: 2.312090\n",
      "[13,  2800] loss: 2.310255\n",
      "[13,  3000] loss: 2.310739\n",
      "[13,  3200] loss: 2.311496\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[14,   200] loss: 2.310496\n",
      "[14,   400] loss: 2.311841\n",
      "[14,   600] loss: 2.311541\n",
      "[14,   800] loss: 2.309561\n",
      "[14,  1000] loss: 2.311529\n",
      "[14,  1200] loss: 2.311976\n",
      "[14,  1400] loss: 2.310972\n",
      "[14,  1600] loss: 2.311954\n",
      "[14,  1800] loss: 2.310556\n",
      "[14,  2000] loss: 2.311238\n",
      "[14,  2200] loss: 2.311261\n",
      "[14,  2400] loss: 2.312147\n",
      "[14,  2600] loss: 2.310655\n",
      "[14,  2800] loss: 2.310436\n",
      "[14,  3000] loss: 2.311568\n",
      "[14,  3200] loss: 2.310700\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[15,   200] loss: 2.310925\n",
      "[15,   400] loss: 2.311101\n",
      "[15,   600] loss: 2.312741\n",
      "[15,   800] loss: 2.311140\n",
      "[15,  1000] loss: 2.311344\n",
      "[15,  1200] loss: 2.310970\n",
      "[15,  1400] loss: 2.309982\n",
      "[15,  1600] loss: 2.309760\n",
      "[15,  1800] loss: 2.311314\n",
      "[15,  2000] loss: 2.310575\n",
      "[15,  2200] loss: 2.311194\n",
      "[15,  2400] loss: 2.311514\n",
      "[15,  2600] loss: 2.310895\n",
      "[15,  2800] loss: 2.310712\n",
      "[15,  3000] loss: 2.733637\n",
      "[15,  3200] loss: 2.573434\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[16,   200] loss: 2.336088\n",
      "[16,   400] loss: 2.311166\n",
      "[16,   600] loss: 2.311432\n",
      "[16,   800] loss: 2.309404\n",
      "[16,  1000] loss: 2.311121\n",
      "[16,  1200] loss: 2.314729\n",
      "[16,  1400] loss: 2.311608\n",
      "[16,  1600] loss: 2.310642\n",
      "[16,  1800] loss: 2.331244\n",
      "[16,  2000] loss: 2.310525\n",
      "[16,  2200] loss: 2.313891\n",
      "[16,  2400] loss: 2.311598\n",
      "[16,  2600] loss: 2.312431\n",
      "[16,  2800] loss: 2.310146\n",
      "[16,  3000] loss: 2.309882\n",
      "[16,  3200] loss: 2.312065\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[17,   200] loss: 2.309881\n",
      "[17,   400] loss: 2.311714\n",
      "[17,   600] loss: 2.310898\n",
      "[17,   800] loss: 2.311156\n",
      "[17,  1000] loss: 2.312564\n",
      "[17,  1200] loss: 2.313443\n",
      "[17,  1400] loss: 2.310622\n",
      "[17,  1600] loss: 2.311810\n",
      "[17,  1800] loss: 2.312210\n",
      "[17,  2000] loss: 2.310147\n",
      "[17,  2200] loss: 2.311092\n",
      "[17,  2400] loss: 2.310269\n",
      "[17,  2600] loss: 2.312279\n",
      "[17,  2800] loss: 2.310821\n",
      "[17,  3000] loss: 2.310321\n",
      "[17,  3200] loss: 2.311329\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[18,   200] loss: 2.312740\n",
      "[18,   400] loss: 2.311525\n",
      "[18,   600] loss: 2.310555\n",
      "[18,   800] loss: 2.311809\n",
      "[18,  1000] loss: 2.310391\n",
      "[18,  1200] loss: 2.311749\n",
      "[18,  1400] loss: 2.310417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,  1600] loss: 2.309572\n",
      "[18,  1800] loss: 2.311227\n",
      "[18,  2000] loss: 2.310047\n",
      "[18,  2200] loss: 2.310004\n",
      "[18,  2400] loss: 2.313919\n",
      "[18,  2600] loss: 2.311832\n",
      "[18,  2800] loss: 2.311329\n",
      "[18,  3000] loss: 2.312383\n",
      "[18,  3200] loss: 2.310662\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[19,   200] loss: 2.312026\n",
      "[19,   400] loss: 2.311295\n",
      "[19,   600] loss: 2.309715\n",
      "[19,   800] loss: 2.309968\n",
      "[19,  1000] loss: 2.673386\n",
      "[19,  1200] loss: 2.655823\n",
      "[19,  1400] loss: 2.312259\n",
      "[19,  1600] loss: 2.310226\n",
      "[19,  1800] loss: 2.310960\n",
      "[19,  2000] loss: 2.308966\n",
      "[19,  2200] loss: 2.324784\n",
      "[19,  2400] loss: 2.321045\n",
      "[19,  2600] loss: 2.310272\n",
      "[19,  2800] loss: 2.310360\n",
      "[19,  3000] loss: 2.311622\n",
      "[19,  3200] loss: 2.311844\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[20,   200] loss: 2.310631\n",
      "[20,   400] loss: 2.308942\n",
      "[20,   600] loss: 2.309677\n",
      "[20,   800] loss: 2.311699\n",
      "[20,  1000] loss: 2.311887\n",
      "[20,  1200] loss: 2.311492\n",
      "[20,  1400] loss: 2.311053\n",
      "[20,  1600] loss: 2.311597\n",
      "[20,  1800] loss: 2.310166\n",
      "[20,  2000] loss: 2.312398\n",
      "[20,  2200] loss: 2.312166\n",
      "[20,  2400] loss: 2.311992\n",
      "[20,  2600] loss: 2.310972\n",
      "[20,  2800] loss: 2.311772\n",
      "[20,  3000] loss: 2.312760\n",
      "[20,  3200] loss: 2.311945\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[21,   200] loss: 2.309799\n",
      "[21,   400] loss: 2.310244\n",
      "[21,   600] loss: 2.310169\n",
      "[21,   800] loss: 2.311316\n",
      "[21,  1000] loss: 2.311253\n",
      "[21,  1200] loss: 2.309944\n",
      "[21,  1400] loss: 2.312712\n",
      "[21,  1600] loss: 2.313108\n",
      "[21,  1800] loss: 2.312326\n",
      "[21,  2000] loss: 2.312314\n",
      "[21,  2200] loss: 2.311227\n",
      "[21,  2400] loss: 2.311930\n",
      "[21,  2600] loss: 2.312020\n",
      "[21,  2800] loss: 2.312441\n",
      "[21,  3000] loss: 2.311479\n",
      "[21,  3200] loss: 2.309431\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[22,   200] loss: 2.310213\n",
      "[22,   400] loss: 2.310372\n",
      "[22,   600] loss: 2.313680\n",
      "[22,   800] loss: 2.312137\n",
      "[22,  1000] loss: 2.312845\n",
      "[22,  1200] loss: 2.310437\n",
      "[22,  1400] loss: 2.312266\n",
      "[22,  1600] loss: 2.310061\n",
      "[22,  1800] loss: 2.310076\n",
      "[22,  2000] loss: 2.310724\n",
      "[22,  2200] loss: 2.312127\n",
      "[22,  2400] loss: 2.310959\n",
      "[22,  2600] loss: 2.312533\n",
      "[22,  2800] loss: 2.311144\n",
      "[22,  3000] loss: 2.313617\n",
      "[22,  3200] loss: 2.314283\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[23,   200] loss: 2.310527\n",
      "[23,   400] loss: 2.309435\n",
      "[23,   600] loss: 2.311826\n",
      "[23,   800] loss: 2.311315\n",
      "[23,  1000] loss: 2.313039\n",
      "[23,  1200] loss: 2.311040\n",
      "[23,  1400] loss: 2.312177\n",
      "[23,  1600] loss: 2.309712\n",
      "[23,  1800] loss: 2.313669\n",
      "[23,  2000] loss: 2.311463\n",
      "[23,  2200] loss: 2.311397\n",
      "[23,  2400] loss: 2.310331\n",
      "[23,  2600] loss: 2.312052\n",
      "[23,  2800] loss: 2.311041\n",
      "[23,  3000] loss: 2.312446\n",
      "[23,  3200] loss: 2.309796\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[24,   200] loss: 2.310690\n",
      "[24,   400] loss: 2.311444\n",
      "[24,   600] loss: 2.312896\n",
      "[24,   800] loss: 2.310957\n",
      "[24,  1000] loss: 2.310688\n",
      "[24,  1200] loss: 2.311478\n",
      "[24,  1400] loss: 2.311672\n",
      "[24,  1600] loss: 2.311895\n",
      "[24,  1800] loss: 2.311390\n",
      "[24,  2000] loss: 2.311618\n",
      "[24,  2200] loss: 2.311254\n",
      "[24,  2400] loss: 2.310455\n",
      "[24,  2600] loss: 2.309248\n",
      "[24,  2800] loss: 2.313050\n",
      "[24,  3000] loss: 2.311599\n",
      "[24,  3200] loss: 2.311391\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[25,   200] loss: 2.310728\n",
      "[25,   400] loss: 2.312733\n",
      "[25,   600] loss: 2.312273\n",
      "[25,   800] loss: 2.315003\n",
      "[25,  1000] loss: 2.311754\n",
      "[25,  1200] loss: 2.311059\n",
      "[25,  1400] loss: 2.312426\n",
      "[25,  1600] loss: 2.312020\n",
      "[25,  1800] loss: 2.311107\n",
      "[25,  2000] loss: 2.311507\n",
      "[25,  2200] loss: 2.310232\n",
      "[25,  2400] loss: 2.310748\n",
      "[25,  2600] loss: 2.311873\n",
      "[25,  2800] loss: 2.310804\n",
      "[25,  3000] loss: 2.311304\n",
      "[25,  3200] loss: 2.311890\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[26,   200] loss: 2.312351\n",
      "[26,   400] loss: 2.310528\n",
      "[26,   600] loss: 3.138565\n",
      "[26,   800] loss: 2.445618\n",
      "[26,  1000] loss: 2.313959\n",
      "[26,  1200] loss: 2.324281\n",
      "[26,  1400] loss: 2.313609\n",
      "[26,  1600] loss: 2.313645\n",
      "[26,  1800] loss: 2.311735\n",
      "[26,  2000] loss: 2.311882\n",
      "[26,  2200] loss: 2.311195\n",
      "[26,  2400] loss: 2.312424\n",
      "[26,  2600] loss: 2.311259\n",
      "[26,  2800] loss: 2.311335\n",
      "[26,  3000] loss: 2.312027\n",
      "[26,  3200] loss: 2.313482\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[27,   200] loss: 2.312331\n",
      "[27,   400] loss: 2.311134\n",
      "[27,   600] loss: 2.311805\n",
      "[27,   800] loss: 2.311816\n",
      "[27,  1000] loss: 2.312954\n",
      "[27,  1200] loss: 2.311662\n",
      "[27,  1400] loss: 2.311192\n",
      "[27,  1600] loss: 2.310964\n",
      "[27,  1800] loss: 2.311819\n",
      "[27,  2000] loss: 2.312974\n",
      "[27,  2200] loss: 2.312377\n",
      "[27,  2400] loss: 2.310938\n",
      "[27,  2600] loss: 2.310733\n",
      "[27,  2800] loss: 2.311714\n",
      "[27,  3000] loss: 2.309599\n",
      "[27,  3200] loss: 2.313623\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[28,   200] loss: 2.310505\n",
      "[28,   400] loss: 2.310872\n",
      "[28,   600] loss: 2.311839\n",
      "[28,   800] loss: 2.310730\n",
      "[28,  1000] loss: 2.309993\n",
      "[28,  1200] loss: 2.312281\n",
      "[28,  1400] loss: 2.314565\n",
      "[28,  1600] loss: 2.311085\n",
      "[28,  1800] loss: 2.311820\n",
      "[28,  2000] loss: 2.312197\n",
      "[28,  2200] loss: 2.310811\n",
      "[28,  2400] loss: 2.310436\n",
      "[28,  2600] loss: 2.313275\n",
      "[28,  2800] loss: 2.312515\n",
      "[28,  3000] loss: 2.313640\n",
      "[28,  3200] loss: 2.312552\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[29,   200] loss: 2.311359\n",
      "[29,   400] loss: 2.310356\n",
      "[29,   600] loss: 2.313031\n",
      "[29,   800] loss: 2.311098\n",
      "[29,  1000] loss: 2.309080\n",
      "[29,  1200] loss: 2.311462\n",
      "[29,  1400] loss: 2.314804\n",
      "[29,  1600] loss: 2.310873\n",
      "[29,  1800] loss: 2.310659\n",
      "[29,  2000] loss: 2.309828\n",
      "[29,  2200] loss: 2.313118\n",
      "[29,  2400] loss: 2.311597\n",
      "[29,  2600] loss: 2.311256\n",
      "[29,  2800] loss: 2.309853\n",
      "[29,  3000] loss: 2.309731\n",
      "[29,  3200] loss: 2.309041\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[30,   200] loss: 2.311042\n",
      "[30,   400] loss: 2.310580\n",
      "[30,   600] loss: 2.312424\n",
      "[30,   800] loss: 2.311975\n",
      "[30,  1000] loss: 2.310056\n",
      "[30,  1200] loss: 2.313443\n",
      "[30,  1400] loss: 2.309812\n",
      "[30,  1600] loss: 2.310214\n",
      "[30,  1800] loss: 2.309301\n",
      "[30,  2000] loss: 2.313777\n",
      "[30,  2200] loss: 2.310039\n",
      "[30,  2400] loss: 2.311460\n",
      "[30,  2600] loss: 2.312656\n",
      "[30,  2800] loss: 2.310526\n",
      "[30,  3000] loss: 2.313357\n",
      "[30,  3200] loss: 2.311595\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[31,   200] loss: 2.311809\n",
      "[31,   400] loss: 2.312555\n",
      "[31,   600] loss: 2.310304\n",
      "[31,   800] loss: 2.312952\n",
      "[31,  1000] loss: 2.312249\n",
      "[31,  1200] loss: 2.312759\n",
      "[31,  1400] loss: 2.311503\n",
      "[31,  1600] loss: 2.312228\n",
      "[31,  1800] loss: 2.310584\n",
      "[31,  2000] loss: 2.311132\n",
      "[31,  2200] loss: 2.310317\n",
      "[31,  2400] loss: 2.311807\n",
      "[31,  2600] loss: 2.312376\n",
      "[31,  2800] loss: 2.312468\n",
      "[31,  3000] loss: 2.311393\n",
      "[31,  3200] loss: 2.312854\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[32,   200] loss: 2.311357\n",
      "[32,   400] loss: 2.311229\n",
      "[32,   600] loss: 2.310984\n",
      "[32,   800] loss: 2.311802\n",
      "[32,  1000] loss: 2.310924\n",
      "[32,  1200] loss: 2.310639\n",
      "[32,  1400] loss: 2.309552\n",
      "[32,  1600] loss: 2.310488\n",
      "[32,  1800] loss: 2.311075\n",
      "[32,  2000] loss: 2.310651\n",
      "[32,  2200] loss: 2.312695\n",
      "[32,  2400] loss: 2.311278\n",
      "[32,  2600] loss: 2.311345\n",
      "[32,  2800] loss: 2.311322\n",
      "[32,  3000] loss: 2.311336\n",
      "[32,  3200] loss: 2.311598\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[33,   200] loss: 2.311057\n",
      "[33,   400] loss: 2.312226\n",
      "[33,   600] loss: 2.310764\n",
      "[33,   800] loss: 2.311775\n",
      "[33,  1000] loss: 2.309703\n",
      "[33,  1200] loss: 2.314614\n",
      "[33,  1400] loss: 2.310742\n",
      "[33,  1600] loss: 2.310610\n",
      "[33,  1800] loss: 2.312665\n",
      "[33,  2000] loss: 2.310024\n",
      "[33,  2200] loss: 2.310321\n",
      "[33,  2400] loss: 2.310271\n",
      "[33,  2600] loss: 2.311667\n",
      "[33,  2800] loss: 2.310220\n",
      "[33,  3000] loss: 2.310073\n",
      "[33,  3200] loss: 2.312598\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[34,   200] loss: 2.310225\n",
      "[34,   400] loss: 2.310532\n",
      "[34,   600] loss: 2.313061\n",
      "[34,   800] loss: 2.310332\n",
      "[34,  1000] loss: 2.312015\n",
      "[34,  1200] loss: 2.312268\n",
      "[34,  1400] loss: 2.310284\n",
      "[34,  1600] loss: 2.313692\n",
      "[34,  1800] loss: 2.313241\n",
      "[34,  2000] loss: 2.309229\n",
      "[34,  2200] loss: 2.311949\n",
      "[34,  2400] loss: 2.310629\n",
      "[34,  2600] loss: 2.312437\n",
      "[34,  2800] loss: 2.311179\n",
      "[34,  3000] loss: 2.313350\n",
      "[34,  3200] loss: 2.311196\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[35,   200] loss: 2.312061\n",
      "[35,   400] loss: 2.313462\n",
      "[35,   600] loss: 2.311480\n",
      "[35,   800] loss: 2.310972\n",
      "[35,  1000] loss: 2.312162\n",
      "[35,  1200] loss: 2.311471\n",
      "[35,  1400] loss: 2.310819\n",
      "[35,  1600] loss: 2.311140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,  1800] loss: 2.311226\n",
      "[35,  2000] loss: 2.311743\n",
      "[35,  2200] loss: 2.311286\n",
      "[35,  2400] loss: 2.312649\n",
      "[35,  2600] loss: 2.312550\n",
      "[35,  2800] loss: 2.310591\n",
      "[35,  3000] loss: 2.311984\n",
      "[35,  3200] loss: 2.311146\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[36,   200] loss: 2.310284\n",
      "[36,   400] loss: 2.310597\n",
      "[36,   600] loss: 2.311051\n",
      "[36,   800] loss: 2.310986\n",
      "[36,  1000] loss: 2.310283\n",
      "[36,  1200] loss: 2.311145\n",
      "[36,  1400] loss: 2.311968\n",
      "[36,  1600] loss: 2.310664\n",
      "[36,  1800] loss: 2.314053\n",
      "[36,  2000] loss: 2.311547\n",
      "[36,  2200] loss: 2.311263\n",
      "[36,  2400] loss: 2.311069\n",
      "[36,  2600] loss: 2.312144\n",
      "[36,  2800] loss: 2.312069\n",
      "[36,  3000] loss: 2.311025\n",
      "[36,  3200] loss: 2.311853\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[37,   200] loss: 2.310271\n",
      "[37,   400] loss: 2.309331\n",
      "[37,   600] loss: 2.310322\n",
      "[37,   800] loss: 2.310728\n",
      "[37,  1000] loss: 2.311789\n",
      "[37,  1200] loss: 2.312531\n",
      "[37,  1400] loss: 2.311667\n",
      "[37,  1600] loss: 2.310934\n",
      "[37,  1800] loss: 2.311500\n",
      "[37,  2000] loss: 2.312377\n",
      "[37,  2200] loss: 2.310995\n",
      "[37,  2400] loss: 2.311575\n",
      "[37,  2600] loss: 2.309687\n",
      "[37,  2800] loss: 2.311331\n",
      "[37,  3000] loss: 2.312851\n",
      "[37,  3200] loss: 2.309903\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[38,   200] loss: 2.313166\n",
      "[38,   400] loss: 2.310443\n",
      "[38,   600] loss: 2.310641\n",
      "[38,   800] loss: 2.311219\n",
      "[38,  1000] loss: 2.310277\n",
      "[38,  1200] loss: 2.310541\n",
      "[38,  1400] loss: 2.310462\n",
      "[38,  1600] loss: 2.311294\n",
      "[38,  1800] loss: 2.310464\n",
      "[38,  2000] loss: 2.311126\n",
      "[38,  2200] loss: 2.311562\n",
      "[38,  2400] loss: 2.312160\n",
      "[38,  2600] loss: 2.311580\n",
      "[38,  2800] loss: 2.311058\n",
      "[38,  3000] loss: 2.311539\n",
      "[38,  3200] loss: 2.312065\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[39,   200] loss: 2.311684\n",
      "[39,   400] loss: 2.312317\n",
      "[39,   600] loss: 2.309018\n",
      "[39,   800] loss: 2.312357\n",
      "[39,  1000] loss: 2.310820\n",
      "[39,  1200] loss: 2.311811\n",
      "[39,  1400] loss: 2.310789\n",
      "[39,  1600] loss: 2.312775\n",
      "[39,  1800] loss: 2.311501\n",
      "[39,  2000] loss: 2.311149\n",
      "[39,  2200] loss: 2.312346\n",
      "[39,  2400] loss: 2.310321\n",
      "[39,  2600] loss: 2.310344\n",
      "[39,  2800] loss: 2.311111\n",
      "[39,  3000] loss: 2.312378\n",
      "[39,  3200] loss: 2.310759\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[40,   200] loss: 2.311634\n",
      "[40,   400] loss: 2.313226\n",
      "[40,   600] loss: 2.309058\n",
      "[40,   800] loss: 2.312938\n",
      "[40,  1000] loss: 2.311554\n",
      "[40,  1200] loss: 2.310163\n",
      "[40,  1400] loss: 2.310035\n",
      "[40,  1600] loss: 2.311712\n",
      "[40,  1800] loss: 2.310758\n",
      "[40,  2000] loss: 2.310227\n",
      "[40,  2200] loss: 2.310628\n",
      "[40,  2400] loss: 2.311108\n",
      "[40,  2600] loss: 2.311773\n",
      "[40,  2800] loss: 2.313885\n",
      "[40,  3000] loss: 2.309518\n",
      "[40,  3200] loss: 2.311539\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[41,   200] loss: 2.309974\n",
      "[41,   400] loss: 2.312432\n",
      "[41,   600] loss: 2.308561\n",
      "[41,   800] loss: 2.311135\n",
      "[41,  1000] loss: 2.311782\n",
      "[41,  1200] loss: 2.311586\n",
      "[41,  1400] loss: 2.310789\n",
      "[41,  1600] loss: 2.310655\n",
      "[41,  1800] loss: 2.311190\n",
      "[41,  2000] loss: 2.312523\n",
      "[41,  2200] loss: 2.311689\n",
      "[41,  2400] loss: 2.313080\n",
      "[41,  2600] loss: 2.311759\n",
      "[41,  2800] loss: 2.311619\n",
      "[41,  3000] loss: 2.313144\n",
      "[41,  3200] loss: 2.311673\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[42,   200] loss: 2.312709\n",
      "[42,   400] loss: 2.310825\n",
      "[42,   600] loss: 2.313642\n",
      "[42,   800] loss: 2.313116\n",
      "[42,  1000] loss: 2.311390\n",
      "[42,  1200] loss: 2.310256\n",
      "[42,  1400] loss: 2.312809\n",
      "[42,  1600] loss: 2.311101\n",
      "[42,  1800] loss: 2.310002\n",
      "[42,  2000] loss: 2.312138\n",
      "[42,  2200] loss: 2.311058\n",
      "[42,  2400] loss: 2.312721\n",
      "[42,  2600] loss: 2.311472\n",
      "[42,  2800] loss: 2.308871\n",
      "[42,  3000] loss: 2.310131\n",
      "[42,  3200] loss: 2.310619\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[43,   200] loss: 2.311644\n",
      "[43,   400] loss: 2.312332\n",
      "[43,   600] loss: 2.310209\n",
      "[43,   800] loss: 2.310028\n",
      "[43,  1000] loss: 2.311627\n",
      "[43,  1200] loss: 2.310814\n",
      "[43,  1400] loss: 2.311514\n",
      "[43,  1600] loss: 2.310183\n",
      "[43,  1800] loss: 2.313865\n",
      "[43,  2000] loss: 2.311060\n",
      "[43,  2200] loss: 2.310831\n",
      "[43,  2400] loss: 2.310513\n",
      "[43,  2600] loss: 2.312330\n",
      "[43,  2800] loss: 2.311983\n",
      "[43,  3000] loss: 2.310532\n",
      "[43,  3200] loss: 2.311095\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[44,   200] loss: 2.309863\n",
      "[44,   400] loss: 2.310562\n",
      "[44,   600] loss: 2.310814\n",
      "[44,   800] loss: 2.313460\n",
      "[44,  1000] loss: 2.309890\n",
      "[44,  1200] loss: 2.312218\n",
      "[44,  1400] loss: 2.311263\n",
      "[44,  1600] loss: 2.311880\n",
      "[44,  1800] loss: 2.311024\n",
      "[44,  2000] loss: 2.311099\n",
      "[44,  2200] loss: 2.310789\n",
      "[44,  2400] loss: 2.311212\n",
      "[44,  2600] loss: 2.311424\n",
      "[44,  2800] loss: 2.312027\n",
      "[44,  3000] loss: 2.313547\n",
      "[44,  3200] loss: 2.311432\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[45,   200] loss: 2.311762\n",
      "[45,   400] loss: 2.311552\n",
      "[45,   600] loss: 2.311247\n",
      "[45,   800] loss: 2.311196\n",
      "[45,  1000] loss: 2.311294\n",
      "[45,  1200] loss: 2.311910\n",
      "[45,  1400] loss: 2.311882\n",
      "[45,  1600] loss: 2.312688\n",
      "[45,  1800] loss: 2.311421\n",
      "[45,  2000] loss: 2.312810\n",
      "[45,  2200] loss: 2.309194\n",
      "[45,  2400] loss: 2.311327\n",
      "[45,  2600] loss: 2.310736\n",
      "[45,  2800] loss: 2.310114\n",
      "[45,  3000] loss: 2.310656\n",
      "[45,  3200] loss: 2.311143\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[46,   200] loss: 2.311220\n",
      "[46,   400] loss: 2.310174\n",
      "[46,   600] loss: 2.311423\n",
      "[46,   800] loss: 2.310944\n",
      "[46,  1000] loss: 2.312749\n",
      "[46,  1200] loss: 2.310262\n",
      "[46,  1400] loss: 2.311332\n",
      "[46,  1600] loss: 2.308629\n",
      "[46,  1800] loss: 2.310484\n",
      "[46,  2000] loss: 2.310373\n",
      "[46,  2200] loss: 2.310933\n",
      "[46,  2400] loss: 2.311594\n",
      "[46,  2600] loss: 2.311548\n",
      "[46,  2800] loss: 2.310854\n",
      "[46,  3000] loss: 2.309727\n",
      "[46,  3200] loss: 2.313190\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[47,   200] loss: 2.310714\n",
      "[47,   400] loss: 2.311432\n",
      "[47,   600] loss: 2.311650\n",
      "[47,   800] loss: 2.311061\n",
      "[47,  1000] loss: 2.312882\n",
      "[47,  1200] loss: 2.312225\n",
      "[47,  1400] loss: 2.311291\n",
      "[47,  1600] loss: 2.310902\n",
      "[47,  1800] loss: 2.313932\n",
      "[47,  2000] loss: 2.309411\n",
      "[47,  2200] loss: 2.311597\n",
      "[47,  2400] loss: 2.313362\n",
      "[47,  2600] loss: 2.311325\n",
      "[47,  2800] loss: 2.310762\n",
      "[47,  3000] loss: 2.312130\n",
      "[47,  3200] loss: 2.311249\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[48,   200] loss: 2.312411\n",
      "[48,   400] loss: 2.312910\n",
      "[48,   600] loss: 2.310665\n",
      "[48,   800] loss: 2.311086\n",
      "[48,  1000] loss: 2.311356\n",
      "[48,  1200] loss: 2.310901\n",
      "[48,  1400] loss: 2.311082\n",
      "[48,  1600] loss: 2.312667\n",
      "[48,  1800] loss: 2.310243\n",
      "[48,  2000] loss: 2.311100\n",
      "[48,  2200] loss: 2.313667\n",
      "[48,  2400] loss: 2.313392\n",
      "[48,  2600] loss: 2.312152\n",
      "[48,  2800] loss: 2.313434\n",
      "[48,  3000] loss: 2.311746\n",
      "[48,  3200] loss: 2.309978\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[49,   200] loss: 2.312753\n",
      "[49,   400] loss: 2.312643\n",
      "[49,   600] loss: 2.310499\n",
      "[49,   800] loss: 2.310878\n",
      "[49,  1000] loss: 2.312380\n",
      "[49,  1200] loss: 2.314299\n",
      "[49,  1400] loss: 2.309076\n",
      "[49,  1600] loss: 2.310973\n",
      "[49,  1800] loss: 2.310716\n",
      "[49,  2000] loss: 2.312609\n",
      "[49,  2200] loss: 2.309971\n",
      "[49,  2400] loss: 2.312308\n",
      "[49,  2600] loss: 2.312595\n",
      "[49,  2800] loss: 2.311961\n",
      "[49,  3000] loss: 2.312328\n",
      "[49,  3200] loss: 2.311113\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[50,   200] loss: 2.312500\n",
      "[50,   400] loss: 2.312975\n",
      "[50,   600] loss: 2.309794\n",
      "[50,   800] loss: 2.311621\n",
      "[50,  1000] loss: 2.310404\n",
      "[50,  1200] loss: 2.311161\n",
      "[50,  1400] loss: 2.313137\n",
      "[50,  1600] loss: 2.309652\n",
      "[50,  1800] loss: 2.310581\n",
      "[50,  2000] loss: 2.312338\n",
      "[50,  2200] loss: 2.311946\n",
      "[50,  2400] loss: 2.310935\n",
      "[50,  2600] loss: 2.310396\n",
      "[50,  2800] loss: 2.312323\n",
      "[50,  3000] loss: 2.312093\n",
      "[50,  3200] loss: 2.311192\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[51,   200] loss: 2.311099\n",
      "[51,   400] loss: 2.310640\n",
      "[51,   600] loss: 2.313521\n",
      "[51,   800] loss: 2.312126\n",
      "[51,  1000] loss: 2.310404\n",
      "[51,  1200] loss: 2.311383\n",
      "[51,  1400] loss: 2.311720\n",
      "[51,  1600] loss: 2.310624\n",
      "[51,  1800] loss: 2.310141\n",
      "[51,  2000] loss: 2.310500\n",
      "[51,  2200] loss: 2.312463\n",
      "[51,  2400] loss: 2.308974\n",
      "[51,  2600] loss: 2.309964\n",
      "[51,  2800] loss: 2.311470\n",
      "[51,  3000] loss: 2.310014\n",
      "[51,  3200] loss: 2.310490\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[52,   200] loss: 2.313170\n",
      "[52,   400] loss: 2.309704\n",
      "[52,   600] loss: 2.313310\n",
      "[52,   800] loss: 2.312894\n",
      "[52,  1000] loss: 2.311261\n",
      "[52,  1200] loss: 2.310713\n",
      "[52,  1400] loss: 2.311386\n",
      "[52,  1600] loss: 2.309801\n",
      "[52,  1800] loss: 2.310818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52,  2000] loss: 2.311560\n",
      "[52,  2200] loss: 2.310510\n",
      "[52,  2400] loss: 2.312170\n",
      "[52,  2600] loss: 2.310195\n",
      "[52,  2800] loss: 2.313063\n",
      "[52,  3000] loss: 2.310026\n",
      "[52,  3200] loss: 2.312866\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[53,   200] loss: 2.311570\n",
      "[53,   400] loss: 2.310538\n",
      "[53,   600] loss: 2.313552\n",
      "[53,   800] loss: 2.310433\n",
      "[53,  1000] loss: 2.311436\n",
      "[53,  1200] loss: 2.310567\n",
      "[53,  1400] loss: 2.312040\n",
      "[53,  1600] loss: 2.309967\n",
      "[53,  1800] loss: 2.312061\n",
      "[53,  2000] loss: 2.311269\n",
      "[53,  2200] loss: 2.311668\n",
      "[53,  2400] loss: 2.311154\n",
      "[53,  2600] loss: 2.313072\n",
      "[53,  2800] loss: 2.310722\n",
      "[53,  3000] loss: 2.312253\n",
      "[53,  3200] loss: 2.310389\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[54,   200] loss: 2.308734\n",
      "[54,   400] loss: 2.312157\n",
      "[54,   600] loss: 2.311518\n",
      "[54,   800] loss: 2.310889\n",
      "[54,  1000] loss: 2.310459\n",
      "[54,  1200] loss: 2.310422\n",
      "[54,  1400] loss: 2.311010\n",
      "[54,  1600] loss: 2.310873\n",
      "[54,  1800] loss: 2.313071\n",
      "[54,  2000] loss: 2.311609\n",
      "[54,  2200] loss: 2.311711\n",
      "[54,  2400] loss: 2.311183\n",
      "[54,  2600] loss: 2.311575\n",
      "[54,  2800] loss: 2.311299\n",
      "[54,  3000] loss: 2.309341\n",
      "[54,  3200] loss: 2.312798\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[55,   200] loss: 2.310531\n",
      "[55,   400] loss: 2.311255\n",
      "[55,   600] loss: 2.312748\n",
      "[55,   800] loss: 2.310584\n",
      "[55,  1000] loss: 2.310371\n",
      "[55,  1200] loss: 2.311038\n",
      "[55,  1400] loss: 2.312011\n",
      "[55,  1600] loss: 2.311325\n",
      "[55,  1800] loss: 2.312227\n",
      "[55,  2000] loss: 2.310807\n",
      "[55,  2200] loss: 2.311806\n",
      "[55,  2400] loss: 2.308530\n",
      "[55,  2600] loss: 2.309104\n",
      "[55,  2800] loss: 2.311837\n",
      "[55,  3000] loss: 2.311014\n",
      "[55,  3200] loss: 2.310116\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[56,   200] loss: 2.310184\n",
      "[56,   400] loss: 2.310361\n",
      "[56,   600] loss: 2.311783\n",
      "[56,   800] loss: 2.312308\n",
      "[56,  1000] loss: 2.310323\n",
      "[56,  1200] loss: 2.313814\n",
      "[56,  1400] loss: 2.311549\n",
      "[56,  1600] loss: 2.310001\n",
      "[56,  1800] loss: 2.311233\n",
      "[56,  2000] loss: 2.312174\n",
      "[56,  2200] loss: 2.309830\n",
      "[56,  2400] loss: 2.312124\n",
      "[56,  2600] loss: 2.311065\n",
      "[56,  2800] loss: 2.310378\n",
      "[56,  3000] loss: 2.310508\n",
      "[56,  3200] loss: 2.313110\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[57,   200] loss: 2.310571\n",
      "[57,   400] loss: 2.309821\n",
      "[57,   600] loss: 2.311274\n",
      "[57,   800] loss: 2.313738\n",
      "[57,  1000] loss: 2.311683\n",
      "[57,  1200] loss: 2.312368\n",
      "[57,  1400] loss: 2.310751\n",
      "[57,  1600] loss: 2.312960\n",
      "[57,  1800] loss: 2.311236\n",
      "[57,  2000] loss: 2.313649\n",
      "[57,  2200] loss: 2.311565\n",
      "[57,  2400] loss: 2.311965\n",
      "[57,  2600] loss: 2.310075\n",
      "[57,  2800] loss: 2.309299\n",
      "[57,  3000] loss: 2.310444\n",
      "[57,  3200] loss: 2.311053\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[58,   200] loss: 2.312152\n",
      "[58,   400] loss: 2.311500\n",
      "[58,   600] loss: 2.309660\n",
      "[58,   800] loss: 2.314819\n",
      "[58,  1000] loss: 2.311793\n",
      "[58,  1200] loss: 2.311075\n",
      "[58,  1400] loss: 2.310244\n",
      "[58,  1600] loss: 2.311121\n",
      "[58,  1800] loss: 2.310839\n",
      "[58,  2000] loss: 2.311089\n",
      "[58,  2200] loss: 2.311454\n",
      "[58,  2400] loss: 2.312852\n",
      "[58,  2600] loss: 2.311265\n",
      "[58,  2800] loss: 2.311738\n",
      "[58,  3000] loss: 2.313301\n",
      "[58,  3200] loss: 2.311492\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[59,   200] loss: 2.309077\n",
      "[59,   400] loss: 2.310596\n",
      "[59,   600] loss: 2.312112\n",
      "[59,   800] loss: 2.309929\n",
      "[59,  1000] loss: 2.311480\n",
      "[59,  1200] loss: 2.312353\n",
      "[59,  1400] loss: 2.311745\n",
      "[59,  1600] loss: 2.310793\n",
      "[59,  1800] loss: 2.310345\n",
      "[59,  2000] loss: 2.313032\n",
      "[59,  2200] loss: 2.312548\n",
      "[59,  2400] loss: 2.310379\n",
      "[59,  2600] loss: 2.311492\n",
      "[59,  2800] loss: 2.312389\n",
      "[59,  3000] loss: 2.312366\n",
      "[59,  3200] loss: 2.311915\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[60,   200] loss: 2.311884\n",
      "[60,   400] loss: 2.312298\n",
      "[60,   600] loss: 2.311251\n",
      "[60,   800] loss: 2.309279\n",
      "[60,  1000] loss: 2.311378\n",
      "[60,  1200] loss: 2.311309\n",
      "[60,  1400] loss: 2.310569\n",
      "[60,  1600] loss: 2.310178\n",
      "[60,  1800] loss: 2.311682\n",
      "[60,  2000] loss: 2.310511\n",
      "[60,  2200] loss: 2.311485\n",
      "[60,  2400] loss: 2.311944\n",
      "[60,  2600] loss: 2.310774\n",
      "[60,  2800] loss: 2.313642\n",
      "[60,  3000] loss: 2.309914\n",
      "[60,  3200] loss: 2.313466\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[61,   200] loss: 2.312843\n",
      "[61,   400] loss: 2.311334\n",
      "[61,   600] loss: 2.311571\n",
      "[61,   800] loss: 2.312711\n",
      "[61,  1000] loss: 2.310442\n",
      "[61,  1200] loss: 2.310386\n",
      "[61,  1400] loss: 2.311947\n",
      "[61,  1600] loss: 2.311664\n",
      "[61,  1800] loss: 2.310007\n",
      "[61,  2000] loss: 2.312401\n",
      "[61,  2200] loss: 2.312785\n",
      "[61,  2400] loss: 2.312983\n",
      "[61,  2600] loss: 2.311820\n",
      "[61,  2800] loss: 2.311386\n",
      "[61,  3000] loss: 2.308496\n",
      "[61,  3200] loss: 2.310575\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[62,   200] loss: 2.311447\n",
      "[62,   400] loss: 2.310690\n",
      "[62,   600] loss: 2.311639\n",
      "[62,   800] loss: 2.310330\n",
      "[62,  1000] loss: 2.312547\n",
      "[62,  1200] loss: 2.311273\n",
      "[62,  1400] loss: 2.310562\n",
      "[62,  1600] loss: 2.310585\n",
      "[62,  1800] loss: 2.313180\n",
      "[62,  2000] loss: 2.312866\n",
      "[62,  2200] loss: 2.311308\n",
      "[62,  2400] loss: 2.312705\n",
      "[62,  2600] loss: 2.308932\n",
      "[62,  2800] loss: 2.312750\n",
      "[62,  3000] loss: 2.311588\n",
      "[62,  3200] loss: 2.311258\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[63,   200] loss: 2.307640\n",
      "[63,   400] loss: 2.313054\n",
      "[63,   600] loss: 2.309886\n",
      "[63,   800] loss: 2.310180\n",
      "[63,  1000] loss: 2.310693\n",
      "[63,  1200] loss: 2.312420\n",
      "[63,  1400] loss: 2.309790\n",
      "[63,  1600] loss: 2.311785\n",
      "[63,  1800] loss: 2.311878\n",
      "[63,  2000] loss: 2.312995\n",
      "[63,  2200] loss: 2.310744\n",
      "[63,  2400] loss: 2.312820\n",
      "[63,  2600] loss: 2.314192\n",
      "[63,  2800] loss: 2.312244\n",
      "[63,  3000] loss: 2.311996\n",
      "[63,  3200] loss: 2.310928\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[64,   200] loss: 2.311491\n",
      "[64,   400] loss: 2.310626\n",
      "[64,   600] loss: 2.310413\n",
      "[64,   800] loss: 2.311683\n",
      "[64,  1000] loss: 2.312098\n",
      "[64,  1200] loss: 2.312067\n",
      "[64,  1400] loss: 2.311045\n",
      "[64,  1600] loss: 2.312453\n",
      "[64,  1800] loss: 2.310960\n",
      "[64,  2000] loss: 2.311404\n",
      "[64,  2200] loss: 2.312287\n",
      "[64,  2400] loss: 2.310886\n",
      "[64,  2600] loss: 2.312317\n",
      "[64,  2800] loss: 2.312915\n",
      "[64,  3000] loss: 2.311389\n",
      "[64,  3200] loss: 2.311339\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[65,   200] loss: 2.310154\n",
      "[65,   400] loss: 2.312585\n",
      "[65,   600] loss: 2.310736\n",
      "[65,   800] loss: 2.310127\n",
      "[65,  1000] loss: 2.310092\n",
      "[65,  1200] loss: 2.311663\n",
      "[65,  1400] loss: 2.312479\n",
      "[65,  1600] loss: 2.313801\n",
      "[65,  1800] loss: 2.312324\n",
      "[65,  2000] loss: 2.310108\n",
      "[65,  2200] loss: 2.311864\n",
      "[65,  2400] loss: 2.312095\n",
      "[65,  2600] loss: 2.311407\n",
      "[65,  2800] loss: 2.310467\n",
      "[65,  3000] loss: 2.311683\n",
      "[65,  3200] loss: 2.310908\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[66,   200] loss: 2.311820\n",
      "[66,   400] loss: 2.311476\n",
      "[66,   600] loss: 2.312070\n",
      "[66,   800] loss: 2.311000\n",
      "[66,  1000] loss: 2.311044\n",
      "[66,  1200] loss: 2.309969\n",
      "[66,  1400] loss: 2.312174\n",
      "[66,  1600] loss: 2.309256\n",
      "[66,  1800] loss: 2.312401\n",
      "[66,  2000] loss: 2.311192\n",
      "[66,  2200] loss: 2.312829\n",
      "[66,  2400] loss: 2.311245\n",
      "[66,  2600] loss: 2.313079\n",
      "[66,  2800] loss: 2.310435\n",
      "[66,  3000] loss: 2.311007\n",
      "[66,  3200] loss: 2.310189\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[67,   200] loss: 2.314426\n",
      "[67,   400] loss: 2.311390\n",
      "[67,   600] loss: 2.310960\n",
      "[67,   800] loss: 2.312566\n",
      "[67,  1000] loss: 2.312559\n",
      "[67,  1200] loss: 2.312315\n",
      "[67,  1400] loss: 2.310195\n",
      "[67,  1600] loss: 2.311323\n",
      "[67,  1800] loss: 2.311990\n",
      "[67,  2000] loss: 2.310894\n",
      "[67,  2200] loss: 2.310610\n",
      "[67,  2400] loss: 2.311137\n",
      "[67,  2600] loss: 2.311204\n",
      "[67,  2800] loss: 2.310896\n",
      "[67,  3000] loss: 2.311717\n",
      "[67,  3200] loss: 2.311690\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[68,   200] loss: 2.312493\n",
      "[68,   400] loss: 2.312097\n",
      "[68,   600] loss: 2.311722\n",
      "[68,   800] loss: 2.311202\n",
      "[68,  1000] loss: 2.312159\n",
      "[68,  1200] loss: 2.311480\n",
      "[68,  1400] loss: 2.311568\n",
      "[68,  1600] loss: 2.311534\n",
      "[68,  1800] loss: 2.311548\n",
      "[68,  2000] loss: 2.309437\n",
      "[68,  2200] loss: 2.311451\n",
      "[68,  2400] loss: 2.310555\n",
      "[68,  2600] loss: 2.311681\n",
      "[68,  2800] loss: 2.310417\n",
      "[68,  3000] loss: 2.310364\n",
      "[68,  3200] loss: 2.311663\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[69,   200] loss: 2.310994\n",
      "[69,   400] loss: 2.311710\n",
      "[69,   600] loss: 2.311153\n",
      "[69,   800] loss: 2.312047\n",
      "[69,  1000] loss: 2.312107\n",
      "[69,  1200] loss: 2.312132\n",
      "[69,  1400] loss: 2.311545\n",
      "[69,  1600] loss: 2.311754\n",
      "[69,  1800] loss: 2.310896\n",
      "[69,  2000] loss: 2.312117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69,  2200] loss: 2.311767\n",
      "[69,  2400] loss: 2.310749\n",
      "[69,  2600] loss: 2.311902\n",
      "[69,  2800] loss: 2.309790\n",
      "[69,  3000] loss: 2.311172\n",
      "[69,  3200] loss: 2.311834\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[70,   200] loss: 2.311817\n",
      "[70,   400] loss: 2.311001\n",
      "[70,   600] loss: 2.311012\n",
      "[70,   800] loss: 2.309621\n",
      "[70,  1000] loss: 2.312037\n",
      "[70,  1200] loss: 2.311672\n",
      "[70,  1400] loss: 2.310114\n",
      "[70,  1600] loss: 2.311990\n",
      "[70,  1800] loss: 2.312362\n",
      "[70,  2000] loss: 2.310609\n",
      "[70,  2200] loss: 2.310101\n",
      "[70,  2400] loss: 2.312051\n",
      "[70,  2600] loss: 2.311925\n",
      "[70,  2800] loss: 2.312894\n",
      "[70,  3000] loss: 2.311488\n",
      "[70,  3200] loss: 2.312056\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[71,   200] loss: 2.312468\n",
      "[71,   400] loss: 2.310838\n",
      "[71,   600] loss: 2.310258\n",
      "[71,   800] loss: 2.311233\n",
      "[71,  1000] loss: 2.311327\n",
      "[71,  1200] loss: 2.311099\n",
      "[71,  1400] loss: 2.312564\n",
      "[71,  1600] loss: 2.312246\n",
      "[71,  1800] loss: 2.311823\n",
      "[71,  2000] loss: 2.311103\n",
      "[71,  2200] loss: 2.309653\n",
      "[71,  2400] loss: 2.312724\n",
      "[71,  2600] loss: 2.313621\n",
      "[71,  2800] loss: 2.313461\n",
      "[71,  3000] loss: 2.310359\n",
      "[71,  3200] loss: 2.310059\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[72,   200] loss: 2.310847\n",
      "[72,   400] loss: 2.311374\n",
      "[72,   600] loss: 2.313083\n",
      "[72,   800] loss: 2.311480\n",
      "[72,  1000] loss: 2.310978\n",
      "[72,  1200] loss: 2.309856\n",
      "[72,  1400] loss: 2.311466\n",
      "[72,  1600] loss: 2.311569\n",
      "[72,  1800] loss: 2.310875\n",
      "[72,  2000] loss: 2.312023\n",
      "[72,  2200] loss: 2.310463\n",
      "[72,  2400] loss: 2.311323\n",
      "[72,  2600] loss: 2.311962\n",
      "[72,  2800] loss: 2.311244\n",
      "[72,  3000] loss: 2.310485\n",
      "[72,  3200] loss: 2.309329\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[73,   200] loss: 2.312004\n",
      "[73,   400] loss: 2.311724\n",
      "[73,   600] loss: 2.311219\n",
      "[73,   800] loss: 2.312835\n",
      "[73,  1000] loss: 2.312447\n",
      "[73,  1200] loss: 2.311169\n",
      "[73,  1400] loss: 2.310779\n",
      "[73,  1600] loss: 2.312979\n",
      "[73,  1800] loss: 2.312652\n",
      "[73,  2000] loss: 2.310057\n",
      "[73,  2200] loss: 2.311805\n",
      "[73,  2400] loss: 2.311289\n",
      "[73,  2600] loss: 2.309833\n",
      "[73,  2800] loss: 2.311529\n",
      "[73,  3000] loss: 2.310257\n",
      "[73,  3200] loss: 2.310800\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[74,   200] loss: 2.312875\n",
      "[74,   400] loss: 2.313023\n",
      "[74,   600] loss: 2.312857\n",
      "[74,   800] loss: 2.312159\n",
      "[74,  1000] loss: 2.311388\n",
      "[74,  1200] loss: 2.311701\n",
      "[74,  1400] loss: 2.312434\n",
      "[74,  1600] loss: 2.311729\n",
      "[74,  1800] loss: 2.312586\n",
      "[74,  2000] loss: 2.310995\n",
      "[74,  2200] loss: 2.310590\n",
      "[74,  2400] loss: 2.312644\n",
      "[74,  2600] loss: 2.310228\n",
      "[74,  2800] loss: 2.311039\n",
      "[74,  3000] loss: 2.311239\n",
      "[74,  3200] loss: 2.309187\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[75,   200] loss: 2.310783\n",
      "[75,   400] loss: 2.313880\n",
      "[75,   600] loss: 2.311845\n",
      "[75,   800] loss: 2.311750\n",
      "[75,  1000] loss: 2.312746\n",
      "[75,  1200] loss: 2.312229\n",
      "[75,  1400] loss: 2.312098\n",
      "[75,  1600] loss: 2.311060\n",
      "[75,  1800] loss: 2.310927\n",
      "[75,  2000] loss: 2.312159\n",
      "[75,  2200] loss: 2.310459\n",
      "[75,  2400] loss: 2.312685\n",
      "[75,  2600] loss: 2.314346\n",
      "[75,  2800] loss: 2.310477\n",
      "[75,  3000] loss: 2.310998\n",
      "[75,  3200] loss: 2.312110\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[76,   200] loss: 2.312387\n",
      "[76,   400] loss: 2.310088\n",
      "[76,   600] loss: 2.311181\n",
      "[76,   800] loss: 2.311624\n",
      "[76,  1000] loss: 2.311379\n",
      "[76,  1200] loss: 2.311681\n",
      "[76,  1400] loss: 2.313393\n",
      "[76,  1600] loss: 2.309982\n",
      "[76,  1800] loss: 2.310933\n",
      "[76,  2000] loss: 2.310627\n",
      "[76,  2200] loss: 2.310262\n",
      "[76,  2400] loss: 2.311359\n",
      "[76,  2600] loss: 2.312675\n",
      "[76,  2800] loss: 2.311536\n",
      "[76,  3000] loss: 2.310333\n",
      "[76,  3200] loss: 2.310441\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[77,   200] loss: 2.311795\n",
      "[77,   400] loss: 2.311471\n",
      "[77,   600] loss: 2.313713\n",
      "[77,   800] loss: 2.312355\n",
      "[77,  1000] loss: 2.312017\n",
      "[77,  1200] loss: 2.312047\n",
      "[77,  1400] loss: 2.311003\n",
      "[77,  1600] loss: 2.310841\n",
      "[77,  1800] loss: 2.311859\n",
      "[77,  2000] loss: 2.311898\n",
      "[77,  2200] loss: 2.310855\n",
      "[77,  2400] loss: 2.311484\n",
      "[77,  2600] loss: 2.313660\n",
      "[77,  2800] loss: 2.311992\n",
      "[77,  3000] loss: 2.311868\n",
      "[77,  3200] loss: 2.310139\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[78,   200] loss: 2.311584\n",
      "[78,   400] loss: 2.310401\n",
      "[78,   600] loss: 2.311404\n",
      "[78,   800] loss: 2.313254\n",
      "[78,  1000] loss: 2.310246\n",
      "[78,  1200] loss: 2.310866\n",
      "[78,  1400] loss: 2.310971\n",
      "[78,  1600] loss: 2.312540\n",
      "[78,  1800] loss: 2.313314\n",
      "[78,  2000] loss: 2.312077\n",
      "[78,  2200] loss: 2.311128\n",
      "[78,  2400] loss: 2.310610\n",
      "[78,  2600] loss: 2.311603\n",
      "[78,  2800] loss: 2.309699\n",
      "[78,  3000] loss: 2.311166\n",
      "[78,  3200] loss: 2.311027\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[79,   200] loss: 2.312070\n",
      "[79,   400] loss: 2.312271\n",
      "[79,   600] loss: 2.310994\n",
      "[79,   800] loss: 2.313124\n",
      "[79,  1000] loss: 2.312233\n",
      "[79,  1200] loss: 2.312210\n",
      "[79,  1400] loss: 2.310265\n",
      "[79,  1600] loss: 2.313688\n",
      "[79,  1800] loss: 2.313154\n",
      "[79,  2000] loss: 2.312260\n",
      "[79,  2200] loss: 2.310330\n",
      "[79,  2400] loss: 2.311049\n",
      "[79,  2600] loss: 2.313294\n",
      "[79,  2800] loss: 2.309250\n",
      "[79,  3000] loss: 2.311367\n",
      "[79,  3200] loss: 2.310425\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[80,   200] loss: 2.312124\n",
      "[80,   400] loss: 2.314557\n",
      "[80,   600] loss: 2.310253\n",
      "[80,   800] loss: 2.313000\n",
      "[80,  1000] loss: 2.312587\n",
      "[80,  1200] loss: 2.312325\n",
      "[80,  1400] loss: 2.309590\n",
      "[80,  1600] loss: 2.311949\n",
      "[80,  1800] loss: 2.311709\n",
      "[80,  2000] loss: 2.311989\n",
      "[80,  2200] loss: 2.312058\n",
      "[80,  2400] loss: 2.311434\n",
      "[80,  2600] loss: 2.310540\n",
      "[80,  2800] loss: 2.310817\n",
      "[80,  3000] loss: 2.311924\n",
      "[80,  3200] loss: 2.309955\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[81,   200] loss: 2.311712\n",
      "[81,   400] loss: 2.311035\n",
      "[81,   600] loss: 2.309788\n",
      "[81,   800] loss: 2.311215\n",
      "[81,  1000] loss: 2.311545\n",
      "[81,  1200] loss: 2.311769\n",
      "[81,  1400] loss: 2.308908\n",
      "[81,  1600] loss: 2.311149\n",
      "[81,  1800] loss: 2.311422\n",
      "[81,  2000] loss: 2.311013\n",
      "[81,  2200] loss: 2.310435\n",
      "[81,  2400] loss: 2.310557\n",
      "[81,  2600] loss: 2.310836\n",
      "[81,  2800] loss: 2.309269\n",
      "[81,  3000] loss: 2.310776\n",
      "[81,  3200] loss: 2.312154\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[82,   200] loss: 2.310007\n",
      "[82,   400] loss: 2.312925\n",
      "[82,   600] loss: 2.311311\n",
      "[82,   800] loss: 2.309951\n",
      "[82,  1000] loss: 2.311031\n",
      "[82,  1200] loss: 2.310532\n",
      "[82,  1400] loss: 2.311795\n",
      "[82,  1600] loss: 2.310467\n",
      "[82,  1800] loss: 2.313846\n",
      "[82,  2000] loss: 2.312495\n",
      "[82,  2200] loss: 2.311408\n",
      "[82,  2400] loss: 2.310899\n",
      "[82,  2600] loss: 2.310933\n",
      "[82,  2800] loss: 2.311610\n",
      "[82,  3000] loss: 2.310746\n",
      "[82,  3200] loss: 2.311102\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[83,   200] loss: 2.312512\n",
      "[83,   400] loss: 2.313699\n",
      "[83,   600] loss: 2.309043\n",
      "[83,   800] loss: 2.310971\n",
      "[83,  1000] loss: 2.310842\n",
      "[83,  1200] loss: 2.312116\n",
      "[83,  1400] loss: 2.311115\n",
      "[83,  1600] loss: 2.312614\n",
      "[83,  1800] loss: 2.309456\n",
      "[83,  2000] loss: 2.311735\n",
      "[83,  2200] loss: 2.309531\n",
      "[83,  2400] loss: 2.313447\n",
      "[83,  2600] loss: 2.310799\n",
      "[83,  2800] loss: 2.311564\n",
      "[83,  3000] loss: 2.310163\n",
      "[83,  3200] loss: 2.311584\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[84,   200] loss: 2.313308\n",
      "[84,   400] loss: 2.310361\n",
      "[84,   600] loss: 2.311064\n",
      "[84,   800] loss: 2.310287\n",
      "[84,  1000] loss: 2.311975\n",
      "[84,  1200] loss: 2.312704\n",
      "[84,  1400] loss: 2.312011\n",
      "[84,  1600] loss: 2.312046\n",
      "[84,  1800] loss: 2.311263\n",
      "[84,  2000] loss: 2.312159\n",
      "[84,  2200] loss: 2.312077\n",
      "[84,  2400] loss: 2.312844\n",
      "[84,  2600] loss: 2.310407\n",
      "[84,  2800] loss: 2.309325\n",
      "[84,  3000] loss: 2.310340\n",
      "[84,  3200] loss: 2.310382\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[85,   200] loss: 2.310133\n",
      "[85,   400] loss: 2.312868\n",
      "[85,   600] loss: 2.309015\n",
      "[85,   800] loss: 2.311707\n",
      "[85,  1000] loss: 2.313530\n",
      "[85,  1200] loss: 2.311564\n",
      "[85,  1400] loss: 2.311124\n",
      "[85,  1600] loss: 2.311058\n",
      "[85,  1800] loss: 2.312320\n",
      "[85,  2000] loss: 2.310064\n",
      "[85,  2200] loss: 2.311449\n",
      "[85,  2400] loss: 2.311771\n",
      "[85,  2600] loss: 2.310845\n",
      "[85,  2800] loss: 2.312195\n",
      "[85,  3000] loss: 2.312755\n",
      "[85,  3200] loss: 2.311253\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[86,   200] loss: 2.311197\n",
      "[86,   400] loss: 2.309975\n",
      "[86,   600] loss: 2.309912\n",
      "[86,   800] loss: 2.311996\n",
      "[86,  1000] loss: 2.310455\n",
      "[86,  1200] loss: 2.310787\n",
      "[86,  1400] loss: 2.313122\n",
      "[86,  1600] loss: 2.311956\n",
      "[86,  1800] loss: 2.312249\n",
      "[86,  2000] loss: 2.310302\n",
      "[86,  2200] loss: 2.311928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86,  2400] loss: 2.312267\n",
      "[86,  2600] loss: 2.310560\n",
      "[86,  2800] loss: 2.310034\n",
      "[86,  3000] loss: 2.312987\n",
      "[86,  3200] loss: 2.312139\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[87,   200] loss: 2.313500\n",
      "[87,   400] loss: 2.310119\n",
      "[87,   600] loss: 2.312492\n",
      "[87,   800] loss: 2.311938\n",
      "[87,  1000] loss: 2.309957\n",
      "[87,  1200] loss: 2.311625\n",
      "[87,  1400] loss: 2.311349\n",
      "[87,  1600] loss: 2.311139\n",
      "[87,  1800] loss: 2.311820\n",
      "[87,  2000] loss: 2.311488\n",
      "[87,  2200] loss: 2.309715\n",
      "[87,  2400] loss: 2.310571\n",
      "[87,  2600] loss: 2.313933\n",
      "[87,  2800] loss: 2.310344\n",
      "[87,  3000] loss: 2.310131\n",
      "[87,  3200] loss: 2.312031\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[88,   200] loss: 2.312396\n",
      "[88,   400] loss: 2.310935\n",
      "[88,   600] loss: 2.312023\n",
      "[88,   800] loss: 2.310893\n",
      "[88,  1000] loss: 2.312196\n",
      "[88,  1200] loss: 2.311350\n",
      "[88,  1400] loss: 2.310492\n",
      "[88,  1600] loss: 2.312035\n",
      "[88,  1800] loss: 2.311308\n",
      "[88,  2000] loss: 2.313239\n",
      "[88,  2200] loss: 2.312494\n",
      "[88,  2400] loss: 2.312024\n",
      "[88,  2600] loss: 2.311970\n",
      "[88,  2800] loss: 2.312084\n",
      "[88,  3000] loss: 2.310396\n",
      "[88,  3200] loss: 2.311318\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[89,   200] loss: 2.311178\n",
      "[89,   400] loss: 2.311643\n",
      "[89,   600] loss: 2.311528\n",
      "[89,   800] loss: 2.311416\n",
      "[89,  1000] loss: 2.310321\n",
      "[89,  1200] loss: 2.312097\n",
      "[89,  1400] loss: 2.312036\n",
      "[89,  1600] loss: 2.311402\n",
      "[89,  1800] loss: 2.311100\n",
      "[89,  2000] loss: 2.310518\n",
      "[89,  2200] loss: 2.311013\n",
      "[89,  2400] loss: 2.311400\n",
      "[89,  2600] loss: 2.312080\n",
      "[89,  2800] loss: 2.311069\n",
      "[89,  3000] loss: 2.311112\n",
      "[89,  3200] loss: 2.311696\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[90,   200] loss: 2.312620\n",
      "[90,   400] loss: 2.311843\n",
      "[90,   600] loss: 2.311551\n",
      "[90,   800] loss: 2.310370\n",
      "[90,  1000] loss: 2.311739\n",
      "[90,  1200] loss: 2.313862\n",
      "[90,  1400] loss: 2.308931\n",
      "[90,  1600] loss: 2.310301\n",
      "[90,  1800] loss: 2.312319\n",
      "[90,  2000] loss: 2.311289\n",
      "[90,  2200] loss: 2.314878\n",
      "[90,  2400] loss: 2.311282\n",
      "[90,  2600] loss: 2.311730\n",
      "[90,  2800] loss: 2.311427\n",
      "[90,  3000] loss: 2.311873\n",
      "[90,  3200] loss: 2.314187\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[91,   200] loss: 2.310845\n",
      "[91,   400] loss: 2.312850\n",
      "[91,   600] loss: 2.310359\n",
      "[91,   800] loss: 2.310765\n",
      "[91,  1000] loss: 2.312689\n",
      "[91,  1200] loss: 2.311574\n",
      "[91,  1400] loss: 2.310184\n",
      "[91,  1600] loss: 2.313111\n",
      "[91,  1800] loss: 2.311152\n",
      "[91,  2000] loss: 2.312743\n",
      "[91,  2200] loss: 2.312312\n",
      "[91,  2400] loss: 2.311180\n",
      "[91,  2600] loss: 2.311270\n",
      "[91,  2800] loss: 2.310162\n",
      "[91,  3000] loss: 2.312089\n",
      "[91,  3200] loss: 2.310783\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[92,   200] loss: 2.310732\n",
      "[92,   400] loss: 2.311253\n",
      "[92,   600] loss: 2.311447\n",
      "[92,   800] loss: 2.311277\n",
      "[92,  1000] loss: 2.312245\n",
      "[92,  1200] loss: 2.309311\n",
      "[92,  1400] loss: 2.314088\n",
      "[92,  1600] loss: 2.312221\n",
      "[92,  1800] loss: 2.310870\n",
      "[92,  2000] loss: 2.312643\n",
      "[92,  2200] loss: 2.310312\n",
      "[92,  2400] loss: 2.311162\n",
      "[92,  2600] loss: 2.311358\n",
      "[92,  2800] loss: 2.312343\n",
      "[92,  3000] loss: 2.309785\n",
      "[92,  3200] loss: 2.313457\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[93,   200] loss: 2.310910\n",
      "[93,   400] loss: 2.312678\n",
      "[93,   600] loss: 2.312510\n",
      "[93,   800] loss: 2.311947\n",
      "[93,  1000] loss: 2.310388\n",
      "[93,  1200] loss: 2.311025\n",
      "[93,  1400] loss: 2.308249\n",
      "[93,  1600] loss: 2.313069\n",
      "[93,  1800] loss: 2.312946\n",
      "[93,  2000] loss: 2.312522\n",
      "[93,  2200] loss: 2.312967\n",
      "[93,  2400] loss: 2.309777\n",
      "[93,  2600] loss: 2.309567\n",
      "[93,  2800] loss: 2.313973\n",
      "[93,  3000] loss: 2.311773\n",
      "[93,  3200] loss: 2.311544\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[94,   200] loss: 2.310898\n",
      "[94,   400] loss: 2.309932\n",
      "[94,   600] loss: 2.312058\n",
      "[94,   800] loss: 2.313219\n",
      "[94,  1000] loss: 2.310837\n",
      "[94,  1200] loss: 2.311690\n",
      "[94,  1400] loss: 2.311522\n",
      "[94,  1600] loss: 2.311440\n",
      "[94,  1800] loss: 2.311215\n",
      "[94,  2000] loss: 2.312363\n",
      "[94,  2200] loss: 2.310025\n",
      "[94,  2400] loss: 2.311135\n",
      "[94,  2600] loss: 2.311312\n",
      "[94,  2800] loss: 2.314675\n",
      "[94,  3000] loss: 2.311557\n",
      "[94,  3200] loss: 2.312115\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[95,   200] loss: 2.311678\n",
      "[95,   400] loss: 2.310205\n",
      "[95,   600] loss: 2.311796\n",
      "[95,   800] loss: 2.312119\n",
      "[95,  1000] loss: 2.310363\n",
      "[95,  1200] loss: 2.310463\n",
      "[95,  1400] loss: 2.310193\n",
      "[95,  1600] loss: 2.311248\n",
      "[95,  1800] loss: 2.312657\n",
      "[95,  2000] loss: 2.312141\n",
      "[95,  2200] loss: 2.313077\n",
      "[95,  2400] loss: 2.311575\n",
      "[95,  2600] loss: 2.313660\n",
      "[95,  2800] loss: 2.311125\n",
      "[95,  3000] loss: 2.312286\n",
      "[95,  3200] loss: 2.311051\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[96,   200] loss: 2.311244\n",
      "[96,   400] loss: 2.313237\n",
      "[96,   600] loss: 2.310546\n",
      "[96,   800] loss: 2.311262\n",
      "[96,  1000] loss: 2.311648\n",
      "[96,  1200] loss: 2.312609\n",
      "[96,  1400] loss: 2.311084\n",
      "[96,  1600] loss: 2.310359\n",
      "[96,  1800] loss: 2.310567\n",
      "[96,  2000] loss: 2.311117\n",
      "[96,  2200] loss: 2.310411\n",
      "[96,  2400] loss: 2.310776\n",
      "[96,  2600] loss: 2.314100\n",
      "[96,  2800] loss: 2.310431\n",
      "[96,  3000] loss: 2.312123\n",
      "[96,  3200] loss: 2.310883\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[97,   200] loss: 2.312745\n",
      "[97,   400] loss: 2.311147\n",
      "[97,   600] loss: 2.312291\n",
      "[97,   800] loss: 2.313774\n",
      "[97,  1000] loss: 2.314145\n",
      "[97,  1200] loss: 2.312528\n",
      "[97,  1400] loss: 2.312870\n",
      "[97,  1600] loss: 2.311146\n",
      "[97,  1800] loss: 2.312704\n",
      "[97,  2000] loss: 2.312108\n",
      "[97,  2200] loss: 2.310710\n",
      "[97,  2400] loss: 2.313183\n",
      "[97,  2600] loss: 2.311811\n",
      "[97,  2800] loss: 2.311604\n",
      "[97,  3000] loss: 2.312286\n",
      "[97,  3200] loss: 2.309554\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[98,   200] loss: 2.310360\n",
      "[98,   400] loss: 2.311360\n",
      "[98,   600] loss: 2.309064\n",
      "[98,   800] loss: 2.310270\n",
      "[98,  1000] loss: 2.310965\n",
      "[98,  1200] loss: 2.310009\n",
      "[98,  1400] loss: 2.311349\n",
      "[98,  1600] loss: 2.312238\n",
      "[98,  1800] loss: 2.313343\n",
      "[98,  2000] loss: 2.311676\n",
      "[98,  2200] loss: 2.311110\n",
      "[98,  2400] loss: 2.311814\n",
      "[98,  2600] loss: 2.313092\n",
      "[98,  2800] loss: 2.311248\n",
      "[98,  3000] loss: 2.310469\n",
      "[98,  3200] loss: 2.311344\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[99,   200] loss: 2.310746\n",
      "[99,   400] loss: 2.312263\n",
      "[99,   600] loss: 2.311454\n",
      "[99,   800] loss: 2.310855\n",
      "[99,  1000] loss: 2.312648\n",
      "[99,  1200] loss: 2.309064\n",
      "[99,  1400] loss: 2.311857\n",
      "[99,  1600] loss: 2.312209\n",
      "[99,  1800] loss: 2.312499\n",
      "[99,  2000] loss: 2.311784\n",
      "[99,  2200] loss: 2.312021\n",
      "[99,  2400] loss: 2.310078\n",
      "[99,  2600] loss: 2.313195\n",
      "[99,  2800] loss: 2.311007\n",
      "[99,  3000] loss: 2.312421\n",
      "[99,  3200] loss: 2.311097\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[100,   200] loss: 2.310527\n",
      "[100,   400] loss: 2.313695\n",
      "[100,   600] loss: 2.309327\n",
      "[100,   800] loss: 2.311060\n",
      "[100,  1000] loss: 2.312881\n",
      "[100,  1200] loss: 2.312510\n",
      "[100,  1400] loss: 2.313341\n",
      "[100,  1600] loss: 2.310704\n",
      "[100,  1800] loss: 2.310959\n",
      "[100,  2000] loss: 2.311903\n",
      "[100,  2200] loss: 2.311751\n",
      "[100,  2400] loss: 2.310024\n",
      "[100,  2600] loss: 2.312761\n",
      "[100,  2800] loss: 2.311149\n",
      "[100,  3000] loss: 2.311296\n",
      "[100,  3200] loss: 2.311021\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[101,   200] loss: 2.311714\n",
      "[101,   400] loss: 2.311441\n",
      "[101,   600] loss: 2.309400\n",
      "[101,   800] loss: 2.311894\n",
      "[101,  1000] loss: 2.310794\n",
      "[101,  1200] loss: 2.308973\n",
      "[101,  1400] loss: 2.309496\n",
      "[101,  1600] loss: 2.312933\n",
      "[101,  1800] loss: 2.314091\n",
      "[101,  2000] loss: 2.310074\n",
      "[101,  2200] loss: 2.312419\n",
      "[101,  2400] loss: 2.311814\n",
      "[101,  2600] loss: 2.310978\n",
      "[101,  2800] loss: 2.311995\n",
      "[101,  3000] loss: 2.312473\n",
      "[101,  3200] loss: 2.310587\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[102,   200] loss: 2.310636\n",
      "[102,   400] loss: 2.309502\n",
      "[102,   600] loss: 2.311497\n",
      "[102,   800] loss: 2.310860\n",
      "[102,  1000] loss: 2.310212\n",
      "[102,  1200] loss: 2.310489\n",
      "[102,  1400] loss: 2.312378\n",
      "[102,  1600] loss: 2.311442\n",
      "[102,  1800] loss: 2.312603\n",
      "[102,  2000] loss: 2.311941\n",
      "[102,  2200] loss: 2.311373\n",
      "[102,  2400] loss: 2.313057\n",
      "[102,  2600] loss: 2.310726\n",
      "[102,  2800] loss: 2.310776\n",
      "[102,  3000] loss: 2.311964\n",
      "[102,  3200] loss: 2.310910\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[103,   200] loss: 2.312325\n",
      "[103,   400] loss: 2.310585\n",
      "[103,   600] loss: 2.313361\n",
      "[103,   800] loss: 2.309858\n",
      "[103,  1000] loss: 2.310886\n",
      "[103,  1200] loss: 2.312356\n",
      "[103,  1400] loss: 2.311738\n",
      "[103,  1600] loss: 2.311729\n",
      "[103,  1800] loss: 2.312820\n",
      "[103,  2000] loss: 2.311399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103,  2200] loss: 2.309598\n",
      "[103,  2400] loss: 2.312118\n",
      "[103,  2600] loss: 2.310909\n",
      "[103,  2800] loss: 2.311741\n",
      "[103,  3000] loss: 2.311541\n",
      "[103,  3200] loss: 2.311010\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[104,   200] loss: 2.312378\n",
      "[104,   400] loss: 2.315424\n",
      "[104,   600] loss: 2.311898\n",
      "[104,   800] loss: 2.312567\n",
      "[104,  1000] loss: 2.313037\n",
      "[104,  1200] loss: 2.311666\n",
      "[104,  1400] loss: 2.312273\n",
      "[104,  1600] loss: 2.312305\n",
      "[104,  1800] loss: 2.311544\n",
      "[104,  2000] loss: 2.311474\n",
      "[104,  2200] loss: 2.310534\n",
      "[104,  2400] loss: 2.314158\n",
      "[104,  2600] loss: 2.307486\n",
      "[104,  2800] loss: 2.311955\n",
      "[104,  3000] loss: 2.310046\n",
      "[104,  3200] loss: 2.312633\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[105,   200] loss: 2.311288\n",
      "[105,   400] loss: 2.311707\n",
      "[105,   600] loss: 2.312274\n",
      "[105,   800] loss: 2.311164\n",
      "[105,  1000] loss: 2.310594\n",
      "[105,  1200] loss: 2.310277\n",
      "[105,  1400] loss: 2.312131\n",
      "[105,  1600] loss: 2.309971\n",
      "[105,  1800] loss: 2.311268\n",
      "[105,  2000] loss: 2.312797\n",
      "[105,  2200] loss: 2.309427\n",
      "[105,  2400] loss: 2.311353\n",
      "[105,  2600] loss: 2.310063\n",
      "[105,  2800] loss: 2.313111\n",
      "[105,  3000] loss: 2.309933\n",
      "[105,  3200] loss: 2.311744\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[106,   200] loss: 2.311790\n",
      "[106,   400] loss: 2.311837\n",
      "[106,   600] loss: 2.312871\n",
      "[106,   800] loss: 2.309863\n",
      "[106,  1000] loss: 2.312068\n",
      "[106,  1200] loss: 2.312922\n",
      "[106,  1400] loss: 2.311934\n",
      "[106,  1600] loss: 2.310993\n",
      "[106,  1800] loss: 2.311994\n",
      "[106,  2000] loss: 2.312486\n",
      "[106,  2200] loss: 2.311151\n",
      "[106,  2400] loss: 2.311205\n",
      "[106,  2600] loss: 2.312974\n",
      "[106,  2800] loss: 2.308509\n",
      "[106,  3000] loss: 2.312040\n",
      "[106,  3200] loss: 2.310245\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[107,   200] loss: 2.311450\n",
      "[107,   400] loss: 2.310056\n",
      "[107,   600] loss: 2.312079\n",
      "[107,   800] loss: 2.311114\n",
      "[107,  1000] loss: 2.314326\n",
      "[107,  1200] loss: 2.312101\n",
      "[107,  1400] loss: 2.312063\n",
      "[107,  1600] loss: 2.311456\n",
      "[107,  1800] loss: 2.312110\n",
      "[107,  2000] loss: 2.313100\n",
      "[107,  2200] loss: 2.310630\n",
      "[107,  2400] loss: 2.311425\n",
      "[107,  2600] loss: 2.310906\n",
      "[107,  2800] loss: 2.309146\n",
      "[107,  3000] loss: 2.311704\n",
      "[107,  3200] loss: 2.311708\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[108,   200] loss: 2.311540\n",
      "[108,   400] loss: 2.310885\n",
      "[108,   600] loss: 2.311827\n",
      "[108,   800] loss: 2.311849\n",
      "[108,  1000] loss: 2.310444\n",
      "[108,  1200] loss: 2.311079\n",
      "[108,  1400] loss: 2.311019\n",
      "[108,  1600] loss: 2.311205\n",
      "[108,  1800] loss: 2.312089\n",
      "[108,  2000] loss: 2.312453\n",
      "[108,  2200] loss: 2.312911\n",
      "[108,  2400] loss: 2.311265\n",
      "[108,  2600] loss: 2.311601\n",
      "[108,  2800] loss: 2.313372\n",
      "[108,  3000] loss: 2.313306\n",
      "[108,  3200] loss: 2.310174\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[109,   200] loss: 2.311331\n",
      "[109,   400] loss: 2.313690\n",
      "[109,   600] loss: 2.309825\n",
      "[109,   800] loss: 2.310305\n",
      "[109,  1000] loss: 2.313197\n",
      "[109,  1200] loss: 2.312742\n",
      "[109,  1400] loss: 2.310516\n",
      "[109,  1600] loss: 2.313116\n",
      "[109,  1800] loss: 2.311264\n",
      "[109,  2000] loss: 2.312182\n",
      "[109,  2200] loss: 2.311655\n",
      "[109,  2400] loss: 2.310981\n",
      "[109,  2600] loss: 2.311834\n",
      "[109,  2800] loss: 2.310323\n",
      "[109,  3000] loss: 2.312625\n",
      "[109,  3200] loss: 2.311327\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[110,   200] loss: 2.311995\n",
      "[110,   400] loss: 2.311048\n",
      "[110,   600] loss: 2.310524\n",
      "[110,   800] loss: 2.309722\n",
      "[110,  1000] loss: 2.312972\n",
      "[110,  1200] loss: 2.310502\n",
      "[110,  1400] loss: 2.310939\n",
      "[110,  1600] loss: 2.312100\n",
      "[110,  1800] loss: 2.310214\n",
      "[110,  2000] loss: 2.314258\n",
      "[110,  2200] loss: 2.309537\n",
      "[110,  2400] loss: 2.311223\n",
      "[110,  2600] loss: 2.308994\n",
      "[110,  2800] loss: 2.310448\n",
      "[110,  3000] loss: 2.311620\n",
      "[110,  3200] loss: 2.311342\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[111,   200] loss: 2.311507\n",
      "[111,   400] loss: 2.311520\n",
      "[111,   600] loss: 2.309471\n",
      "[111,   800] loss: 2.311133\n",
      "[111,  1000] loss: 2.311551\n",
      "[111,  1200] loss: 2.311636\n",
      "[111,  1400] loss: 2.312433\n",
      "[111,  1600] loss: 2.311339\n",
      "[111,  1800] loss: 2.309796\n",
      "[111,  2000] loss: 2.313207\n",
      "[111,  2200] loss: 2.313288\n",
      "[111,  2400] loss: 2.312241\n",
      "[111,  2600] loss: 2.311390\n",
      "[111,  2800] loss: 2.309605\n",
      "[111,  3000] loss: 2.311993\n",
      "[111,  3200] loss: 2.311270\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[112,   200] loss: 2.310133\n",
      "[112,   400] loss: 2.311504\n",
      "[112,   600] loss: 2.311737\n",
      "[112,   800] loss: 2.311087\n",
      "[112,  1000] loss: 2.311708\n",
      "[112,  1200] loss: 2.311765\n",
      "[112,  1400] loss: 2.311990\n",
      "[112,  1600] loss: 2.308970\n",
      "[112,  1800] loss: 2.309354\n",
      "[112,  2000] loss: 2.311155\n",
      "[112,  2200] loss: 2.312604\n",
      "[112,  2400] loss: 2.312111\n",
      "[112,  2600] loss: 2.311729\n",
      "[112,  2800] loss: 2.311127\n",
      "[112,  3000] loss: 2.310632\n",
      "[112,  3200] loss: 2.311354\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[113,   200] loss: 2.310879\n",
      "[113,   400] loss: 2.310303\n",
      "[113,   600] loss: 2.311343\n",
      "[113,   800] loss: 2.309066\n",
      "[113,  1000] loss: 2.312115\n",
      "[113,  1200] loss: 2.310224\n",
      "[113,  1400] loss: 2.311977\n",
      "[113,  1600] loss: 2.311693\n",
      "[113,  1800] loss: 2.313855\n",
      "[113,  2000] loss: 2.309067\n",
      "[113,  2200] loss: 2.312212\n",
      "[113,  2400] loss: 2.311254\n",
      "[113,  2600] loss: 2.310469\n",
      "[113,  2800] loss: 2.311321\n",
      "[113,  3000] loss: 2.314179\n",
      "[113,  3200] loss: 2.313918\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[114,   200] loss: 2.311214\n",
      "[114,   400] loss: 2.309636\n",
      "[114,   600] loss: 2.311440\n",
      "[114,   800] loss: 2.311054\n",
      "[114,  1000] loss: 2.311404\n",
      "[114,  1200] loss: 2.312124\n",
      "[114,  1400] loss: 2.312564\n",
      "[114,  1600] loss: 2.310797\n",
      "[114,  1800] loss: 2.311429\n",
      "[114,  2000] loss: 2.313675\n",
      "[114,  2200] loss: 2.310215\n",
      "[114,  2400] loss: 2.311021\n",
      "[114,  2600] loss: 2.311004\n",
      "[114,  2800] loss: 2.311582\n",
      "[114,  3000] loss: 2.311868\n",
      "[114,  3200] loss: 2.310675\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[115,   200] loss: 2.312328\n",
      "[115,   400] loss: 2.309867\n",
      "[115,   600] loss: 2.310814\n",
      "[115,   800] loss: 2.310131\n",
      "[115,  1000] loss: 2.311263\n",
      "[115,  1200] loss: 2.311298\n",
      "[115,  1400] loss: 2.312034\n",
      "[115,  1600] loss: 2.312271\n",
      "[115,  1800] loss: 2.311166\n",
      "[115,  2000] loss: 2.311426\n",
      "[115,  2200] loss: 2.312192\n",
      "[115,  2400] loss: 2.311802\n",
      "[115,  2600] loss: 2.310549\n",
      "[115,  2800] loss: 2.311401\n",
      "[115,  3000] loss: 2.314054\n",
      "[115,  3200] loss: 2.310800\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[116,   200] loss: 2.308983\n",
      "[116,   400] loss: 2.311363\n",
      "[116,   600] loss: 2.312078\n",
      "[116,   800] loss: 2.311341\n",
      "[116,  1000] loss: 2.311256\n",
      "[116,  1200] loss: 2.312832\n",
      "[116,  1400] loss: 2.309753\n",
      "[116,  1600] loss: 2.311817\n",
      "[116,  1800] loss: 2.309786\n",
      "[116,  2000] loss: 2.309668\n",
      "[116,  2200] loss: 2.310680\n",
      "[116,  2400] loss: 2.309043\n",
      "[116,  2600] loss: 2.311444\n",
      "[116,  2800] loss: 2.311312\n",
      "[116,  3000] loss: 2.310952\n",
      "[116,  3200] loss: 2.311525\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[117,   200] loss: 2.311140\n",
      "[117,   400] loss: 2.311904\n",
      "[117,   600] loss: 2.313012\n",
      "[117,   800] loss: 2.311766\n",
      "[117,  1000] loss: 2.310222\n",
      "[117,  1200] loss: 2.309418\n",
      "[117,  1400] loss: 2.312075\n",
      "[117,  1600] loss: 2.311685\n",
      "[117,  1800] loss: 2.311388\n",
      "[117,  2000] loss: 2.312135\n",
      "[117,  2200] loss: 2.311242\n",
      "[117,  2400] loss: 2.310100\n",
      "[117,  2600] loss: 2.311771\n",
      "[117,  2800] loss: 2.311076\n",
      "[117,  3000] loss: 2.310475\n",
      "[117,  3200] loss: 2.310131\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[118,   200] loss: 2.311747\n",
      "[118,   400] loss: 2.309879\n",
      "[118,   600] loss: 2.309527\n",
      "[118,   800] loss: 2.311467\n",
      "[118,  1000] loss: 2.310377\n",
      "[118,  1200] loss: 2.311218\n",
      "[118,  1400] loss: 2.312655\n",
      "[118,  1600] loss: 2.311667\n",
      "[118,  1800] loss: 2.311000\n",
      "[118,  2000] loss: 2.310821\n",
      "[118,  2200] loss: 2.310425\n",
      "[118,  2400] loss: 2.311492\n",
      "[118,  2600] loss: 2.312379\n",
      "[118,  2800] loss: 2.310933\n",
      "[118,  3000] loss: 2.312384\n",
      "[118,  3200] loss: 2.313446\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[119,   200] loss: 2.312017\n",
      "[119,   400] loss: 2.310032\n",
      "[119,   600] loss: 2.310152\n",
      "[119,   800] loss: 2.311955\n",
      "[119,  1000] loss: 2.309648\n",
      "[119,  1200] loss: 2.311985\n",
      "[119,  1400] loss: 2.312793\n",
      "[119,  1600] loss: 2.311206\n",
      "[119,  1800] loss: 2.312476\n",
      "[119,  2000] loss: 2.310944\n",
      "[119,  2200] loss: 2.312028\n",
      "[119,  2400] loss: 2.310974\n",
      "[119,  2600] loss: 2.311365\n",
      "[119,  2800] loss: 2.310760\n",
      "[119,  3000] loss: 2.311473\n",
      "[119,  3200] loss: 2.312572\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[120,   200] loss: 2.311970\n",
      "[120,   400] loss: 2.312415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120,   600] loss: 2.311826\n",
      "[120,   800] loss: 2.310928\n",
      "[120,  1000] loss: 2.310914\n",
      "[120,  1200] loss: 2.310164\n",
      "[120,  1400] loss: 2.310505\n",
      "[120,  1600] loss: 2.311178\n",
      "[120,  1800] loss: 2.310938\n",
      "[120,  2000] loss: 2.312425\n",
      "[120,  2200] loss: 2.311290\n",
      "[120,  2400] loss: 2.312637\n",
      "[120,  2600] loss: 2.310564\n",
      "[120,  2800] loss: 2.311970\n",
      "[120,  3000] loss: 2.313766\n",
      "[120,  3200] loss: 2.311132\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[121,   200] loss: 2.311293\n",
      "[121,   400] loss: 2.310684\n",
      "[121,   600] loss: 2.311044\n",
      "[121,   800] loss: 2.311349\n",
      "[121,  1000] loss: 2.311680\n",
      "[121,  1200] loss: 2.311723\n",
      "[121,  1400] loss: 2.313313\n",
      "[121,  1600] loss: 2.309556\n",
      "[121,  1800] loss: 2.309713\n",
      "[121,  2000] loss: 2.311614\n",
      "[121,  2200] loss: 2.311129\n",
      "[121,  2400] loss: 2.310170\n",
      "[121,  2600] loss: 2.310538\n",
      "[121,  2800] loss: 2.312626\n",
      "[121,  3000] loss: 2.310437\n",
      "[121,  3200] loss: 2.310815\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[122,   200] loss: 2.311705\n",
      "[122,   400] loss: 2.311284\n",
      "[122,   600] loss: 2.311382\n",
      "[122,   800] loss: 2.313389\n",
      "[122,  1000] loss: 2.313044\n",
      "[122,  1200] loss: 2.310863\n",
      "[122,  1400] loss: 2.310389\n",
      "[122,  1600] loss: 2.312511\n",
      "[122,  1800] loss: 2.308822\n",
      "[122,  2000] loss: 2.310773\n",
      "[122,  2200] loss: 2.310442\n",
      "[122,  2400] loss: 2.312333\n",
      "[122,  2600] loss: 2.310821\n",
      "[122,  2800] loss: 2.310972\n",
      "[122,  3000] loss: 2.311376\n",
      "[122,  3200] loss: 2.310034\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[123,   200] loss: 2.310356\n",
      "[123,   400] loss: 2.313288\n",
      "[123,   600] loss: 2.310626\n",
      "[123,   800] loss: 2.311772\n",
      "[123,  1000] loss: 2.310587\n",
      "[123,  1200] loss: 2.313399\n",
      "[123,  1400] loss: 2.312580\n",
      "[123,  1600] loss: 2.310317\n",
      "[123,  1800] loss: 2.310335\n",
      "[123,  2000] loss: 2.313426\n",
      "[123,  2200] loss: 2.310223\n",
      "[123,  2400] loss: 2.309846\n",
      "[123,  2600] loss: 2.313408\n",
      "[123,  2800] loss: 2.311580\n",
      "[123,  3000] loss: 2.310475\n",
      "[123,  3200] loss: 2.310190\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[124,   200] loss: 2.312822\n",
      "[124,   400] loss: 2.310297\n",
      "[124,   600] loss: 2.310841\n",
      "[124,   800] loss: 2.312331\n",
      "[124,  1000] loss: 2.312389\n",
      "[124,  1200] loss: 2.312453\n",
      "[124,  1400] loss: 2.313055\n",
      "[124,  1600] loss: 2.311732\n",
      "[124,  1800] loss: 2.312084\n",
      "[124,  2000] loss: 2.311119\n",
      "[124,  2200] loss: 2.309985\n",
      "[124,  2400] loss: 2.311470\n",
      "[124,  2600] loss: 2.311847\n",
      "[124,  2800] loss: 2.309641\n",
      "[124,  3000] loss: 2.309914\n",
      "[124,  3200] loss: 2.309469\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[125,   200] loss: 2.312833\n",
      "[125,   400] loss: 2.310438\n",
      "[125,   600] loss: 2.312327\n",
      "[125,   800] loss: 2.313252\n",
      "[125,  1000] loss: 2.311178\n",
      "[125,  1200] loss: 2.313331\n",
      "[125,  1400] loss: 2.311923\n",
      "[125,  1600] loss: 2.312283\n",
      "[125,  1800] loss: 2.311311\n",
      "[125,  2000] loss: 2.311322\n",
      "[125,  2200] loss: 2.311278\n",
      "[125,  2400] loss: 2.309703\n",
      "[125,  2600] loss: 2.311047\n",
      "[125,  2800] loss: 2.310644\n",
      "[125,  3000] loss: 2.309221\n",
      "[125,  3200] loss: 2.311421\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[126,   200] loss: 2.311859\n",
      "[126,   400] loss: 2.323061\n",
      "[126,   600] loss: 2.311541\n",
      "[126,   800] loss: 2.312082\n",
      "[126,  1000] loss: 2.313303\n",
      "[126,  1200] loss: 2.311431\n",
      "[126,  1400] loss: 2.309944\n",
      "[126,  1600] loss: 2.312297\n",
      "[126,  1800] loss: 2.312671\n",
      "[126,  2000] loss: 2.310333\n",
      "[126,  2200] loss: 2.312399\n",
      "[126,  2400] loss: 2.310755\n",
      "[126,  2600] loss: 2.313618\n",
      "[126,  2800] loss: 2.311859\n",
      "[126,  3000] loss: 2.311504\n",
      "[126,  3200] loss: 2.312913\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[127,   200] loss: 2.311797\n",
      "[127,   400] loss: 2.310751\n",
      "[127,   600] loss: 2.312702\n",
      "[127,   800] loss: 2.309696\n",
      "[127,  1000] loss: 2.313096\n",
      "[127,  1200] loss: 2.311866\n",
      "[127,  1400] loss: 2.311211\n",
      "[127,  1600] loss: 2.312395\n",
      "[127,  1800] loss: 2.316219\n",
      "[127,  2000] loss: 2.318096\n",
      "[127,  2200] loss: 2.312766\n",
      "[127,  2400] loss: 2.309743\n",
      "[127,  2600] loss: 2.312523\n",
      "[127,  2800] loss: 2.312726\n",
      "[127,  3000] loss: 2.310852\n",
      "[127,  3200] loss: 2.311775\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[128,   200] loss: 2.315076\n",
      "[128,   400] loss: 2.310752\n",
      "[128,   600] loss: 2.309524\n",
      "[128,   800] loss: 2.311272\n",
      "[128,  1000] loss: 2.311252\n",
      "[128,  1200] loss: 2.311456\n",
      "[128,  1400] loss: 2.312288\n",
      "[128,  1600] loss: 2.311374\n",
      "[128,  1800] loss: 2.311862\n",
      "[128,  2000] loss: 2.310825\n",
      "[128,  2200] loss: 2.310691\n",
      "[128,  2400] loss: 2.311243\n",
      "[128,  2600] loss: 2.310605\n",
      "[128,  2800] loss: 2.329796\n",
      "[128,  3000] loss: 2.310812\n",
      "[128,  3200] loss: 2.311408\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[129,   200] loss: 2.312765\n",
      "[129,   400] loss: 2.311199\n",
      "[129,   600] loss: 2.311771\n",
      "[129,   800] loss: 2.310744\n",
      "[129,  1000] loss: 2.310466\n",
      "[129,  1200] loss: 2.309793\n",
      "[129,  1400] loss: 2.313303\n",
      "[129,  1600] loss: 2.311858\n",
      "[129,  1800] loss: 2.312505\n",
      "[129,  2000] loss: 2.310604\n",
      "[129,  2200] loss: 2.310733\n",
      "[129,  2400] loss: 2.311319\n",
      "[129,  2600] loss: 2.312897\n",
      "[129,  2800] loss: 2.311309\n",
      "[129,  3000] loss: 2.310387\n",
      "[129,  3200] loss: 2.311245\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[130,   200] loss: 2.311046\n",
      "[130,   400] loss: 2.311614\n",
      "[130,   600] loss: 2.310911\n",
      "[130,   800] loss: 2.312167\n",
      "[130,  1000] loss: 2.310306\n",
      "[130,  1200] loss: 2.310970\n",
      "[130,  1400] loss: 2.311439\n",
      "[130,  1600] loss: 2.310038\n",
      "[130,  1800] loss: 2.310614\n",
      "[130,  2000] loss: 2.312659\n",
      "[130,  2200] loss: 2.311032\n",
      "[130,  2400] loss: 2.311258\n",
      "[130,  2600] loss: 2.311374\n",
      "[130,  2800] loss: 2.311179\n",
      "[130,  3000] loss: 2.310693\n",
      "[130,  3200] loss: 2.310369\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[131,   200] loss: 2.312878\n",
      "[131,   400] loss: 2.309514\n",
      "[131,   600] loss: 2.310940\n",
      "[131,   800] loss: 2.312043\n",
      "[131,  1000] loss: 2.310318\n",
      "[131,  1200] loss: 2.311057\n",
      "[131,  1400] loss: 2.311004\n",
      "[131,  1600] loss: 2.310029\n",
      "[131,  1800] loss: 2.312961\n",
      "[131,  2000] loss: 2.310791\n",
      "[131,  2200] loss: 2.311270\n",
      "[131,  2400] loss: 2.310389\n",
      "[131,  2600] loss: 2.310454\n",
      "[131,  2800] loss: 2.312239\n",
      "[131,  3000] loss: 2.312240\n",
      "[131,  3200] loss: 2.311900\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[132,   200] loss: 2.311020\n",
      "[132,   400] loss: 2.310870\n",
      "[132,   600] loss: 2.311496\n",
      "[132,   800] loss: 2.311930\n",
      "[132,  1000] loss: 2.311522\n",
      "[132,  1200] loss: 2.311518\n",
      "[132,  1400] loss: 2.310629\n",
      "[132,  1600] loss: 2.312068\n",
      "[132,  1800] loss: 2.312205\n",
      "[132,  2000] loss: 2.311200\n",
      "[132,  2200] loss: 2.313658\n",
      "[132,  2400] loss: 2.309333\n",
      "[132,  2600] loss: 2.311357\n",
      "[132,  2800] loss: 2.311928\n",
      "[132,  3000] loss: 2.311656\n",
      "[132,  3200] loss: 2.312645\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[133,   200] loss: 2.307660\n",
      "[133,   400] loss: 2.311799\n",
      "[133,   600] loss: 2.311282\n",
      "[133,   800] loss: 2.312008\n",
      "[133,  1000] loss: 2.311336\n",
      "[133,  1200] loss: 2.312919\n",
      "[133,  1400] loss: 2.312093\n",
      "[133,  1600] loss: 2.311968\n",
      "[133,  1800] loss: 2.309571\n",
      "[133,  2000] loss: 2.312908\n",
      "[133,  2200] loss: 2.310085\n",
      "[133,  2400] loss: 2.310844\n",
      "[133,  2600] loss: 2.313304\n",
      "[133,  2800] loss: 2.311651\n",
      "[133,  3000] loss: 2.310411\n",
      "[133,  3200] loss: 2.312396\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[134,   200] loss: 2.313971\n",
      "[134,   400] loss: 2.308093\n",
      "[134,   600] loss: 2.311122\n",
      "[134,   800] loss: 2.310302\n",
      "[134,  1000] loss: 2.310807\n",
      "[134,  1200] loss: 2.311786\n",
      "[134,  1400] loss: 2.311309\n",
      "[134,  1600] loss: 2.311662\n",
      "[134,  1800] loss: 2.312238\n",
      "[134,  2000] loss: 2.313232\n",
      "[134,  2200] loss: 2.310500\n",
      "[134,  2400] loss: 2.313928\n",
      "[134,  2600] loss: 2.312510\n",
      "[134,  2800] loss: 2.309688\n",
      "[134,  3000] loss: 2.311708\n",
      "[134,  3200] loss: 2.310801\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[135,   200] loss: 2.312985\n",
      "[135,   400] loss: 2.312324\n",
      "[135,   600] loss: 2.311809\n",
      "[135,   800] loss: 2.311518\n",
      "[135,  1000] loss: 2.310229\n",
      "[135,  1200] loss: 2.312960\n",
      "[135,  1400] loss: 2.311784\n",
      "[135,  1600] loss: 2.313275\n",
      "[135,  1800] loss: 2.309482\n",
      "[135,  2000] loss: 2.312382\n",
      "[135,  2200] loss: 2.312643\n",
      "[135,  2400] loss: 2.310918\n",
      "[135,  2600] loss: 2.311398\n",
      "[135,  2800] loss: 2.310763\n",
      "[135,  3000] loss: 2.311377\n",
      "[135,  3200] loss: 2.312042\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[136,   200] loss: 2.311110\n",
      "[136,   400] loss: 2.311528\n",
      "[136,   600] loss: 2.312853\n",
      "[136,   800] loss: 2.309884\n",
      "[136,  1000] loss: 2.312253\n",
      "[136,  1200] loss: 2.311238\n",
      "[136,  1400] loss: 2.312886\n",
      "[136,  1600] loss: 2.311565\n",
      "[136,  1800] loss: 2.310863\n",
      "[136,  2000] loss: 2.313394\n",
      "[136,  2200] loss: 2.313002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136,  2400] loss: 2.311890\n",
      "[136,  2600] loss: 2.310778\n",
      "[136,  2800] loss: 2.312542\n",
      "[136,  3000] loss: 2.311720\n",
      "[136,  3200] loss: 2.313777\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[137,   200] loss: 2.309599\n",
      "[137,   400] loss: 2.307917\n",
      "[137,   600] loss: 2.312762\n",
      "[137,   800] loss: 2.311423\n",
      "[137,  1000] loss: 2.314104\n",
      "[137,  1200] loss: 2.311300\n",
      "[137,  1400] loss: 2.310027\n",
      "[137,  1600] loss: 2.310406\n",
      "[137,  1800] loss: 2.311414\n",
      "[137,  2000] loss: 2.311339\n",
      "[137,  2200] loss: 2.311683\n",
      "[137,  2400] loss: 2.310944\n",
      "[137,  2600] loss: 2.312237\n",
      "[137,  2800] loss: 2.310626\n",
      "[137,  3000] loss: 2.312002\n",
      "[137,  3200] loss: 2.312919\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[138,   200] loss: 2.312441\n",
      "[138,   400] loss: 2.314342\n",
      "[138,   600] loss: 2.312552\n",
      "[138,   800] loss: 2.312516\n",
      "[138,  1000] loss: 2.311881\n",
      "[138,  1200] loss: 2.310922\n",
      "[138,  1400] loss: 2.313833\n",
      "[138,  1600] loss: 2.312010\n",
      "[138,  1800] loss: 2.313527\n",
      "[138,  2000] loss: 2.311292\n",
      "[138,  2200] loss: 2.313171\n",
      "[138,  2400] loss: 2.311398\n",
      "[138,  2600] loss: 2.311398\n",
      "[138,  2800] loss: 2.310797\n",
      "[138,  3000] loss: 2.312186\n",
      "[138,  3200] loss: 2.312214\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[139,   200] loss: 2.312361\n",
      "[139,   400] loss: 2.312315\n",
      "[139,   600] loss: 2.311891\n",
      "[139,   800] loss: 2.311647\n",
      "[139,  1000] loss: 2.310043\n",
      "[139,  1200] loss: 2.311330\n",
      "[139,  1400] loss: 2.310401\n",
      "[139,  1600] loss: 2.312877\n",
      "[139,  1800] loss: 2.316032\n",
      "[139,  2000] loss: 2.310502\n",
      "[139,  2200] loss: 2.311728\n",
      "[139,  2400] loss: 2.310318\n",
      "[139,  2600] loss: 2.309588\n",
      "[139,  2800] loss: 2.311695\n",
      "[139,  3000] loss: 2.311604\n",
      "[139,  3200] loss: 2.312500\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[140,   200] loss: 2.312784\n",
      "[140,   400] loss: 2.311426\n",
      "[140,   600] loss: 2.310139\n",
      "[140,   800] loss: 2.310975\n",
      "[140,  1000] loss: 2.310723\n",
      "[140,  1200] loss: 2.310124\n",
      "[140,  1400] loss: 2.311469\n",
      "[140,  1600] loss: 2.311984\n",
      "[140,  1800] loss: 2.310769\n",
      "[140,  2000] loss: 2.312036\n",
      "[140,  2200] loss: 2.311945\n",
      "[140,  2400] loss: 2.311537\n",
      "[140,  2600] loss: 2.311955\n",
      "[140,  2800] loss: 2.309903\n",
      "[140,  3000] loss: 2.310820\n",
      "[140,  3200] loss: 2.312027\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[141,   200] loss: 2.310706\n",
      "[141,   400] loss: 2.311483\n",
      "[141,   600] loss: 2.310598\n",
      "[141,   800] loss: 2.311065\n",
      "[141,  1000] loss: 2.310670\n",
      "[141,  1200] loss: 2.311412\n",
      "[141,  1400] loss: 2.311005\n",
      "[141,  1600] loss: 2.311768\n",
      "[141,  1800] loss: 2.312298\n",
      "[141,  2000] loss: 2.312305\n",
      "[141,  2200] loss: 2.311801\n",
      "[141,  2400] loss: 2.312250\n",
      "[141,  2600] loss: 2.310903\n",
      "[141,  2800] loss: 2.314278\n",
      "[141,  3000] loss: 2.310706\n",
      "[141,  3200] loss: 2.311104\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[142,   200] loss: 2.309970\n",
      "[142,   400] loss: 2.312277\n",
      "[142,   600] loss: 2.311073\n",
      "[142,   800] loss: 2.312250\n",
      "[142,  1000] loss: 2.310869\n",
      "[142,  1200] loss: 2.311063\n",
      "[142,  1400] loss: 2.312576\n",
      "[142,  1600] loss: 2.311700\n",
      "[142,  1800] loss: 2.312201\n",
      "[142,  2000] loss: 2.311970\n",
      "[142,  2200] loss: 2.311943\n",
      "[142,  2400] loss: 2.311626\n",
      "[142,  2600] loss: 2.309662\n",
      "[142,  2800] loss: 2.313747\n",
      "[142,  3000] loss: 2.311574\n",
      "[142,  3200] loss: 2.311671\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[143,   200] loss: 2.311347\n",
      "[143,   400] loss: 2.309792\n",
      "[143,   600] loss: 2.309162\n",
      "[143,   800] loss: 2.311727\n",
      "[143,  1000] loss: 2.311451\n",
      "[143,  1200] loss: 2.312764\n",
      "[143,  1400] loss: 2.312200\n",
      "[143,  1600] loss: 2.309357\n",
      "[143,  1800] loss: 2.312202\n",
      "[143,  2000] loss: 2.312795\n",
      "[143,  2200] loss: 2.312908\n",
      "[143,  2400] loss: 2.310573\n",
      "[143,  2600] loss: 2.311179\n",
      "[143,  2800] loss: 2.311786\n",
      "[143,  3000] loss: 2.311406\n",
      "[143,  3200] loss: 2.311337\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[144,   200] loss: 2.311658\n",
      "[144,   400] loss: 2.312493\n",
      "[144,   600] loss: 2.309584\n",
      "[144,   800] loss: 2.311523\n",
      "[144,  1000] loss: 2.312043\n",
      "[144,  1200] loss: 2.311853\n",
      "[144,  1400] loss: 2.310804\n",
      "[144,  1600] loss: 2.311829\n",
      "[144,  1800] loss: 2.312456\n",
      "[144,  2000] loss: 2.310297\n",
      "[144,  2200] loss: 2.310460\n",
      "[144,  2400] loss: 2.311591\n",
      "[144,  2600] loss: 2.309415\n",
      "[144,  2800] loss: 2.311378\n",
      "[144,  3000] loss: 2.310504\n",
      "[144,  3200] loss: 2.310950\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[145,   200] loss: 2.310886\n",
      "[145,   400] loss: 2.310808\n",
      "[145,   600] loss: 2.311686\n",
      "[145,   800] loss: 2.311720\n",
      "[145,  1000] loss: 2.311153\n",
      "[145,  1200] loss: 2.310788\n",
      "[145,  1400] loss: 2.312008\n",
      "[145,  1600] loss: 2.311427\n",
      "[145,  1800] loss: 2.311504\n",
      "[145,  2000] loss: 2.310623\n",
      "[145,  2200] loss: 2.312314\n",
      "[145,  2400] loss: 2.310941\n",
      "[145,  2600] loss: 2.313456\n",
      "[145,  2800] loss: 2.312466\n",
      "[145,  3000] loss: 2.311027\n",
      "[145,  3200] loss: 2.310317\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[146,   200] loss: 2.311098\n",
      "[146,   400] loss: 2.310926\n",
      "[146,   600] loss: 2.312592\n",
      "[146,   800] loss: 2.311492\n",
      "[146,  1000] loss: 2.313125\n",
      "[146,  1200] loss: 2.309762\n",
      "[146,  1400] loss: 2.312195\n",
      "[146,  1600] loss: 2.312444\n",
      "[146,  1800] loss: 2.312886\n",
      "[146,  2000] loss: 2.312464\n",
      "[146,  2200] loss: 2.312881\n",
      "[146,  2400] loss: 2.310922\n",
      "[146,  2600] loss: 2.309719\n",
      "[146,  2800] loss: 2.313487\n",
      "[146,  3000] loss: 2.312374\n",
      "[146,  3200] loss: 2.311348\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[147,   200] loss: 2.310752\n",
      "[147,   400] loss: 2.312461\n",
      "[147,   600] loss: 2.312151\n",
      "[147,   800] loss: 2.311047\n",
      "[147,  1000] loss: 2.313127\n",
      "[147,  1200] loss: 2.311086\n",
      "[147,  1400] loss: 2.311135\n",
      "[147,  1600] loss: 2.309870\n",
      "[147,  1800] loss: 2.312016\n",
      "[147,  2000] loss: 2.310729\n",
      "[147,  2200] loss: 2.311341\n",
      "[147,  2400] loss: 2.312457\n",
      "[147,  2600] loss: 2.311608\n",
      "[147,  2800] loss: 2.310120\n",
      "[147,  3000] loss: 2.311353\n",
      "[147,  3200] loss: 2.310012\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[148,   200] loss: 2.310220\n",
      "[148,   400] loss: 2.310222\n",
      "[148,   600] loss: 2.313251\n",
      "[148,   800] loss: 2.311516\n",
      "[148,  1000] loss: 2.310712\n",
      "[148,  1200] loss: 2.311801\n",
      "[148,  1400] loss: 2.311321\n",
      "[148,  1600] loss: 2.309456\n",
      "[148,  1800] loss: 2.309526\n",
      "[148,  2000] loss: 2.312086\n",
      "[148,  2200] loss: 2.310211\n",
      "[148,  2400] loss: 2.312635\n",
      "[148,  2600] loss: 2.315443\n",
      "[148,  2800] loss: 2.314729\n",
      "[148,  3000] loss: 2.311538\n",
      "[148,  3200] loss: 2.312226\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[149,   200] loss: 2.312496\n",
      "[149,   400] loss: 2.311709\n",
      "[149,   600] loss: 2.310602\n",
      "[149,   800] loss: 2.312503\n",
      "[149,  1000] loss: 2.311907\n",
      "[149,  1200] loss: 2.312941\n",
      "[149,  1400] loss: 2.312199\n",
      "[149,  1600] loss: 2.311788\n",
      "[149,  1800] loss: 2.312357\n",
      "[149,  2000] loss: 2.311251\n",
      "[149,  2200] loss: 2.311787\n",
      "[149,  2400] loss: 2.311322\n",
      "[149,  2600] loss: 2.314396\n",
      "[149,  2800] loss: 2.310428\n",
      "[149,  3000] loss: 2.310515\n",
      "[149,  3200] loss: 2.310846\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[150,   200] loss: 2.311185\n",
      "[150,   400] loss: 2.310877\n",
      "[150,   600] loss: 2.310957\n",
      "[150,   800] loss: 2.311451\n",
      "[150,  1000] loss: 2.314997\n",
      "[150,  1200] loss: 2.311979\n",
      "[150,  1400] loss: 2.312394\n",
      "[150,  1600] loss: 2.311458\n",
      "[150,  1800] loss: 2.309931\n",
      "[150,  2000] loss: 2.312302\n",
      "[150,  2200] loss: 2.312156\n",
      "[150,  2400] loss: 2.311023\n",
      "[150,  2600] loss: 2.310744\n",
      "[150,  2800] loss: 2.308973\n",
      "[150,  3000] loss: 2.310065\n",
      "[150,  3200] loss: 2.311684\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[151,   200] loss: 2.305846\n",
      "[151,   400] loss: 2.303132\n",
      "[151,   600] loss: 2.303437\n",
      "[151,   800] loss: 2.303205\n",
      "[151,  1000] loss: 2.302994\n",
      "[151,  1200] loss: 2.303069\n",
      "[151,  1400] loss: 2.303542\n",
      "[151,  1600] loss: 2.303356\n",
      "[151,  1800] loss: 2.303716\n",
      "[151,  2000] loss: 2.303102\n",
      "[151,  2200] loss: 2.303473\n",
      "[151,  2400] loss: 2.303518\n",
      "[151,  2600] loss: 2.303363\n",
      "[151,  2800] loss: 2.303579\n",
      "[151,  3000] loss: 2.303566\n",
      "[151,  3200] loss: 2.303820\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[152,   200] loss: 2.303646\n",
      "[152,   400] loss: 2.303712\n",
      "[152,   600] loss: 2.303812\n",
      "[152,   800] loss: 2.303801\n",
      "[152,  1000] loss: 2.303886\n",
      "[152,  1200] loss: 2.303510\n",
      "[152,  1400] loss: 2.303746\n",
      "[152,  1600] loss: 2.303561\n",
      "[152,  1800] loss: 2.303742\n",
      "[152,  2000] loss: 2.303711\n",
      "[152,  2200] loss: 2.303325\n",
      "[152,  2400] loss: 2.303727\n",
      "[152,  2600] loss: 2.303448\n",
      "[152,  2800] loss: 2.303531\n",
      "[152,  3000] loss: 2.303409\n",
      "[152,  3200] loss: 2.303370\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[153,   200] loss: 2.303644\n",
      "[153,   400] loss: 2.303749\n",
      "[153,   600] loss: 2.303640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153,   800] loss: 2.303267\n",
      "[153,  1000] loss: 2.303413\n",
      "[153,  1200] loss: 2.303254\n",
      "[153,  1400] loss: 2.303661\n",
      "[153,  1600] loss: 2.303454\n",
      "[153,  1800] loss: 2.303505\n",
      "[153,  2000] loss: 2.303631\n",
      "[153,  2200] loss: 2.303292\n",
      "[153,  2400] loss: 2.302917\n",
      "[153,  2600] loss: 2.303894\n",
      "[153,  2800] loss: 2.303018\n",
      "[153,  3000] loss: 2.303616\n",
      "[153,  3200] loss: 2.303543\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[154,   200] loss: 2.303337\n",
      "[154,   400] loss: 2.303335\n",
      "[154,   600] loss: 2.303734\n",
      "[154,   800] loss: 2.303518\n",
      "[154,  1000] loss: 2.303720\n",
      "[154,  1200] loss: 2.303587\n",
      "[154,  1400] loss: 2.303593\n",
      "[154,  1600] loss: 2.303282\n",
      "[154,  1800] loss: 2.303297\n",
      "[154,  2000] loss: 2.303580\n",
      "[154,  2200] loss: 2.303641\n",
      "[154,  2400] loss: 2.303091\n",
      "[154,  2600] loss: 2.303618\n",
      "[154,  2800] loss: 2.303451\n",
      "[154,  3000] loss: 2.303470\n",
      "[154,  3200] loss: 2.303345\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[155,   200] loss: 2.303555\n",
      "[155,   400] loss: 2.303860\n",
      "[155,   600] loss: 2.303074\n",
      "[155,   800] loss: 2.303558\n",
      "[155,  1000] loss: 2.303376\n",
      "[155,  1200] loss: 2.303403\n",
      "[155,  1400] loss: 2.303889\n",
      "[155,  1600] loss: 2.302925\n",
      "[155,  1800] loss: 2.303516\n",
      "[155,  2000] loss: 2.303906\n",
      "[155,  2200] loss: 2.303316\n",
      "[155,  2400] loss: 2.303421\n",
      "[155,  2600] loss: 2.303914\n",
      "[155,  2800] loss: 2.303496\n",
      "[155,  3000] loss: 2.303348\n",
      "[155,  3200] loss: 2.303571\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[156,   200] loss: 2.303608\n",
      "[156,   400] loss: 2.303430\n",
      "[156,   600] loss: 2.303375\n",
      "[156,   800] loss: 2.303370\n",
      "[156,  1000] loss: 2.303393\n",
      "[156,  1200] loss: 2.303143\n",
      "[156,  1400] loss: 2.303565\n",
      "[156,  1600] loss: 2.303441\n",
      "[156,  1800] loss: 2.303633\n",
      "[156,  2000] loss: 2.303297\n",
      "[156,  2200] loss: 2.303418\n",
      "[156,  2400] loss: 2.303396\n",
      "[156,  2600] loss: 2.303471\n",
      "[156,  2800] loss: 2.303321\n",
      "[156,  3000] loss: 2.303581\n",
      "[156,  3200] loss: 2.303556\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[157,   200] loss: 2.303918\n",
      "[157,   400] loss: 2.303620\n",
      "[157,   600] loss: 2.303077\n",
      "[157,   800] loss: 2.303555\n",
      "[157,  1000] loss: 2.303395\n",
      "[157,  1200] loss: 2.303208\n",
      "[157,  1400] loss: 2.303919\n",
      "[157,  1600] loss: 2.303274\n",
      "[157,  1800] loss: 2.303327\n",
      "[157,  2000] loss: 2.303690\n",
      "[157,  2200] loss: 2.303468\n",
      "[157,  2400] loss: 2.303518\n",
      "[157,  2600] loss: 2.303526\n",
      "[157,  2800] loss: 2.303388\n",
      "[157,  3000] loss: 2.303297\n",
      "[157,  3200] loss: 2.303675\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[158,   200] loss: 2.303845\n",
      "[158,   400] loss: 2.303564\n",
      "[158,   600] loss: 2.303477\n",
      "[158,   800] loss: 2.303627\n",
      "[158,  1000] loss: 2.303464\n",
      "[158,  1200] loss: 2.303241\n",
      "[158,  1400] loss: 2.303687\n",
      "[158,  1600] loss: 2.303196\n",
      "[158,  1800] loss: 2.303758\n",
      "[158,  2000] loss: 2.303538\n",
      "[158,  2200] loss: 2.303425\n",
      "[158,  2400] loss: 2.303542\n",
      "[158,  2600] loss: 2.303174\n",
      "[158,  2800] loss: 2.303584\n",
      "[158,  3000] loss: 2.303926\n",
      "[158,  3200] loss: 2.303166\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[159,   200] loss: 2.303388\n",
      "[159,   400] loss: 2.303520\n",
      "[159,   600] loss: 2.303769\n",
      "[159,   800] loss: 2.303477\n",
      "[159,  1000] loss: 2.303523\n",
      "[159,  1200] loss: 2.302916\n",
      "[159,  1400] loss: 2.303669\n",
      "[159,  1600] loss: 2.302922\n",
      "[159,  1800] loss: 2.303384\n",
      "[159,  2000] loss: 2.303554\n",
      "[159,  2200] loss: 2.303834\n",
      "[159,  2400] loss: 2.303160\n",
      "[159,  2600] loss: 2.303142\n",
      "[159,  2800] loss: 2.303687\n",
      "[159,  3000] loss: 2.303633\n",
      "[159,  3200] loss: 2.303268\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[160,   200] loss: 2.303487\n",
      "[160,   400] loss: 2.303332\n",
      "[160,   600] loss: 2.303540\n",
      "[160,   800] loss: 2.303311\n",
      "[160,  1000] loss: 2.303221\n",
      "[160,  1200] loss: 2.303027\n",
      "[160,  1400] loss: 2.303719\n",
      "[160,  1600] loss: 2.303653\n",
      "[160,  1800] loss: 2.303793\n",
      "[160,  2000] loss: 2.303117\n",
      "[160,  2200] loss: 2.303737\n",
      "[160,  2400] loss: 2.303659\n",
      "[160,  2600] loss: 2.303571\n",
      "[160,  2800] loss: 2.303120\n",
      "[160,  3000] loss: 2.303436\n",
      "[160,  3200] loss: 2.303361\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[161,   200] loss: 2.302479\n",
      "[161,   400] loss: 2.303626\n",
      "[161,   600] loss: 2.303320\n",
      "[161,   800] loss: 2.303369\n",
      "[161,  1000] loss: 2.303618\n",
      "[161,  1200] loss: 2.303351\n",
      "[161,  1400] loss: 2.303517\n",
      "[161,  1600] loss: 2.303706\n",
      "[161,  1800] loss: 2.303113\n",
      "[161,  2000] loss: 2.303323\n",
      "[161,  2200] loss: 2.303854\n",
      "[161,  2400] loss: 2.303579\n",
      "[161,  2600] loss: 2.303081\n",
      "[161,  2800] loss: 2.303358\n",
      "[161,  3000] loss: 2.303689\n",
      "[161,  3200] loss: 2.303324\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[162,   200] loss: 2.303471\n",
      "[162,   400] loss: 2.303751\n",
      "[162,   600] loss: 2.303393\n",
      "[162,   800] loss: 2.303559\n",
      "[162,  1000] loss: 2.303395\n",
      "[162,  1200] loss: 2.303488\n",
      "[162,  1400] loss: 2.303466\n",
      "[162,  1600] loss: 2.303184\n",
      "[162,  1800] loss: 2.303826\n",
      "[162,  2000] loss: 2.303015\n",
      "[162,  2200] loss: 2.303623\n",
      "[162,  2400] loss: 2.303470\n",
      "[162,  2600] loss: 2.303445\n",
      "[162,  2800] loss: 2.303712\n",
      "[162,  3000] loss: 2.303416\n",
      "[162,  3200] loss: 2.303602\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[163,   200] loss: 2.303599\n",
      "[163,   400] loss: 2.303712\n",
      "[163,   600] loss: 2.302880\n",
      "[163,   800] loss: 2.303124\n",
      "[163,  1000] loss: 2.303500\n",
      "[163,  1200] loss: 2.303145\n",
      "[163,  1400] loss: 2.302989\n",
      "[163,  1600] loss: 2.303205\n",
      "[163,  1800] loss: 2.303233\n",
      "[163,  2000] loss: 2.303373\n",
      "[163,  2200] loss: 2.303993\n",
      "[163,  2400] loss: 2.303625\n",
      "[163,  2600] loss: 2.303079\n",
      "[163,  2800] loss: 2.303034\n",
      "[163,  3000] loss: 2.303485\n",
      "[163,  3200] loss: 2.303575\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[164,   200] loss: 2.303278\n",
      "[164,   400] loss: 2.303133\n",
      "[164,   600] loss: 2.303522\n",
      "[164,   800] loss: 2.303121\n",
      "[164,  1000] loss: 2.303226\n",
      "[164,  1200] loss: 2.303674\n",
      "[164,  1400] loss: 2.302950\n",
      "[164,  1600] loss: 2.303652\n",
      "[164,  1800] loss: 2.303894\n",
      "[164,  2000] loss: 2.303540\n",
      "[164,  2200] loss: 2.303064\n",
      "[164,  2400] loss: 2.303569\n",
      "[164,  2600] loss: 2.303195\n",
      "[164,  2800] loss: 2.303814\n",
      "[164,  3000] loss: 2.303154\n",
      "[164,  3200] loss: 2.302770\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[165,   200] loss: 2.303782\n",
      "[165,   400] loss: 2.303192\n",
      "[165,   600] loss: 2.303592\n",
      "[165,   800] loss: 2.303617\n",
      "[165,  1000] loss: 2.303571\n",
      "[165,  1200] loss: 2.303198\n",
      "[165,  1400] loss: 2.303347\n",
      "[165,  1600] loss: 2.303586\n",
      "[165,  1800] loss: 2.303645\n",
      "[165,  2000] loss: 2.303550\n",
      "[165,  2200] loss: 2.303540\n",
      "[165,  2400] loss: 2.303621\n",
      "[165,  2600] loss: 2.303661\n",
      "[165,  2800] loss: 2.303106\n",
      "[165,  3000] loss: 2.303693\n",
      "[165,  3200] loss: 2.303430\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[166,   200] loss: 2.303366\n",
      "[166,   400] loss: 2.303528\n",
      "[166,   600] loss: 2.303281\n",
      "[166,   800] loss: 2.303328\n",
      "[166,  1000] loss: 2.303566\n",
      "[166,  1200] loss: 2.303785\n",
      "[166,  1400] loss: 2.303629\n",
      "[166,  1600] loss: 2.303229\n",
      "[166,  1800] loss: 2.303353\n",
      "[166,  2000] loss: 2.303348\n",
      "[166,  2200] loss: 2.303735\n",
      "[166,  2400] loss: 2.303577\n",
      "[166,  2600] loss: 2.303445\n",
      "[166,  2800] loss: 2.302992\n",
      "[166,  3000] loss: 2.303492\n",
      "[166,  3200] loss: 2.303761\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[167,   200] loss: 2.303606\n",
      "[167,   400] loss: 2.303568\n",
      "[167,   600] loss: 2.303613\n",
      "[167,   800] loss: 2.303348\n",
      "[167,  1000] loss: 2.303078\n",
      "[167,  1200] loss: 2.304048\n",
      "[167,  1400] loss: 2.303511\n",
      "[167,  1600] loss: 2.303871\n",
      "[167,  1800] loss: 2.302998\n",
      "[167,  2000] loss: 2.303899\n",
      "[167,  2200] loss: 2.303091\n",
      "[167,  2400] loss: 2.303304\n",
      "[167,  2600] loss: 2.303432\n",
      "[167,  2800] loss: 2.303386\n",
      "[167,  3000] loss: 2.303452\n",
      "[167,  3200] loss: 2.303561\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[168,   200] loss: 2.303532\n",
      "[168,   400] loss: 2.303591\n",
      "[168,   600] loss: 2.303739\n",
      "[168,   800] loss: 2.303378\n",
      "[168,  1000] loss: 2.302925\n",
      "[168,  1200] loss: 2.303434\n",
      "[168,  1400] loss: 2.303379\n",
      "[168,  1600] loss: 2.303563\n",
      "[168,  1800] loss: 2.303494\n",
      "[168,  2000] loss: 2.303285\n",
      "[168,  2200] loss: 2.303357\n",
      "[168,  2400] loss: 2.303396\n",
      "[168,  2600] loss: 2.303810\n",
      "[168,  2800] loss: 2.303492\n",
      "[168,  3000] loss: 2.303492\n",
      "[168,  3200] loss: 2.303348\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[169,   200] loss: 2.303718\n",
      "[169,   400] loss: 2.303286\n",
      "[169,   600] loss: 2.303673\n",
      "[169,   800] loss: 2.303478\n",
      "[169,  1000] loss: 2.303726\n",
      "[169,  1200] loss: 2.303511\n",
      "[169,  1400] loss: 2.303786\n",
      "[169,  1600] loss: 2.303425\n",
      "[169,  1800] loss: 2.303292\n",
      "[169,  2000] loss: 2.303577\n",
      "[169,  2200] loss: 2.303516\n",
      "[169,  2400] loss: 2.303567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169,  2600] loss: 2.303475\n",
      "[169,  2800] loss: 2.303462\n",
      "[169,  3000] loss: 2.303607\n",
      "[169,  3200] loss: 2.303261\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[170,   200] loss: 2.303833\n",
      "[170,   400] loss: 2.303523\n",
      "[170,   600] loss: 2.303881\n",
      "[170,   800] loss: 2.303028\n",
      "[170,  1000] loss: 2.303360\n",
      "[170,  1200] loss: 2.303593\n",
      "[170,  1400] loss: 2.303683\n",
      "[170,  1600] loss: 2.303347\n",
      "[170,  1800] loss: 2.303955\n",
      "[170,  2000] loss: 2.303603\n",
      "[170,  2200] loss: 2.303851\n",
      "[170,  2400] loss: 2.303101\n",
      "[170,  2600] loss: 2.303078\n",
      "[170,  2800] loss: 2.303694\n",
      "[170,  3000] loss: 2.303512\n",
      "[170,  3200] loss: 2.303462\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[171,   200] loss: 2.303294\n",
      "[171,   400] loss: 2.303974\n",
      "[171,   600] loss: 2.303293\n",
      "[171,   800] loss: 2.303745\n",
      "[171,  1000] loss: 2.303243\n",
      "[171,  1200] loss: 2.303389\n",
      "[171,  1400] loss: 2.303374\n",
      "[171,  1600] loss: 2.303286\n",
      "[171,  1800] loss: 2.303359\n",
      "[171,  2000] loss: 2.303162\n",
      "[171,  2200] loss: 2.303601\n",
      "[171,  2400] loss: 2.303632\n",
      "[171,  2600] loss: 2.303357\n",
      "[171,  2800] loss: 2.303670\n",
      "[171,  3000] loss: 2.303650\n",
      "[171,  3200] loss: 2.303304\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[172,   200] loss: 2.303371\n",
      "[172,   400] loss: 2.303976\n",
      "[172,   600] loss: 2.303568\n",
      "[172,   800] loss: 2.303528\n",
      "[172,  1000] loss: 2.303435\n",
      "[172,  1200] loss: 2.303187\n",
      "[172,  1400] loss: 2.303313\n",
      "[172,  1600] loss: 2.303126\n",
      "[172,  1800] loss: 2.303407\n",
      "[172,  2000] loss: 2.303024\n",
      "[172,  2200] loss: 2.303163\n",
      "[172,  2400] loss: 2.303337\n",
      "[172,  2600] loss: 2.303808\n",
      "[172,  2800] loss: 2.303478\n",
      "[172,  3000] loss: 2.303441\n",
      "[172,  3200] loss: 2.303541\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[173,   200] loss: 2.303436\n",
      "[173,   400] loss: 2.303599\n",
      "[173,   600] loss: 2.303564\n",
      "[173,   800] loss: 2.303611\n",
      "[173,  1000] loss: 2.303453\n",
      "[173,  1200] loss: 2.303165\n",
      "[173,  1400] loss: 2.303481\n",
      "[173,  1600] loss: 2.303287\n",
      "[173,  1800] loss: 2.303509\n",
      "[173,  2000] loss: 2.303279\n",
      "[173,  2200] loss: 2.303269\n",
      "[173,  2400] loss: 2.303655\n",
      "[173,  2600] loss: 2.302819\n",
      "[173,  2800] loss: 2.303477\n",
      "[173,  3000] loss: 2.303426\n",
      "[173,  3200] loss: 2.303610\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[174,   200] loss: 2.303173\n",
      "[174,   400] loss: 2.303568\n",
      "[174,   600] loss: 2.303694\n",
      "[174,   800] loss: 2.303320\n",
      "[174,  1000] loss: 2.303907\n",
      "[174,  1200] loss: 2.303291\n",
      "[174,  1400] loss: 2.303214\n",
      "[174,  1600] loss: 2.303649\n",
      "[174,  1800] loss: 2.303434\n",
      "[174,  2000] loss: 2.303680\n",
      "[174,  2200] loss: 2.303379\n",
      "[174,  2400] loss: 2.303510\n",
      "[174,  2600] loss: 2.303202\n",
      "[174,  2800] loss: 2.303761\n",
      "[174,  3000] loss: 2.303099\n",
      "[174,  3200] loss: 2.303705\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[175,   200] loss: 2.303362\n",
      "[175,   400] loss: 2.303462\n",
      "[175,   600] loss: 2.303363\n",
      "[175,   800] loss: 2.303522\n",
      "[175,  1000] loss: 2.302794\n",
      "[175,  1200] loss: 2.303221\n",
      "[175,  1400] loss: 2.303752\n",
      "[175,  1600] loss: 2.303525\n",
      "[175,  1800] loss: 2.303350\n",
      "[175,  2000] loss: 2.303819\n",
      "[175,  2200] loss: 2.303591\n",
      "[175,  2400] loss: 2.303492\n",
      "[175,  2600] loss: 2.303782\n",
      "[175,  2800] loss: 2.303302\n",
      "[175,  3000] loss: 2.303566\n",
      "[175,  3200] loss: 2.303049\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[176,   200] loss: 2.303344\n",
      "[176,   400] loss: 2.303555\n",
      "[176,   600] loss: 2.303405\n",
      "[176,   800] loss: 2.303776\n",
      "[176,  1000] loss: 2.303115\n",
      "[176,  1200] loss: 2.302749\n",
      "[176,  1400] loss: 2.303574\n",
      "[176,  1600] loss: 2.303361\n",
      "[176,  1800] loss: 2.303624\n",
      "[176,  2000] loss: 2.303632\n",
      "[176,  2200] loss: 2.304009\n",
      "[176,  2400] loss: 2.303767\n",
      "[176,  2600] loss: 2.303227\n",
      "[176,  2800] loss: 2.303312\n",
      "[176,  3000] loss: 2.303679\n",
      "[176,  3200] loss: 2.303503\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[177,   200] loss: 2.303465\n",
      "[177,   400] loss: 2.303338\n",
      "[177,   600] loss: 2.303759\n",
      "[177,   800] loss: 2.303740\n",
      "[177,  1000] loss: 2.303359\n",
      "[177,  1200] loss: 2.302809\n",
      "[177,  1400] loss: 2.303842\n",
      "[177,  1600] loss: 2.303488\n",
      "[177,  1800] loss: 2.303673\n",
      "[177,  2000] loss: 2.303690\n",
      "[177,  2200] loss: 2.303315\n",
      "[177,  2400] loss: 2.303703\n",
      "[177,  2600] loss: 2.303767\n",
      "[177,  2800] loss: 2.302885\n",
      "[177,  3000] loss: 2.303381\n",
      "[177,  3200] loss: 2.303592\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[178,   200] loss: 2.303845\n",
      "[178,   400] loss: 2.303320\n",
      "[178,   600] loss: 2.303755\n",
      "[178,   800] loss: 2.303748\n",
      "[178,  1000] loss: 2.303536\n",
      "[178,  1200] loss: 2.303680\n",
      "[178,  1400] loss: 2.303891\n",
      "[178,  1600] loss: 2.303543\n",
      "[178,  1800] loss: 2.303455\n",
      "[178,  2000] loss: 2.303641\n",
      "[178,  2200] loss: 2.303513\n",
      "[178,  2400] loss: 2.303406\n",
      "[178,  2600] loss: 2.303460\n",
      "[178,  2800] loss: 2.303335\n",
      "[178,  3000] loss: 2.303740\n",
      "[178,  3200] loss: 2.303482\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[179,   200] loss: 2.303589\n",
      "[179,   400] loss: 2.303462\n",
      "[179,   600] loss: 2.303384\n",
      "[179,   800] loss: 2.303478\n",
      "[179,  1000] loss: 2.303285\n",
      "[179,  1200] loss: 2.303787\n",
      "[179,  1400] loss: 2.303638\n",
      "[179,  1600] loss: 2.303707\n",
      "[179,  1800] loss: 2.303300\n",
      "[179,  2000] loss: 2.303834\n",
      "[179,  2200] loss: 2.303599\n",
      "[179,  2400] loss: 2.303906\n",
      "[179,  2600] loss: 2.303099\n",
      "[179,  2800] loss: 2.303553\n",
      "[179,  3000] loss: 2.303928\n",
      "[179,  3200] loss: 2.303760\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[180,   200] loss: 2.303621\n",
      "[180,   400] loss: 2.302623\n",
      "[180,   600] loss: 2.303697\n",
      "[180,   800] loss: 2.303245\n",
      "[180,  1000] loss: 2.303679\n",
      "[180,  1200] loss: 2.303971\n",
      "[180,  1400] loss: 2.303819\n",
      "[180,  1600] loss: 2.303719\n",
      "[180,  1800] loss: 2.303521\n",
      "[180,  2000] loss: 2.303120\n",
      "[180,  2200] loss: 2.303417\n",
      "[180,  2400] loss: 2.303400\n",
      "[180,  2600] loss: 2.303836\n",
      "[180,  2800] loss: 2.303503\n",
      "[180,  3000] loss: 2.303655\n",
      "[180,  3200] loss: 2.303490\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[181,   200] loss: 2.303327\n",
      "[181,   400] loss: 2.303530\n",
      "[181,   600] loss: 2.303314\n",
      "[181,   800] loss: 2.303302\n",
      "[181,  1000] loss: 2.303436\n",
      "[181,  1200] loss: 2.303539\n",
      "[181,  1400] loss: 2.303752\n",
      "[181,  1600] loss: 2.303692\n",
      "[181,  1800] loss: 2.303323\n",
      "[181,  2000] loss: 2.303653\n",
      "[181,  2200] loss: 2.303310\n",
      "[181,  2400] loss: 2.303840\n",
      "[181,  2600] loss: 2.303370\n",
      "[181,  2800] loss: 2.303660\n",
      "[181,  3000] loss: 2.303448\n",
      "[181,  3200] loss: 2.303223\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[182,   200] loss: 2.304006\n",
      "[182,   400] loss: 2.303617\n",
      "[182,   600] loss: 2.303656\n",
      "[182,   800] loss: 2.303467\n",
      "[182,  1000] loss: 2.303343\n",
      "[182,  1200] loss: 2.303772\n",
      "[182,  1400] loss: 2.303562\n",
      "[182,  1600] loss: 2.303631\n",
      "[182,  1800] loss: 2.303319\n",
      "[182,  2000] loss: 2.303582\n",
      "[182,  2200] loss: 2.303569\n",
      "[182,  2400] loss: 2.303766\n",
      "[182,  2600] loss: 2.302950\n",
      "[182,  2800] loss: 2.303345\n",
      "[182,  3000] loss: 2.303520\n",
      "[182,  3200] loss: 2.303456\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[183,   200] loss: 2.303282\n",
      "[183,   400] loss: 2.303706\n",
      "[183,   600] loss: 2.303604\n",
      "[183,   800] loss: 2.303614\n",
      "[183,  1000] loss: 2.303562\n",
      "[183,  1200] loss: 2.302913\n",
      "[183,  1400] loss: 2.303941\n",
      "[183,  1600] loss: 2.303609\n",
      "[183,  1800] loss: 2.303466\n",
      "[183,  2000] loss: 2.303759\n",
      "[183,  2200] loss: 2.303517\n",
      "[183,  2400] loss: 2.303261\n",
      "[183,  2600] loss: 2.303372\n",
      "[183,  2800] loss: 2.303228\n",
      "[183,  3000] loss: 2.303255\n",
      "[183,  3200] loss: 2.303577\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[184,   200] loss: 2.302991\n",
      "[184,   400] loss: 2.303397\n",
      "[184,   600] loss: 2.303718\n",
      "[184,   800] loss: 2.303663\n",
      "[184,  1000] loss: 2.303806\n",
      "[184,  1200] loss: 2.303682\n",
      "[184,  1400] loss: 2.303673\n",
      "[184,  1600] loss: 2.303771\n",
      "[184,  1800] loss: 2.303477\n",
      "[184,  2000] loss: 2.303172\n",
      "[184,  2200] loss: 2.303529\n",
      "[184,  2400] loss: 2.303081\n",
      "[184,  2600] loss: 2.303877\n",
      "[184,  2800] loss: 2.303555\n",
      "[184,  3000] loss: 2.302774\n",
      "[184,  3200] loss: 2.303290\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[185,   200] loss: 2.303821\n",
      "[185,   400] loss: 2.303687\n",
      "[185,   600] loss: 2.303536\n",
      "[185,   800] loss: 2.303687\n",
      "[185,  1000] loss: 2.303494\n",
      "[185,  1200] loss: 2.303941\n",
      "[185,  1400] loss: 2.303412\n",
      "[185,  1600] loss: 2.303317\n",
      "[185,  1800] loss: 2.303509\n",
      "[185,  2000] loss: 2.303586\n",
      "[185,  2200] loss: 2.303277\n",
      "[185,  2400] loss: 2.303127\n",
      "[185,  2600] loss: 2.303544\n",
      "[185,  2800] loss: 2.302940\n",
      "[185,  3000] loss: 2.303585\n",
      "[185,  3200] loss: 2.303363\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[186,   200] loss: 2.303345\n",
      "[186,   400] loss: 2.303696\n",
      "[186,   600] loss: 2.303405\n",
      "[186,   800] loss: 2.303533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186,  1000] loss: 2.303042\n",
      "[186,  1200] loss: 2.303468\n",
      "[186,  1400] loss: 2.303220\n",
      "[186,  1600] loss: 2.303715\n",
      "[186,  1800] loss: 2.303471\n",
      "[186,  2000] loss: 2.303557\n",
      "[186,  2200] loss: 2.303326\n",
      "[186,  2400] loss: 2.303400\n",
      "[186,  2600] loss: 2.303495\n",
      "[186,  2800] loss: 2.303562\n",
      "[186,  3000] loss: 2.303602\n",
      "[186,  3200] loss: 2.303656\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[187,   200] loss: 2.303340\n",
      "[187,   400] loss: 2.303440\n",
      "[187,   600] loss: 2.303517\n",
      "[187,   800] loss: 2.303584\n",
      "[187,  1000] loss: 2.303198\n",
      "[187,  1200] loss: 2.303822\n",
      "[187,  1400] loss: 2.303386\n",
      "[187,  1600] loss: 2.303630\n",
      "[187,  1800] loss: 2.303207\n",
      "[187,  2000] loss: 2.303250\n",
      "[187,  2200] loss: 2.303463\n",
      "[187,  2400] loss: 2.303536\n",
      "[187,  2600] loss: 2.303360\n",
      "[187,  2800] loss: 2.303429\n",
      "[187,  3000] loss: 2.303136\n",
      "[187,  3200] loss: 2.303419\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[188,   200] loss: 2.303191\n",
      "[188,   400] loss: 2.303346\n",
      "[188,   600] loss: 2.302992\n",
      "[188,   800] loss: 2.303792\n",
      "[188,  1000] loss: 2.303267\n",
      "[188,  1200] loss: 2.303465\n",
      "[188,  1400] loss: 2.303516\n",
      "[188,  1600] loss: 2.303007\n",
      "[188,  1800] loss: 2.303563\n",
      "[188,  2000] loss: 2.303178\n",
      "[188,  2200] loss: 2.303381\n",
      "[188,  2400] loss: 2.303156\n",
      "[188,  2600] loss: 2.303925\n",
      "[188,  2800] loss: 2.303757\n",
      "[188,  3000] loss: 2.303129\n",
      "[188,  3200] loss: 2.303602\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[189,   200] loss: 2.303280\n",
      "[189,   400] loss: 2.303607\n",
      "[189,   600] loss: 2.303545\n",
      "[189,   800] loss: 2.303291\n",
      "[189,  1000] loss: 2.303344\n",
      "[189,  1200] loss: 2.303732\n",
      "[189,  1400] loss: 2.303133\n",
      "[189,  1600] loss: 2.303518\n",
      "[189,  1800] loss: 2.303353\n",
      "[189,  2000] loss: 2.303427\n",
      "[189,  2200] loss: 2.303324\n",
      "[189,  2400] loss: 2.303942\n",
      "[189,  2600] loss: 2.303532\n",
      "[189,  2800] loss: 2.303541\n",
      "[189,  3000] loss: 2.303284\n",
      "[189,  3200] loss: 2.303829\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[190,   200] loss: 2.303198\n",
      "[190,   400] loss: 2.303542\n",
      "[190,   600] loss: 2.303033\n",
      "[190,   800] loss: 2.303497\n",
      "[190,  1000] loss: 2.303374\n",
      "[190,  1200] loss: 2.303185\n",
      "[190,  1400] loss: 2.303602\n",
      "[190,  1600] loss: 2.303727\n",
      "[190,  1800] loss: 2.303299\n",
      "[190,  2000] loss: 2.303687\n",
      "[190,  2200] loss: 2.303582\n",
      "[190,  2400] loss: 2.302999\n",
      "[190,  2600] loss: 2.303581\n",
      "[190,  2800] loss: 2.303222\n",
      "[190,  3000] loss: 2.303629\n",
      "[190,  3200] loss: 2.303247\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[191,   200] loss: 2.303024\n",
      "[191,   400] loss: 2.303551\n",
      "[191,   600] loss: 2.303646\n",
      "[191,   800] loss: 2.303579\n",
      "[191,  1000] loss: 2.303519\n",
      "[191,  1200] loss: 2.303543\n",
      "[191,  1400] loss: 2.303326\n",
      "[191,  1600] loss: 2.303800\n",
      "[191,  1800] loss: 2.303446\n",
      "[191,  2000] loss: 2.304105\n",
      "[191,  2200] loss: 2.303123\n",
      "[191,  2400] loss: 2.303445\n",
      "[191,  2600] loss: 2.303058\n",
      "[191,  2800] loss: 2.303708\n",
      "[191,  3000] loss: 2.303303\n",
      "[191,  3200] loss: 2.303949\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[192,   200] loss: 2.303424\n",
      "[192,   400] loss: 2.303293\n",
      "[192,   600] loss: 2.303330\n",
      "[192,   800] loss: 2.303593\n",
      "[192,  1000] loss: 2.303243\n",
      "[192,  1200] loss: 2.303432\n",
      "[192,  1400] loss: 2.303289\n",
      "[192,  1600] loss: 2.303347\n",
      "[192,  1800] loss: 2.303794\n",
      "[192,  2000] loss: 2.303886\n",
      "[192,  2200] loss: 2.303684\n",
      "[192,  2400] loss: 2.303224\n",
      "[192,  2600] loss: 2.303602\n",
      "[192,  2800] loss: 2.303440\n",
      "[192,  3000] loss: 2.303800\n",
      "[192,  3200] loss: 2.303616\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[193,   200] loss: 2.303453\n",
      "[193,   400] loss: 2.303568\n",
      "[193,   600] loss: 2.303443\n",
      "[193,   800] loss: 2.303752\n",
      "[193,  1000] loss: 2.303320\n",
      "[193,  1200] loss: 2.303686\n",
      "[193,  1400] loss: 2.303615\n",
      "[193,  1600] loss: 2.303100\n",
      "[193,  1800] loss: 2.303641\n",
      "[193,  2000] loss: 2.303548\n",
      "[193,  2200] loss: 2.303574\n",
      "[193,  2400] loss: 2.303420\n",
      "[193,  2600] loss: 2.303407\n",
      "[193,  2800] loss: 2.303599\n",
      "[193,  3000] loss: 2.303535\n",
      "[193,  3200] loss: 2.303382\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[194,   200] loss: 2.303548\n",
      "[194,   400] loss: 2.303272\n",
      "[194,   600] loss: 2.303693\n",
      "[194,   800] loss: 2.303689\n",
      "[194,  1000] loss: 2.302648\n",
      "[194,  1200] loss: 2.303787\n",
      "[194,  1400] loss: 2.303241\n",
      "[194,  1600] loss: 2.303933\n",
      "[194,  1800] loss: 2.303633\n",
      "[194,  2000] loss: 2.303937\n",
      "[194,  2200] loss: 2.303355\n",
      "[194,  2400] loss: 2.303378\n",
      "[194,  2600] loss: 2.303516\n",
      "[194,  2800] loss: 2.303367\n",
      "[194,  3000] loss: 2.303676\n",
      "[194,  3200] loss: 2.303269\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[195,   200] loss: 2.303601\n",
      "[195,   400] loss: 2.303495\n",
      "[195,   600] loss: 2.303576\n",
      "[195,   800] loss: 2.303464\n",
      "[195,  1000] loss: 2.303377\n",
      "[195,  1200] loss: 2.303152\n",
      "[195,  1400] loss: 2.303696\n",
      "[195,  1600] loss: 2.303411\n",
      "[195,  1800] loss: 2.303614\n",
      "[195,  2000] loss: 2.303881\n",
      "[195,  2200] loss: 2.303137\n",
      "[195,  2400] loss: 2.303526\n",
      "[195,  2600] loss: 2.303786\n",
      "[195,  2800] loss: 2.303442\n",
      "[195,  3000] loss: 2.303763\n",
      "[195,  3200] loss: 2.302907\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[196,   200] loss: 2.303632\n",
      "[196,   400] loss: 2.303386\n",
      "[196,   600] loss: 2.303348\n",
      "[196,   800] loss: 2.303521\n",
      "[196,  1000] loss: 2.303211\n",
      "[196,  1200] loss: 2.304008\n",
      "[196,  1400] loss: 2.303763\n",
      "[196,  1600] loss: 2.303721\n",
      "[196,  1800] loss: 2.303298\n",
      "[196,  2000] loss: 2.303186\n",
      "[196,  2200] loss: 2.303336\n",
      "[196,  2400] loss: 2.303937\n",
      "[196,  2600] loss: 2.303064\n",
      "[196,  2800] loss: 2.303060\n",
      "[196,  3000] loss: 2.303765\n",
      "[196,  3200] loss: 2.303217\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[197,   200] loss: 2.303786\n",
      "[197,   400] loss: 2.303386\n",
      "[197,   600] loss: 2.304087\n",
      "[197,   800] loss: 2.303284\n",
      "[197,  1000] loss: 2.303902\n",
      "[197,  1200] loss: 2.303284\n",
      "[197,  1400] loss: 2.304034\n",
      "[197,  1600] loss: 2.303508\n",
      "[197,  1800] loss: 2.303516\n",
      "[197,  2000] loss: 2.303553\n",
      "[197,  2200] loss: 2.303707\n",
      "[197,  2400] loss: 2.303463\n",
      "[197,  2600] loss: 2.303199\n",
      "[197,  2800] loss: 2.303215\n",
      "[197,  3000] loss: 2.303582\n",
      "[197,  3200] loss: 2.303474\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[198,   200] loss: 2.303202\n",
      "[198,   400] loss: 2.303775\n",
      "[198,   600] loss: 2.303248\n",
      "[198,   800] loss: 2.303489\n",
      "[198,  1000] loss: 2.303732\n",
      "[198,  1200] loss: 2.303435\n",
      "[198,  1400] loss: 2.303159\n",
      "[198,  1600] loss: 2.303599\n",
      "[198,  1800] loss: 2.303627\n",
      "[198,  2000] loss: 2.303891\n",
      "[198,  2200] loss: 2.303483\n",
      "[198,  2400] loss: 2.303787\n",
      "[198,  2600] loss: 2.303749\n",
      "[198,  2800] loss: 2.302845\n",
      "[198,  3000] loss: 2.302949\n",
      "[198,  3200] loss: 2.303664\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[199,   200] loss: 2.303691\n",
      "[199,   400] loss: 2.303483\n",
      "[199,   600] loss: 2.302878\n",
      "[199,   800] loss: 2.303560\n",
      "[199,  1000] loss: 2.303293\n",
      "[199,  1200] loss: 2.303575\n",
      "[199,  1400] loss: 2.303750\n",
      "[199,  1600] loss: 2.303846\n",
      "[199,  1800] loss: 2.303679\n",
      "[199,  2000] loss: 2.303232\n",
      "[199,  2200] loss: 2.303454\n",
      "[199,  2400] loss: 2.302933\n",
      "[199,  2600] loss: 2.303233\n",
      "[199,  2800] loss: 2.303760\n",
      "[199,  3000] loss: 2.303660\n",
      "[199,  3200] loss: 2.303953\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[200,   200] loss: 2.303555\n",
      "[200,   400] loss: 2.303442\n",
      "[200,   600] loss: 2.303802\n",
      "[200,   800] loss: 2.303659\n",
      "[200,  1000] loss: 2.303436\n",
      "[200,  1200] loss: 2.303195\n",
      "[200,  1400] loss: 2.303665\n",
      "[200,  1600] loss: 2.303104\n",
      "[200,  1800] loss: 2.303186\n",
      "[200,  2000] loss: 2.303440\n",
      "[200,  2200] loss: 2.303662\n",
      "[200,  2400] loss: 2.303328\n",
      "[200,  2600] loss: 2.303663\n",
      "[200,  2800] loss: 2.303733\n",
      "[200,  3000] loss: 2.303612\n",
      "[200,  3200] loss: 2.303266\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[201,   200] loss: 2.303648\n",
      "[201,   400] loss: 2.303318\n",
      "[201,   600] loss: 2.303417\n",
      "[201,   800] loss: 2.303620\n",
      "[201,  1000] loss: 2.303233\n",
      "[201,  1200] loss: 2.303551\n",
      "[201,  1400] loss: 2.303605\n",
      "[201,  1600] loss: 2.303833\n",
      "[201,  1800] loss: 2.303677\n",
      "[201,  2000] loss: 2.303680\n",
      "[201,  2200] loss: 2.303313\n",
      "[201,  2400] loss: 2.303498\n",
      "[201,  2600] loss: 2.303895\n",
      "[201,  2800] loss: 2.303429\n",
      "[201,  3000] loss: 2.303653\n",
      "[201,  3200] loss: 2.303533\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[202,   200] loss: 2.302898\n",
      "[202,   400] loss: 2.304050\n",
      "[202,   600] loss: 2.303299\n",
      "[202,   800] loss: 2.303375\n",
      "[202,  1000] loss: 2.303670\n",
      "[202,  1200] loss: 2.303205\n",
      "[202,  1400] loss: 2.303390\n",
      "[202,  1600] loss: 2.303645\n",
      "[202,  1800] loss: 2.303579\n",
      "[202,  2000] loss: 2.303524\n",
      "[202,  2200] loss: 2.303350\n",
      "[202,  2400] loss: 2.303282\n",
      "[202,  2600] loss: 2.303810\n",
      "[202,  2800] loss: 2.303445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202,  3000] loss: 2.302989\n",
      "[202,  3200] loss: 2.303445\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[203,   200] loss: 2.303285\n",
      "[203,   400] loss: 2.303278\n",
      "[203,   600] loss: 2.303834\n",
      "[203,   800] loss: 2.303584\n",
      "[203,  1000] loss: 2.303533\n",
      "[203,  1200] loss: 2.303962\n",
      "[203,  1400] loss: 2.303504\n",
      "[203,  1600] loss: 2.303322\n",
      "[203,  1800] loss: 2.303484\n",
      "[203,  2000] loss: 2.303584\n",
      "[203,  2200] loss: 2.303408\n",
      "[203,  2400] loss: 2.303056\n",
      "[203,  2600] loss: 2.303641\n",
      "[203,  2800] loss: 2.303985\n",
      "[203,  3000] loss: 2.303688\n",
      "[203,  3200] loss: 2.302848\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[204,   200] loss: 2.303841\n",
      "[204,   400] loss: 2.303131\n",
      "[204,   600] loss: 2.302856\n",
      "[204,   800] loss: 2.303705\n",
      "[204,  1000] loss: 2.304004\n",
      "[204,  1200] loss: 2.303161\n",
      "[204,  1400] loss: 2.303380\n",
      "[204,  1600] loss: 2.303098\n",
      "[204,  1800] loss: 2.303797\n",
      "[204,  2000] loss: 2.303334\n",
      "[204,  2200] loss: 2.303164\n",
      "[204,  2400] loss: 2.303594\n",
      "[204,  2600] loss: 2.303582\n",
      "[204,  2800] loss: 2.303507\n",
      "[204,  3000] loss: 2.303442\n",
      "[204,  3200] loss: 2.303988\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[205,   200] loss: 2.303579\n",
      "[205,   400] loss: 2.303153\n",
      "[205,   600] loss: 2.303650\n",
      "[205,   800] loss: 2.303665\n",
      "[205,  1000] loss: 2.303588\n",
      "[205,  1200] loss: 2.303188\n",
      "[205,  1400] loss: 2.303514\n",
      "[205,  1600] loss: 2.303663\n",
      "[205,  1800] loss: 2.303933\n",
      "[205,  2000] loss: 2.303245\n",
      "[205,  2200] loss: 2.303876\n",
      "[205,  2400] loss: 2.303617\n",
      "[205,  2600] loss: 2.303337\n",
      "[205,  2800] loss: 2.303638\n",
      "[205,  3000] loss: 2.303598\n",
      "[205,  3200] loss: 2.303328\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[206,   200] loss: 2.303781\n",
      "[206,   400] loss: 2.303686\n",
      "[206,   600] loss: 2.303623\n",
      "[206,   800] loss: 2.303780\n",
      "[206,  1000] loss: 2.303277\n",
      "[206,  1200] loss: 2.303213\n",
      "[206,  1400] loss: 2.303314\n",
      "[206,  1600] loss: 2.303372\n",
      "[206,  1800] loss: 2.303757\n",
      "[206,  2000] loss: 2.303819\n",
      "[206,  2200] loss: 2.302831\n",
      "[206,  2400] loss: 2.303353\n",
      "[206,  2600] loss: 2.303815\n",
      "[206,  2800] loss: 2.303863\n",
      "[206,  3000] loss: 2.303458\n",
      "[206,  3200] loss: 2.303472\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[207,   200] loss: 2.303773\n",
      "[207,   400] loss: 2.303644\n",
      "[207,   600] loss: 2.303398\n",
      "[207,   800] loss: 2.303324\n",
      "[207,  1000] loss: 2.303626\n",
      "[207,  1200] loss: 2.303351\n",
      "[207,  1400] loss: 2.303650\n",
      "[207,  1600] loss: 2.303479\n",
      "[207,  1800] loss: 2.303914\n",
      "[207,  2000] loss: 2.303270\n",
      "[207,  2200] loss: 2.303794\n",
      "[207,  2400] loss: 2.303313\n",
      "[207,  2600] loss: 2.303477\n",
      "[207,  2800] loss: 2.303722\n",
      "[207,  3000] loss: 2.303318\n",
      "[207,  3200] loss: 2.303626\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[208,   200] loss: 2.303564\n",
      "[208,   400] loss: 2.303855\n",
      "[208,   600] loss: 2.303242\n",
      "[208,   800] loss: 2.303821\n",
      "[208,  1000] loss: 2.303765\n",
      "[208,  1200] loss: 2.303303\n",
      "[208,  1400] loss: 2.303336\n",
      "[208,  1600] loss: 2.303611\n",
      "[208,  1800] loss: 2.303522\n",
      "[208,  2000] loss: 2.303215\n",
      "[208,  2200] loss: 2.303818\n",
      "[208,  2400] loss: 2.303702\n",
      "[208,  2600] loss: 2.303673\n",
      "[208,  2800] loss: 2.303757\n",
      "[208,  3000] loss: 2.303598\n",
      "[208,  3200] loss: 2.303223\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[209,   200] loss: 2.303373\n",
      "[209,   400] loss: 2.303849\n",
      "[209,   600] loss: 2.303547\n",
      "[209,   800] loss: 2.303533\n",
      "[209,  1000] loss: 2.303363\n",
      "[209,  1200] loss: 2.303432\n",
      "[209,  1400] loss: 2.303488\n",
      "[209,  1600] loss: 2.303614\n",
      "[209,  1800] loss: 2.303226\n",
      "[209,  2000] loss: 2.303254\n",
      "[209,  2200] loss: 2.303097\n",
      "[209,  2400] loss: 2.303083\n",
      "[209,  2600] loss: 2.303700\n",
      "[209,  2800] loss: 2.303498\n",
      "[209,  3000] loss: 2.303873\n",
      "[209,  3200] loss: 2.303438\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[210,   200] loss: 2.303481\n",
      "[210,   400] loss: 2.302923\n",
      "[210,   600] loss: 2.303824\n",
      "[210,   800] loss: 2.303856\n",
      "[210,  1000] loss: 2.303154\n",
      "[210,  1200] loss: 2.303530\n",
      "[210,  1400] loss: 2.303270\n",
      "[210,  1600] loss: 2.303459\n",
      "[210,  1800] loss: 2.303517\n",
      "[210,  2000] loss: 2.303278\n",
      "[210,  2200] loss: 2.303559\n",
      "[210,  2400] loss: 2.303730\n",
      "[210,  2600] loss: 2.303469\n",
      "[210,  2800] loss: 2.303205\n",
      "[210,  3000] loss: 2.303733\n",
      "[210,  3200] loss: 2.303510\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[211,   200] loss: 2.303439\n",
      "[211,   400] loss: 2.303140\n",
      "[211,   600] loss: 2.303317\n",
      "[211,   800] loss: 2.303529\n",
      "[211,  1000] loss: 2.303750\n",
      "[211,  1200] loss: 2.303630\n",
      "[211,  1400] loss: 2.303437\n",
      "[211,  1600] loss: 2.303568\n",
      "[211,  1800] loss: 2.303244\n",
      "[211,  2000] loss: 2.303310\n",
      "[211,  2200] loss: 2.303632\n",
      "[211,  2400] loss: 2.303602\n",
      "[211,  2600] loss: 2.303568\n",
      "[211,  2800] loss: 2.303296\n",
      "[211,  3000] loss: 2.304041\n",
      "[211,  3200] loss: 2.303187\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[212,   200] loss: 2.303006\n",
      "[212,   400] loss: 2.303398\n",
      "[212,   600] loss: 2.303840\n",
      "[212,   800] loss: 2.303803\n",
      "[212,  1000] loss: 2.303591\n",
      "[212,  1200] loss: 2.303491\n",
      "[212,  1400] loss: 2.303341\n",
      "[212,  1600] loss: 2.303336\n",
      "[212,  1800] loss: 2.303771\n",
      "[212,  2000] loss: 2.303324\n",
      "[212,  2200] loss: 2.303510\n",
      "[212,  2400] loss: 2.303607\n",
      "[212,  2600] loss: 2.303587\n",
      "[212,  2800] loss: 2.303426\n",
      "[212,  3000] loss: 2.303515\n",
      "[212,  3200] loss: 2.303014\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[213,   200] loss: 2.303485\n",
      "[213,   400] loss: 2.303456\n",
      "[213,   600] loss: 2.303455\n",
      "[213,   800] loss: 2.303725\n",
      "[213,  1000] loss: 2.303523\n",
      "[213,  1200] loss: 2.303593\n",
      "[213,  1400] loss: 2.303349\n",
      "[213,  1600] loss: 2.303945\n",
      "[213,  1800] loss: 2.303598\n",
      "[213,  2000] loss: 2.303333\n",
      "[213,  2200] loss: 2.303230\n",
      "[213,  2400] loss: 2.303584\n",
      "[213,  2600] loss: 2.303362\n",
      "[213,  2800] loss: 2.303290\n",
      "[213,  3000] loss: 2.303600\n",
      "[213,  3200] loss: 2.302985\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[214,   200] loss: 2.303548\n",
      "[214,   400] loss: 2.303122\n",
      "[214,   600] loss: 2.303679\n",
      "[214,   800] loss: 2.303577\n",
      "[214,  1000] loss: 2.303577\n",
      "[214,  1200] loss: 2.303318\n",
      "[214,  1400] loss: 2.303627\n",
      "[214,  1600] loss: 2.303830\n",
      "[214,  1800] loss: 2.303480\n",
      "[214,  2000] loss: 2.303392\n",
      "[214,  2200] loss: 2.303700\n",
      "[214,  2400] loss: 2.303468\n",
      "[214,  2600] loss: 2.303593\n",
      "[214,  2800] loss: 2.303665\n",
      "[214,  3000] loss: 2.303847\n",
      "[214,  3200] loss: 2.303392\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[215,   200] loss: 2.303599\n",
      "[215,   400] loss: 2.303436\n",
      "[215,   600] loss: 2.303389\n",
      "[215,   800] loss: 2.303533\n",
      "[215,  1000] loss: 2.303809\n",
      "[215,  1200] loss: 2.303673\n",
      "[215,  1400] loss: 2.303308\n",
      "[215,  1600] loss: 2.303699\n",
      "[215,  1800] loss: 2.303823\n",
      "[215,  2000] loss: 2.303176\n",
      "[215,  2200] loss: 2.303559\n",
      "[215,  2400] loss: 2.303418\n",
      "[215,  2600] loss: 2.303415\n",
      "[215,  2800] loss: 2.303331\n",
      "[215,  3000] loss: 2.303743\n",
      "[215,  3200] loss: 2.303447\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[216,   200] loss: 2.303064\n",
      "[216,   400] loss: 2.303848\n",
      "[216,   600] loss: 2.303082\n",
      "[216,   800] loss: 2.303216\n",
      "[216,  1000] loss: 2.303484\n",
      "[216,  1200] loss: 2.303560\n",
      "[216,  1400] loss: 2.303560\n",
      "[216,  1600] loss: 2.303868\n",
      "[216,  1800] loss: 2.303314\n",
      "[216,  2000] loss: 2.303403\n",
      "[216,  2200] loss: 2.303693\n",
      "[216,  2400] loss: 2.304083\n",
      "[216,  2600] loss: 2.303228\n",
      "[216,  2800] loss: 2.302631\n",
      "[216,  3000] loss: 2.303633\n",
      "[216,  3200] loss: 2.303832\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[217,   200] loss: 2.303820\n",
      "[217,   400] loss: 2.303203\n",
      "[217,   600] loss: 2.303891\n",
      "[217,   800] loss: 2.303738\n",
      "[217,  1000] loss: 2.303587\n",
      "[217,  1200] loss: 2.303554\n",
      "[217,  1400] loss: 2.303035\n",
      "[217,  1600] loss: 2.304090\n",
      "[217,  1800] loss: 2.303331\n",
      "[217,  2000] loss: 2.303542\n",
      "[217,  2200] loss: 2.303075\n",
      "[217,  2400] loss: 2.303883\n",
      "[217,  2600] loss: 2.303598\n",
      "[217,  2800] loss: 2.303851\n",
      "[217,  3000] loss: 2.303452\n",
      "[217,  3200] loss: 2.303361\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[218,   200] loss: 2.303179\n",
      "[218,   400] loss: 2.303798\n",
      "[218,   600] loss: 2.303590\n",
      "[218,   800] loss: 2.303419\n",
      "[218,  1000] loss: 2.303207\n",
      "[218,  1200] loss: 2.303567\n",
      "[218,  1400] loss: 2.303403\n",
      "[218,  1600] loss: 2.303235\n",
      "[218,  1800] loss: 2.303551\n",
      "[218,  2000] loss: 2.303612\n",
      "[218,  2200] loss: 2.303413\n",
      "[218,  2400] loss: 2.303779\n",
      "[218,  2600] loss: 2.303410\n",
      "[218,  2800] loss: 2.303779\n",
      "[218,  3000] loss: 2.303587\n",
      "[218,  3200] loss: 2.303511\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[219,   200] loss: 2.303527\n",
      "[219,   400] loss: 2.303576\n",
      "[219,   600] loss: 2.303575\n",
      "[219,   800] loss: 2.303260\n",
      "[219,  1000] loss: 2.303508\n",
      "[219,  1200] loss: 2.303493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[219,  1400] loss: 2.303769\n",
      "[219,  1600] loss: 2.302934\n",
      "[219,  1800] loss: 2.303739\n",
      "[219,  2000] loss: 2.303601\n",
      "[219,  2200] loss: 2.303473\n",
      "[219,  2400] loss: 2.303678\n",
      "[219,  2600] loss: 2.303373\n",
      "[219,  2800] loss: 2.303694\n",
      "[219,  3000] loss: 2.303289\n",
      "[219,  3200] loss: 2.303675\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[220,   200] loss: 2.303858\n",
      "[220,   400] loss: 2.303332\n",
      "[220,   600] loss: 2.303662\n",
      "[220,   800] loss: 2.303104\n",
      "[220,  1000] loss: 2.303579\n",
      "[220,  1200] loss: 2.303464\n",
      "[220,  1400] loss: 2.303070\n",
      "[220,  1600] loss: 2.303973\n",
      "[220,  1800] loss: 2.303111\n",
      "[220,  2000] loss: 2.303576\n",
      "[220,  2200] loss: 2.303271\n",
      "[220,  2400] loss: 2.303780\n",
      "[220,  2600] loss: 2.303060\n",
      "[220,  2800] loss: 2.303339\n",
      "[220,  3000] loss: 2.303881\n",
      "[220,  3200] loss: 2.303349\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[221,   200] loss: 2.303525\n",
      "[221,   400] loss: 2.303294\n",
      "[221,   600] loss: 2.302698\n",
      "[221,   800] loss: 2.304212\n",
      "[221,  1000] loss: 2.303390\n",
      "[221,  1200] loss: 2.303292\n",
      "[221,  1400] loss: 2.303340\n",
      "[221,  1600] loss: 2.302872\n",
      "[221,  1800] loss: 2.303967\n",
      "[221,  2000] loss: 2.303526\n",
      "[221,  2200] loss: 2.303365\n",
      "[221,  2400] loss: 2.303270\n",
      "[221,  2600] loss: 2.303333\n",
      "[221,  2800] loss: 2.303132\n",
      "[221,  3000] loss: 2.303237\n",
      "[221,  3200] loss: 2.303190\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[222,   200] loss: 2.303435\n",
      "[222,   400] loss: 2.303434\n",
      "[222,   600] loss: 2.303391\n",
      "[222,   800] loss: 2.303630\n",
      "[222,  1000] loss: 2.303441\n",
      "[222,  1200] loss: 2.302953\n",
      "[222,  1400] loss: 2.303098\n",
      "[222,  1600] loss: 2.303157\n",
      "[222,  1800] loss: 2.303256\n",
      "[222,  2000] loss: 2.304083\n",
      "[222,  2200] loss: 2.303236\n",
      "[222,  2400] loss: 2.303317\n",
      "[222,  2600] loss: 2.303564\n",
      "[222,  2800] loss: 2.303149\n",
      "[222,  3000] loss: 2.303356\n",
      "[222,  3200] loss: 2.303708\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[223,   200] loss: 2.303620\n",
      "[223,   400] loss: 2.303687\n",
      "[223,   600] loss: 2.303298\n",
      "[223,   800] loss: 2.303709\n",
      "[223,  1000] loss: 2.303787\n",
      "[223,  1200] loss: 2.303497\n",
      "[223,  1400] loss: 2.303629\n",
      "[223,  1600] loss: 2.303158\n",
      "[223,  1800] loss: 2.303749\n",
      "[223,  2000] loss: 2.303643\n",
      "[223,  2200] loss: 2.303555\n",
      "[223,  2400] loss: 2.303308\n",
      "[223,  2600] loss: 2.303627\n",
      "[223,  2800] loss: 2.303301\n",
      "[223,  3000] loss: 2.303423\n",
      "[223,  3200] loss: 2.303628\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[224,   200] loss: 2.303430\n",
      "[224,   400] loss: 2.303552\n",
      "[224,   600] loss: 2.303057\n",
      "[224,   800] loss: 2.303680\n",
      "[224,  1000] loss: 2.303445\n",
      "[224,  1200] loss: 2.303749\n",
      "[224,  1400] loss: 2.303368\n",
      "[224,  1600] loss: 2.303441\n",
      "[224,  1800] loss: 2.303574\n",
      "[224,  2000] loss: 2.303332\n",
      "[224,  2200] loss: 2.303126\n",
      "[224,  2400] loss: 2.303589\n",
      "[224,  2600] loss: 2.303619\n",
      "[224,  2800] loss: 2.303820\n",
      "[224,  3000] loss: 2.303282\n",
      "[224,  3200] loss: 2.303415\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[225,   200] loss: 2.303800\n",
      "[225,   400] loss: 2.303493\n",
      "[225,   600] loss: 2.303701\n",
      "[225,   800] loss: 2.303074\n",
      "[225,  1000] loss: 2.303111\n",
      "[225,  1200] loss: 2.303288\n",
      "[225,  1400] loss: 2.303442\n",
      "[225,  1600] loss: 2.303246\n",
      "[225,  1800] loss: 2.303476\n",
      "[225,  2000] loss: 2.303248\n",
      "[225,  2200] loss: 2.303789\n",
      "[225,  2400] loss: 2.303179\n",
      "[225,  2600] loss: 2.303586\n",
      "[225,  2800] loss: 2.303612\n",
      "[225,  3000] loss: 2.303426\n",
      "[225,  3200] loss: 2.303933\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[226,   200] loss: 2.304209\n",
      "[226,   400] loss: 2.303673\n",
      "[226,   600] loss: 2.303086\n",
      "[226,   800] loss: 2.303936\n",
      "[226,  1000] loss: 2.303593\n",
      "[226,  1200] loss: 2.303353\n",
      "[226,  1400] loss: 2.303718\n",
      "[226,  1600] loss: 2.303333\n",
      "[226,  1800] loss: 2.303484\n",
      "[226,  2000] loss: 2.303832\n",
      "[226,  2200] loss: 2.303422\n",
      "[226,  2400] loss: 2.303641\n",
      "[226,  2600] loss: 2.303004\n",
      "[226,  2800] loss: 2.303774\n",
      "[226,  3000] loss: 2.303862\n",
      "[226,  3200] loss: 2.303361\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[227,   200] loss: 2.302985\n",
      "[227,   400] loss: 2.303524\n",
      "[227,   600] loss: 2.303476\n",
      "[227,   800] loss: 2.303555\n",
      "[227,  1000] loss: 2.303863\n",
      "[227,  1200] loss: 2.303486\n",
      "[227,  1400] loss: 2.303407\n",
      "[227,  1600] loss: 2.303507\n",
      "[227,  1800] loss: 2.303691\n",
      "[227,  2000] loss: 2.303406\n",
      "[227,  2200] loss: 2.303200\n",
      "[227,  2400] loss: 2.303322\n",
      "[227,  2600] loss: 2.303756\n",
      "[227,  2800] loss: 2.303704\n",
      "[227,  3000] loss: 2.303167\n",
      "[227,  3200] loss: 2.303817\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[228,   200] loss: 2.303591\n",
      "[228,   400] loss: 2.303695\n",
      "[228,   600] loss: 2.303690\n",
      "[228,   800] loss: 2.303395\n",
      "[228,  1000] loss: 2.303441\n",
      "[228,  1200] loss: 2.303590\n",
      "[228,  1400] loss: 2.303773\n",
      "[228,  1600] loss: 2.303617\n",
      "[228,  1800] loss: 2.303684\n",
      "[228,  2000] loss: 2.303484\n",
      "[228,  2200] loss: 2.304046\n",
      "[228,  2400] loss: 2.303583\n",
      "[228,  2600] loss: 2.302956\n",
      "[228,  2800] loss: 2.303974\n",
      "[228,  3000] loss: 2.303488\n",
      "[228,  3200] loss: 2.303748\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[229,   200] loss: 2.303592\n",
      "[229,   400] loss: 2.303328\n",
      "[229,   600] loss: 2.303500\n",
      "[229,   800] loss: 2.303527\n",
      "[229,  1000] loss: 2.303570\n",
      "[229,  1200] loss: 2.303629\n",
      "[229,  1400] loss: 2.303648\n",
      "[229,  1600] loss: 2.303460\n",
      "[229,  1800] loss: 2.303202\n",
      "[229,  2000] loss: 2.303579\n",
      "[229,  2200] loss: 2.303413\n",
      "[229,  2400] loss: 2.303596\n",
      "[229,  2600] loss: 2.303560\n",
      "[229,  2800] loss: 2.303609\n",
      "[229,  3000] loss: 2.303396\n",
      "[229,  3200] loss: 2.303503\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[230,   200] loss: 2.303410\n",
      "[230,   400] loss: 2.303391\n",
      "[230,   600] loss: 2.303473\n",
      "[230,   800] loss: 2.303221\n",
      "[230,  1000] loss: 2.303365\n",
      "[230,  1200] loss: 2.303523\n",
      "[230,  1400] loss: 2.303354\n",
      "[230,  1600] loss: 2.303962\n",
      "[230,  1800] loss: 2.303065\n",
      "[230,  2000] loss: 2.303364\n",
      "[230,  2200] loss: 2.303294\n",
      "[230,  2400] loss: 2.303422\n",
      "[230,  2600] loss: 2.303459\n",
      "[230,  2800] loss: 2.303399\n",
      "[230,  3000] loss: 2.303320\n",
      "[230,  3200] loss: 2.303398\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[231,   200] loss: 2.303170\n",
      "[231,   400] loss: 2.303566\n",
      "[231,   600] loss: 2.303649\n",
      "[231,   800] loss: 2.303434\n",
      "[231,  1000] loss: 2.303397\n",
      "[231,  1200] loss: 2.303445\n",
      "[231,  1400] loss: 2.303567\n",
      "[231,  1600] loss: 2.303445\n",
      "[231,  1800] loss: 2.303218\n",
      "[231,  2000] loss: 2.303613\n",
      "[231,  2200] loss: 2.303413\n",
      "[231,  2400] loss: 2.303251\n",
      "[231,  2600] loss: 2.303854\n",
      "[231,  2800] loss: 2.303022\n",
      "[231,  3000] loss: 2.303541\n",
      "[231,  3200] loss: 2.303452\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[232,   200] loss: 2.303480\n",
      "[232,   400] loss: 2.303239\n",
      "[232,   600] loss: 2.303543\n",
      "[232,   800] loss: 2.303696\n",
      "[232,  1000] loss: 2.303462\n",
      "[232,  1200] loss: 2.303393\n",
      "[232,  1400] loss: 2.303519\n",
      "[232,  1600] loss: 2.303560\n",
      "[232,  1800] loss: 2.303372\n",
      "[232,  2000] loss: 2.303115\n",
      "[232,  2200] loss: 2.303570\n",
      "[232,  2400] loss: 2.303367\n",
      "[232,  2600] loss: 2.303061\n",
      "[232,  2800] loss: 2.303670\n",
      "[232,  3000] loss: 2.303698\n",
      "[232,  3200] loss: 2.303444\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[233,   200] loss: 2.303823\n",
      "[233,   400] loss: 2.303267\n",
      "[233,   600] loss: 2.303795\n",
      "[233,   800] loss: 2.303202\n",
      "[233,  1000] loss: 2.302987\n",
      "[233,  1200] loss: 2.303549\n",
      "[233,  1400] loss: 2.303697\n",
      "[233,  1600] loss: 2.303146\n",
      "[233,  1800] loss: 2.303135\n",
      "[233,  2000] loss: 2.303645\n",
      "[233,  2200] loss: 2.303346\n",
      "[233,  2400] loss: 2.303498\n",
      "[233,  2600] loss: 2.303361\n",
      "[233,  2800] loss: 2.303602\n",
      "[233,  3000] loss: 2.303373\n",
      "[233,  3200] loss: 2.303908\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[234,   200] loss: 2.303524\n",
      "[234,   400] loss: 2.303654\n",
      "[234,   600] loss: 2.303334\n",
      "[234,   800] loss: 2.303062\n",
      "[234,  1000] loss: 2.303435\n",
      "[234,  1200] loss: 2.303487\n",
      "[234,  1400] loss: 2.303217\n",
      "[234,  1600] loss: 2.303978\n",
      "[234,  1800] loss: 2.303391\n",
      "[234,  2000] loss: 2.303188\n",
      "[234,  2200] loss: 2.303255\n",
      "[234,  2400] loss: 2.303872\n",
      "[234,  2600] loss: 2.303237\n",
      "[234,  2800] loss: 2.303111\n",
      "[234,  3000] loss: 2.303748\n",
      "[234,  3200] loss: 2.303185\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[235,   200] loss: 2.303565\n",
      "[235,   400] loss: 2.303705\n",
      "[235,   600] loss: 2.303522\n",
      "[235,   800] loss: 2.302928\n",
      "[235,  1000] loss: 2.303221\n",
      "[235,  1200] loss: 2.303321\n",
      "[235,  1400] loss: 2.303351\n",
      "[235,  1600] loss: 2.303466\n",
      "[235,  1800] loss: 2.303360\n",
      "[235,  2000] loss: 2.303993\n",
      "[235,  2200] loss: 2.303451\n",
      "[235,  2400] loss: 2.303743\n",
      "[235,  2600] loss: 2.303623\n",
      "[235,  2800] loss: 2.303276\n",
      "[235,  3000] loss: 2.303494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235,  3200] loss: 2.303394\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[236,   200] loss: 2.303718\n",
      "[236,   400] loss: 2.303628\n",
      "[236,   600] loss: 2.303848\n",
      "[236,   800] loss: 2.303274\n",
      "[236,  1000] loss: 2.303496\n",
      "[236,  1200] loss: 2.303537\n",
      "[236,  1400] loss: 2.303176\n",
      "[236,  1600] loss: 2.303757\n",
      "[236,  1800] loss: 2.303555\n",
      "[236,  2000] loss: 2.303490\n",
      "[236,  2200] loss: 2.303317\n",
      "[236,  2400] loss: 2.303784\n",
      "[236,  2600] loss: 2.303413\n",
      "[236,  2800] loss: 2.303261\n",
      "[236,  3000] loss: 2.303495\n",
      "[236,  3200] loss: 2.303607\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[237,   200] loss: 2.303608\n",
      "[237,   400] loss: 2.303644\n",
      "[237,   600] loss: 2.303769\n",
      "[237,   800] loss: 2.303602\n",
      "[237,  1000] loss: 2.303508\n",
      "[237,  1200] loss: 2.303432\n",
      "[237,  1400] loss: 2.303258\n",
      "[237,  1600] loss: 2.303904\n",
      "[237,  1800] loss: 2.303032\n",
      "[237,  2000] loss: 2.303713\n",
      "[237,  2200] loss: 2.303545\n",
      "[237,  2400] loss: 2.302786\n",
      "[237,  2600] loss: 2.303633\n",
      "[237,  2800] loss: 2.303578\n",
      "[237,  3000] loss: 2.303318\n",
      "[237,  3200] loss: 2.303607\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[238,   200] loss: 2.303509\n",
      "[238,   400] loss: 2.303129\n",
      "[238,   600] loss: 2.303390\n",
      "[238,   800] loss: 2.303190\n",
      "[238,  1000] loss: 2.303135\n",
      "[238,  1200] loss: 2.303577\n",
      "[238,  1400] loss: 2.303528\n",
      "[238,  1600] loss: 2.303392\n",
      "[238,  1800] loss: 2.303302\n",
      "[238,  2000] loss: 2.303500\n",
      "[238,  2200] loss: 2.302878\n",
      "[238,  2400] loss: 2.303348\n",
      "[238,  2600] loss: 2.303698\n",
      "[238,  2800] loss: 2.303394\n",
      "[238,  3000] loss: 2.303429\n",
      "[238,  3200] loss: 2.303456\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[239,   200] loss: 2.303449\n",
      "[239,   400] loss: 2.303112\n",
      "[239,   600] loss: 2.303046\n",
      "[239,   800] loss: 2.303449\n",
      "[239,  1000] loss: 2.303600\n",
      "[239,  1200] loss: 2.303660\n",
      "[239,  1400] loss: 2.303214\n",
      "[239,  1600] loss: 2.303609\n",
      "[239,  1800] loss: 2.303824\n",
      "[239,  2000] loss: 2.303557\n",
      "[239,  2200] loss: 2.303460\n",
      "[239,  2400] loss: 2.303257\n",
      "[239,  2600] loss: 2.303616\n",
      "[239,  2800] loss: 2.303868\n",
      "[239,  3000] loss: 2.303251\n",
      "[239,  3200] loss: 2.303510\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[240,   200] loss: 2.303735\n",
      "[240,   400] loss: 2.303013\n",
      "[240,   600] loss: 2.303637\n",
      "[240,   800] loss: 2.302990\n",
      "[240,  1000] loss: 2.303404\n",
      "[240,  1200] loss: 2.303622\n",
      "[240,  1400] loss: 2.303870\n",
      "[240,  1600] loss: 2.303756\n",
      "[240,  1800] loss: 2.303495\n",
      "[240,  2000] loss: 2.303158\n",
      "[240,  2200] loss: 2.303794\n",
      "[240,  2400] loss: 2.303645\n",
      "[240,  2600] loss: 2.303581\n",
      "[240,  2800] loss: 2.302339\n",
      "[240,  3000] loss: 2.303461\n",
      "[240,  3200] loss: 2.303915\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[241,   200] loss: 2.303959\n",
      "[241,   400] loss: 2.303249\n",
      "[241,   600] loss: 2.303129\n",
      "[241,   800] loss: 2.303824\n",
      "[241,  1000] loss: 2.303590\n",
      "[241,  1200] loss: 2.303270\n",
      "[241,  1400] loss: 2.303752\n",
      "[241,  1600] loss: 2.303220\n",
      "[241,  1800] loss: 2.303729\n",
      "[241,  2000] loss: 2.302950\n",
      "[241,  2200] loss: 2.303273\n",
      "[241,  2400] loss: 2.303451\n",
      "[241,  2600] loss: 2.303877\n",
      "[241,  2800] loss: 2.303186\n",
      "[241,  3000] loss: 2.303473\n",
      "[241,  3200] loss: 2.303519\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[242,   200] loss: 2.303729\n",
      "[242,   400] loss: 2.303287\n",
      "[242,   600] loss: 2.303801\n",
      "[242,   800] loss: 2.303598\n",
      "[242,  1000] loss: 2.303683\n",
      "[242,  1200] loss: 2.303233\n",
      "[242,  1400] loss: 2.303722\n",
      "[242,  1600] loss: 2.303214\n",
      "[242,  1800] loss: 2.303072\n",
      "[242,  2000] loss: 2.303658\n",
      "[242,  2200] loss: 2.303705\n",
      "[242,  2400] loss: 2.303518\n",
      "[242,  2600] loss: 2.303322\n",
      "[242,  2800] loss: 2.303671\n",
      "[242,  3000] loss: 2.303325\n",
      "[242,  3200] loss: 2.303826\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[243,   200] loss: 2.303711\n",
      "[243,   400] loss: 2.303586\n",
      "[243,   600] loss: 2.303796\n",
      "[243,   800] loss: 2.303711\n",
      "[243,  1000] loss: 2.303789\n",
      "[243,  1200] loss: 2.303391\n",
      "[243,  1400] loss: 2.303287\n",
      "[243,  1600] loss: 2.303507\n",
      "[243,  1800] loss: 2.303685\n",
      "[243,  2000] loss: 2.303542\n",
      "[243,  2200] loss: 2.303137\n",
      "[243,  2400] loss: 2.303359\n",
      "[243,  2600] loss: 2.303375\n",
      "[243,  2800] loss: 2.303734\n",
      "[243,  3000] loss: 2.303365\n",
      "[243,  3200] loss: 2.303869\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[244,   200] loss: 2.303166\n",
      "[244,   400] loss: 2.303615\n",
      "[244,   600] loss: 2.303468\n",
      "[244,   800] loss: 2.303345\n",
      "[244,  1000] loss: 2.303478\n",
      "[244,  1200] loss: 2.303233\n",
      "[244,  1400] loss: 2.303825\n",
      "[244,  1600] loss: 2.303259\n",
      "[244,  1800] loss: 2.303309\n",
      "[244,  2000] loss: 2.303941\n",
      "[244,  2200] loss: 2.303872\n",
      "[244,  2400] loss: 2.303748\n",
      "[244,  2600] loss: 2.303672\n",
      "[244,  2800] loss: 2.303332\n",
      "[244,  3000] loss: 2.303735\n",
      "[244,  3200] loss: 2.303731\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[245,   200] loss: 2.302801\n",
      "[245,   400] loss: 2.303405\n",
      "[245,   600] loss: 2.303538\n",
      "[245,   800] loss: 2.303740\n",
      "[245,  1000] loss: 2.303880\n",
      "[245,  1200] loss: 2.303545\n",
      "[245,  1400] loss: 2.303713\n",
      "[245,  1600] loss: 2.303193\n",
      "[245,  1800] loss: 2.303060\n",
      "[245,  2000] loss: 2.303444\n",
      "[245,  2200] loss: 2.303560\n",
      "[245,  2400] loss: 2.303412\n",
      "[245,  2600] loss: 2.304088\n",
      "[245,  2800] loss: 2.303442\n",
      "[245,  3000] loss: 2.303368\n",
      "[245,  3200] loss: 2.303234\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[246,   200] loss: 2.303277\n",
      "[246,   400] loss: 2.303694\n",
      "[246,   600] loss: 2.303155\n",
      "[246,   800] loss: 2.303228\n",
      "[246,  1000] loss: 2.303989\n",
      "[246,  1200] loss: 2.303008\n",
      "[246,  1400] loss: 2.303500\n",
      "[246,  1600] loss: 2.303272\n",
      "[246,  1800] loss: 2.303478\n",
      "[246,  2000] loss: 2.303338\n",
      "[246,  2200] loss: 2.303189\n",
      "[246,  2400] loss: 2.303772\n",
      "[246,  2600] loss: 2.303300\n",
      "[246,  2800] loss: 2.303794\n",
      "[246,  3000] loss: 2.303434\n",
      "[246,  3200] loss: 2.303521\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[247,   200] loss: 2.303472\n",
      "[247,   400] loss: 2.303301\n",
      "[247,   600] loss: 2.303551\n",
      "[247,   800] loss: 2.303496\n",
      "[247,  1000] loss: 2.303610\n",
      "[247,  1200] loss: 2.303737\n",
      "[247,  1400] loss: 2.303904\n",
      "[247,  1600] loss: 2.303493\n",
      "[247,  1800] loss: 2.303139\n",
      "[247,  2000] loss: 2.303689\n",
      "[247,  2200] loss: 2.303475\n",
      "[247,  2400] loss: 2.303729\n",
      "[247,  2600] loss: 2.303439\n",
      "[247,  2800] loss: 2.303322\n",
      "[247,  3000] loss: 2.303305\n",
      "[247,  3200] loss: 2.303408\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[248,   200] loss: 2.303550\n",
      "[248,   400] loss: 2.303617\n",
      "[248,   600] loss: 2.303449\n",
      "[248,   800] loss: 2.303378\n",
      "[248,  1000] loss: 2.303051\n",
      "[248,  1200] loss: 2.303657\n",
      "[248,  1400] loss: 2.303140\n",
      "[248,  1600] loss: 2.303541\n",
      "[248,  1800] loss: 2.303713\n",
      "[248,  2000] loss: 2.303246\n",
      "[248,  2200] loss: 2.303282\n",
      "[248,  2400] loss: 2.303540\n",
      "[248,  2600] loss: 2.302946\n",
      "[248,  2800] loss: 2.302708\n",
      "[248,  3000] loss: 2.304233\n",
      "[248,  3200] loss: 2.303456\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[249,   200] loss: 2.303612\n",
      "[249,   400] loss: 2.303667\n",
      "[249,   600] loss: 2.303826\n",
      "[249,   800] loss: 2.303304\n",
      "[249,  1000] loss: 2.303297\n",
      "[249,  1200] loss: 2.303223\n",
      "[249,  1400] loss: 2.303805\n",
      "[249,  1600] loss: 2.303603\n",
      "[249,  1800] loss: 2.303643\n",
      "[249,  2000] loss: 2.303374\n",
      "[249,  2200] loss: 2.303611\n",
      "[249,  2400] loss: 2.303620\n",
      "[249,  2600] loss: 2.302931\n",
      "[249,  2800] loss: 2.303520\n",
      "[249,  3000] loss: 2.303247\n",
      "[249,  3200] loss: 2.303436\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[250,   200] loss: 2.303902\n",
      "[250,   400] loss: 2.303565\n",
      "[250,   600] loss: 2.303822\n",
      "[250,   800] loss: 2.303756\n",
      "[250,  1000] loss: 2.303526\n",
      "[250,  1200] loss: 2.303720\n",
      "[250,  1400] loss: 2.303656\n",
      "[250,  1600] loss: 2.303764\n",
      "[250,  1800] loss: 2.303317\n",
      "[250,  2000] loss: 2.303691\n",
      "[250,  2200] loss: 2.303427\n",
      "[250,  2400] loss: 2.302960\n",
      "[250,  2600] loss: 2.303202\n",
      "[250,  2800] loss: 2.303639\n",
      "[250,  3000] loss: 2.303817\n",
      "[250,  3200] loss: 2.303136\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[251,   200] loss: 2.303295\n",
      "[251,   400] loss: 2.302985\n",
      "[251,   600] loss: 2.302805\n",
      "[251,   800] loss: 2.302693\n",
      "[251,  1000] loss: 2.302677\n",
      "[251,  1200] loss: 2.302408\n",
      "[251,  1400] loss: 2.302599\n",
      "[251,  1600] loss: 2.302554\n",
      "[251,  1800] loss: 2.302564\n",
      "[251,  2000] loss: 2.302472\n",
      "[251,  2200] loss: 2.302968\n",
      "[251,  2400] loss: 2.302549\n",
      "[251,  2600] loss: 2.302692\n",
      "[251,  2800] loss: 2.302613\n",
      "[251,  3000] loss: 2.302848\n",
      "[251,  3200] loss: 2.302532\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[252,   200] loss: 2.302453\n",
      "[252,   400] loss: 2.302551\n",
      "[252,   600] loss: 2.302355\n",
      "[252,   800] loss: 2.302874\n",
      "[252,  1000] loss: 2.302699\n",
      "[252,  1200] loss: 2.302446\n",
      "[252,  1400] loss: 2.302623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[252,  1600] loss: 2.302799\n",
      "[252,  1800] loss: 2.302808\n",
      "[252,  2000] loss: 2.302594\n",
      "[252,  2200] loss: 2.302610\n",
      "[252,  2400] loss: 2.302472\n",
      "[252,  2600] loss: 2.302782\n",
      "[252,  2800] loss: 2.302438\n",
      "[252,  3000] loss: 2.302885\n",
      "[252,  3200] loss: 2.302749\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[253,   200] loss: 2.302669\n",
      "[253,   400] loss: 2.302554\n",
      "[253,   600] loss: 2.302807\n",
      "[253,   800] loss: 2.302585\n",
      "[253,  1000] loss: 2.302240\n",
      "[253,  1200] loss: 2.302761\n",
      "[253,  1400] loss: 2.302649\n",
      "[253,  1600] loss: 2.302507\n",
      "[253,  1800] loss: 2.302675\n",
      "[253,  2000] loss: 2.302829\n",
      "[253,  2200] loss: 2.302662\n",
      "[253,  2400] loss: 2.302780\n",
      "[253,  2600] loss: 2.302632\n",
      "[253,  2800] loss: 2.302688\n",
      "[253,  3000] loss: 2.302700\n",
      "[253,  3200] loss: 2.302647\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[254,   200] loss: 2.302645\n",
      "[254,   400] loss: 2.302717\n",
      "[254,   600] loss: 2.302717\n",
      "[254,   800] loss: 2.302694\n",
      "[254,  1000] loss: 2.302691\n",
      "[254,  1200] loss: 2.302682\n",
      "[254,  1400] loss: 2.302692\n",
      "[254,  1600] loss: 2.302484\n",
      "[254,  1800] loss: 2.302470\n",
      "[254,  2000] loss: 2.302706\n",
      "[254,  2200] loss: 2.302573\n",
      "[254,  2400] loss: 2.302636\n",
      "[254,  2600] loss: 2.302621\n",
      "[254,  2800] loss: 2.302726\n",
      "[254,  3000] loss: 2.302533\n",
      "[254,  3200] loss: 2.302371\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[255,   200] loss: 2.302534\n",
      "[255,   400] loss: 2.302731\n",
      "[255,   600] loss: 2.302519\n",
      "[255,   800] loss: 2.302647\n",
      "[255,  1000] loss: 2.302797\n",
      "[255,  1200] loss: 2.302416\n",
      "[255,  1400] loss: 2.302715\n",
      "[255,  1600] loss: 2.302673\n",
      "[255,  1800] loss: 2.302488\n",
      "[255,  2000] loss: 2.302849\n",
      "[255,  2200] loss: 2.302789\n",
      "[255,  2400] loss: 2.302569\n",
      "[255,  2600] loss: 2.302732\n",
      "[255,  2800] loss: 2.302601\n",
      "[255,  3000] loss: 2.302613\n",
      "[255,  3200] loss: 2.302619\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[256,   200] loss: 2.302788\n",
      "[256,   400] loss: 2.302752\n",
      "[256,   600] loss: 2.302855\n",
      "[256,   800] loss: 2.302678\n",
      "[256,  1000] loss: 2.302683\n",
      "[256,  1200] loss: 2.302669\n",
      "[256,  1400] loss: 2.302704\n",
      "[256,  1600] loss: 2.302780\n",
      "[256,  1800] loss: 2.302778\n",
      "[256,  2000] loss: 2.302465\n",
      "[256,  2200] loss: 2.302817\n",
      "[256,  2400] loss: 2.302466\n",
      "[256,  2600] loss: 2.302628\n",
      "[256,  2800] loss: 2.302592\n",
      "[256,  3000] loss: 2.302482\n",
      "[256,  3200] loss: 2.302225\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[257,   200] loss: 2.302874\n",
      "[257,   400] loss: 2.302357\n",
      "[257,   600] loss: 2.302821\n",
      "[257,   800] loss: 2.302716\n",
      "[257,  1000] loss: 2.302688\n",
      "[257,  1200] loss: 2.302594\n",
      "[257,  1400] loss: 2.302590\n",
      "[257,  1600] loss: 2.302552\n",
      "[257,  1800] loss: 2.302810\n",
      "[257,  2000] loss: 2.302406\n",
      "[257,  2200] loss: 2.302667\n",
      "[257,  2400] loss: 2.302814\n",
      "[257,  2600] loss: 2.302754\n",
      "[257,  2800] loss: 2.302568\n",
      "[257,  3000] loss: 2.302533\n",
      "[257,  3200] loss: 2.302826\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[258,   200] loss: 2.302519\n",
      "[258,   400] loss: 2.302651\n",
      "[258,   600] loss: 2.302538\n",
      "[258,   800] loss: 2.302772\n",
      "[258,  1000] loss: 2.302757\n",
      "[258,  1200] loss: 2.302388\n",
      "[258,  1400] loss: 2.302679\n",
      "[258,  1600] loss: 2.302885\n",
      "[258,  1800] loss: 2.302557\n",
      "[258,  2000] loss: 2.302759\n",
      "[258,  2200] loss: 2.302470\n",
      "[258,  2400] loss: 2.302673\n",
      "[258,  2600] loss: 2.302522\n",
      "[258,  2800] loss: 2.302740\n",
      "[258,  3000] loss: 2.302364\n",
      "[258,  3200] loss: 2.303095\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[259,   200] loss: 2.302700\n",
      "[259,   400] loss: 2.302570\n",
      "[259,   600] loss: 2.302549\n",
      "[259,   800] loss: 2.302781\n",
      "[259,  1000] loss: 2.302712\n",
      "[259,  1200] loss: 2.302773\n",
      "[259,  1400] loss: 2.302564\n",
      "[259,  1600] loss: 2.302343\n",
      "[259,  1800] loss: 2.302586\n",
      "[259,  2000] loss: 2.302703\n",
      "[259,  2200] loss: 2.302553\n",
      "[259,  2400] loss: 2.302535\n",
      "[259,  2600] loss: 2.302625\n",
      "[259,  2800] loss: 2.302786\n",
      "[259,  3000] loss: 2.302772\n",
      "[259,  3200] loss: 2.302809\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[260,   200] loss: 2.302822\n",
      "[260,   400] loss: 2.302534\n",
      "[260,   600] loss: 2.302460\n",
      "[260,   800] loss: 2.302888\n",
      "[260,  1000] loss: 2.302651\n",
      "[260,  1200] loss: 2.302624\n",
      "[260,  1400] loss: 2.302661\n",
      "[260,  1600] loss: 2.302385\n",
      "[260,  1800] loss: 2.302555\n",
      "[260,  2000] loss: 2.302729\n",
      "[260,  2200] loss: 2.302744\n",
      "[260,  2400] loss: 2.302708\n",
      "[260,  2600] loss: 2.302742\n",
      "[260,  2800] loss: 2.302684\n",
      "[260,  3000] loss: 2.302606\n",
      "[260,  3200] loss: 2.302675\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[261,   200] loss: 2.302673\n",
      "[261,   400] loss: 2.302509\n",
      "[261,   600] loss: 2.302811\n",
      "[261,   800] loss: 2.302797\n",
      "[261,  1000] loss: 2.302391\n",
      "[261,  1200] loss: 2.302848\n",
      "[261,  1400] loss: 2.302392\n",
      "[261,  1600] loss: 2.302701\n",
      "[261,  1800] loss: 2.302548\n",
      "[261,  2000] loss: 2.302860\n",
      "[261,  2200] loss: 2.302625\n",
      "[261,  2400] loss: 2.302733\n",
      "[261,  2600] loss: 2.302773\n",
      "[261,  2800] loss: 2.302522\n",
      "[261,  3000] loss: 2.302645\n",
      "[261,  3200] loss: 2.302865\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[262,   200] loss: 2.302603\n",
      "[262,   400] loss: 2.302685\n",
      "[262,   600] loss: 2.302652\n",
      "[262,   800] loss: 2.302643\n",
      "[262,  1000] loss: 2.302687\n",
      "[262,  1200] loss: 2.302605\n",
      "[262,  1400] loss: 2.302632\n",
      "[262,  1600] loss: 2.302584\n",
      "[262,  1800] loss: 2.302505\n",
      "[262,  2000] loss: 2.302705\n",
      "[262,  2200] loss: 2.302716\n",
      "[262,  2400] loss: 2.302727\n",
      "[262,  2600] loss: 2.302441\n",
      "[262,  2800] loss: 2.302710\n",
      "[262,  3000] loss: 2.302667\n",
      "[262,  3200] loss: 2.302791\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[263,   200] loss: 2.302527\n",
      "[263,   400] loss: 2.302601\n",
      "[263,   600] loss: 2.302412\n",
      "[263,   800] loss: 2.302567\n",
      "[263,  1000] loss: 2.302643\n",
      "[263,  1200] loss: 2.302652\n",
      "[263,  1400] loss: 2.302601\n",
      "[263,  1600] loss: 2.302688\n",
      "[263,  1800] loss: 2.302638\n",
      "[263,  2000] loss: 2.302686\n",
      "[263,  2200] loss: 2.302649\n",
      "[263,  2400] loss: 2.302665\n",
      "[263,  2600] loss: 2.302723\n",
      "[263,  2800] loss: 2.302757\n",
      "[263,  3000] loss: 2.302630\n",
      "[263,  3200] loss: 2.302525\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[264,   200] loss: 2.302560\n",
      "[264,   400] loss: 2.302559\n",
      "[264,   600] loss: 2.302633\n",
      "[264,   800] loss: 2.302682\n",
      "[264,  1000] loss: 2.302743\n",
      "[264,  1200] loss: 2.302760\n",
      "[264,  1400] loss: 2.302676\n",
      "[264,  1600] loss: 2.302679\n",
      "[264,  1800] loss: 2.302644\n",
      "[264,  2000] loss: 2.302623\n",
      "[264,  2200] loss: 2.302744\n",
      "[264,  2400] loss: 2.302679\n",
      "[264,  2600] loss: 2.302525\n",
      "[264,  2800] loss: 2.302693\n",
      "[264,  3000] loss: 2.302463\n",
      "[264,  3200] loss: 2.302546\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[265,   200] loss: 2.302395\n",
      "[265,   400] loss: 2.302487\n",
      "[265,   600] loss: 2.302244\n",
      "[265,   800] loss: 2.302921\n",
      "[265,  1000] loss: 2.302499\n",
      "[265,  1200] loss: 2.302788\n",
      "[265,  1400] loss: 2.302610\n",
      "[265,  1600] loss: 2.302454\n",
      "[265,  1800] loss: 2.302536\n",
      "[265,  2000] loss: 2.302851\n",
      "[265,  2200] loss: 2.302745\n",
      "[265,  2400] loss: 2.302577\n",
      "[265,  2600] loss: 2.302797\n",
      "[265,  2800] loss: 2.302553\n",
      "[265,  3000] loss: 2.302710\n",
      "[265,  3200] loss: 2.302716\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[266,   200] loss: 2.302641\n",
      "[266,   400] loss: 2.302643\n",
      "[266,   600] loss: 2.302655\n",
      "[266,   800] loss: 2.302787\n",
      "[266,  1000] loss: 2.302254\n",
      "[266,  1200] loss: 2.302715\n",
      "[266,  1400] loss: 2.302545\n",
      "[266,  1600] loss: 2.302710\n",
      "[266,  1800] loss: 2.302659\n",
      "[266,  2000] loss: 2.302704\n",
      "[266,  2200] loss: 2.302695\n",
      "[266,  2400] loss: 2.302702\n",
      "[266,  2600] loss: 2.302416\n",
      "[266,  2800] loss: 2.302790\n",
      "[266,  3000] loss: 2.302491\n",
      "[266,  3200] loss: 2.302804\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[267,   200] loss: 2.302554\n",
      "[267,   400] loss: 2.302631\n",
      "[267,   600] loss: 2.302534\n",
      "[267,   800] loss: 2.302602\n",
      "[267,  1000] loss: 2.302855\n",
      "[267,  1200] loss: 2.302544\n",
      "[267,  1400] loss: 2.302678\n",
      "[267,  1600] loss: 2.302617\n",
      "[267,  1800] loss: 2.302479\n",
      "[267,  2000] loss: 2.302905\n",
      "[267,  2200] loss: 2.302583\n",
      "[267,  2400] loss: 2.302713\n",
      "[267,  2600] loss: 2.302557\n",
      "[267,  2800] loss: 2.302585\n",
      "[267,  3000] loss: 2.302901\n",
      "[267,  3200] loss: 2.302716\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[268,   200] loss: 2.302600\n",
      "[268,   400] loss: 2.302548\n",
      "[268,   600] loss: 2.302787\n",
      "[268,   800] loss: 2.302691\n",
      "[268,  1000] loss: 2.302548\n",
      "[268,  1200] loss: 2.302758\n",
      "[268,  1400] loss: 2.302607\n",
      "[268,  1600] loss: 2.302730\n",
      "[268,  1800] loss: 2.302761\n",
      "[268,  2000] loss: 2.302391\n",
      "[268,  2200] loss: 2.302802\n",
      "[268,  2400] loss: 2.302667\n",
      "[268,  2600] loss: 2.302490\n",
      "[268,  2800] loss: 2.302626\n",
      "[268,  3000] loss: 2.302663\n",
      "[268,  3200] loss: 2.302695\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269,   200] loss: 2.302846\n",
      "[269,   400] loss: 2.302281\n",
      "[269,   600] loss: 2.302911\n",
      "[269,   800] loss: 2.302598\n",
      "[269,  1000] loss: 2.302572\n",
      "[269,  1200] loss: 2.302382\n",
      "[269,  1400] loss: 2.302611\n",
      "[269,  1600] loss: 2.302683\n",
      "[269,  1800] loss: 2.302596\n",
      "[269,  2000] loss: 2.302543\n",
      "[269,  2200] loss: 2.302780\n",
      "[269,  2400] loss: 2.302774\n",
      "[269,  2600] loss: 2.302659\n",
      "[269,  2800] loss: 2.302726\n",
      "[269,  3000] loss: 2.302649\n",
      "[269,  3200] loss: 2.302478\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[270,   200] loss: 2.302631\n",
      "[270,   400] loss: 2.302827\n",
      "[270,   600] loss: 2.302448\n",
      "[270,   800] loss: 2.302525\n",
      "[270,  1000] loss: 2.302766\n",
      "[270,  1200] loss: 2.302406\n",
      "[270,  1400] loss: 2.302573\n",
      "[270,  1600] loss: 2.302769\n",
      "[270,  1800] loss: 2.302727\n",
      "[270,  2000] loss: 2.302669\n",
      "[270,  2200] loss: 2.302746\n",
      "[270,  2400] loss: 2.302682\n",
      "[270,  2600] loss: 2.302739\n",
      "[270,  2800] loss: 2.302661\n",
      "[270,  3000] loss: 2.302705\n",
      "[270,  3200] loss: 2.302557\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[271,   200] loss: 2.302604\n",
      "[271,   400] loss: 2.302491\n",
      "[271,   600] loss: 2.302810\n",
      "[271,   800] loss: 2.302450\n",
      "[271,  1000] loss: 2.302567\n",
      "[271,  1200] loss: 2.302615\n",
      "[271,  1400] loss: 2.302408\n",
      "[271,  1600] loss: 2.302988\n",
      "[271,  1800] loss: 2.302734\n",
      "[271,  2000] loss: 2.302670\n",
      "[271,  2200] loss: 2.302770\n",
      "[271,  2400] loss: 2.302761\n",
      "[271,  2600] loss: 2.302672\n",
      "[271,  2800] loss: 2.302744\n",
      "[271,  3000] loss: 2.302698\n",
      "[271,  3200] loss: 2.302588\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[272,   200] loss: 2.302553\n",
      "[272,   400] loss: 2.302380\n",
      "[272,   600] loss: 2.302524\n",
      "[272,   800] loss: 2.302422\n",
      "[272,  1000] loss: 2.302827\n",
      "[272,  1200] loss: 2.302603\n",
      "[272,  1400] loss: 2.302457\n",
      "[272,  1600] loss: 2.302673\n",
      "[272,  1800] loss: 2.302691\n",
      "[272,  2000] loss: 2.302508\n",
      "[272,  2200] loss: 2.302682\n",
      "[272,  2400] loss: 2.302673\n",
      "[272,  2600] loss: 2.302842\n",
      "[272,  2800] loss: 2.302577\n",
      "[272,  3000] loss: 2.302624\n",
      "[272,  3200] loss: 2.302469\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[273,   200] loss: 2.302444\n",
      "[273,   400] loss: 2.303056\n",
      "[273,   600] loss: 2.302700\n",
      "[273,   800] loss: 2.302662\n",
      "[273,  1000] loss: 2.302543\n",
      "[273,  1200] loss: 2.302657\n",
      "[273,  1400] loss: 2.302562\n",
      "[273,  1600] loss: 2.302587\n",
      "[273,  1800] loss: 2.302520\n",
      "[273,  2000] loss: 2.302725\n",
      "[273,  2200] loss: 2.302736\n",
      "[273,  2400] loss: 2.302802\n",
      "[273,  2600] loss: 2.302643\n",
      "[273,  2800] loss: 2.302543\n",
      "[273,  3000] loss: 2.302642\n",
      "[273,  3200] loss: 2.302706\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[274,   200] loss: 2.302712\n",
      "[274,   400] loss: 2.302557\n",
      "[274,   600] loss: 2.302753\n",
      "[274,   800] loss: 2.302681\n",
      "[274,  1000] loss: 2.302693\n",
      "[274,  1200] loss: 2.302416\n",
      "[274,  1400] loss: 2.302533\n",
      "[274,  1600] loss: 2.302681\n",
      "[274,  1800] loss: 2.302610\n",
      "[274,  2000] loss: 2.302630\n",
      "[274,  2200] loss: 2.302813\n",
      "[274,  2400] loss: 2.302583\n",
      "[274,  2600] loss: 2.302607\n",
      "[274,  2800] loss: 2.302723\n",
      "[274,  3000] loss: 2.302725\n",
      "[274,  3200] loss: 2.302619\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[275,   200] loss: 2.302567\n",
      "[275,   400] loss: 2.302702\n",
      "[275,   600] loss: 2.302418\n",
      "[275,   800] loss: 2.302564\n",
      "[275,  1000] loss: 2.302638\n",
      "[275,  1200] loss: 2.302741\n",
      "[275,  1400] loss: 2.302672\n",
      "[275,  1600] loss: 2.302631\n",
      "[275,  1800] loss: 2.302681\n",
      "[275,  2000] loss: 2.302655\n",
      "[275,  2200] loss: 2.302695\n",
      "[275,  2400] loss: 2.302587\n",
      "[275,  2600] loss: 2.302631\n",
      "[275,  2800] loss: 2.302818\n",
      "[275,  3000] loss: 2.302695\n",
      "[275,  3200] loss: 2.302643\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[276,   200] loss: 2.302691\n",
      "[276,   400] loss: 2.302530\n",
      "[276,   600] loss: 2.302399\n",
      "[276,   800] loss: 2.302717\n",
      "[276,  1000] loss: 2.302454\n",
      "[276,  1200] loss: 2.302769\n",
      "[276,  1400] loss: 2.302719\n",
      "[276,  1600] loss: 2.302756\n",
      "[276,  1800] loss: 2.302539\n",
      "[276,  2000] loss: 2.302818\n",
      "[276,  2200] loss: 2.302577\n",
      "[276,  2400] loss: 2.302741\n",
      "[276,  2600] loss: 2.302690\n",
      "[276,  2800] loss: 2.302803\n",
      "[276,  3000] loss: 2.302513\n",
      "[276,  3200] loss: 2.302674\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[277,   200] loss: 2.302437\n",
      "[277,   400] loss: 2.302781\n",
      "[277,   600] loss: 2.302855\n",
      "[277,   800] loss: 2.302456\n",
      "[277,  1000] loss: 2.302429\n",
      "[277,  1200] loss: 2.302329\n",
      "[277,  1400] loss: 2.302799\n",
      "[277,  1600] loss: 2.302664\n",
      "[277,  1800] loss: 2.302701\n",
      "[277,  2000] loss: 2.302771\n",
      "[277,  2200] loss: 2.302698\n",
      "[277,  2400] loss: 2.302675\n",
      "[277,  2600] loss: 2.302743\n",
      "[277,  2800] loss: 2.302708\n",
      "[277,  3000] loss: 2.302498\n",
      "[277,  3200] loss: 2.302738\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[278,   200] loss: 2.302798\n",
      "[278,   400] loss: 2.302641\n",
      "[278,   600] loss: 2.302644\n",
      "[278,   800] loss: 2.302702\n",
      "[278,  1000] loss: 2.302670\n",
      "[278,  1200] loss: 2.302714\n",
      "[278,  1400] loss: 2.302422\n",
      "[278,  1600] loss: 2.302585\n",
      "[278,  1800] loss: 2.302663\n",
      "[278,  2000] loss: 2.302700\n",
      "[278,  2200] loss: 2.302707\n",
      "[278,  2400] loss: 2.302769\n",
      "[278,  2600] loss: 2.302676\n",
      "[278,  2800] loss: 2.302582\n",
      "[278,  3000] loss: 2.302487\n",
      "[278,  3200] loss: 2.302711\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[279,   200] loss: 2.302513\n",
      "[279,   400] loss: 2.302552\n",
      "[279,   600] loss: 2.302537\n",
      "[279,   800] loss: 2.302717\n",
      "[279,  1000] loss: 2.302637\n",
      "[279,  1200] loss: 2.302722\n",
      "[279,  1400] loss: 2.302352\n",
      "[279,  1600] loss: 2.302694\n",
      "[279,  1800] loss: 2.302628\n",
      "[279,  2000] loss: 2.302767\n",
      "[279,  2200] loss: 2.302709\n",
      "[279,  2400] loss: 2.302756\n",
      "[279,  2600] loss: 2.302695\n",
      "[279,  2800] loss: 2.302559\n",
      "[279,  3000] loss: 2.302770\n",
      "[279,  3200] loss: 2.302492\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[280,   200] loss: 2.302764\n",
      "[280,   400] loss: 2.302692\n",
      "[280,   600] loss: 2.302699\n",
      "[280,   800] loss: 2.302672\n",
      "[280,  1000] loss: 2.302705\n",
      "[280,  1200] loss: 2.302659\n",
      "[280,  1400] loss: 2.302625\n",
      "[280,  1600] loss: 2.302534\n",
      "[280,  1800] loss: 2.302670\n",
      "[280,  2000] loss: 2.302519\n",
      "[280,  2200] loss: 2.302642\n",
      "[280,  2400] loss: 2.302564\n",
      "[280,  2600] loss: 2.302630\n",
      "[280,  2800] loss: 2.302681\n",
      "[280,  3000] loss: 2.302593\n",
      "[280,  3200] loss: 2.302726\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[281,   200] loss: 2.302634\n",
      "[281,   400] loss: 2.302756\n",
      "[281,   600] loss: 2.302658\n",
      "[281,   800] loss: 2.302713\n",
      "[281,  1000] loss: 2.302771\n",
      "[281,  1200] loss: 2.302684\n",
      "[281,  1400] loss: 2.302693\n",
      "[281,  1600] loss: 2.302500\n",
      "[281,  1800] loss: 2.302795\n",
      "[281,  2000] loss: 2.302723\n",
      "[281,  2200] loss: 2.302598\n",
      "[281,  2400] loss: 2.302638\n",
      "[281,  2600] loss: 2.302675\n",
      "[281,  2800] loss: 2.302664\n",
      "[281,  3000] loss: 2.302148\n",
      "[281,  3200] loss: 2.302617\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[282,   200] loss: 2.302671\n",
      "[282,   400] loss: 2.302425\n",
      "[282,   600] loss: 2.302858\n",
      "[282,   800] loss: 2.302578\n",
      "[282,  1000] loss: 2.302636\n",
      "[282,  1200] loss: 2.302678\n",
      "[282,  1400] loss: 2.302764\n",
      "[282,  1600] loss: 2.302696\n",
      "[282,  1800] loss: 2.302735\n",
      "[282,  2000] loss: 2.302670\n",
      "[282,  2200] loss: 2.302595\n",
      "[282,  2400] loss: 2.302813\n",
      "[282,  2600] loss: 2.302523\n",
      "[282,  2800] loss: 2.302618\n",
      "[282,  3000] loss: 2.302813\n",
      "[282,  3200] loss: 2.302472\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[283,   200] loss: 2.302343\n",
      "[283,   400] loss: 2.302738\n",
      "[283,   600] loss: 2.302612\n",
      "[283,   800] loss: 2.302759\n",
      "[283,  1000] loss: 2.302670\n",
      "[283,  1200] loss: 2.302485\n",
      "[283,  1400] loss: 2.302681\n",
      "[283,  1600] loss: 2.302405\n",
      "[283,  1800] loss: 2.302674\n",
      "[283,  2000] loss: 2.302829\n",
      "[283,  2200] loss: 2.302480\n",
      "[283,  2400] loss: 2.302602\n",
      "[283,  2600] loss: 2.302685\n",
      "[283,  2800] loss: 2.302776\n",
      "[283,  3000] loss: 2.302640\n",
      "[283,  3200] loss: 2.302600\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[284,   200] loss: 2.302637\n",
      "[284,   400] loss: 2.302669\n",
      "[284,   600] loss: 2.302713\n",
      "[284,   800] loss: 2.302330\n",
      "[284,  1000] loss: 2.302565\n",
      "[284,  1200] loss: 2.302735\n",
      "[284,  1400] loss: 2.302624\n",
      "[284,  1600] loss: 2.302574\n",
      "[284,  1800] loss: 2.302636\n",
      "[284,  2000] loss: 2.302626\n",
      "[284,  2200] loss: 2.302610\n",
      "[284,  2400] loss: 2.302710\n",
      "[284,  2600] loss: 2.302628\n",
      "[284,  2800] loss: 2.302545\n",
      "[284,  3000] loss: 2.302520\n",
      "[284,  3200] loss: 2.302575\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[285,   200] loss: 2.302612\n",
      "[285,   400] loss: 2.302586\n",
      "[285,   600] loss: 2.302782\n",
      "[285,   800] loss: 2.302627\n",
      "[285,  1000] loss: 2.302544\n",
      "[285,  1200] loss: 2.302653\n",
      "[285,  1400] loss: 2.302838\n",
      "[285,  1600] loss: 2.302600\n",
      "[285,  1800] loss: 2.302686\n",
      "[285,  2000] loss: 2.302426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285,  2200] loss: 2.302770\n",
      "[285,  2400] loss: 2.302662\n",
      "[285,  2600] loss: 2.302814\n",
      "[285,  2800] loss: 2.302720\n",
      "[285,  3000] loss: 2.302778\n",
      "[285,  3200] loss: 2.302437\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[286,   200] loss: 2.302715\n",
      "[286,   400] loss: 2.302661\n",
      "[286,   600] loss: 2.302573\n",
      "[286,   800] loss: 2.302742\n",
      "[286,  1000] loss: 2.302755\n",
      "[286,  1200] loss: 2.302591\n",
      "[286,  1400] loss: 2.302578\n",
      "[286,  1600] loss: 2.302280\n",
      "[286,  1800] loss: 2.302786\n",
      "[286,  2000] loss: 2.302582\n",
      "[286,  2200] loss: 2.302774\n",
      "[286,  2400] loss: 2.302659\n",
      "[286,  2600] loss: 2.302657\n",
      "[286,  2800] loss: 2.302690\n",
      "[286,  3000] loss: 2.302694\n",
      "[286,  3200] loss: 2.302674\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[287,   200] loss: 2.302727\n",
      "[287,   400] loss: 2.302723\n",
      "[287,   600] loss: 2.302738\n",
      "[287,   800] loss: 2.302550\n",
      "[287,  1000] loss: 2.302645\n",
      "[287,  1200] loss: 2.302493\n",
      "[287,  1400] loss: 2.302720\n",
      "[287,  1600] loss: 2.302756\n",
      "[287,  1800] loss: 2.302562\n",
      "[287,  2000] loss: 2.302668\n",
      "[287,  2200] loss: 2.302748\n",
      "[287,  2400] loss: 2.302717\n",
      "[287,  2600] loss: 2.302437\n",
      "[287,  2800] loss: 2.302659\n",
      "[287,  3000] loss: 2.302689\n",
      "[287,  3200] loss: 2.302584\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[288,   200] loss: 2.302341\n",
      "[288,   400] loss: 2.302532\n",
      "[288,   600] loss: 2.302693\n",
      "[288,   800] loss: 2.302716\n",
      "[288,  1000] loss: 2.302474\n",
      "[288,  1200] loss: 2.302673\n",
      "[288,  1400] loss: 2.302660\n",
      "[288,  1600] loss: 2.302833\n",
      "[288,  1800] loss: 2.302670\n",
      "[288,  2000] loss: 2.302720\n",
      "[288,  2200] loss: 2.302524\n",
      "[288,  2400] loss: 2.302460\n",
      "[288,  2600] loss: 2.302875\n",
      "[288,  2800] loss: 2.302679\n",
      "[288,  3000] loss: 2.302726\n",
      "[288,  3200] loss: 2.302601\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[289,   200] loss: 2.302616\n",
      "[289,   400] loss: 2.302558\n",
      "[289,   600] loss: 2.302806\n",
      "[289,   800] loss: 2.302540\n",
      "[289,  1000] loss: 2.302804\n",
      "[289,  1200] loss: 2.302642\n",
      "[289,  1400] loss: 2.302723\n",
      "[289,  1600] loss: 2.302659\n",
      "[289,  1800] loss: 2.302478\n",
      "[289,  2000] loss: 2.302572\n",
      "[289,  2200] loss: 2.302881\n",
      "[289,  2400] loss: 2.302553\n",
      "[289,  2600] loss: 2.302742\n",
      "[289,  2800] loss: 2.302338\n",
      "[289,  3000] loss: 2.302374\n",
      "[289,  3200] loss: 2.302437\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[290,   200] loss: 2.302606\n",
      "[290,   400] loss: 2.302444\n",
      "[290,   600] loss: 2.302866\n",
      "[290,   800] loss: 2.302603\n",
      "[290,  1000] loss: 2.302569\n",
      "[290,  1200] loss: 2.302715\n",
      "[290,  1400] loss: 2.302486\n",
      "[290,  1600] loss: 2.302863\n",
      "[290,  1800] loss: 2.302610\n",
      "[290,  2000] loss: 2.302447\n",
      "[290,  2200] loss: 2.302653\n",
      "[290,  2400] loss: 2.302686\n",
      "[290,  2600] loss: 2.302619\n",
      "[290,  2800] loss: 2.302546\n",
      "[290,  3000] loss: 2.302784\n",
      "[290,  3200] loss: 2.302647\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[291,   200] loss: 2.302692\n",
      "[291,   400] loss: 2.302731\n",
      "[291,   600] loss: 2.302640\n",
      "[291,   800] loss: 2.302447\n",
      "[291,  1000] loss: 2.302278\n",
      "[291,  1200] loss: 2.302807\n",
      "[291,  1400] loss: 2.302634\n",
      "[291,  1600] loss: 2.302799\n",
      "[291,  1800] loss: 2.302650\n",
      "[291,  2000] loss: 2.302831\n",
      "[291,  2200] loss: 2.302612\n",
      "[291,  2400] loss: 2.302738\n",
      "[291,  2600] loss: 2.302576\n",
      "[291,  2800] loss: 2.302600\n",
      "[291,  3000] loss: 2.302467\n",
      "[291,  3200] loss: 2.302769\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[292,   200] loss: 2.302687\n",
      "[292,   400] loss: 2.302644\n",
      "[292,   600] loss: 2.302727\n",
      "[292,   800] loss: 2.302620\n",
      "[292,  1000] loss: 2.302510\n",
      "[292,  1200] loss: 2.302547\n",
      "[292,  1400] loss: 2.302627\n",
      "[292,  1600] loss: 2.302712\n",
      "[292,  1800] loss: 2.302678\n",
      "[292,  2000] loss: 2.302641\n",
      "[292,  2200] loss: 2.302849\n",
      "[292,  2400] loss: 2.302665\n",
      "[292,  2600] loss: 2.302598\n",
      "[292,  2800] loss: 2.302693\n",
      "[292,  3000] loss: 2.302471\n",
      "[292,  3200] loss: 2.302688\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[293,   200] loss: 2.302458\n",
      "[293,   400] loss: 2.302480\n",
      "[293,   600] loss: 2.302694\n",
      "[293,   800] loss: 2.302609\n",
      "[293,  1000] loss: 2.302756\n",
      "[293,  1200] loss: 2.302696\n",
      "[293,  1400] loss: 2.302582\n",
      "[293,  1600] loss: 2.302513\n",
      "[293,  1800] loss: 2.302888\n",
      "[293,  2000] loss: 2.302572\n",
      "[293,  2200] loss: 2.302790\n",
      "[293,  2400] loss: 2.302783\n",
      "[293,  2600] loss: 2.302647\n",
      "[293,  2800] loss: 2.302704\n",
      "[293,  3000] loss: 2.302427\n",
      "[293,  3200] loss: 2.302671\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[294,   200] loss: 2.302868\n",
      "[294,   400] loss: 2.302550\n",
      "[294,   600] loss: 2.302678\n",
      "[294,   800] loss: 2.302644\n",
      "[294,  1000] loss: 2.302408\n",
      "[294,  1200] loss: 2.302586\n",
      "[294,  1400] loss: 2.302650\n",
      "[294,  1600] loss: 2.302624\n",
      "[294,  1800] loss: 2.302638\n",
      "[294,  2000] loss: 2.302626\n",
      "[294,  2200] loss: 2.302710\n",
      "[294,  2400] loss: 2.302781\n",
      "[294,  2600] loss: 2.302596\n",
      "[294,  2800] loss: 2.302495\n",
      "[294,  3000] loss: 2.302781\n",
      "[294,  3200] loss: 2.302736\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[295,   200] loss: 2.302490\n",
      "[295,   400] loss: 2.302794\n",
      "[295,   600] loss: 2.302755\n",
      "[295,   800] loss: 2.302600\n",
      "[295,  1000] loss: 2.302578\n",
      "[295,  1200] loss: 2.302710\n",
      "[295,  1400] loss: 2.302680\n",
      "[295,  1600] loss: 2.302342\n",
      "[295,  1800] loss: 2.302955\n",
      "[295,  2000] loss: 2.302798\n",
      "[295,  2200] loss: 2.302530\n",
      "[295,  2400] loss: 2.302650\n",
      "[295,  2600] loss: 2.302670\n",
      "[295,  2800] loss: 2.302652\n",
      "[295,  3000] loss: 2.302741\n",
      "[295,  3200] loss: 2.302662\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[296,   200] loss: 2.302547\n",
      "[296,   400] loss: 2.302658\n",
      "[296,   600] loss: 2.302517\n",
      "[296,   800] loss: 2.302533\n",
      "[296,  1000] loss: 2.302579\n",
      "[296,  1200] loss: 2.302198\n",
      "[296,  1400] loss: 2.302759\n",
      "[296,  1600] loss: 2.302618\n",
      "[296,  1800] loss: 2.302800\n",
      "[296,  2000] loss: 2.302747\n",
      "[296,  2200] loss: 2.302527\n",
      "[296,  2400] loss: 2.302658\n",
      "[296,  2600] loss: 2.302644\n",
      "[296,  2800] loss: 2.302739\n",
      "[296,  3000] loss: 2.302768\n",
      "[296,  3200] loss: 2.302734\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[297,   200] loss: 2.302825\n",
      "[297,   400] loss: 2.302499\n",
      "[297,   600] loss: 2.302504\n",
      "[297,   800] loss: 2.302398\n",
      "[297,  1000] loss: 2.302538\n",
      "[297,  1200] loss: 2.302516\n",
      "[297,  1400] loss: 2.302651\n",
      "[297,  1600] loss: 2.302610\n",
      "[297,  1800] loss: 2.302635\n",
      "[297,  2000] loss: 2.302539\n",
      "[297,  2200] loss: 2.302748\n",
      "[297,  2400] loss: 2.302829\n",
      "[297,  2600] loss: 2.302638\n",
      "[297,  2800] loss: 2.302617\n",
      "[297,  3000] loss: 2.302556\n",
      "[297,  3200] loss: 2.302751\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[298,   200] loss: 2.302520\n",
      "[298,   400] loss: 2.302671\n",
      "[298,   600] loss: 2.302584\n",
      "[298,   800] loss: 2.302868\n",
      "[298,  1000] loss: 2.302437\n",
      "[298,  1200] loss: 2.302867\n",
      "[298,  1400] loss: 2.302687\n",
      "[298,  1600] loss: 2.302641\n",
      "[298,  1800] loss: 2.302771\n",
      "[298,  2000] loss: 2.302598\n",
      "[298,  2200] loss: 2.302774\n",
      "[298,  2400] loss: 2.302599\n",
      "[298,  2600] loss: 2.302623\n",
      "[298,  2800] loss: 2.302448\n",
      "[298,  3000] loss: 2.302593\n",
      "[298,  3200] loss: 2.302796\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[299,   200] loss: 2.302937\n",
      "[299,   400] loss: 2.302758\n",
      "[299,   600] loss: 2.302734\n",
      "[299,   800] loss: 2.302712\n",
      "[299,  1000] loss: 2.302689\n",
      "[299,  1200] loss: 2.302554\n",
      "[299,  1400] loss: 2.302577\n",
      "[299,  1600] loss: 2.302621\n",
      "[299,  1800] loss: 2.302855\n",
      "[299,  2000] loss: 2.302385\n",
      "[299,  2200] loss: 2.302275\n",
      "[299,  2400] loss: 2.302975\n",
      "[299,  2600] loss: 2.302774\n",
      "[299,  2800] loss: 2.302543\n",
      "[299,  3000] loss: 2.302540\n",
      "[299,  3200] loss: 2.302614\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[300,   200] loss: 2.302559\n",
      "[300,   400] loss: 2.302783\n",
      "[300,   600] loss: 2.302542\n",
      "[300,   800] loss: 2.302622\n",
      "[300,  1000] loss: 2.302626\n",
      "[300,  1200] loss: 2.302647\n",
      "[300,  1400] loss: 2.302770\n",
      "[300,  1600] loss: 2.302751\n",
      "[300,  1800] loss: 2.302775\n",
      "[300,  2000] loss: 2.302676\n",
      "[300,  2200] loss: 2.302626\n",
      "[300,  2400] loss: 2.302535\n",
      "[300,  2600] loss: 2.302471\n",
      "[300,  2800] loss: 2.302758\n",
      "[300,  3000] loss: 2.302586\n",
      "[300,  3200] loss: 2.302873\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[301,   200] loss: 2.302530\n",
      "[301,   400] loss: 2.302832\n",
      "[301,   600] loss: 2.302649\n",
      "[301,   800] loss: 2.302596\n",
      "[301,  1000] loss: 2.302804\n",
      "[301,  1200] loss: 2.302537\n",
      "[301,  1400] loss: 2.302487\n",
      "[301,  1600] loss: 2.302609\n",
      "[301,  1800] loss: 2.302688\n",
      "[301,  2000] loss: 2.302761\n",
      "[301,  2200] loss: 2.302759\n",
      "[301,  2400] loss: 2.302561\n",
      "[301,  2600] loss: 2.302647\n",
      "[301,  2800] loss: 2.302598\n",
      "[301,  3000] loss: 2.302723\n",
      "[301,  3200] loss: 2.302610\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[302,   200] loss: 2.302664\n",
      "[302,   400] loss: 2.302444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302,   600] loss: 2.302744\n",
      "[302,   800] loss: 2.302837\n",
      "[302,  1000] loss: 2.302589\n",
      "[302,  1200] loss: 2.302364\n",
      "[302,  1400] loss: 2.302737\n",
      "[302,  1600] loss: 2.302516\n",
      "[302,  1800] loss: 2.302645\n",
      "[302,  2000] loss: 2.302698\n",
      "[302,  2200] loss: 2.302466\n",
      "[302,  2400] loss: 2.302732\n",
      "[302,  2600] loss: 2.302696\n",
      "[302,  2800] loss: 2.302657\n",
      "[302,  3000] loss: 2.302538\n",
      "[302,  3200] loss: 2.302771\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[303,   200] loss: 2.302419\n",
      "[303,   400] loss: 2.302450\n",
      "[303,   600] loss: 2.302549\n",
      "[303,   800] loss: 2.302363\n",
      "[303,  1000] loss: 2.302889\n",
      "[303,  1200] loss: 2.302731\n",
      "[303,  1400] loss: 2.302716\n",
      "[303,  1600] loss: 2.302717\n",
      "[303,  1800] loss: 2.302495\n",
      "[303,  2000] loss: 2.302485\n",
      "[303,  2200] loss: 2.302750\n",
      "[303,  2400] loss: 2.302380\n",
      "[303,  2600] loss: 2.302643\n",
      "[303,  2800] loss: 2.302752\n",
      "[303,  3000] loss: 2.302635\n",
      "[303,  3200] loss: 2.302789\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[304,   200] loss: 2.302696\n",
      "[304,   400] loss: 2.302610\n",
      "[304,   600] loss: 2.302685\n",
      "[304,   800] loss: 2.302704\n",
      "[304,  1000] loss: 2.302743\n",
      "[304,  1200] loss: 2.302580\n",
      "[304,  1400] loss: 2.302625\n",
      "[304,  1600] loss: 2.302672\n",
      "[304,  1800] loss: 2.302534\n",
      "[304,  2000] loss: 2.302643\n",
      "[304,  2200] loss: 2.302784\n",
      "[304,  2400] loss: 2.302717\n",
      "[304,  2600] loss: 2.302599\n",
      "[304,  2800] loss: 2.302610\n",
      "[304,  3000] loss: 2.302677\n",
      "[304,  3200] loss: 2.302679\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[305,   200] loss: 2.302447\n",
      "[305,   400] loss: 2.302761\n",
      "[305,   600] loss: 2.302595\n",
      "[305,   800] loss: 2.302661\n",
      "[305,  1000] loss: 2.302600\n",
      "[305,  1200] loss: 2.302535\n",
      "[305,  1400] loss: 2.302730\n",
      "[305,  1600] loss: 2.302672\n",
      "[305,  1800] loss: 2.302508\n",
      "[305,  2000] loss: 2.302531\n",
      "[305,  2200] loss: 2.302677\n",
      "[305,  2400] loss: 2.302649\n",
      "[305,  2600] loss: 2.302786\n",
      "[305,  2800] loss: 2.302666\n",
      "[305,  3000] loss: 2.302527\n",
      "[305,  3200] loss: 2.302783\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[306,   200] loss: 2.302647\n",
      "[306,   400] loss: 2.302606\n",
      "[306,   600] loss: 2.302681\n",
      "[306,   800] loss: 2.302747\n",
      "[306,  1000] loss: 2.302724\n",
      "[306,  1200] loss: 2.302660\n",
      "[306,  1400] loss: 2.302724\n",
      "[306,  1600] loss: 2.302388\n",
      "[306,  1800] loss: 2.302641\n",
      "[306,  2000] loss: 2.302610\n",
      "[306,  2200] loss: 2.302544\n",
      "[306,  2400] loss: 2.302813\n",
      "[306,  2600] loss: 2.302755\n",
      "[306,  2800] loss: 2.302742\n",
      "[306,  3000] loss: 2.302659\n",
      "[306,  3200] loss: 2.302479\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[307,   200] loss: 2.302518\n",
      "[307,   400] loss: 2.302804\n",
      "[307,   600] loss: 2.302727\n",
      "[307,   800] loss: 2.302553\n",
      "[307,  1000] loss: 2.302441\n",
      "[307,  1200] loss: 2.302801\n",
      "[307,  1400] loss: 2.302800\n",
      "[307,  1600] loss: 2.302662\n",
      "[307,  1800] loss: 2.302673\n",
      "[307,  2000] loss: 2.302621\n",
      "[307,  2200] loss: 2.302465\n",
      "[307,  2400] loss: 2.302767\n",
      "[307,  2600] loss: 2.302502\n",
      "[307,  2800] loss: 2.302552\n",
      "[307,  3000] loss: 2.302672\n",
      "[307,  3200] loss: 2.302720\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[308,   200] loss: 2.302675\n",
      "[308,   400] loss: 2.302679\n",
      "[308,   600] loss: 2.302645\n",
      "[308,   800] loss: 2.302791\n",
      "[308,  1000] loss: 2.302336\n",
      "[308,  1200] loss: 2.302697\n",
      "[308,  1400] loss: 2.302648\n",
      "[308,  1600] loss: 2.302586\n",
      "[308,  1800] loss: 2.302797\n",
      "[308,  2000] loss: 2.302507\n",
      "[308,  2200] loss: 2.302836\n",
      "[308,  2400] loss: 2.302577\n",
      "[308,  2600] loss: 2.302409\n",
      "[308,  2800] loss: 2.302783\n",
      "[308,  3000] loss: 2.302487\n",
      "[308,  3200] loss: 2.302867\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[309,   200] loss: 2.302595\n",
      "[309,   400] loss: 2.302585\n",
      "[309,   600] loss: 2.302509\n",
      "[309,   800] loss: 2.302892\n",
      "[309,  1000] loss: 2.302751\n",
      "[309,  1200] loss: 2.302505\n",
      "[309,  1400] loss: 2.302712\n",
      "[309,  1600] loss: 2.302585\n",
      "[309,  1800] loss: 2.302637\n",
      "[309,  2000] loss: 2.302853\n",
      "[309,  2200] loss: 2.302765\n",
      "[309,  2400] loss: 2.302623\n",
      "[309,  2600] loss: 2.302570\n",
      "[309,  2800] loss: 2.302491\n",
      "[309,  3000] loss: 2.302670\n",
      "[309,  3200] loss: 2.302541\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[310,   200] loss: 2.302656\n",
      "[310,   400] loss: 2.302586\n",
      "[310,   600] loss: 2.302652\n",
      "[310,   800] loss: 2.302640\n",
      "[310,  1000] loss: 2.302655\n",
      "[310,  1200] loss: 2.302754\n",
      "[310,  1400] loss: 2.302677\n",
      "[310,  1600] loss: 2.302755\n",
      "[310,  1800] loss: 2.302720\n",
      "[310,  2000] loss: 2.302709\n",
      "[310,  2200] loss: 2.302496\n",
      "[310,  2400] loss: 2.302384\n",
      "[310,  2600] loss: 2.302719\n",
      "[310,  2800] loss: 2.302543\n",
      "[310,  3000] loss: 2.302729\n",
      "[310,  3200] loss: 2.302765\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[311,   200] loss: 2.302750\n",
      "[311,   400] loss: 2.302678\n",
      "[311,   600] loss: 2.302681\n",
      "[311,   800] loss: 2.302627\n",
      "[311,  1000] loss: 2.302682\n",
      "[311,  1200] loss: 2.302557\n",
      "[311,  1400] loss: 2.302821\n",
      "[311,  1600] loss: 2.302547\n",
      "[311,  1800] loss: 2.302653\n",
      "[311,  2000] loss: 2.302667\n",
      "[311,  2200] loss: 2.302732\n",
      "[311,  2400] loss: 2.302849\n",
      "[311,  2600] loss: 2.302507\n",
      "[311,  2800] loss: 2.302582\n",
      "[311,  3000] loss: 2.302771\n",
      "[311,  3200] loss: 2.302645\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[312,   200] loss: 2.302400\n",
      "[312,   400] loss: 2.302431\n",
      "[312,   600] loss: 2.302773\n",
      "[312,   800] loss: 2.302843\n",
      "[312,  1000] loss: 2.302437\n",
      "[312,  1200] loss: 2.302845\n",
      "[312,  1400] loss: 2.302550\n",
      "[312,  1600] loss: 2.302551\n",
      "[312,  1800] loss: 2.302795\n",
      "[312,  2000] loss: 2.302513\n",
      "[312,  2200] loss: 2.302676\n",
      "[312,  2400] loss: 2.302626\n",
      "[312,  2600] loss: 2.302864\n",
      "[312,  2800] loss: 2.302470\n",
      "[312,  3000] loss: 2.302762\n",
      "[312,  3200] loss: 2.302546\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[313,   200] loss: 2.302628\n",
      "[313,   400] loss: 2.302560\n",
      "[313,   600] loss: 2.302470\n",
      "[313,   800] loss: 2.302709\n",
      "[313,  1000] loss: 2.302567\n",
      "[313,  1200] loss: 2.302699\n",
      "[313,  1400] loss: 2.302810\n",
      "[313,  1600] loss: 2.302623\n",
      "[313,  1800] loss: 2.302559\n",
      "[313,  2000] loss: 2.302668\n",
      "[313,  2200] loss: 2.302612\n",
      "[313,  2400] loss: 2.302564\n",
      "[313,  2600] loss: 2.302625\n",
      "[313,  2800] loss: 2.302683\n",
      "[313,  3000] loss: 2.302831\n",
      "[313,  3200] loss: 2.302684\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[314,   200] loss: 2.302676\n",
      "[314,   400] loss: 2.302613\n",
      "[314,   600] loss: 2.302414\n",
      "[314,   800] loss: 2.302665\n",
      "[314,  1000] loss: 2.302908\n",
      "[314,  1200] loss: 2.302643\n",
      "[314,  1400] loss: 2.302786\n",
      "[314,  1600] loss: 2.302252\n",
      "[314,  1800] loss: 2.302829\n",
      "[314,  2000] loss: 2.302752\n",
      "[314,  2200] loss: 2.302622\n",
      "[314,  2400] loss: 2.302784\n",
      "[314,  2600] loss: 2.302649\n",
      "[314,  2800] loss: 2.302724\n",
      "[314,  3000] loss: 2.302639\n",
      "[314,  3200] loss: 2.302562\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[315,   200] loss: 2.302710\n",
      "[315,   400] loss: 2.302722\n",
      "[315,   600] loss: 2.302658\n",
      "[315,   800] loss: 2.302539\n",
      "[315,  1000] loss: 2.302664\n",
      "[315,  1200] loss: 2.302726\n",
      "[315,  1400] loss: 2.302693\n",
      "[315,  1600] loss: 2.302703\n",
      "[315,  1800] loss: 2.302605\n",
      "[315,  2000] loss: 2.302727\n",
      "[315,  2200] loss: 2.302548\n",
      "[315,  2400] loss: 2.302748\n",
      "[315,  2600] loss: 2.302497\n",
      "[315,  2800] loss: 2.302784\n",
      "[315,  3000] loss: 2.302717\n",
      "[315,  3200] loss: 2.302580\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[316,   200] loss: 2.302402\n",
      "[316,   400] loss: 2.302677\n",
      "[316,   600] loss: 2.302448\n",
      "[316,   800] loss: 2.302736\n",
      "[316,  1000] loss: 2.302806\n",
      "[316,  1200] loss: 2.302605\n",
      "[316,  1400] loss: 2.302653\n",
      "[316,  1600] loss: 2.302371\n",
      "[316,  1800] loss: 2.302689\n",
      "[316,  2000] loss: 2.302709\n",
      "[316,  2200] loss: 2.302681\n",
      "[316,  2400] loss: 2.302386\n",
      "[316,  2600] loss: 2.303071\n",
      "[316,  2800] loss: 2.302507\n",
      "[316,  3000] loss: 2.302812\n",
      "[316,  3200] loss: 2.302778\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[317,   200] loss: 2.302658\n",
      "[317,   400] loss: 2.302676\n",
      "[317,   600] loss: 2.302609\n",
      "[317,   800] loss: 2.302802\n",
      "[317,  1000] loss: 2.302583\n",
      "[317,  1200] loss: 2.302504\n",
      "[317,  1400] loss: 2.302694\n",
      "[317,  1600] loss: 2.302544\n",
      "[317,  1800] loss: 2.302425\n",
      "[317,  2000] loss: 2.302984\n",
      "[317,  2200] loss: 2.302742\n",
      "[317,  2400] loss: 2.302570\n",
      "[317,  2600] loss: 2.302782\n",
      "[317,  2800] loss: 2.302812\n",
      "[317,  3000] loss: 2.302544\n",
      "[317,  3200] loss: 2.302701\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[318,   200] loss: 2.302694\n",
      "[318,   400] loss: 2.302785\n",
      "[318,   600] loss: 2.302670\n",
      "[318,   800] loss: 2.302592\n",
      "[318,  1000] loss: 2.302670\n",
      "[318,  1200] loss: 2.302789\n",
      "[318,  1400] loss: 2.302707\n",
      "[318,  1600] loss: 2.302581\n",
      "[318,  1800] loss: 2.302626\n",
      "[318,  2000] loss: 2.302569\n",
      "[318,  2200] loss: 2.302571\n",
      "[318,  2400] loss: 2.302764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318,  2600] loss: 2.302585\n",
      "[318,  2800] loss: 2.302719\n",
      "[318,  3000] loss: 2.302683\n",
      "[318,  3200] loss: 2.302670\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[319,   200] loss: 2.302618\n",
      "[319,   400] loss: 2.302549\n",
      "[319,   600] loss: 2.302709\n",
      "[319,   800] loss: 2.302502\n",
      "[319,  1000] loss: 2.302700\n",
      "[319,  1200] loss: 2.302886\n",
      "[319,  1400] loss: 2.302648\n",
      "[319,  1600] loss: 2.302785\n",
      "[319,  1800] loss: 2.302646\n",
      "[319,  2000] loss: 2.302726\n",
      "[319,  2200] loss: 2.302508\n",
      "[319,  2400] loss: 2.302527\n",
      "[319,  2600] loss: 2.302712\n",
      "[319,  2800] loss: 2.302421\n",
      "[319,  3000] loss: 2.302830\n",
      "[319,  3200] loss: 2.302830\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[320,   200] loss: 2.302713\n",
      "[320,   400] loss: 2.302769\n",
      "[320,   600] loss: 2.302569\n",
      "[320,   800] loss: 2.302584\n",
      "[320,  1000] loss: 2.302689\n",
      "[320,  1200] loss: 2.302637\n",
      "[320,  1400] loss: 2.302718\n",
      "[320,  1600] loss: 2.302712\n",
      "[320,  1800] loss: 2.302387\n",
      "[320,  2000] loss: 2.302125\n",
      "[320,  2200] loss: 2.302776\n",
      "[320,  2400] loss: 2.302533\n",
      "[320,  2600] loss: 2.302777\n",
      "[320,  2800] loss: 2.302676\n",
      "[320,  3000] loss: 2.302807\n",
      "[320,  3200] loss: 2.302593\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[321,   200] loss: 2.302445\n",
      "[321,   400] loss: 2.302511\n",
      "[321,   600] loss: 2.302732\n",
      "[321,   800] loss: 2.302474\n",
      "[321,  1000] loss: 2.302635\n",
      "[321,  1200] loss: 2.302526\n",
      "[321,  1400] loss: 2.302858\n",
      "[321,  1600] loss: 2.302596\n",
      "[321,  1800] loss: 2.302363\n",
      "[321,  2000] loss: 2.302227\n",
      "[321,  2200] loss: 2.302659\n",
      "[321,  2400] loss: 2.302731\n",
      "[321,  2600] loss: 2.302837\n",
      "[321,  2800] loss: 2.302554\n",
      "[321,  3000] loss: 2.302696\n",
      "[321,  3200] loss: 2.302698\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[322,   200] loss: 2.302688\n",
      "[322,   400] loss: 2.302600\n",
      "[322,   600] loss: 2.302479\n",
      "[322,   800] loss: 2.302692\n",
      "[322,  1000] loss: 2.302795\n",
      "[322,  1200] loss: 2.302382\n",
      "[322,  1400] loss: 2.302761\n",
      "[322,  1600] loss: 2.302469\n",
      "[322,  1800] loss: 2.302699\n",
      "[322,  2000] loss: 2.302674\n",
      "[322,  2200] loss: 2.302738\n",
      "[322,  2400] loss: 2.302765\n",
      "[322,  2600] loss: 2.302661\n",
      "[322,  2800] loss: 2.302699\n",
      "[322,  3000] loss: 2.302564\n",
      "[322,  3200] loss: 2.302697\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[323,   200] loss: 2.302681\n",
      "[323,   400] loss: 2.302596\n",
      "[323,   600] loss: 2.302593\n",
      "[323,   800] loss: 2.302682\n",
      "[323,  1000] loss: 2.302574\n",
      "[323,  1200] loss: 2.302608\n",
      "[323,  1400] loss: 2.302520\n",
      "[323,  1600] loss: 2.302638\n",
      "[323,  1800] loss: 2.302508\n",
      "[323,  2000] loss: 2.302731\n",
      "[323,  2200] loss: 2.302594\n",
      "[323,  2400] loss: 2.302670\n",
      "[323,  2600] loss: 2.302707\n",
      "[323,  2800] loss: 2.302526\n",
      "[323,  3000] loss: 2.302651\n",
      "[323,  3200] loss: 2.302871\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[324,   200] loss: 2.302621\n",
      "[324,   400] loss: 2.302672\n",
      "[324,   600] loss: 2.302655\n",
      "[324,   800] loss: 2.302765\n",
      "[324,  1000] loss: 2.302517\n",
      "[324,  1200] loss: 2.302797\n",
      "[324,  1400] loss: 2.302598\n",
      "[324,  1600] loss: 2.302699\n",
      "[324,  1800] loss: 2.302830\n",
      "[324,  2000] loss: 2.302634\n",
      "[324,  2200] loss: 2.302392\n",
      "[324,  2400] loss: 2.302586\n",
      "[324,  2600] loss: 2.302698\n",
      "[324,  2800] loss: 2.302595\n",
      "[324,  3000] loss: 2.302738\n",
      "[324,  3200] loss: 2.302609\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[325,   200] loss: 2.302721\n",
      "[325,   400] loss: 2.302650\n",
      "[325,   600] loss: 2.302687\n",
      "[325,   800] loss: 2.302721\n",
      "[325,  1000] loss: 2.302653\n",
      "[325,  1200] loss: 2.302715\n",
      "[325,  1400] loss: 2.302731\n",
      "[325,  1600] loss: 2.302659\n",
      "[325,  1800] loss: 2.302565\n",
      "[325,  2000] loss: 2.302693\n",
      "[325,  2200] loss: 2.302786\n",
      "[325,  2400] loss: 2.302534\n",
      "[325,  2600] loss: 2.302447\n",
      "[325,  2800] loss: 2.302719\n",
      "[325,  3000] loss: 2.302722\n",
      "[325,  3200] loss: 2.302512\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[326,   200] loss: 2.302471\n",
      "[326,   400] loss: 2.302768\n",
      "[326,   600] loss: 2.302640\n",
      "[326,   800] loss: 2.302596\n",
      "[326,  1000] loss: 2.302720\n",
      "[326,  1200] loss: 2.302623\n",
      "[326,  1400] loss: 2.302320\n",
      "[326,  1600] loss: 2.302866\n",
      "[326,  1800] loss: 2.302576\n",
      "[326,  2000] loss: 2.302762\n",
      "[326,  2200] loss: 2.302735\n",
      "[326,  2400] loss: 2.302616\n",
      "[326,  2600] loss: 2.302564\n",
      "[326,  2800] loss: 2.302644\n",
      "[326,  3000] loss: 2.302764\n",
      "[326,  3200] loss: 2.302619\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[327,   200] loss: 2.302699\n",
      "[327,   400] loss: 2.302661\n",
      "[327,   600] loss: 2.302771\n",
      "[327,   800] loss: 2.302673\n",
      "[327,  1000] loss: 2.302426\n",
      "[327,  1200] loss: 2.302549\n",
      "[327,  1400] loss: 2.302635\n",
      "[327,  1600] loss: 2.302792\n",
      "[327,  1800] loss: 2.302683\n",
      "[327,  2000] loss: 2.302671\n",
      "[327,  2200] loss: 2.302705\n",
      "[327,  2400] loss: 2.302647\n",
      "[327,  2600] loss: 2.302714\n",
      "[327,  2800] loss: 2.302585\n",
      "[327,  3000] loss: 2.302435\n",
      "[327,  3200] loss: 2.302502\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[328,   200] loss: 2.302829\n",
      "[328,   400] loss: 2.302562\n",
      "[328,   600] loss: 2.302643\n",
      "[328,   800] loss: 2.302510\n",
      "[328,  1000] loss: 2.302811\n",
      "[328,  1200] loss: 2.302680\n",
      "[328,  1400] loss: 2.302706\n",
      "[328,  1600] loss: 2.302494\n",
      "[328,  1800] loss: 2.302703\n",
      "[328,  2000] loss: 2.302687\n",
      "[328,  2200] loss: 2.302718\n",
      "[328,  2400] loss: 2.302642\n",
      "[328,  2600] loss: 2.302472\n",
      "[328,  2800] loss: 2.302506\n",
      "[328,  3000] loss: 2.302718\n",
      "[328,  3200] loss: 2.302577\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[329,   200] loss: 2.302732\n",
      "[329,   400] loss: 2.302365\n",
      "[329,   600] loss: 2.302871\n",
      "[329,   800] loss: 2.302657\n",
      "[329,  1000] loss: 2.302905\n",
      "[329,  1200] loss: 2.302414\n",
      "[329,  1400] loss: 2.302691\n",
      "[329,  1600] loss: 2.302677\n",
      "[329,  1800] loss: 2.302539\n",
      "[329,  2000] loss: 2.302726\n",
      "[329,  2200] loss: 2.302765\n",
      "[329,  2400] loss: 2.302705\n",
      "[329,  2600] loss: 2.302602\n",
      "[329,  2800] loss: 2.302648\n",
      "[329,  3000] loss: 2.302676\n",
      "[329,  3200] loss: 2.302692\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[330,   200] loss: 2.302803\n",
      "[330,   400] loss: 2.302577\n",
      "[330,   600] loss: 2.302805\n",
      "[330,   800] loss: 2.302774\n",
      "[330,  1000] loss: 2.302701\n",
      "[330,  1200] loss: 2.302647\n",
      "[330,  1400] loss: 2.302591\n",
      "[330,  1600] loss: 2.302610\n",
      "[330,  1800] loss: 2.302509\n",
      "[330,  2000] loss: 2.302982\n",
      "[330,  2200] loss: 2.302547\n",
      "[330,  2400] loss: 2.302609\n",
      "[330,  2600] loss: 2.302668\n",
      "[330,  2800] loss: 2.302642\n",
      "[330,  3000] loss: 2.302351\n",
      "[330,  3200] loss: 2.302918\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[331,   200] loss: 2.302475\n",
      "[331,   400] loss: 2.302488\n",
      "[331,   600] loss: 2.302888\n",
      "[331,   800] loss: 2.302507\n",
      "[331,  1000] loss: 2.302483\n",
      "[331,  1200] loss: 2.302817\n",
      "[331,  1400] loss: 2.302530\n",
      "[331,  1600] loss: 2.302761\n",
      "[331,  1800] loss: 2.302585\n",
      "[331,  2000] loss: 2.302596\n",
      "[331,  2200] loss: 2.302800\n",
      "[331,  2400] loss: 2.302566\n",
      "[331,  2600] loss: 2.302613\n",
      "[331,  2800] loss: 2.302762\n",
      "[331,  3000] loss: 2.302651\n",
      "[331,  3200] loss: 2.302712\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[332,   200] loss: 2.302556\n",
      "[332,   400] loss: 2.302804\n",
      "[332,   600] loss: 2.302531\n",
      "[332,   800] loss: 2.302753\n",
      "[332,  1000] loss: 2.302614\n",
      "[332,  1200] loss: 2.302618\n",
      "[332,  1400] loss: 2.302411\n",
      "[332,  1600] loss: 2.302381\n",
      "[332,  1800] loss: 2.302772\n",
      "[332,  2000] loss: 2.302796\n",
      "[332,  2200] loss: 2.302559\n",
      "[332,  2400] loss: 2.302708\n",
      "[332,  2600] loss: 2.302501\n",
      "[332,  2800] loss: 2.302634\n",
      "[332,  3000] loss: 2.302802\n",
      "[332,  3200] loss: 2.302729\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[333,   200] loss: 2.302626\n",
      "[333,   400] loss: 2.302699\n",
      "[333,   600] loss: 2.302771\n",
      "[333,   800] loss: 2.302511\n",
      "[333,  1000] loss: 2.302420\n",
      "[333,  1200] loss: 2.302826\n",
      "[333,  1400] loss: 2.302650\n",
      "[333,  1600] loss: 2.302670\n",
      "[333,  1800] loss: 2.302710\n",
      "[333,  2000] loss: 2.302709\n",
      "[333,  2200] loss: 2.302658\n",
      "[333,  2400] loss: 2.302564\n",
      "[333,  2600] loss: 2.302528\n",
      "[333,  2800] loss: 2.302619\n",
      "[333,  3000] loss: 2.302689\n",
      "[333,  3200] loss: 2.302686\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[334,   200] loss: 2.302713\n",
      "[334,   400] loss: 2.302642\n",
      "[334,   600] loss: 2.302721\n",
      "[334,   800] loss: 2.302676\n",
      "[334,  1000] loss: 2.302499\n",
      "[334,  1200] loss: 2.302648\n",
      "[334,  1400] loss: 2.302682\n",
      "[334,  1600] loss: 2.302681\n",
      "[334,  1800] loss: 2.302620\n",
      "[334,  2000] loss: 2.302489\n",
      "[334,  2200] loss: 2.302860\n",
      "[334,  2400] loss: 2.302688\n",
      "[334,  2600] loss: 2.302371\n",
      "[334,  2800] loss: 2.302704\n",
      "[334,  3000] loss: 2.302573\n",
      "[334,  3200] loss: 2.302658\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[335,   200] loss: 2.302546\n",
      "[335,   400] loss: 2.302517\n",
      "[335,   600] loss: 2.302777\n",
      "[335,   800] loss: 2.302570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335,  1000] loss: 2.302676\n",
      "[335,  1200] loss: 2.302675\n",
      "[335,  1400] loss: 2.302818\n",
      "[335,  1600] loss: 2.302516\n",
      "[335,  1800] loss: 2.302776\n",
      "[335,  2000] loss: 2.302652\n",
      "[335,  2200] loss: 2.302552\n",
      "[335,  2400] loss: 2.302603\n",
      "[335,  2600] loss: 2.302645\n",
      "[335,  2800] loss: 2.302769\n",
      "[335,  3000] loss: 2.302367\n",
      "[335,  3200] loss: 2.302846\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[336,   200] loss: 2.302534\n",
      "[336,   400] loss: 2.302823\n",
      "[336,   600] loss: 2.302590\n",
      "[336,   800] loss: 2.302770\n",
      "[336,  1000] loss: 2.302492\n",
      "[336,  1200] loss: 2.302744\n",
      "[336,  1400] loss: 2.302336\n",
      "[336,  1600] loss: 2.302626\n",
      "[336,  1800] loss: 2.302672\n",
      "[336,  2000] loss: 2.302440\n",
      "[336,  2200] loss: 2.302781\n",
      "[336,  2400] loss: 2.302679\n",
      "[336,  2600] loss: 2.302893\n",
      "[336,  2800] loss: 2.302640\n",
      "[336,  3000] loss: 2.302635\n",
      "[336,  3200] loss: 2.302721\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[337,   200] loss: 2.302712\n",
      "[337,   400] loss: 2.302673\n",
      "[337,   600] loss: 2.302761\n",
      "[337,   800] loss: 2.302597\n",
      "[337,  1000] loss: 2.302587\n",
      "[337,  1200] loss: 2.302338\n",
      "[337,  1400] loss: 2.302437\n",
      "[337,  1600] loss: 2.302454\n",
      "[337,  1800] loss: 2.302618\n",
      "[337,  2000] loss: 2.302777\n",
      "[337,  2200] loss: 2.302744\n",
      "[337,  2400] loss: 2.302550\n",
      "[337,  2600] loss: 2.302698\n",
      "[337,  2800] loss: 2.302607\n",
      "[337,  3000] loss: 2.302753\n",
      "[337,  3200] loss: 2.302792\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[338,   200] loss: 2.302687\n",
      "[338,   400] loss: 2.302525\n",
      "[338,   600] loss: 2.302667\n",
      "[338,   800] loss: 2.302805\n",
      "[338,  1000] loss: 2.302384\n",
      "[338,  1200] loss: 2.302749\n",
      "[338,  1400] loss: 2.302755\n",
      "[338,  1600] loss: 2.302745\n",
      "[338,  1800] loss: 2.302597\n",
      "[338,  2000] loss: 2.302771\n",
      "[338,  2200] loss: 2.302593\n",
      "[338,  2400] loss: 2.302823\n",
      "[338,  2600] loss: 2.302629\n",
      "[338,  2800] loss: 2.302578\n",
      "[338,  3000] loss: 2.302636\n",
      "[338,  3200] loss: 2.302772\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[339,   200] loss: 2.302576\n",
      "[339,   400] loss: 2.302427\n",
      "[339,   600] loss: 2.302561\n",
      "[339,   800] loss: 2.302720\n",
      "[339,  1000] loss: 2.302561\n",
      "[339,  1200] loss: 2.302951\n",
      "[339,  1400] loss: 2.302664\n",
      "[339,  1600] loss: 2.302490\n",
      "[339,  1800] loss: 2.302767\n",
      "[339,  2000] loss: 2.302612\n",
      "[339,  2200] loss: 2.302765\n",
      "[339,  2400] loss: 2.302737\n",
      "[339,  2600] loss: 2.302642\n",
      "[339,  2800] loss: 2.302576\n",
      "[339,  3000] loss: 2.302641\n",
      "[339,  3200] loss: 2.302559\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[340,   200] loss: 2.302876\n",
      "[340,   400] loss: 2.302499\n",
      "[340,   600] loss: 2.302703\n",
      "[340,   800] loss: 2.302215\n",
      "[340,  1000] loss: 2.302902\n",
      "[340,  1200] loss: 2.302616\n",
      "[340,  1400] loss: 2.302849\n",
      "[340,  1600] loss: 2.302593\n",
      "[340,  1800] loss: 2.302678\n",
      "[340,  2000] loss: 2.302545\n",
      "[340,  2200] loss: 2.302538\n",
      "[340,  2400] loss: 2.302693\n",
      "[340,  2600] loss: 2.302613\n",
      "[340,  2800] loss: 2.302633\n",
      "[340,  3000] loss: 2.302604\n",
      "[340,  3200] loss: 2.302727\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[341,   200] loss: 2.302781\n",
      "[341,   400] loss: 2.302659\n",
      "[341,   600] loss: 2.302635\n",
      "[341,   800] loss: 2.302622\n",
      "[341,  1000] loss: 2.302779\n",
      "[341,  1200] loss: 2.302746\n",
      "[341,  1400] loss: 2.302755\n",
      "[341,  1600] loss: 2.302667\n",
      "[341,  1800] loss: 2.302657\n",
      "[341,  2000] loss: 2.302581\n",
      "[341,  2200] loss: 2.302444\n",
      "[341,  2400] loss: 2.302558\n",
      "[341,  2600] loss: 2.302630\n",
      "[341,  2800] loss: 2.302633\n",
      "[341,  3000] loss: 2.302518\n",
      "[341,  3200] loss: 2.302569\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[342,   200] loss: 2.302628\n",
      "[342,   400] loss: 2.302711\n",
      "[342,   600] loss: 2.302374\n",
      "[342,   800] loss: 2.302649\n",
      "[342,  1000] loss: 2.302795\n",
      "[342,  1200] loss: 2.302664\n",
      "[342,  1400] loss: 2.302466\n",
      "[342,  1600] loss: 2.302838\n",
      "[342,  1800] loss: 2.302443\n",
      "[342,  2000] loss: 2.302404\n",
      "[342,  2200] loss: 2.302348\n",
      "[342,  2400] loss: 2.302850\n",
      "[342,  2600] loss: 2.302573\n",
      "[342,  2800] loss: 2.302795\n",
      "[342,  3000] loss: 2.302863\n",
      "[342,  3200] loss: 2.302608\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[343,   200] loss: 2.302669\n",
      "[343,   400] loss: 2.302606\n",
      "[343,   600] loss: 2.302766\n",
      "[343,   800] loss: 2.302660\n",
      "[343,  1000] loss: 2.302614\n",
      "[343,  1200] loss: 2.302578\n",
      "[343,  1400] loss: 2.302744\n",
      "[343,  1600] loss: 2.302639\n",
      "[343,  1800] loss: 2.302535\n",
      "[343,  2000] loss: 2.302828\n",
      "[343,  2200] loss: 2.302700\n",
      "[343,  2400] loss: 2.302720\n",
      "[343,  2600] loss: 2.302527\n",
      "[343,  2800] loss: 2.302456\n",
      "[343,  3000] loss: 2.302796\n",
      "[343,  3200] loss: 2.302558\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[344,   200] loss: 2.302513\n",
      "[344,   400] loss: 2.302757\n",
      "[344,   600] loss: 2.302574\n",
      "[344,   800] loss: 2.302540\n",
      "[344,  1000] loss: 2.302830\n",
      "[344,  1200] loss: 2.302392\n",
      "[344,  1400] loss: 2.302495\n",
      "[344,  1600] loss: 2.302978\n",
      "[344,  1800] loss: 2.302568\n",
      "[344,  2000] loss: 2.302659\n",
      "[344,  2200] loss: 2.302792\n",
      "[344,  2400] loss: 2.302549\n",
      "[344,  2600] loss: 2.302764\n",
      "[344,  2800] loss: 2.302544\n",
      "[344,  3000] loss: 2.302797\n",
      "[344,  3200] loss: 2.302689\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[345,   200] loss: 2.302593\n",
      "[345,   400] loss: 2.302522\n",
      "[345,   600] loss: 2.302377\n",
      "[345,   800] loss: 2.302934\n",
      "[345,  1000] loss: 2.302609\n",
      "[345,  1200] loss: 2.302641\n",
      "[345,  1400] loss: 2.302364\n",
      "[345,  1600] loss: 2.302867\n",
      "[345,  1800] loss: 2.302563\n",
      "[345,  2000] loss: 2.302750\n",
      "[345,  2200] loss: 2.302594\n",
      "[345,  2400] loss: 2.302697\n",
      "[345,  2600] loss: 2.302841\n",
      "[345,  2800] loss: 2.302577\n",
      "[345,  3000] loss: 2.302626\n",
      "[345,  3200] loss: 2.302740\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[346,   200] loss: 2.302611\n",
      "[346,   400] loss: 2.302762\n",
      "[346,   600] loss: 2.302461\n",
      "[346,   800] loss: 2.302677\n",
      "[346,  1000] loss: 2.302528\n",
      "[346,  1200] loss: 2.302877\n",
      "[346,  1400] loss: 2.302692\n",
      "[346,  1600] loss: 2.302761\n",
      "[346,  1800] loss: 2.302767\n",
      "[346,  2000] loss: 2.302724\n",
      "[346,  2200] loss: 2.302551\n",
      "[346,  2400] loss: 2.302689\n",
      "[346,  2600] loss: 2.302798\n",
      "[346,  2800] loss: 2.302459\n",
      "[346,  3000] loss: 2.302522\n",
      "[346,  3200] loss: 2.302782\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[347,   200] loss: 2.302519\n",
      "[347,   400] loss: 2.302594\n",
      "[347,   600] loss: 2.302800\n",
      "[347,   800] loss: 2.302502\n",
      "[347,  1000] loss: 2.302532\n",
      "[347,  1200] loss: 2.302626\n",
      "[347,  1400] loss: 2.302816\n",
      "[347,  1600] loss: 2.302786\n",
      "[347,  1800] loss: 2.302753\n",
      "[347,  2000] loss: 2.302675\n",
      "[347,  2200] loss: 2.302679\n",
      "[347,  2400] loss: 2.302608\n",
      "[347,  2600] loss: 2.302804\n",
      "[347,  2800] loss: 2.302689\n",
      "[347,  3000] loss: 2.302482\n",
      "[347,  3200] loss: 2.302685\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[348,   200] loss: 2.302455\n",
      "[348,   400] loss: 2.302814\n",
      "[348,   600] loss: 2.302600\n",
      "[348,   800] loss: 2.302454\n",
      "[348,  1000] loss: 2.302959\n",
      "[348,  1200] loss: 2.302558\n",
      "[348,  1400] loss: 2.302682\n",
      "[348,  1600] loss: 2.302814\n",
      "[348,  1800] loss: 2.302567\n",
      "[348,  2000] loss: 2.302704\n",
      "[348,  2200] loss: 2.302587\n",
      "[348,  2400] loss: 2.302493\n",
      "[348,  2600] loss: 2.302754\n",
      "[348,  2800] loss: 2.302489\n",
      "[348,  3000] loss: 2.302727\n",
      "[348,  3200] loss: 2.302784\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[349,   200] loss: 2.302440\n",
      "[349,   400] loss: 2.302772\n",
      "[349,   600] loss: 2.302678\n",
      "[349,   800] loss: 2.302660\n",
      "[349,  1000] loss: 2.302639\n",
      "[349,  1200] loss: 2.302774\n",
      "[349,  1400] loss: 2.302746\n",
      "[349,  1600] loss: 2.302427\n",
      "[349,  1800] loss: 2.302518\n",
      "[349,  2000] loss: 2.302816\n",
      "[349,  2200] loss: 2.302610\n",
      "[349,  2400] loss: 2.302626\n",
      "[349,  2600] loss: 2.302862\n",
      "[349,  2800] loss: 2.302622\n",
      "[349,  3000] loss: 2.302308\n",
      "[349,  3200] loss: 2.302763\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[350,   200] loss: 2.302692\n",
      "[350,   400] loss: 2.302697\n",
      "[350,   600] loss: 2.302650\n",
      "[350,   800] loss: 2.302519\n",
      "[350,  1000] loss: 2.302584\n",
      "[350,  1200] loss: 2.302718\n",
      "[350,  1400] loss: 2.302671\n",
      "[350,  1600] loss: 2.302688\n",
      "[350,  1800] loss: 2.302664\n",
      "[350,  2000] loss: 2.302729\n",
      "[350,  2200] loss: 2.302585\n",
      "[350,  2400] loss: 2.302639\n",
      "[350,  2600] loss: 2.302500\n",
      "[350,  2800] loss: 2.302707\n",
      "[350,  3000] loss: 2.302615\n",
      "[350,  3200] loss: 2.302640\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# define and train the network\n",
    "red_stylised_model_path_torch = './cifar32_style_red_torch.pth'\n",
    "red_stylised_model_torch = torchvision.models.resnet50().to(device)\n",
    "lr=0.1\n",
    "red_stylised_optimizer_torch = optim.Adam(red_stylised_model_torch.parameters(), lr=lr)\n",
    "red_stylised_lr_scheduler_torch = optim.lr_scheduler.MultiStepLR(red_stylised_optimizer_torch, milestones=[150, 250])\n",
    "train_part(red_stylised_model_torch, data_loader_train_style_red, data_loader_val_style_red, red_stylised_model_path_torch, red_stylised_optimizer_torch, red_stylised_lr_scheduler_torch, epochs = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Reduced Stylised ResNet50 Torch on Normal CIFAR10 Dataset\n",
    "\n",
    "The below code tests the reduced stylised ResNet50 torch model on the normal CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 997 / 10000 correct, accuracy of the dataset is: 9.970 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "red_stylised_model_torch = torchvision.models.resnet50().to(device)\n",
    "red_stylised_model_torch.load_state_dict(torch.load('./cifar32_style_red_torch.pth'))\n",
    "red_stylised_model_torch.to(device)\n",
    "check_accuracy(data_loader_test, red_stylised_model_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Reduced Stylised ResNet50 Torch on Reduced Stylised CIFAR10 Dataset\n",
    "\n",
    "The below code tests the reduced stylised ResNet50 torch model on the reduced stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1000 / 10000 correct, accuracy of the dataset is: 10.000 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "red_stylised_model_torch = torchvision.models.resnet50().to(device)\n",
    "red_stylised_model_torch.load_state_dict(torch.load('./cifar32_style_red_torch.pth'))\n",
    "red_stylised_model_torch.to(device)\n",
    "check_accuracy(data_loader_test_style_red, red_stylised_model_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
