{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Contextural Bias of ResNet50 on CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook trains and tests a vanilla ResNet50 model and a stylised ResNet50 model with the CIFAR10 dataset. It includes functions for loading the dataset, turning them into tensors, model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv2d, AvgPool2d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following cell provides a class that loads the CIFAR dataset given the relevant path, processes it into a dictionary format of class labels and content then processes the images into tensors. The class also has helper functions to extract information about the dataset needed for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        super(CifarDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        \n",
    "        classes_list = []\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            classes_list.append(class_name)\n",
    "        classes_list.sort()\n",
    "        self.classes = [dict(class_idx = k, class_name = v) for k, v in enumerate(classes_list)]\n",
    "        \n",
    "\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "        tr = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        img = tr(img)\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_datasets(data_path):\n",
    "    dataset = CifarDataset(data_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used for this experiment are normal CIFAR10 dataset and the stylised version of the CIFAR10 dataset created using AdaIN style transfer.\n",
    "\n",
    "The following cell calls the function created above to load the training, validation and testing datasets of both normal and stylised CIFAR10 and transforms them into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 36000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load normal CIFAR10\n",
    "data_path_train = \"../../CIFAR/cifar32/training\"\n",
    "dataset_train = get_cifar_datasets(data_path_train)\n",
    "\n",
    "data_path_val = \"../../CIFAR/cifar32/validation/\"\n",
    "dataset_val = get_cifar_datasets(data_path_val)\n",
    "\n",
    "data_path_test = \"../../CIFAR/cifar32/testing/\"\n",
    "dataset_test = get_cifar_datasets(data_path_test)\n",
    "\n",
    "print(f\"Number of train samples {dataset_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of val samples {dataset_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of test samples {dataset_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val = DataLoader(dataset_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with original kaggle images\n",
    "data_path_train_style = \"../../CIFAR/cifar32_style/training\"\n",
    "dataset_train_style = get_cifar_datasets(data_path_train_style)\n",
    "\n",
    "data_path_val_style = \"../../CIFAR/cifar32_style/validation/\"\n",
    "dataset_val_style = get_cifar_datasets(data_path_val_style)\n",
    "\n",
    "data_path_test_style = \"../../CIFAR/cifar32_style/testing/\"\n",
    "dataset_test_style = get_cifar_datasets(data_path_test_style)\n",
    "\n",
    "print(f\"Number of stylised train samples {dataset_train_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised val samples {dataset_val_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised test samples {dataset_test_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style = DataLoader(dataset_train_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style = DataLoader(dataset_val_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style = DataLoader(dataset_test_style, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with reduced kaggle images\n",
    "data_path_train_style_red = \"../../CIFAR/cifar32_style_red/training\"\n",
    "dataset_train_style_red = get_cifar_datasets(data_path_train_style_red)\n",
    "\n",
    "data_path_val_style_red = \"../../CIFAR/cifar32_style_red/validation/\"\n",
    "dataset_val_style_red = get_cifar_datasets(data_path_val_style_red)\n",
    "\n",
    "data_path_test_style_red = \"../../CIFAR/cifar32_style_red/testing/\"\n",
    "dataset_test_style_red = get_cifar_datasets(data_path_test_style_red)\n",
    "\n",
    "print(f\"Number of reduced stylised train samples {dataset_train_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised val samples {dataset_val_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised test samples {dataset_test_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style_red.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style_red = DataLoader(dataset_train_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style_red = DataLoader(dataset_val_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style_red = DataLoader(dataset_test_style_red, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch['image'] torch.Size([64, 3, 32, 32])\n",
      "Shape of batch['cls'] torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the data was loaded correctly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(8, 8))\n",
    "\n",
    "for batch in data_loader_test_style:\n",
    "\n",
    "    print(f\"Shape of batch['image'] {batch['image'].shape}\")\n",
    "    print(f\"Shape of batch['cls'] {batch['cls'].shape}\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        col = i % 4\n",
    "        row = i // 4\n",
    "\n",
    "        img = batch['image'][i].numpy()\n",
    "\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(batch['class_name'][i])\n",
    "        axes[row,col].imshow(np.transpose(img,(1,2,0)))\n",
    "                         \n",
    "        if i >= 15:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining ResNet50\n",
    "\n",
    "The following code defines Resnet50 architecture that will be used to train and test the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resnet building blocks\n",
    "class ResidualBlock(nn.Module): \n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inchannel, outchannel, stride=1): \n",
    "        \n",
    "        super(ResidualBlock, self).__init__() \n",
    "        \n",
    "        self.left = nn.Sequential(\n",
    "            Conv2d(inchannel, outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, self.expansion*outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*outchannel)\n",
    "        ) \n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or inchannel != self.expansion*outchannel: \n",
    "            self.shortcut = nn.Sequential(\n",
    "                Conv2d(inchannel, self.expansion*outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*outchannel)\n",
    "            ) \n",
    "            \n",
    "    def forward(self, x): \n",
    "        out = self.left(x) \n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out) \n",
    "        return out\n",
    "\n",
    "    \n",
    "# define resnet\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ResidualBlock, num_classes = 10):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv2d(3, 64, kernel_size = 3, stride = 1,padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 3, stride = 1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 4, stride = 2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 6, stride = 2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 3, stride = 2)\n",
    "        self.avgpool = AvgPool2d(4)\n",
    "        self.fc = nn.Linear(512*ResidualBlock.expansion, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet50():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the training, validation and testing functions for the experiment. train_part() function trains and updates gradients on each batch of the training set and once that is done, it tests its accuracy on the validation set. Every time the validation test returns a better accuracy than the current maximum, the model is saved and carries on with the next epoch. Then it checks with the learning reate scheduler for any changes in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "    \n",
    "print_every = 100\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "            scores = model(inputs)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct, accuracy of the dataset is: %.3f %%' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_part(model, train_data, val_data, model_path, optimizer, lr_scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "    val_acc = 0\n",
    "    num_epoch = 2\n",
    "    # Main Loop\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        val_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for i, batch in enumerate(train_data, 0):\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "\n",
    "            # get outputs from the input data and calculate the cross entropy loss\n",
    "            scores = model(inputs)\n",
    "            loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "            # zero and update the gradients and optimise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i, batch in enumerate(val_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['cls'].to(device)\n",
    "\n",
    "                # get the outputs from the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # compute accuracy based on the outputs\n",
    "                _, preds = outputs.max(1)\n",
    "                num_correct += (preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "            if acc > val_acc:\n",
    "                print('saving model')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                val_acc = acc\n",
    "            else:\n",
    "                print('skip model saving')\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla ResNet50 Training\n",
    "\n",
    "The model used in this experiment is ResNet50 with Adam optimiser with learning rate scheduler and default settings. Learning rate changes every 80, 120, 160, 180th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 3.598853\n",
      "[1,   400] loss: 2.021459\n",
      "Got 974 / 4000 correct (24.35)\n",
      "saving model\n",
      "[2,   200] loss: 1.851733\n",
      "[2,   400] loss: 1.798656\n",
      "Got 1530 / 4000 correct (38.25)\n",
      "saving model\n",
      "[3,   200] loss: 1.665520\n",
      "[3,   400] loss: 1.584996\n",
      "Got 1760 / 4000 correct (44.00)\n",
      "saving model\n",
      "[4,   200] loss: 1.441373\n",
      "[4,   400] loss: 1.483538\n",
      "Got 1861 / 4000 correct (46.52)\n",
      "saving model\n",
      "[5,   200] loss: 1.304863\n",
      "[5,   400] loss: 1.293829\n",
      "Got 2073 / 4000 correct (51.82)\n",
      "saving model\n",
      "[6,   200] loss: 1.258562\n",
      "[6,   400] loss: 1.205399\n",
      "Got 2268 / 4000 correct (56.70)\n",
      "saving model\n",
      "[7,   200] loss: 1.097656\n",
      "[7,   400] loss: 1.091432\n",
      "Got 2234 / 4000 correct (55.85)\n",
      "skip model saving\n",
      "[8,   200] loss: 1.009811\n",
      "[8,   400] loss: 1.004631\n",
      "Got 2524 / 4000 correct (63.10)\n",
      "saving model\n",
      "[9,   200] loss: 0.960038\n",
      "[9,   400] loss: 0.943464\n",
      "Got 2581 / 4000 correct (64.53)\n",
      "saving model\n",
      "[10,   200] loss: 0.902805\n",
      "[10,   400] loss: 1.030621\n",
      "Got 2108 / 4000 correct (52.70)\n",
      "skip model saving\n",
      "[11,   200] loss: 0.828310\n",
      "[11,   400] loss: 0.838679\n",
      "Got 2651 / 4000 correct (66.27)\n",
      "saving model\n",
      "[12,   200] loss: 0.770959\n",
      "[12,   400] loss: 0.776816\n",
      "Got 2772 / 4000 correct (69.30)\n",
      "saving model\n",
      "[13,   200] loss: 0.737984\n",
      "[13,   400] loss: 0.775721\n",
      "Got 2606 / 4000 correct (65.15)\n",
      "skip model saving\n",
      "[14,   200] loss: 0.676484\n",
      "[14,   400] loss: 0.713433\n",
      "Got 2939 / 4000 correct (73.47)\n",
      "saving model\n",
      "[15,   200] loss: 0.611226\n",
      "[15,   400] loss: 0.639139\n",
      "Got 2899 / 4000 correct (72.47)\n",
      "skip model saving\n",
      "[16,   200] loss: 0.582869\n",
      "[16,   400] loss: 0.634607\n",
      "Got 2620 / 4000 correct (65.50)\n",
      "skip model saving\n",
      "[17,   200] loss: 0.530226\n",
      "[17,   400] loss: 0.561482\n",
      "Got 3007 / 4000 correct (75.17)\n",
      "saving model\n",
      "[18,   200] loss: 0.494938\n",
      "[18,   400] loss: 0.507437\n",
      "Got 3019 / 4000 correct (75.48)\n",
      "saving model\n",
      "[19,   200] loss: 0.436132\n",
      "[19,   400] loss: 0.501509\n",
      "Got 2992 / 4000 correct (74.80)\n",
      "skip model saving\n",
      "[20,   200] loss: 0.407134\n",
      "[20,   400] loss: 0.453832\n",
      "Got 2913 / 4000 correct (72.82)\n",
      "skip model saving\n",
      "[21,   200] loss: 0.359086\n",
      "[21,   400] loss: 0.404211\n",
      "Got 2928 / 4000 correct (73.20)\n",
      "skip model saving\n",
      "[22,   200] loss: 0.386262\n",
      "[22,   400] loss: 0.387145\n",
      "Got 3036 / 4000 correct (75.90)\n",
      "saving model\n",
      "[23,   200] loss: 0.288443\n",
      "[23,   400] loss: 0.320813\n",
      "Got 2952 / 4000 correct (73.80)\n",
      "skip model saving\n",
      "[24,   200] loss: 0.275802\n",
      "[24,   400] loss: 0.283324\n",
      "Got 3011 / 4000 correct (75.28)\n",
      "skip model saving\n",
      "[25,   200] loss: 0.260716\n",
      "[25,   400] loss: 0.289704\n",
      "Got 3051 / 4000 correct (76.28)\n",
      "saving model\n",
      "[26,   200] loss: 0.215794\n",
      "[26,   400] loss: 0.225222\n",
      "Got 3079 / 4000 correct (76.98)\n",
      "saving model\n",
      "[27,   200] loss: 0.191556\n",
      "[27,   400] loss: 0.234857\n",
      "Got 3026 / 4000 correct (75.65)\n",
      "skip model saving\n",
      "[28,   200] loss: 0.177980\n",
      "[28,   400] loss: 0.222914\n",
      "Got 3037 / 4000 correct (75.92)\n",
      "skip model saving\n",
      "[29,   200] loss: 0.158635\n",
      "[29,   400] loss: 0.237986\n",
      "Got 3085 / 4000 correct (77.12)\n",
      "saving model\n",
      "[30,   200] loss: 0.146515\n",
      "[30,   400] loss: 0.172536\n",
      "Got 2925 / 4000 correct (73.12)\n",
      "skip model saving\n",
      "[31,   200] loss: 0.143632\n",
      "[31,   400] loss: 0.140385\n",
      "Got 3070 / 4000 correct (76.75)\n",
      "skip model saving\n",
      "[32,   200] loss: 0.133335\n",
      "[32,   400] loss: 0.146153\n",
      "Got 2800 / 4000 correct (70.00)\n",
      "skip model saving\n",
      "[33,   200] loss: 0.220263\n",
      "[33,   400] loss: 0.167741\n",
      "Got 3065 / 4000 correct (76.62)\n",
      "skip model saving\n",
      "[34,   200] loss: 0.094389\n",
      "[34,   400] loss: 0.119799\n",
      "Got 3075 / 4000 correct (76.88)\n",
      "skip model saving\n",
      "[35,   200] loss: 0.123370\n",
      "[35,   400] loss: 0.141666\n",
      "Got 3148 / 4000 correct (78.70)\n",
      "saving model\n",
      "[36,   200] loss: 0.118013\n",
      "[36,   400] loss: 0.125066\n",
      "Got 3094 / 4000 correct (77.35)\n",
      "skip model saving\n",
      "[37,   200] loss: 0.103564\n",
      "[37,   400] loss: 0.108611\n",
      "Got 3124 / 4000 correct (78.10)\n",
      "skip model saving\n",
      "[38,   200] loss: 0.093618\n",
      "[38,   400] loss: 0.152449\n",
      "Got 3064 / 4000 correct (76.60)\n",
      "skip model saving\n",
      "[39,   200] loss: 0.100903\n",
      "[39,   400] loss: 0.106134\n",
      "Got 3098 / 4000 correct (77.45)\n",
      "skip model saving\n",
      "[40,   200] loss: 0.095278\n",
      "[40,   400] loss: 0.120544\n",
      "Got 3143 / 4000 correct (78.57)\n",
      "skip model saving\n",
      "[41,   200] loss: 0.083830\n",
      "[41,   400] loss: 0.099683\n",
      "Got 3080 / 4000 correct (77.00)\n",
      "skip model saving\n",
      "[42,   200] loss: 0.158879\n",
      "[42,   400] loss: 0.105026\n",
      "Got 3114 / 4000 correct (77.85)\n",
      "skip model saving\n",
      "[43,   200] loss: 0.084372\n",
      "[43,   400] loss: 0.101655\n",
      "Got 3102 / 4000 correct (77.55)\n",
      "skip model saving\n",
      "[44,   200] loss: 0.100151\n",
      "[44,   400] loss: 0.088228\n",
      "Got 3131 / 4000 correct (78.27)\n",
      "skip model saving\n",
      "[45,   200] loss: 0.075565\n",
      "[45,   400] loss: 0.095296\n",
      "Got 3129 / 4000 correct (78.22)\n",
      "skip model saving\n",
      "[46,   200] loss: 0.074085\n",
      "[46,   400] loss: 0.084926\n",
      "Got 3108 / 4000 correct (77.70)\n",
      "skip model saving\n",
      "[47,   200] loss: 0.059235\n",
      "[47,   400] loss: 0.073563\n",
      "Got 3063 / 4000 correct (76.58)\n",
      "skip model saving\n",
      "[48,   200] loss: 0.081040\n",
      "[48,   400] loss: 0.092036\n",
      "Got 3110 / 4000 correct (77.75)\n",
      "skip model saving\n",
      "[49,   200] loss: 0.103564\n",
      "[49,   400] loss: 0.094614\n",
      "Got 3122 / 4000 correct (78.05)\n",
      "skip model saving\n",
      "[50,   200] loss: 0.053818\n",
      "[50,   400] loss: 0.064696\n",
      "Got 3114 / 4000 correct (77.85)\n",
      "skip model saving\n",
      "[51,   200] loss: 0.056202\n",
      "[51,   400] loss: 0.077045\n",
      "Got 3141 / 4000 correct (78.53)\n",
      "skip model saving\n",
      "[52,   200] loss: 0.070587\n",
      "[52,   400] loss: 0.091344\n",
      "Got 3094 / 4000 correct (77.35)\n",
      "skip model saving\n",
      "[53,   200] loss: 0.115818\n",
      "[53,   400] loss: 0.072542\n",
      "Got 3104 / 4000 correct (77.60)\n",
      "skip model saving\n",
      "[54,   200] loss: 0.052832\n",
      "[54,   400] loss: 0.061686\n",
      "Got 3072 / 4000 correct (76.80)\n",
      "skip model saving\n",
      "[55,   200] loss: 0.075815\n",
      "[55,   400] loss: 0.081549\n",
      "Got 3130 / 4000 correct (78.25)\n",
      "skip model saving\n",
      "[56,   200] loss: 0.056549\n",
      "[56,   400] loss: 0.061896\n",
      "Got 3123 / 4000 correct (78.08)\n",
      "skip model saving\n",
      "[57,   200] loss: 0.053549\n",
      "[57,   400] loss: 0.070051\n",
      "Got 3090 / 4000 correct (77.25)\n",
      "skip model saving\n",
      "[58,   200] loss: 0.060722\n",
      "[58,   400] loss: 0.067787\n",
      "Got 3122 / 4000 correct (78.05)\n",
      "skip model saving\n",
      "[59,   200] loss: 0.059341\n",
      "[59,   400] loss: 0.047814\n",
      "Got 3165 / 4000 correct (79.12)\n",
      "saving model\n",
      "[60,   200] loss: 0.038558\n",
      "[60,   400] loss: 0.116733\n",
      "Got 3123 / 4000 correct (78.08)\n",
      "skip model saving\n",
      "[61,   200] loss: 0.060411\n",
      "[61,   400] loss: 0.045074\n",
      "Got 3154 / 4000 correct (78.85)\n",
      "skip model saving\n",
      "[62,   200] loss: 0.040189\n",
      "[62,   400] loss: 0.065679\n",
      "Got 3149 / 4000 correct (78.72)\n",
      "skip model saving\n",
      "[63,   200] loss: 0.055354\n",
      "[63,   400] loss: 0.061728\n",
      "Got 3174 / 4000 correct (79.35)\n",
      "saving model\n",
      "[64,   200] loss: 0.065142\n",
      "[64,   400] loss: 0.086195\n",
      "Got 3146 / 4000 correct (78.65)\n",
      "skip model saving\n",
      "[65,   200] loss: 0.063604\n",
      "[65,   400] loss: 0.044101\n",
      "Got 3108 / 4000 correct (77.70)\n",
      "skip model saving\n",
      "[66,   200] loss: 0.042550\n",
      "[66,   400] loss: 0.058060\n",
      "Got 3137 / 4000 correct (78.42)\n",
      "skip model saving\n",
      "[67,   200] loss: 0.055269\n",
      "[67,   400] loss: 0.038509\n",
      "Got 3102 / 4000 correct (77.55)\n",
      "skip model saving\n",
      "[68,   200] loss: 0.055298\n",
      "[68,   400] loss: 0.057069\n",
      "Got 3159 / 4000 correct (78.97)\n",
      "skip model saving\n",
      "[69,   200] loss: 0.046560\n",
      "[69,   400] loss: 0.078651\n",
      "Got 3144 / 4000 correct (78.60)\n",
      "skip model saving\n",
      "[70,   200] loss: 0.050210\n",
      "[70,   400] loss: 0.046519\n",
      "Got 3115 / 4000 correct (77.88)\n",
      "skip model saving\n",
      "[71,   200] loss: 0.030566\n",
      "[71,   400] loss: 0.035222\n",
      "Got 3012 / 4000 correct (75.30)\n",
      "skip model saving\n",
      "[72,   200] loss: 0.072053\n",
      "[72,   400] loss: 0.050287\n",
      "Got 3168 / 4000 correct (79.20)\n",
      "skip model saving\n",
      "[73,   200] loss: 0.044594\n",
      "[73,   400] loss: 0.044084\n",
      "Got 3105 / 4000 correct (77.62)\n",
      "skip model saving\n",
      "[74,   200] loss: 0.052994\n",
      "[74,   400] loss: 0.068044\n",
      "Got 3186 / 4000 correct (79.65)\n",
      "saving model\n",
      "[75,   200] loss: 0.036930\n",
      "[75,   400] loss: 0.061951\n",
      "Got 3181 / 4000 correct (79.53)\n",
      "skip model saving\n",
      "[76,   200] loss: 0.030376\n",
      "[76,   400] loss: 0.048972\n",
      "Got 3160 / 4000 correct (79.00)\n",
      "skip model saving\n",
      "[77,   200] loss: 0.054937\n",
      "[77,   400] loss: 0.039436\n",
      "Got 3116 / 4000 correct (77.90)\n",
      "skip model saving\n",
      "[78,   200] loss: 0.046180\n",
      "[78,   400] loss: 0.082116\n",
      "Got 3132 / 4000 correct (78.30)\n",
      "skip model saving\n",
      "[79,   200] loss: 0.031899\n",
      "[79,   400] loss: 0.035927\n",
      "Got 3140 / 4000 correct (78.50)\n",
      "skip model saving\n",
      "[80,   200] loss: 0.054901\n",
      "[80,   400] loss: 0.076131\n",
      "Got 3145 / 4000 correct (78.62)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81,   200] loss: 0.037780\n",
      "[81,   400] loss: 0.052734\n",
      "Got 3176 / 4000 correct (79.40)\n",
      "skip model saving\n",
      "[82,   200] loss: 0.041081\n",
      "[82,   400] loss: 0.044112\n",
      "Got 3135 / 4000 correct (78.38)\n",
      "skip model saving\n",
      "[83,   200] loss: 0.061669\n",
      "[83,   400] loss: 0.060997\n",
      "Got 3174 / 4000 correct (79.35)\n",
      "skip model saving\n",
      "[84,   200] loss: 0.024607\n",
      "[84,   400] loss: 0.045983\n",
      "Got 3180 / 4000 correct (79.50)\n",
      "skip model saving\n",
      "[85,   200] loss: 0.041620\n",
      "[85,   400] loss: 0.042359\n",
      "Got 3159 / 4000 correct (78.97)\n",
      "skip model saving\n",
      "[86,   200] loss: 0.045329\n",
      "[86,   400] loss: 0.051611\n",
      "Got 3122 / 4000 correct (78.05)\n",
      "skip model saving\n",
      "[87,   200] loss: 0.032658\n",
      "[87,   400] loss: 0.050101\n",
      "Got 3203 / 4000 correct (80.08)\n",
      "saving model\n",
      "[88,   200] loss: 0.030600\n",
      "[88,   400] loss: 0.035854\n",
      "Got 3216 / 4000 correct (80.40)\n",
      "saving model\n",
      "[89,   200] loss: 0.050119\n",
      "[89,   400] loss: 0.056535\n",
      "Got 3155 / 4000 correct (78.88)\n",
      "skip model saving\n",
      "[90,   200] loss: 0.040081\n",
      "[90,   400] loss: 0.039269\n",
      "Got 3075 / 4000 correct (76.88)\n",
      "skip model saving\n",
      "[91,   200] loss: 0.036528\n",
      "[91,   400] loss: 0.027408\n",
      "Got 3177 / 4000 correct (79.42)\n",
      "skip model saving\n",
      "[92,   200] loss: 0.043613\n",
      "[92,   400] loss: 0.057317\n",
      "Got 3148 / 4000 correct (78.70)\n",
      "skip model saving\n",
      "[93,   200] loss: 0.032089\n",
      "[93,   400] loss: 0.036196\n",
      "Got 3167 / 4000 correct (79.17)\n",
      "skip model saving\n",
      "[94,   200] loss: 0.036274\n",
      "[94,   400] loss: 0.035004\n",
      "Got 3145 / 4000 correct (78.62)\n",
      "skip model saving\n",
      "[95,   200] loss: 0.050246\n",
      "[95,   400] loss: 0.050818\n",
      "Got 3170 / 4000 correct (79.25)\n",
      "skip model saving\n",
      "[96,   200] loss: 0.025688\n",
      "[96,   400] loss: 0.028628\n",
      "Got 3098 / 4000 correct (77.45)\n",
      "skip model saving\n",
      "[97,   200] loss: 0.048932\n",
      "[97,   400] loss: 0.045435\n",
      "Got 3163 / 4000 correct (79.07)\n",
      "skip model saving\n",
      "[98,   200] loss: 0.023460\n",
      "[98,   400] loss: 0.021139\n",
      "Got 3155 / 4000 correct (78.88)\n",
      "skip model saving\n",
      "[99,   200] loss: 0.029194\n",
      "[99,   400] loss: 0.031403\n",
      "Got 3166 / 4000 correct (79.15)\n",
      "skip model saving\n",
      "[100,   200] loss: 0.040317\n",
      "[100,   400] loss: 0.029650\n",
      "Got 3161 / 4000 correct (79.03)\n",
      "skip model saving\n",
      "[101,   200] loss: 0.041235\n",
      "[101,   400] loss: 0.029362\n",
      "Got 3180 / 4000 correct (79.50)\n",
      "skip model saving\n",
      "[102,   200] loss: 0.029324\n",
      "[102,   400] loss: 0.045921\n",
      "Got 3155 / 4000 correct (78.88)\n",
      "skip model saving\n",
      "[103,   200] loss: 0.044254\n",
      "[103,   400] loss: 0.041371\n",
      "Got 3184 / 4000 correct (79.60)\n",
      "skip model saving\n",
      "[104,   200] loss: 0.018350\n",
      "[104,   400] loss: 0.026622\n",
      "Got 3180 / 4000 correct (79.50)\n",
      "skip model saving\n",
      "[105,   200] loss: 0.026011\n",
      "[105,   400] loss: 0.054818\n",
      "Got 3134 / 4000 correct (78.35)\n",
      "skip model saving\n",
      "[106,   200] loss: 0.055228\n",
      "[106,   400] loss: 0.042480\n",
      "Got 3153 / 4000 correct (78.83)\n",
      "skip model saving\n",
      "[107,   200] loss: 0.035250\n",
      "[107,   400] loss: 0.029348\n",
      "Got 3130 / 4000 correct (78.25)\n",
      "skip model saving\n",
      "[108,   200] loss: 0.029364\n",
      "[108,   400] loss: 0.023359\n",
      "Got 3125 / 4000 correct (78.12)\n",
      "skip model saving\n",
      "[109,   200] loss: 0.024284\n",
      "[109,   400] loss: 0.043270\n",
      "Got 3127 / 4000 correct (78.17)\n",
      "skip model saving\n",
      "[110,   200] loss: 0.046176\n",
      "[110,   400] loss: 0.035299\n",
      "Got 2954 / 4000 correct (73.85)\n",
      "skip model saving\n",
      "[111,   200] loss: 0.028883\n",
      "[111,   400] loss: 0.035145\n",
      "Got 3151 / 4000 correct (78.77)\n",
      "skip model saving\n",
      "[112,   200] loss: 0.054222\n",
      "[112,   400] loss: 0.046070\n",
      "Got 3144 / 4000 correct (78.60)\n",
      "skip model saving\n",
      "[113,   200] loss: 0.031010\n",
      "[113,   400] loss: 0.026887\n",
      "Got 3156 / 4000 correct (78.90)\n",
      "skip model saving\n",
      "[114,   200] loss: 0.025404\n",
      "[114,   400] loss: 0.026957\n",
      "Got 3167 / 4000 correct (79.17)\n",
      "skip model saving\n",
      "[115,   200] loss: 0.024841\n",
      "[115,   400] loss: 0.031303\n",
      "Got 3184 / 4000 correct (79.60)\n",
      "skip model saving\n",
      "[116,   200] loss: 0.024702\n",
      "[116,   400] loss: 0.029005\n",
      "Got 3122 / 4000 correct (78.05)\n",
      "skip model saving\n",
      "[117,   200] loss: 0.033638\n",
      "[117,   400] loss: 0.024365\n",
      "Got 3169 / 4000 correct (79.22)\n",
      "skip model saving\n",
      "[118,   200] loss: 0.028202\n",
      "[118,   400] loss: 0.027727\n",
      "Got 3190 / 4000 correct (79.75)\n",
      "skip model saving\n",
      "[119,   200] loss: 0.029043\n",
      "[119,   400] loss: 0.025385\n",
      "Got 3173 / 4000 correct (79.33)\n",
      "skip model saving\n",
      "[120,   200] loss: 0.032464\n",
      "[120,   400] loss: 0.030065\n",
      "Got 3146 / 4000 correct (78.65)\n",
      "skip model saving\n",
      "[121,   200] loss: 0.022030\n",
      "[121,   400] loss: 0.043926\n",
      "Got 3171 / 4000 correct (79.27)\n",
      "skip model saving\n",
      "[122,   200] loss: 0.023472\n",
      "[122,   400] loss: 0.021669\n",
      "Got 3162 / 4000 correct (79.05)\n",
      "skip model saving\n",
      "[123,   200] loss: 0.021420\n",
      "[123,   400] loss: 0.026048\n",
      "Got 3122 / 4000 correct (78.05)\n",
      "skip model saving\n",
      "[124,   200] loss: 0.056229\n",
      "[124,   400] loss: 0.048001\n",
      "Got 3142 / 4000 correct (78.55)\n",
      "skip model saving\n",
      "[125,   200] loss: 0.030406\n",
      "[125,   400] loss: 0.018698\n",
      "Got 3179 / 4000 correct (79.47)\n",
      "skip model saving\n",
      "[126,   200] loss: 0.023132\n",
      "[126,   400] loss: 0.022423\n",
      "Got 3188 / 4000 correct (79.70)\n",
      "skip model saving\n",
      "[127,   200] loss: 0.032153\n",
      "[127,   400] loss: 0.026576\n",
      "Got 3131 / 4000 correct (78.27)\n",
      "skip model saving\n",
      "[128,   200] loss: 0.037287\n",
      "[128,   400] loss: 0.030292\n",
      "Got 3171 / 4000 correct (79.27)\n",
      "skip model saving\n",
      "[129,   200] loss: 0.025277\n",
      "[129,   400] loss: 0.024945\n",
      "Got 3189 / 4000 correct (79.72)\n",
      "skip model saving\n",
      "[130,   200] loss: 0.025199\n",
      "[130,   400] loss: 0.025389\n",
      "Got 3181 / 4000 correct (79.53)\n",
      "skip model saving\n",
      "[131,   200] loss: 0.021069\n",
      "[131,   400] loss: 0.027913\n",
      "Got 3116 / 4000 correct (77.90)\n",
      "skip model saving\n",
      "[132,   200] loss: 0.041784\n",
      "[132,   400] loss: 0.033341\n",
      "Got 3168 / 4000 correct (79.20)\n",
      "skip model saving\n",
      "[133,   200] loss: 0.026790\n",
      "[133,   400] loss: 0.023764\n",
      "Got 3156 / 4000 correct (78.90)\n",
      "skip model saving\n",
      "[134,   200] loss: 0.024004\n",
      "[134,   400] loss: 0.025742\n",
      "Got 3171 / 4000 correct (79.27)\n",
      "skip model saving\n",
      "[135,   200] loss: 0.023775\n",
      "[135,   400] loss: 0.030787\n",
      "Got 3131 / 4000 correct (78.27)\n",
      "skip model saving\n",
      "[136,   200] loss: 0.030853\n",
      "[136,   400] loss: 0.027800\n",
      "Got 3211 / 4000 correct (80.27)\n",
      "skip model saving\n",
      "[137,   200] loss: 0.021328\n",
      "[137,   400] loss: 0.020056\n",
      "Got 3170 / 4000 correct (79.25)\n",
      "skip model saving\n",
      "[138,   200] loss: 0.022273\n",
      "[138,   400] loss: 0.029473\n",
      "Got 3178 / 4000 correct (79.45)\n",
      "skip model saving\n",
      "[139,   200] loss: 0.025470\n",
      "[139,   400] loss: 0.032953\n",
      "Got 3133 / 4000 correct (78.33)\n",
      "skip model saving\n",
      "[140,   200] loss: 0.029172\n",
      "[140,   400] loss: 0.036958\n",
      "Got 3189 / 4000 correct (79.72)\n",
      "skip model saving\n",
      "[141,   200] loss: 0.013812\n",
      "[141,   400] loss: 0.065219\n",
      "Got 3164 / 4000 correct (79.10)\n",
      "skip model saving\n",
      "[142,   200] loss: 0.025469\n",
      "[142,   400] loss: 0.018013\n",
      "Got 3147 / 4000 correct (78.67)\n",
      "skip model saving\n",
      "[143,   200] loss: 0.019100\n",
      "[143,   400] loss: 0.015026\n",
      "Got 3125 / 4000 correct (78.12)\n",
      "skip model saving\n",
      "[144,   200] loss: 0.053972\n",
      "[144,   400] loss: 0.028474\n",
      "Got 3196 / 4000 correct (79.90)\n",
      "skip model saving\n",
      "[145,   200] loss: 0.025385\n",
      "[145,   400] loss: 0.025294\n",
      "Got 3156 / 4000 correct (78.90)\n",
      "skip model saving\n",
      "[146,   200] loss: 0.017823\n",
      "[146,   400] loss: 0.096165\n",
      "Got 3194 / 4000 correct (79.85)\n",
      "skip model saving\n",
      "[147,   200] loss: 0.027389\n",
      "[147,   400] loss: 0.021076\n",
      "Got 3162 / 4000 correct (79.05)\n",
      "skip model saving\n",
      "[148,   200] loss: 0.016794\n",
      "[148,   400] loss: 0.015972\n",
      "Got 3155 / 4000 correct (78.88)\n",
      "skip model saving\n",
      "[149,   200] loss: 0.025602\n",
      "[149,   400] loss: 0.025956\n",
      "Got 3198 / 4000 correct (79.95)\n",
      "skip model saving\n",
      "[150,   200] loss: 0.019538\n",
      "[150,   400] loss: 0.022121\n",
      "Got 3183 / 4000 correct (79.57)\n",
      "skip model saving\n",
      "[151,   200] loss: 0.014989\n",
      "[151,   400] loss: 0.008348\n",
      "Got 3230 / 4000 correct (80.75)\n",
      "saving model\n",
      "[152,   200] loss: 0.002923\n",
      "[152,   400] loss: 0.003457\n",
      "Got 3247 / 4000 correct (81.17)\n",
      "saving model\n",
      "[153,   200] loss: 0.001076\n",
      "[153,   400] loss: 0.001779\n",
      "Got 3246 / 4000 correct (81.15)\n",
      "skip model saving\n",
      "[154,   200] loss: 0.001115\n",
      "[154,   400] loss: 0.001077\n",
      "Got 3250 / 4000 correct (81.25)\n",
      "saving model\n",
      "[155,   200] loss: 0.001046\n",
      "[155,   400] loss: 0.000703\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "saving model\n",
      "[156,   200] loss: 0.000634\n",
      "[156,   400] loss: 0.000505\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "saving model\n",
      "[157,   200] loss: 0.000462\n",
      "[157,   400] loss: 0.000286\n",
      "Got 3256 / 4000 correct (81.40)\n",
      "skip model saving\n",
      "[158,   200] loss: 0.000218\n",
      "[158,   400] loss: 0.000276\n",
      "Got 3244 / 4000 correct (81.10)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159,   200] loss: 0.000210\n",
      "[159,   400] loss: 0.000206\n",
      "Got 3255 / 4000 correct (81.38)\n",
      "skip model saving\n",
      "[160,   200] loss: 0.000241\n",
      "[160,   400] loss: 0.000416\n",
      "Got 3252 / 4000 correct (81.30)\n",
      "skip model saving\n",
      "[161,   200] loss: 0.000203\n",
      "[161,   400] loss: 0.000615\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "saving model\n",
      "[162,   200] loss: 0.000178\n",
      "[162,   400] loss: 0.000289\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[163,   200] loss: 0.000664\n",
      "[163,   400] loss: 0.000306\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "saving model\n",
      "[164,   200] loss: 0.000245\n",
      "[164,   400] loss: 0.000292\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[165,   200] loss: 0.000108\n",
      "[165,   400] loss: 0.000118\n",
      "Got 3286 / 4000 correct (82.15)\n",
      "saving model\n",
      "[166,   200] loss: 0.000104\n",
      "[166,   400] loss: 0.000150\n",
      "Got 3250 / 4000 correct (81.25)\n",
      "skip model saving\n",
      "[167,   200] loss: 0.000076\n",
      "[167,   400] loss: 0.000713\n",
      "Got 3253 / 4000 correct (81.33)\n",
      "skip model saving\n",
      "[168,   200] loss: 0.000441\n",
      "[168,   400] loss: 0.000547\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[169,   200] loss: 0.000098\n",
      "[169,   400] loss: 0.000189\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[170,   200] loss: 0.000769\n",
      "[170,   400] loss: 0.000132\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[171,   200] loss: 0.000099\n",
      "[171,   400] loss: 0.000088\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[172,   200] loss: 0.000078\n",
      "[172,   400] loss: 0.000371\n",
      "Got 3277 / 4000 correct (81.92)\n",
      "skip model saving\n",
      "[173,   200] loss: 0.000052\n",
      "[173,   400] loss: 0.000150\n",
      "Got 3277 / 4000 correct (81.92)\n",
      "skip model saving\n",
      "[174,   200] loss: 0.000046\n",
      "[174,   400] loss: 0.000087\n",
      "Got 3282 / 4000 correct (82.05)\n",
      "skip model saving\n",
      "[175,   200] loss: 0.000037\n",
      "[175,   400] loss: 0.000053\n",
      "Got 3280 / 4000 correct (82.00)\n",
      "skip model saving\n",
      "[176,   200] loss: 0.000031\n",
      "[176,   400] loss: 0.000068\n",
      "Got 3281 / 4000 correct (82.03)\n",
      "skip model saving\n",
      "[177,   200] loss: 0.000305\n",
      "[177,   400] loss: 0.000120\n",
      "Got 3253 / 4000 correct (81.33)\n",
      "skip model saving\n",
      "[178,   200] loss: 0.000076\n",
      "[178,   400] loss: 0.000131\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[179,   200] loss: 0.000160\n",
      "[179,   400] loss: 0.000103\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[180,   200] loss: 0.000181\n",
      "[180,   400] loss: 0.000507\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[181,   200] loss: 0.000053\n",
      "[181,   400] loss: 0.000285\n",
      "Got 3271 / 4000 correct (81.77)\n",
      "skip model saving\n",
      "[182,   200] loss: 0.000150\n",
      "[182,   400] loss: 0.000224\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[183,   200] loss: 0.000365\n",
      "[183,   400] loss: 0.000102\n",
      "Got 3275 / 4000 correct (81.88)\n",
      "skip model saving\n",
      "[184,   200] loss: 0.000078\n",
      "[184,   400] loss: 0.000062\n",
      "Got 3285 / 4000 correct (82.12)\n",
      "skip model saving\n",
      "[185,   200] loss: 0.000112\n",
      "[185,   400] loss: 0.000182\n",
      "Got 3276 / 4000 correct (81.90)\n",
      "skip model saving\n",
      "[186,   200] loss: 0.000081\n",
      "[186,   400] loss: 0.000034\n",
      "Got 3272 / 4000 correct (81.80)\n",
      "skip model saving\n",
      "[187,   200] loss: 0.000134\n",
      "[187,   400] loss: 0.001202\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[188,   200] loss: 0.000141\n",
      "[188,   400] loss: 0.000091\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[189,   200] loss: 0.000661\n",
      "[189,   400] loss: 0.000105\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[190,   200] loss: 0.000062\n",
      "[190,   400] loss: 0.000169\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[191,   200] loss: 0.000451\n",
      "[191,   400] loss: 0.000252\n",
      "Got 3246 / 4000 correct (81.15)\n",
      "skip model saving\n",
      "[192,   200] loss: 0.000253\n",
      "[192,   400] loss: 0.000164\n",
      "Got 3254 / 4000 correct (81.35)\n",
      "skip model saving\n",
      "[193,   200] loss: 0.000116\n",
      "[193,   400] loss: 0.000137\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[194,   200] loss: 0.000037\n",
      "[194,   400] loss: 0.000020\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[195,   200] loss: 0.000033\n",
      "[195,   400] loss: 0.000020\n",
      "Got 3271 / 4000 correct (81.77)\n",
      "skip model saving\n",
      "[196,   200] loss: 0.000028\n",
      "[196,   400] loss: 0.000019\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[197,   200] loss: 0.000037\n",
      "[197,   400] loss: 0.000028\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[198,   200] loss: 0.000278\n",
      "[198,   400] loss: 0.000530\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[199,   200] loss: 0.000360\n",
      "[199,   400] loss: 0.000346\n",
      "Got 3255 / 4000 correct (81.38)\n",
      "skip model saving\n",
      "[200,   200] loss: 0.000123\n",
      "[200,   400] loss: 0.000245\n",
      "Got 3273 / 4000 correct (81.83)\n",
      "skip model saving\n",
      "[201,   200] loss: 0.000087\n",
      "[201,   400] loss: 0.000288\n",
      "Got 3246 / 4000 correct (81.15)\n",
      "skip model saving\n",
      "[202,   200] loss: 0.000183\n",
      "[202,   400] loss: 0.000114\n",
      "Got 3273 / 4000 correct (81.83)\n",
      "skip model saving\n",
      "[203,   200] loss: 0.000033\n",
      "[203,   400] loss: 0.000105\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[204,   200] loss: 0.000016\n",
      "[204,   400] loss: 0.000025\n",
      "Got 3255 / 4000 correct (81.38)\n",
      "skip model saving\n",
      "[205,   200] loss: 0.000023\n",
      "[205,   400] loss: 0.000087\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[206,   200] loss: 0.000168\n",
      "[206,   400] loss: 0.000148\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[207,   200] loss: 0.000054\n",
      "[207,   400] loss: 0.000443\n",
      "Got 3271 / 4000 correct (81.77)\n",
      "skip model saving\n",
      "[208,   200] loss: 0.000022\n",
      "[208,   400] loss: 0.000018\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[209,   200] loss: 0.000035\n",
      "[209,   400] loss: 0.000153\n",
      "Got 3256 / 4000 correct (81.40)\n",
      "skip model saving\n",
      "[210,   200] loss: 0.000146\n",
      "[210,   400] loss: 0.000252\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[211,   200] loss: 0.000245\n",
      "[211,   400] loss: 0.000057\n",
      "Got 3254 / 4000 correct (81.35)\n",
      "skip model saving\n",
      "[212,   200] loss: 0.000094\n",
      "[212,   400] loss: 0.000144\n",
      "Got 3271 / 4000 correct (81.77)\n",
      "skip model saving\n",
      "[213,   200] loss: 0.001130\n",
      "[213,   400] loss: 0.000123\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[214,   200] loss: 0.000298\n",
      "[214,   400] loss: 0.000298\n",
      "Got 3261 / 4000 correct (81.53)\n",
      "skip model saving\n",
      "[215,   200] loss: 0.000094\n",
      "[215,   400] loss: 0.000033\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[216,   200] loss: 0.000056\n",
      "[216,   400] loss: 0.000026\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[217,   200] loss: 0.000058\n",
      "[217,   400] loss: 0.000068\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[218,   200] loss: 0.000099\n",
      "[218,   400] loss: 0.000154\n",
      "Got 3256 / 4000 correct (81.40)\n",
      "skip model saving\n",
      "[219,   200] loss: 0.000026\n",
      "[219,   400] loss: 0.000024\n",
      "Got 3255 / 4000 correct (81.38)\n",
      "skip model saving\n",
      "[220,   200] loss: 0.000058\n",
      "[220,   400] loss: 0.000017\n",
      "Got 3247 / 4000 correct (81.17)\n",
      "skip model saving\n",
      "[221,   200] loss: 0.000156\n",
      "[221,   400] loss: 0.000365\n",
      "Got 3245 / 4000 correct (81.12)\n",
      "skip model saving\n",
      "[222,   200] loss: 0.000705\n",
      "[222,   400] loss: 0.000211\n",
      "Got 3243 / 4000 correct (81.08)\n",
      "skip model saving\n",
      "[223,   200] loss: 0.000171\n",
      "[223,   400] loss: 0.000652\n",
      "Got 3253 / 4000 correct (81.33)\n",
      "skip model saving\n",
      "[224,   200] loss: 0.000023\n",
      "[224,   400] loss: 0.000100\n",
      "Got 3247 / 4000 correct (81.17)\n",
      "skip model saving\n",
      "[225,   200] loss: 0.000168\n",
      "[225,   400] loss: 0.000080\n",
      "Got 3246 / 4000 correct (81.15)\n",
      "skip model saving\n",
      "[226,   200] loss: 0.000323\n",
      "[226,   400] loss: 0.000133\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[227,   200] loss: 0.000056\n",
      "[227,   400] loss: 0.000575\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[228,   200] loss: 0.000029\n",
      "[228,   400] loss: 0.000673\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[229,   200] loss: 0.000201\n",
      "[229,   400] loss: 0.000095\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[230,   200] loss: 0.000016\n",
      "[230,   400] loss: 0.000099\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[231,   200] loss: 0.000175\n",
      "[231,   400] loss: 0.000106\n",
      "Got 3256 / 4000 correct (81.40)\n",
      "skip model saving\n",
      "[232,   200] loss: 0.000151\n",
      "[232,   400] loss: 0.000043\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[233,   200] loss: 0.000303\n",
      "[233,   400] loss: 0.000279\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[234,   200] loss: 0.000028\n",
      "[234,   400] loss: 0.000018\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[235,   200] loss: 0.000024\n",
      "[235,   400] loss: 0.000007\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[236,   200] loss: 0.000019\n",
      "[236,   400] loss: 0.000071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[237,   200] loss: 0.000467\n",
      "[237,   400] loss: 0.000161\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[238,   200] loss: 0.000050\n",
      "[238,   400] loss: 0.000024\n",
      "Got 3249 / 4000 correct (81.23)\n",
      "skip model saving\n",
      "[239,   200] loss: 0.000577\n",
      "[239,   400] loss: 0.000658\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[240,   200] loss: 0.000052\n",
      "[240,   400] loss: 0.000891\n",
      "Got 3253 / 4000 correct (81.33)\n",
      "skip model saving\n",
      "[241,   200] loss: 0.000164\n",
      "[241,   400] loss: 0.000078\n",
      "Got 3248 / 4000 correct (81.20)\n",
      "skip model saving\n",
      "[242,   200] loss: 0.000034\n",
      "[242,   400] loss: 0.000018\n",
      "Got 3250 / 4000 correct (81.25)\n",
      "skip model saving\n",
      "[243,   200] loss: 0.000039\n",
      "[243,   400] loss: 0.000081\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[244,   200] loss: 0.000022\n",
      "[244,   400] loss: 0.000027\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[245,   200] loss: 0.000099\n",
      "[245,   400] loss: 0.000068\n",
      "Got 3276 / 4000 correct (81.90)\n",
      "skip model saving\n",
      "[246,   200] loss: 0.000026\n",
      "[246,   400] loss: 0.000117\n",
      "Got 3274 / 4000 correct (81.85)\n",
      "skip model saving\n",
      "[247,   200] loss: 0.000034\n",
      "[247,   400] loss: 0.000017\n",
      "Got 3273 / 4000 correct (81.83)\n",
      "skip model saving\n",
      "[248,   200] loss: 0.000014\n",
      "[248,   400] loss: 0.000305\n",
      "Got 3272 / 4000 correct (81.80)\n",
      "skip model saving\n",
      "[249,   200] loss: 0.000441\n",
      "[249,   400] loss: 0.000029\n",
      "Got 3249 / 4000 correct (81.23)\n",
      "skip model saving\n",
      "[250,   200] loss: 0.000049\n",
      "[250,   400] loss: 0.000049\n",
      "Got 3255 / 4000 correct (81.38)\n",
      "skip model saving\n",
      "[251,   200] loss: 0.000217\n",
      "[251,   400] loss: 0.000164\n",
      "Got 3245 / 4000 correct (81.12)\n",
      "skip model saving\n",
      "[252,   200] loss: 0.000060\n",
      "[252,   400] loss: 0.000018\n",
      "Got 3251 / 4000 correct (81.27)\n",
      "skip model saving\n",
      "[253,   200] loss: 0.000159\n",
      "[253,   400] loss: 0.000008\n",
      "Got 3250 / 4000 correct (81.25)\n",
      "skip model saving\n",
      "[254,   200] loss: 0.000114\n",
      "[254,   400] loss: 0.000074\n",
      "Got 3250 / 4000 correct (81.25)\n",
      "skip model saving\n",
      "[255,   200] loss: 0.000269\n",
      "[255,   400] loss: 0.000046\n",
      "Got 3249 / 4000 correct (81.23)\n",
      "skip model saving\n",
      "[256,   200] loss: 0.000006\n",
      "[256,   400] loss: 0.000011\n",
      "Got 3258 / 4000 correct (81.45)\n",
      "skip model saving\n",
      "[257,   200] loss: 0.000006\n",
      "[257,   400] loss: 0.000099\n",
      "Got 3243 / 4000 correct (81.08)\n",
      "skip model saving\n",
      "[258,   200] loss: 0.000008\n",
      "[258,   400] loss: 0.000022\n",
      "Got 3253 / 4000 correct (81.33)\n",
      "skip model saving\n",
      "[259,   200] loss: 0.000006\n",
      "[259,   400] loss: 0.000058\n",
      "Got 3259 / 4000 correct (81.47)\n",
      "skip model saving\n",
      "[260,   200] loss: 0.000010\n",
      "[260,   400] loss: 0.000010\n",
      "Got 3255 / 4000 correct (81.38)\n",
      "skip model saving\n",
      "[261,   200] loss: 0.000217\n",
      "[261,   400] loss: 0.000040\n",
      "Got 3256 / 4000 correct (81.40)\n",
      "skip model saving\n",
      "[262,   200] loss: 0.000016\n",
      "[262,   400] loss: 0.000016\n",
      "Got 3258 / 4000 correct (81.45)\n",
      "skip model saving\n",
      "[263,   200] loss: 0.000021\n",
      "[263,   400] loss: 0.000009\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[264,   200] loss: 0.000153\n",
      "[264,   400] loss: 0.000015\n",
      "Got 3261 / 4000 correct (81.53)\n",
      "skip model saving\n",
      "[265,   200] loss: 0.000006\n",
      "[265,   400] loss: 0.000006\n",
      "Got 3259 / 4000 correct (81.47)\n",
      "skip model saving\n",
      "[266,   200] loss: 0.000012\n",
      "[266,   400] loss: 0.000020\n",
      "Got 3252 / 4000 correct (81.30)\n",
      "skip model saving\n",
      "[267,   200] loss: 0.000138\n",
      "[267,   400] loss: 0.000005\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[268,   200] loss: 0.000082\n",
      "[268,   400] loss: 0.000033\n",
      "Got 3273 / 4000 correct (81.83)\n",
      "skip model saving\n",
      "[269,   200] loss: 0.000007\n",
      "[269,   400] loss: 0.000251\n",
      "Got 3259 / 4000 correct (81.47)\n",
      "skip model saving\n",
      "[270,   200] loss: 0.000005\n",
      "[270,   400] loss: 0.000008\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[271,   200] loss: 0.000008\n",
      "[271,   400] loss: 0.000007\n",
      "Got 3261 / 4000 correct (81.53)\n",
      "skip model saving\n",
      "[272,   200] loss: 0.000005\n",
      "[272,   400] loss: 0.000009\n",
      "Got 3258 / 4000 correct (81.45)\n",
      "skip model saving\n",
      "[273,   200] loss: 0.000009\n",
      "[273,   400] loss: 0.000005\n",
      "Got 3259 / 4000 correct (81.47)\n",
      "skip model saving\n",
      "[274,   200] loss: 0.000008\n",
      "[274,   400] loss: 0.000008\n",
      "Got 3258 / 4000 correct (81.45)\n",
      "skip model saving\n",
      "[275,   200] loss: 0.000005\n",
      "[275,   400] loss: 0.000007\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[276,   200] loss: 0.000003\n",
      "[276,   400] loss: 0.000005\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[277,   200] loss: 0.000005\n",
      "[277,   400] loss: 0.000007\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[278,   200] loss: 0.000005\n",
      "[278,   400] loss: 0.000013\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[279,   200] loss: 0.000007\n",
      "[279,   400] loss: 0.000006\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[280,   200] loss: 0.000003\n",
      "[280,   400] loss: 0.000005\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[281,   200] loss: 0.000011\n",
      "[281,   400] loss: 0.000012\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[282,   200] loss: 0.000006\n",
      "[282,   400] loss: 0.000004\n",
      "Got 3271 / 4000 correct (81.77)\n",
      "skip model saving\n",
      "[283,   200] loss: 0.000003\n",
      "[283,   400] loss: 0.000004\n",
      "Got 3274 / 4000 correct (81.85)\n",
      "skip model saving\n",
      "[284,   200] loss: 0.000003\n",
      "[284,   400] loss: 0.000015\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[285,   200] loss: 0.000004\n",
      "[285,   400] loss: 0.000006\n",
      "Got 3258 / 4000 correct (81.45)\n",
      "skip model saving\n",
      "[286,   200] loss: 0.000002\n",
      "[286,   400] loss: 0.000003\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[287,   200] loss: 0.000007\n",
      "[287,   400] loss: 0.000003\n",
      "Got 3266 / 4000 correct (81.65)\n",
      "skip model saving\n",
      "[288,   200] loss: 0.000006\n",
      "[288,   400] loss: 0.000002\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[289,   200] loss: 0.000001\n",
      "[289,   400] loss: 0.000006\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[290,   200] loss: 0.000013\n",
      "[290,   400] loss: 0.000005\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[291,   200] loss: 0.000287\n",
      "[291,   400] loss: 0.000005\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[292,   200] loss: 0.000006\n",
      "[292,   400] loss: 0.000002\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[293,   200] loss: 0.000002\n",
      "[293,   400] loss: 0.000007\n",
      "Got 3255 / 4000 correct (81.38)\n",
      "skip model saving\n",
      "[294,   200] loss: 0.000013\n",
      "[294,   400] loss: 0.000003\n",
      "Got 3257 / 4000 correct (81.42)\n",
      "skip model saving\n",
      "[295,   200] loss: 0.000038\n",
      "[295,   400] loss: 0.000003\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[296,   200] loss: 0.000007\n",
      "[296,   400] loss: 0.000004\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[297,   200] loss: 0.000007\n",
      "[297,   400] loss: 0.000002\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[298,   200] loss: 0.000001\n",
      "[298,   400] loss: 0.000004\n",
      "Got 3266 / 4000 correct (81.65)\n",
      "skip model saving\n",
      "[299,   200] loss: 0.000007\n",
      "[299,   400] loss: 0.000106\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[300,   200] loss: 0.000005\n",
      "[300,   400] loss: 0.000026\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[301,   200] loss: 0.000003\n",
      "[301,   400] loss: 0.000004\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[302,   200] loss: 0.000039\n",
      "[302,   400] loss: 0.000002\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[303,   200] loss: 0.000005\n",
      "[303,   400] loss: 0.000002\n",
      "Got 3256 / 4000 correct (81.40)\n",
      "skip model saving\n",
      "[304,   200] loss: 0.000002\n",
      "[304,   400] loss: 0.000003\n",
      "Got 3261 / 4000 correct (81.53)\n",
      "skip model saving\n",
      "[305,   200] loss: 0.000025\n",
      "[305,   400] loss: 0.000008\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[306,   200] loss: 0.000003\n",
      "[306,   400] loss: 0.000003\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[307,   200] loss: 0.000002\n",
      "[307,   400] loss: 0.000003\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[308,   200] loss: 0.000003\n",
      "[308,   400] loss: 0.000002\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[309,   200] loss: 0.000003\n",
      "[309,   400] loss: 0.000002\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[310,   200] loss: 0.000003\n",
      "[310,   400] loss: 0.000017\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[311,   200] loss: 0.000010\n",
      "[311,   400] loss: 0.000002\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[312,   200] loss: 0.000008\n",
      "[312,   400] loss: 0.000001\n",
      "Got 3256 / 4000 correct (81.40)\n",
      "skip model saving\n",
      "[313,   200] loss: 0.000008\n",
      "[313,   400] loss: 0.000002\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[314,   200] loss: 0.000002\n",
      "[314,   400] loss: 0.000006\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[315,   200] loss: 0.000002\n",
      "[315,   400] loss: 0.000003\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[316,   200] loss: 0.000003\n",
      "[316,   400] loss: 0.000003\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[317,   200] loss: 0.000003\n",
      "[317,   400] loss: 0.000001\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[318,   200] loss: 0.000008\n",
      "[318,   400] loss: 0.000002\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[319,   200] loss: 0.000001\n",
      "[319,   400] loss: 0.000040\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[320,   200] loss: 0.000003\n",
      "[320,   400] loss: 0.000001\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[321,   200] loss: 0.000002\n",
      "[321,   400] loss: 0.000002\n",
      "Got 3271 / 4000 correct (81.77)\n",
      "skip model saving\n",
      "[322,   200] loss: 0.000005\n",
      "[322,   400] loss: 0.000010\n",
      "Got 3266 / 4000 correct (81.65)\n",
      "skip model saving\n",
      "[323,   200] loss: 0.000002\n",
      "[323,   400] loss: 0.000093\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[324,   200] loss: 0.000001\n",
      "[324,   400] loss: 0.000001\n",
      "Got 3272 / 4000 correct (81.80)\n",
      "skip model saving\n",
      "[325,   200] loss: 0.000001\n",
      "[325,   400] loss: 0.000002\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[326,   200] loss: 0.000002\n",
      "[326,   400] loss: 0.000002\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[327,   200] loss: 0.000002\n",
      "[327,   400] loss: 0.000005\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[328,   200] loss: 0.000002\n",
      "[328,   400] loss: 0.000002\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[329,   200] loss: 0.000003\n",
      "[329,   400] loss: 0.000001\n",
      "Got 3275 / 4000 correct (81.88)\n",
      "skip model saving\n",
      "[330,   200] loss: 0.000001\n",
      "[330,   400] loss: 0.000004\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[331,   200] loss: 0.000005\n",
      "[331,   400] loss: 0.000006\n",
      "Got 3261 / 4000 correct (81.53)\n",
      "skip model saving\n",
      "[332,   200] loss: 0.000002\n",
      "[332,   400] loss: 0.000002\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[333,   200] loss: 0.000009\n",
      "[333,   400] loss: 0.000002\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[334,   200] loss: 0.000001\n",
      "[334,   400] loss: 0.000001\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[335,   200] loss: 0.000002\n",
      "[335,   400] loss: 0.000002\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[336,   200] loss: 0.000001\n",
      "[336,   400] loss: 0.000001\n",
      "Got 3269 / 4000 correct (81.73)\n",
      "skip model saving\n",
      "[337,   200] loss: 0.000002\n",
      "[337,   400] loss: 0.000001\n",
      "Got 3267 / 4000 correct (81.67)\n",
      "skip model saving\n",
      "[338,   200] loss: 0.000003\n",
      "[338,   400] loss: 0.000002\n",
      "Got 3264 / 4000 correct (81.60)\n",
      "skip model saving\n",
      "[339,   200] loss: 0.000002\n",
      "[339,   400] loss: 0.000001\n",
      "Got 3265 / 4000 correct (81.62)\n",
      "skip model saving\n",
      "[340,   200] loss: 0.000001\n",
      "[340,   400] loss: 0.000002\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[341,   200] loss: 0.000005\n",
      "[341,   400] loss: 0.000002\n",
      "Got 3262 / 4000 correct (81.55)\n",
      "skip model saving\n",
      "[342,   200] loss: 0.000002\n",
      "[342,   400] loss: 0.000001\n",
      "Got 3266 / 4000 correct (81.65)\n",
      "skip model saving\n",
      "[343,   200] loss: 0.000001\n",
      "[343,   400] loss: 0.000001\n",
      "Got 3271 / 4000 correct (81.77)\n",
      "skip model saving\n",
      "[344,   200] loss: 0.000026\n",
      "[344,   400] loss: 0.000001\n",
      "Got 3260 / 4000 correct (81.50)\n",
      "skip model saving\n",
      "[345,   200] loss: 0.000002\n",
      "[345,   400] loss: 0.000001\n",
      "Got 3270 / 4000 correct (81.75)\n",
      "skip model saving\n",
      "[346,   200] loss: 0.000002\n",
      "[346,   400] loss: 0.000002\n",
      "Got 3268 / 4000 correct (81.70)\n",
      "skip model saving\n",
      "[347,   200] loss: 0.000001\n",
      "[347,   400] loss: 0.000003\n",
      "Got 3261 / 4000 correct (81.53)\n",
      "skip model saving\n",
      "[348,   200] loss: 0.000015\n",
      "[348,   400] loss: 0.000001\n",
      "Got 3261 / 4000 correct (81.53)\n",
      "skip model saving\n",
      "[349,   200] loss: 0.000001\n",
      "[349,   400] loss: 0.000002\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n",
      "[350,   200] loss: 0.000003\n",
      "[350,   400] loss: 0.000002\n",
      "Got 3263 / 4000 correct (81.58)\n",
      "skip model saving\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# define and train the network\n",
    "vanilla_model_path = './cifar32_model.pth'\n",
    "vanilla_model = ResNet50()\n",
    "lr=0.1\n",
    "vanilla_optimizer = optim.Adam(vanilla_model.parameters(), lr=lr)\n",
    "vanilla_lr_scheduler = optim.lr_scheduler.MultiStepLR(vanilla_optimizer, milestones=[150, 250])\n",
    "train_part(vanilla_model, data_loader_train, data_loader_val, vanilla_model_path, vanilla_optimizer, vanilla_lr_scheduler, epochs = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vanilla ResNet50 Model on Normal CIFAR10 Dataset\n",
    "\n",
    "The below code tests the vanilla ResNet50 model on the normal CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8022 / 10000 correct, accuracy of the dataset is: 80.220 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "vanilla_model = ResNet50()\n",
    "vanilla_model.load_state_dict(torch.load('./cifar32_model.pth'))\n",
    "vanilla_model.to(device)\n",
    "check_accuracy(data_loader_test, vanilla_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vanilla ResNet50 Model on Stylised CIFAR10 Dataset\n",
    "\n",
    "The below code tests the vanilla ResNet50 model on the stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1154 / 10000 correct, accuracy of the dataset is: 11.540 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "vanilla_model = ResNet50()\n",
    "vanilla_model.load_state_dict(torch.load('./cifar32_model.pth'))\n",
    "vanilla_model.to(device)\n",
    "check_accuracy(data_loader_test_style, vanilla_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vanilla ResNet50 Model on Reduced Stylised CIFAR10 Dataset\n",
    "\n",
    "The below code tests the vanilla ResNet50 model on the reduced stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 947 / 10000 correct, accuracy of the dataset is: 9.470 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "vanilla_model = ResNet50()\n",
    "vanilla_model.load_state_dict(torch.load('./cifar32_model.pth'))\n",
    "vanilla_model.to(device)\n",
    "check_accuracy(data_loader_test_style_red, vanilla_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
