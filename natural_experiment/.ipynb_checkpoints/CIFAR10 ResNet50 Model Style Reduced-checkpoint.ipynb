{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Contextural Bias of ResNet50 on CIFAR10 Dataset - Reduced Stylised ResNet50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook trains and tests a reduced stylised ResNet50 model with the CIFAR10 dataset. It includes functions for loading the dataset, turning them into tensors, model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv2d, AvgPool2d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following cell provides a class that loads the CIFAR dataset given the relevant path, processes it into a dictionary format of class labels and content then processes the images into tensors. The class also has helper functions to extract information about the dataset needed for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        super(CifarDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        \n",
    "        classes_list = []\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            classes_list.append(class_name)\n",
    "        classes_list.sort()\n",
    "        self.classes = [dict(class_idx = k, class_name = v) for k, v in enumerate(classes_list)]\n",
    "        \n",
    "\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "        tr = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        img = tr(img)\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_datasets(data_path):\n",
    "    dataset = CifarDataset(data_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used for this experiment are normal CIFAR10 dataset, stylised CIFAR10 dataset and stylised CIFAR10 dataset created by using reduced style images where stylisation was done by AdaIN style transfer.\n",
    "\n",
    "The following cells call the function created above to load the training, validation and testing datasets of the normal, stylised and reduced stylised CIFAR10 datasets and transform them into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 36000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load normal CIFAR10\n",
    "data_path_train = \"../../CIFAR/cifar32/training\"\n",
    "dataset_train = get_cifar_datasets(data_path_train)\n",
    "\n",
    "data_path_val = \"../../CIFAR/cifar32/validation/\"\n",
    "dataset_val = get_cifar_datasets(data_path_val)\n",
    "\n",
    "data_path_test = \"../../CIFAR/cifar32/testing/\"\n",
    "dataset_test = get_cifar_datasets(data_path_test)\n",
    "\n",
    "print(f\"Number of train samples {dataset_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of val samples {dataset_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of test samples {dataset_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val = DataLoader(dataset_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with original kaggle images\n",
    "data_path_train_style = \"../../CIFAR/cifar32_style/training\"\n",
    "dataset_train_style = get_cifar_datasets(data_path_train_style)\n",
    "\n",
    "data_path_val_style = \"../../CIFAR/cifar32_style/validation/\"\n",
    "dataset_val_style = get_cifar_datasets(data_path_val_style)\n",
    "\n",
    "data_path_test_style = \"../../CIFAR/cifar32_style/testing/\"\n",
    "dataset_test_style = get_cifar_datasets(data_path_test_style)\n",
    "\n",
    "print(f\"Number of stylised train samples {dataset_train_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised val samples {dataset_val_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised test samples {dataset_test_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style = DataLoader(dataset_train_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style = DataLoader(dataset_val_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style = DataLoader(dataset_test_style, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with reduced kaggle images\n",
    "data_path_train_style_red = \"../../CIFAR/cifar32_style_red/training\"\n",
    "dataset_train_style_red = get_cifar_datasets(data_path_train_style_red)\n",
    "\n",
    "data_path_val_style_red = \"../../CIFAR/cifar32_style_red/validation/\"\n",
    "dataset_val_style_red = get_cifar_datasets(data_path_val_style_red)\n",
    "\n",
    "data_path_test_style_red = \"../../CIFAR/cifar32_style_red/testing/\"\n",
    "dataset_test_style_red = get_cifar_datasets(data_path_test_style_red)\n",
    "\n",
    "print(f\"Number of reduced stylised train samples {dataset_train_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised val samples {dataset_val_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised test samples {dataset_test_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style_red.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style_red = DataLoader(dataset_train_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style_red = DataLoader(dataset_val_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style_red = DataLoader(dataset_test_style_red, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch['image'] torch.Size([64, 3, 32, 32])\n",
      "Shape of batch['cls'] torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the data was loaded correctly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(8, 8))\n",
    "\n",
    "for batch in data_loader_test_style:\n",
    "\n",
    "    print(f\"Shape of batch['image'] {batch['image'].shape}\")\n",
    "    print(f\"Shape of batch['cls'] {batch['cls'].shape}\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        col = i % 4\n",
    "        row = i // 4\n",
    "\n",
    "        img = batch['image'][i].numpy()\n",
    "\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(batch['class_name'][i])\n",
    "        axes[row,col].imshow(np.transpose(img,(1,2,0)))\n",
    "                         \n",
    "        if i >= 15:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining ResNet50\n",
    "\n",
    "The following code defines Resnet50 architecture that will be used to train and test the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resnet building blocks\n",
    "class ResidualBlock(nn.Module): \n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inchannel, outchannel, stride=1): \n",
    "        \n",
    "        super(ResidualBlock, self).__init__() \n",
    "        \n",
    "        self.left = nn.Sequential(\n",
    "            Conv2d(inchannel, outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv2d(outchannel, self.expansion*outchannel, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*outchannel)\n",
    "        ) \n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or inchannel != self.expansion*outchannel: \n",
    "            self.shortcut = nn.Sequential(\n",
    "                Conv2d(inchannel, self.expansion*outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*outchannel)\n",
    "            ) \n",
    "            \n",
    "    def forward(self, x): \n",
    "        out = self.left(x) \n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out) \n",
    "        return out\n",
    "\n",
    "    \n",
    "# define resnet\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ResidualBlock, num_classes = 10):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            Conv2d(3, 64, kernel_size = 3, stride = 1,padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 3, stride = 1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 4, stride = 2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 6, stride = 2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 3, stride = 2)\n",
    "        self.avgpool = AvgPool2d(4)\n",
    "        self.fc = nn.Linear(512*ResidualBlock.expansion, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet50():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the training, validation and testing functions for the experiment. train_part() function trains and updates gradients on each batch of the training set and once that is done, it tests its accuracy on the validation set. Every time the validation test returns a better accuracy than the current maximum, the model is saved and carries on with the next epoch. Then it checks with the learning reate scheduler for any changes in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "    \n",
    "print_every = 100\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "            scores = model(inputs)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct, accuracy of the dataset is: %.3f %%' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_part(model, train_data, val_data, model_path, optimizer, lr_scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "    val_acc = 0\n",
    "    num_epoch = 2\n",
    "    # Main Loop\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        val_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for i, batch in enumerate(train_data, 0):\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "\n",
    "            # get outputs from the input data and calculate the cross entropy loss\n",
    "            scores = model(inputs)\n",
    "            loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "            # zero and update the gradients and optimise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i, batch in enumerate(val_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['cls'].to(device)\n",
    "\n",
    "                # get the outputs from the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # compute accuracy based on the outputs\n",
    "                _, preds = outputs.max(1)\n",
    "                num_correct += (preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "            if acc > val_acc:\n",
    "                print('saving model')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                val_acc = acc\n",
    "            else:\n",
    "                print('skip model saving')\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Stylised ResNet50 Training\n",
    "\n",
    "The model used in this experiment is ResNet50 trained for 350 epochs with Adam optimiser, learning rate scheduler and default settings. The dataset used to train the model is reduced stylised CIFAR10. Learning rate starts at 0.1 and changes every 150, 250th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 3.829911\n",
      "[1,   400] loss: 2.311669\n",
      "[1,   600] loss: 2.310162\n",
      "[1,   800] loss: 2.315243\n",
      "[1,  1000] loss: 2.313610\n",
      "[1,  1200] loss: 2.311830\n",
      "[1,  1400] loss: 2.317964\n",
      "[1,  1600] loss: 2.313508\n",
      "[1,  1800] loss: 2.312932\n",
      "[1,  2000] loss: 2.311918\n",
      "[1,  2200] loss: 2.311965\n",
      "[1,  2400] loss: 2.312951\n",
      "[1,  2600] loss: 2.319043\n",
      "[1,  2800] loss: 2.310715\n",
      "[1,  3000] loss: 2.310016\n",
      "[1,  3200] loss: 2.310646\n",
      "Got 387 / 4000 correct (9.68)\n",
      "saving model\n",
      "[2,   200] loss: 2.311524\n",
      "[2,   400] loss: 2.314173\n",
      "[2,   600] loss: 2.310895\n",
      "[2,   800] loss: 2.312667\n",
      "[2,  1000] loss: 2.310868\n",
      "[2,  1200] loss: 2.309332\n",
      "[2,  1400] loss: 2.312541\n",
      "[2,  1600] loss: 2.312326\n",
      "[2,  1800] loss: 2.311097\n",
      "[2,  2000] loss: 2.311375\n",
      "[2,  2200] loss: 2.310381\n",
      "[2,  2400] loss: 2.310580\n",
      "[2,  2600] loss: 2.311341\n",
      "[2,  2800] loss: 2.313052\n",
      "[2,  3000] loss: 2.310472\n",
      "[2,  3200] loss: 2.311201\n",
      "Got 385 / 4000 correct (9.62)\n",
      "skip model saving\n",
      "[3,   200] loss: 2.313041\n",
      "[3,   400] loss: 2.312309\n",
      "[3,   600] loss: 2.310494\n",
      "[3,   800] loss: 2.312819\n",
      "[3,  1000] loss: 2.312814\n",
      "[3,  1200] loss: 2.312123\n",
      "[3,  1400] loss: 2.310045\n",
      "[3,  1600] loss: 2.310983\n",
      "[3,  1800] loss: 2.311048\n",
      "[3,  2000] loss: 2.309635\n",
      "[3,  2200] loss: 2.311670\n",
      "[3,  2400] loss: 2.311003\n",
      "[3,  2600] loss: 2.311748\n",
      "[3,  2800] loss: 2.311843\n",
      "[3,  3000] loss: 4.611233\n",
      "[3,  3200] loss: 2.313216\n",
      "Got 401 / 4000 correct (10.03)\n",
      "saving model\n",
      "[4,   200] loss: 2.311139\n",
      "[4,   400] loss: 2.308758\n",
      "[4,   600] loss: 2.310453\n",
      "[4,   800] loss: 2.310714\n",
      "[4,  1000] loss: 2.311045\n",
      "[4,  1200] loss: 2.310296\n",
      "[4,  1400] loss: 2.310061\n",
      "[4,  1600] loss: 2.311843\n",
      "[4,  1800] loss: 2.310904\n",
      "[4,  2000] loss: 2.310451\n",
      "[4,  2200] loss: 2.311422\n",
      "[4,  2400] loss: 2.311108\n",
      "[4,  2600] loss: 2.309809\n",
      "[4,  2800] loss: 2.311153\n",
      "[4,  3000] loss: 2.312942\n",
      "[4,  3200] loss: 2.311254\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[5,   200] loss: 2.311067\n",
      "[5,   400] loss: 2.310800\n",
      "[5,   600] loss: 2.313158\n",
      "[5,   800] loss: 2.310500\n",
      "[5,  1000] loss: 2.311347\n",
      "[5,  1200] loss: 2.311573\n",
      "[5,  1400] loss: 2.311549\n",
      "[5,  1600] loss: 2.312117\n",
      "[5,  1800] loss: 2.311101\n",
      "[5,  2000] loss: 2.312021\n",
      "[5,  2200] loss: 2.310955\n",
      "[5,  2400] loss: 2.311161\n",
      "[5,  2600] loss: 2.312794\n",
      "[5,  2800] loss: 2.311231\n",
      "[5,  3000] loss: 2.310442\n",
      "[5,  3200] loss: 2.310113\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[6,   200] loss: 2.311270\n",
      "[6,   400] loss: 2.310576\n",
      "[6,   600] loss: 2.347597\n",
      "[6,   800] loss: 2.326986\n",
      "[6,  1000] loss: 2.312821\n",
      "[6,  1200] loss: 2.311459\n",
      "[6,  1400] loss: 2.312266\n",
      "[6,  1600] loss: 2.310600\n",
      "[6,  1800] loss: 2.311187\n",
      "[6,  2000] loss: 2.310745\n",
      "[6,  2200] loss: 2.311463\n",
      "[6,  2400] loss: 2.313130\n",
      "[6,  2600] loss: 2.312541\n",
      "[6,  2800] loss: 2.310943\n",
      "[6,  3000] loss: 2.310811\n",
      "[6,  3200] loss: 2.312093\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[7,   200] loss: 2.311062\n",
      "[7,   400] loss: 2.311367\n",
      "[7,   600] loss: 2.311297\n",
      "[7,   800] loss: 2.310039\n",
      "[7,  1000] loss: 2.311318\n",
      "[7,  1200] loss: 2.310918\n",
      "[7,  1400] loss: 2.312529\n",
      "[7,  1600] loss: 2.313004\n",
      "[7,  1800] loss: 2.310817\n",
      "[7,  2000] loss: 2.312430\n",
      "[7,  2200] loss: 2.313061\n",
      "[7,  2400] loss: 2.313759\n",
      "[7,  2600] loss: 2.310692\n",
      "[7,  2800] loss: 2.310336\n",
      "[7,  3000] loss: 2.311035\n",
      "[7,  3200] loss: 2.311963\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[8,   200] loss: 2.312475\n",
      "[8,   400] loss: 2.311649\n",
      "[8,   600] loss: 2.313415\n",
      "[8,   800] loss: 2.313920\n",
      "[8,  1000] loss: 2.311760\n",
      "[8,  1200] loss: 2.312442\n",
      "[8,  1400] loss: 2.313133\n",
      "[8,  1600] loss: 2.311124\n",
      "[8,  1800] loss: 2.309599\n",
      "[8,  2000] loss: 2.310941\n",
      "[8,  2200] loss: 2.311762\n",
      "[8,  2400] loss: 2.311004\n",
      "[8,  2600] loss: 2.310733\n",
      "[8,  2800] loss: 2.312523\n",
      "[8,  3000] loss: 2.310871\n",
      "[8,  3200] loss: 2.310456\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[9,   200] loss: 2.311740\n",
      "[9,   400] loss: 2.313428\n",
      "[9,   600] loss: 2.310464\n",
      "[9,   800] loss: 2.312866\n",
      "[9,  1000] loss: 2.312066\n",
      "[9,  1200] loss: 2.312827\n",
      "[9,  1400] loss: 2.310239\n",
      "[9,  1600] loss: 2.311198\n",
      "[9,  1800] loss: 2.311280\n",
      "[9,  2000] loss: 2.308750\n",
      "[9,  2200] loss: 2.313963\n",
      "[9,  2400] loss: 2.311872\n",
      "[9,  2600] loss: 2.311460\n",
      "[9,  2800] loss: 2.310830\n",
      "[9,  3000] loss: 2.311502\n",
      "[9,  3200] loss: 2.311816\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[10,   200] loss: 2.311365\n",
      "[10,   400] loss: 2.310406\n",
      "[10,   600] loss: 2.311046\n",
      "[10,   800] loss: 2.311223\n",
      "[10,  1000] loss: 2.311196\n",
      "[10,  1200] loss: 2.311476\n",
      "[10,  1400] loss: 2.311752\n",
      "[10,  1600] loss: 2.311900\n",
      "[10,  1800] loss: 2.312632\n",
      "[10,  2000] loss: 2.311913\n",
      "[10,  2200] loss: 2.312581\n",
      "[10,  2400] loss: 2.311320\n",
      "[10,  2600] loss: 2.310638\n",
      "[10,  2800] loss: 2.312145\n",
      "[10,  3000] loss: 2.310861\n",
      "[10,  3200] loss: 2.311639\n",
      "Got 415 / 4000 correct (10.38)\n",
      "saving model\n",
      "[11,   200] loss: 2.311013\n",
      "[11,   400] loss: 2.314033\n",
      "[11,   600] loss: 2.312432\n",
      "[11,   800] loss: 2.309871\n",
      "[11,  1000] loss: 2.311098\n",
      "[11,  1200] loss: 2.312708\n",
      "[11,  1400] loss: 2.313234\n",
      "[11,  1600] loss: 2.312604\n",
      "[11,  1800] loss: 2.312116\n",
      "[11,  2000] loss: 2.311783\n",
      "[11,  2200] loss: 2.311841\n",
      "[11,  2400] loss: 2.311832\n",
      "[11,  2600] loss: 2.309584\n",
      "[11,  2800] loss: 2.310781\n",
      "[11,  3000] loss: 2.311500\n",
      "[11,  3200] loss: 2.308230\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[12,   200] loss: 2.311413\n",
      "[12,   400] loss: 2.311397\n",
      "[12,   600] loss: 2.314092\n",
      "[12,   800] loss: 2.311991\n",
      "[12,  1000] loss: 2.311789\n",
      "[12,  1200] loss: 2.311322\n",
      "[12,  1400] loss: 2.311502\n",
      "[12,  1600] loss: 2.311760\n",
      "[12,  1800] loss: 2.310887\n",
      "[12,  2000] loss: 2.311939\n",
      "[12,  2200] loss: 2.309546\n",
      "[12,  2400] loss: 2.311906\n",
      "[12,  2600] loss: 2.310909\n",
      "[12,  2800] loss: 2.312048\n",
      "[12,  3000] loss: 2.309460\n",
      "[12,  3200] loss: 2.312280\n",
      "Got 422 / 4000 correct (10.55)\n",
      "saving model\n",
      "[13,   200] loss: 2.309710\n",
      "[13,   400] loss: 2.312310\n",
      "[13,   600] loss: 2.313030\n",
      "[13,   800] loss: 2.310772\n",
      "[13,  1000] loss: 2.311631\n",
      "[13,  1200] loss: 2.309439\n",
      "[13,  1400] loss: 2.311488\n",
      "[13,  1600] loss: 2.311324\n",
      "[13,  1800] loss: 2.312127\n",
      "[13,  2000] loss: 2.312142\n",
      "[13,  2200] loss: 2.313353\n",
      "[13,  2400] loss: 2.310954\n",
      "[13,  2600] loss: 2.311343\n",
      "[13,  2800] loss: 2.311075\n",
      "[13,  3000] loss: 2.311379\n",
      "[13,  3200] loss: 2.312523\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[14,   200] loss: 2.311953\n",
      "[14,   400] loss: 2.310500\n",
      "[14,   600] loss: 2.311611\n",
      "[14,   800] loss: 2.312457\n",
      "[14,  1000] loss: 2.310925\n",
      "[14,  1200] loss: 2.310947\n",
      "[14,  1400] loss: 2.310860\n",
      "[14,  1600] loss: 2.310439\n",
      "[14,  1800] loss: 2.313825\n",
      "[14,  2000] loss: 2.312883\n",
      "[14,  2200] loss: 2.312534\n",
      "[14,  2400] loss: 2.312192\n",
      "[14,  2600] loss: 2.312480\n",
      "[14,  2800] loss: 2.309811\n",
      "[14,  3000] loss: 2.312315\n",
      "[14,  3200] loss: 2.311285\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[15,   200] loss: 2.310563\n",
      "[15,   400] loss: 2.314532\n",
      "[15,   600] loss: 2.311290\n",
      "[15,   800] loss: 2.311275\n",
      "[15,  1000] loss: 2.311684\n",
      "[15,  1200] loss: 2.311472\n",
      "[15,  1400] loss: 2.310821\n",
      "[15,  1600] loss: 2.310480\n",
      "[15,  1800] loss: 2.312489\n",
      "[15,  2000] loss: 2.311746\n",
      "[15,  2200] loss: 2.311184\n",
      "[15,  2400] loss: 2.312656\n",
      "[15,  2600] loss: 2.310166\n",
      "[15,  2800] loss: 2.313009\n",
      "[15,  3000] loss: 2.312866\n",
      "[15,  3200] loss: 2.310824\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[16,   200] loss: 2.311797\n",
      "[16,   400] loss: 2.312656\n",
      "[16,   600] loss: 2.311975\n",
      "[16,   800] loss: 2.311552\n",
      "[16,  1000] loss: 2.312566\n",
      "[16,  1200] loss: 2.310380\n",
      "[16,  1400] loss: 2.311893\n",
      "[16,  1600] loss: 2.311209\n",
      "[16,  1800] loss: 2.310535\n",
      "[16,  2000] loss: 2.312080\n",
      "[16,  2200] loss: 2.310538\n",
      "[16,  2400] loss: 2.310038\n",
      "[16,  2600] loss: 2.311959\n",
      "[16,  2800] loss: 2.310563\n",
      "[16,  3000] loss: 2.311399\n",
      "[16,  3200] loss: 2.312347\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[17,   200] loss: 2.312502\n",
      "[17,   400] loss: 2.311182\n",
      "[17,   600] loss: 2.313824\n",
      "[17,   800] loss: 2.312130\n",
      "[17,  1000] loss: 2.313235\n",
      "[17,  1200] loss: 2.310571\n",
      "[17,  1400] loss: 2.312624\n",
      "[17,  1600] loss: 2.310156\n",
      "[17,  1800] loss: 2.311752\n",
      "[17,  2000] loss: 2.312140\n",
      "[17,  2200] loss: 2.312196\n",
      "[17,  2400] loss: 2.312204\n",
      "[17,  2600] loss: 2.310704\n",
      "[17,  2800] loss: 2.311704\n",
      "[17,  3000] loss: 2.310729\n",
      "[17,  3200] loss: 2.311546\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[18,   200] loss: 2.312515\n",
      "[18,   400] loss: 2.312214\n",
      "[18,   600] loss: 2.312305\n",
      "[18,   800] loss: 2.311270\n",
      "[18,  1000] loss: 2.312126\n",
      "[18,  1200] loss: 2.310952\n",
      "[18,  1400] loss: 2.313531\n",
      "[18,  1600] loss: 2.312307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,  1800] loss: 2.309686\n",
      "[18,  2000] loss: 2.312715\n",
      "[18,  2200] loss: 2.311938\n",
      "[18,  2400] loss: 2.311115\n",
      "[18,  2600] loss: 2.310907\n",
      "[18,  2800] loss: 2.311505\n",
      "[18,  3000] loss: 2.311333\n",
      "[18,  3200] loss: 2.313432\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[19,   200] loss: 2.310957\n",
      "[19,   400] loss: 2.310971\n",
      "[19,   600] loss: 2.310708\n",
      "[19,   800] loss: 2.311349\n",
      "[19,  1000] loss: 2.310089\n",
      "[19,  1200] loss: 2.311457\n",
      "[19,  1400] loss: 2.312177\n",
      "[19,  1600] loss: 2.312166\n",
      "[19,  1800] loss: 2.310523\n",
      "[19,  2000] loss: 2.310375\n",
      "[19,  2200] loss: 2.309100\n",
      "[19,  2400] loss: 2.310020\n",
      "[19,  2600] loss: 2.311251\n",
      "[19,  2800] loss: 2.309864\n",
      "[19,  3000] loss: 2.311944\n",
      "[19,  3200] loss: 2.310389\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[20,   200] loss: 2.311150\n",
      "[20,   400] loss: 2.310807\n",
      "[20,   600] loss: 2.311103\n",
      "[20,   800] loss: 2.310628\n",
      "[20,  1000] loss: 2.311586\n",
      "[20,  1200] loss: 2.313512\n",
      "[20,  1400] loss: 2.310883\n",
      "[20,  1600] loss: 2.310651\n",
      "[20,  1800] loss: 2.310783\n",
      "[20,  2000] loss: 2.312016\n",
      "[20,  2200] loss: 2.310983\n",
      "[20,  2400] loss: 2.310772\n",
      "[20,  2600] loss: 2.311071\n",
      "[20,  2800] loss: 2.309466\n",
      "[20,  3000] loss: 2.310584\n",
      "[20,  3200] loss: 2.309817\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[21,   200] loss: 2.311864\n",
      "[21,   400] loss: 2.312732\n",
      "[21,   600] loss: 2.311200\n",
      "[21,   800] loss: 2.312610\n",
      "[21,  1000] loss: 2.309224\n",
      "[21,  1200] loss: 2.313138\n",
      "[21,  1400] loss: 2.309284\n",
      "[21,  1600] loss: 2.310919\n",
      "[21,  1800] loss: 2.311912\n",
      "[21,  2000] loss: 2.312211\n",
      "[21,  2200] loss: 2.309431\n",
      "[21,  2400] loss: 2.311279\n",
      "[21,  2600] loss: 2.312473\n",
      "[21,  2800] loss: 2.312911\n",
      "[21,  3000] loss: 2.311163\n",
      "[21,  3200] loss: 2.310598\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[22,   200] loss: 2.311975\n",
      "[22,   400] loss: 2.310894\n",
      "[22,   600] loss: 2.312936\n",
      "[22,   800] loss: 2.311450\n",
      "[22,  1000] loss: 2.309881\n",
      "[22,  1200] loss: 2.310651\n",
      "[22,  1400] loss: 2.311977\n",
      "[22,  1600] loss: 2.311550\n",
      "[22,  1800] loss: 2.312646\n",
      "[22,  2000] loss: 2.311585\n",
      "[22,  2200] loss: 2.311505\n",
      "[22,  2400] loss: 2.311239\n",
      "[22,  2600] loss: 2.309309\n",
      "[22,  2800] loss: 2.312295\n",
      "[22,  3000] loss: 2.312927\n",
      "[22,  3200] loss: 2.312823\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[23,   200] loss: 2.312090\n",
      "[23,   400] loss: 2.313439\n",
      "[23,   600] loss: 2.311300\n",
      "[23,   800] loss: 2.312401\n",
      "[23,  1000] loss: 2.311027\n",
      "[23,  1200] loss: 2.311994\n",
      "[23,  1400] loss: 2.311230\n",
      "[23,  1600] loss: 2.312076\n",
      "[23,  1800] loss: 2.311454\n",
      "[23,  2000] loss: 2.311823\n",
      "[23,  2200] loss: 2.312024\n",
      "[23,  2400] loss: 2.310267\n",
      "[23,  2600] loss: 2.310766\n",
      "[23,  2800] loss: 2.310514\n",
      "[23,  3000] loss: 2.309686\n",
      "[23,  3200] loss: 2.310081\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[24,   200] loss: 2.311507\n",
      "[24,   400] loss: 2.312786\n",
      "[24,   600] loss: 2.311786\n",
      "[24,   800] loss: 2.311180\n",
      "[24,  1000] loss: 2.311708\n",
      "[24,  1200] loss: 2.312156\n",
      "[24,  1400] loss: 2.310738\n",
      "[24,  1600] loss: 2.310437\n",
      "[24,  1800] loss: 2.310047\n",
      "[24,  2000] loss: 2.313382\n",
      "[24,  2200] loss: 2.310006\n",
      "[24,  2400] loss: 2.310752\n",
      "[24,  2600] loss: 2.311491\n",
      "[24,  2800] loss: 2.312447\n",
      "[24,  3000] loss: 2.313478\n",
      "[24,  3200] loss: 2.311030\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[25,   200] loss: 2.311751\n",
      "[25,   400] loss: 2.312258\n",
      "[25,   600] loss: 2.310928\n",
      "[25,   800] loss: 2.310680\n",
      "[25,  1000] loss: 2.311227\n",
      "[25,  1200] loss: 2.312398\n",
      "[25,  1400] loss: 2.311192\n",
      "[25,  1600] loss: 2.311024\n",
      "[25,  1800] loss: 2.312226\n",
      "[25,  2000] loss: 2.311878\n",
      "[25,  2200] loss: 2.311482\n",
      "[25,  2400] loss: 2.311718\n",
      "[25,  2600] loss: 2.310904\n",
      "[25,  2800] loss: 2.310578\n",
      "[25,  3000] loss: 2.310113\n",
      "[25,  3200] loss: 2.312158\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[26,   200] loss: 2.310768\n",
      "[26,   400] loss: 2.312367\n",
      "[26,   600] loss: 2.311484\n",
      "[26,   800] loss: 2.311430\n",
      "[26,  1000] loss: 2.311926\n",
      "[26,  1200] loss: 2.312803\n",
      "[26,  1400] loss: 2.314098\n",
      "[26,  1600] loss: 2.310775\n",
      "[26,  1800] loss: 2.311108\n",
      "[26,  2000] loss: 2.311536\n",
      "[26,  2200] loss: 2.312746\n",
      "[26,  2400] loss: 2.309982\n",
      "[26,  2600] loss: 2.312665\n",
      "[26,  2800] loss: 2.310136\n",
      "[26,  3000] loss: 2.311537\n",
      "[26,  3200] loss: 2.312867\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[27,   200] loss: 2.311806\n",
      "[27,   400] loss: 2.311587\n",
      "[27,   600] loss: 2.313365\n",
      "[27,   800] loss: 2.310007\n",
      "[27,  1000] loss: 2.310560\n",
      "[27,  1200] loss: 2.310868\n",
      "[27,  1400] loss: 2.310286\n",
      "[27,  1600] loss: 2.310936\n",
      "[27,  1800] loss: 2.310292\n",
      "[27,  2000] loss: 2.312597\n",
      "[27,  2200] loss: 2.311872\n",
      "[27,  2400] loss: 2.311590\n",
      "[27,  2600] loss: 2.310793\n",
      "[27,  2800] loss: 2.310834\n",
      "[27,  3000] loss: 2.310404\n",
      "[27,  3200] loss: 2.311840\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[28,   200] loss: 2.311734\n",
      "[28,   400] loss: 2.310514\n",
      "[28,   600] loss: 2.312313\n",
      "[28,   800] loss: 2.311308\n",
      "[28,  1000] loss: 2.309289\n",
      "[28,  1200] loss: 2.311460\n",
      "[28,  1400] loss: 2.312724\n",
      "[28,  1600] loss: 2.309795\n",
      "[28,  1800] loss: 2.310176\n",
      "[28,  2000] loss: 2.310154\n",
      "[28,  2200] loss: 2.310067\n",
      "[28,  2400] loss: 2.311492\n",
      "[28,  2600] loss: 2.313228\n",
      "[28,  2800] loss: 2.311094\n",
      "[28,  3000] loss: 2.311940\n",
      "[28,  3200] loss: 2.309676\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[29,   200] loss: 2.312065\n",
      "[29,   400] loss: 2.312084\n",
      "[29,   600] loss: 2.310796\n",
      "[29,   800] loss: 2.314295\n",
      "[29,  1000] loss: 2.311905\n",
      "[29,  1200] loss: 2.310504\n",
      "[29,  1400] loss: 2.311200\n",
      "[29,  1600] loss: 2.311252\n",
      "[29,  1800] loss: 2.309829\n",
      "[29,  2000] loss: 2.312818\n",
      "[29,  2200] loss: 2.310869\n",
      "[29,  2400] loss: 2.309153\n",
      "[29,  2600] loss: 2.310416\n",
      "[29,  2800] loss: 2.311176\n",
      "[29,  3000] loss: 2.312600\n",
      "[29,  3200] loss: 2.312033\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[30,   200] loss: 2.310339\n",
      "[30,   400] loss: 2.312434\n",
      "[30,   600] loss: 2.311574\n",
      "[30,   800] loss: 2.311431\n",
      "[30,  1000] loss: 2.311316\n",
      "[30,  1200] loss: 2.310955\n",
      "[30,  1400] loss: 2.311635\n",
      "[30,  1600] loss: 2.313032\n",
      "[30,  1800] loss: 2.311097\n",
      "[30,  2000] loss: 2.311230\n",
      "[30,  2200] loss: 2.310207\n",
      "[30,  2400] loss: 2.312027\n",
      "[30,  2600] loss: 2.311072\n",
      "[30,  2800] loss: 2.311617\n",
      "[30,  3000] loss: 2.309888\n",
      "[30,  3200] loss: 2.313713\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[31,   200] loss: 2.310887\n",
      "[31,   400] loss: 2.313076\n",
      "[31,   600] loss: 2.310611\n",
      "[31,   800] loss: 2.310995\n",
      "[31,  1000] loss: 2.310103\n",
      "[31,  1200] loss: 2.310803\n",
      "[31,  1400] loss: 2.312645\n",
      "[31,  1600] loss: 2.312002\n",
      "[31,  1800] loss: 2.311657\n",
      "[31,  2000] loss: 2.311498\n",
      "[31,  2200] loss: 2.311375\n",
      "[31,  2400] loss: 2.311351\n",
      "[31,  2600] loss: 2.310601\n",
      "[31,  2800] loss: 2.310136\n",
      "[31,  3000] loss: 2.312815\n",
      "[31,  3200] loss: 2.311188\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[32,   200] loss: 2.313161\n",
      "[32,   400] loss: 2.309707\n",
      "[32,   600] loss: 2.311449\n",
      "[32,   800] loss: 2.311491\n",
      "[32,  1000] loss: 2.310916\n",
      "[32,  1200] loss: 2.311088\n",
      "[32,  1400] loss: 2.310934\n",
      "[32,  1600] loss: 2.311924\n",
      "[32,  1800] loss: 2.309848\n",
      "[32,  2000] loss: 2.313140\n",
      "[32,  2200] loss: 2.310261\n",
      "[32,  2400] loss: 2.312159\n",
      "[32,  2600] loss: 2.313536\n",
      "[32,  2800] loss: 2.311624\n",
      "[32,  3000] loss: 2.310149\n",
      "[32,  3200] loss: 2.312041\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[33,   200] loss: 2.313138\n",
      "[33,   400] loss: 2.309778\n",
      "[33,   600] loss: 2.313930\n",
      "[33,   800] loss: 2.311395\n",
      "[33,  1000] loss: 2.312136\n",
      "[33,  1200] loss: 2.312735\n",
      "[33,  1400] loss: 2.312558\n",
      "[33,  1600] loss: 2.310426\n",
      "[33,  1800] loss: 2.311229\n",
      "[33,  2000] loss: 2.309968\n",
      "[33,  2200] loss: 2.312161\n",
      "[33,  2400] loss: 2.311037\n",
      "[33,  2600] loss: 2.312096\n",
      "[33,  2800] loss: 2.311295\n",
      "[33,  3000] loss: 2.310393\n",
      "[33,  3200] loss: 2.310976\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[34,   200] loss: 2.310517\n",
      "[34,   400] loss: 2.310182\n",
      "[34,   600] loss: 2.312623\n",
      "[34,   800] loss: 2.311442\n",
      "[34,  1000] loss: 2.312825\n",
      "[34,  1200] loss: 2.311584\n",
      "[34,  1400] loss: 2.310218\n",
      "[34,  1600] loss: 2.312936\n",
      "[34,  1800] loss: 2.312771\n",
      "[34,  2000] loss: 2.311709\n",
      "[34,  2200] loss: 2.312687\n",
      "[34,  2400] loss: 2.311138\n",
      "[34,  2600] loss: 2.310901\n",
      "[34,  2800] loss: 2.311253\n",
      "[34,  3000] loss: 2.313549\n",
      "[34,  3200] loss: 2.311785\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[35,   200] loss: 2.311375\n",
      "[35,   400] loss: 2.313028\n",
      "[35,   600] loss: 2.312123\n",
      "[35,   800] loss: 2.312899\n",
      "[35,  1000] loss: 2.311488\n",
      "[35,  1200] loss: 2.310833\n",
      "[35,  1400] loss: 2.312980\n",
      "[35,  1600] loss: 2.310991\n",
      "[35,  1800] loss: 2.312442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,  2000] loss: 2.311766\n",
      "[35,  2200] loss: 2.311429\n",
      "[35,  2400] loss: 2.310433\n",
      "[35,  2600] loss: 2.311816\n",
      "[35,  2800] loss: 2.310438\n",
      "[35,  3000] loss: 2.312051\n",
      "[35,  3200] loss: 2.313000\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[36,   200] loss: 2.312814\n",
      "[36,   400] loss: 2.311447\n",
      "[36,   600] loss: 2.311000\n",
      "[36,   800] loss: 2.310777\n",
      "[36,  1000] loss: 2.310720\n",
      "[36,  1200] loss: 2.311830\n",
      "[36,  1400] loss: 2.309771\n",
      "[36,  1600] loss: 2.310830\n",
      "[36,  1800] loss: 2.311262\n",
      "[36,  2000] loss: 2.312458\n",
      "[36,  2200] loss: 2.310938\n",
      "[36,  2400] loss: 2.312381\n",
      "[36,  2600] loss: 2.311269\n",
      "[36,  2800] loss: 2.310451\n",
      "[36,  3000] loss: 2.311165\n",
      "[36,  3200] loss: 2.310230\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[37,   200] loss: 2.314425\n",
      "[37,   400] loss: 2.310472\n",
      "[37,   600] loss: 2.311890\n",
      "[37,   800] loss: 2.314226\n",
      "[37,  1000] loss: 2.309463\n",
      "[37,  1200] loss: 2.312452\n",
      "[37,  1400] loss: 2.312723\n",
      "[37,  1600] loss: 2.313306\n",
      "[37,  1800] loss: 2.311129\n",
      "[37,  2000] loss: 2.311459\n",
      "[37,  2200] loss: 2.311870\n",
      "[37,  2400] loss: 2.311377\n",
      "[37,  2600] loss: 2.310091\n",
      "[37,  2800] loss: 2.310742\n",
      "[37,  3000] loss: 2.310597\n",
      "[37,  3200] loss: 2.312629\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[38,   200] loss: 2.312745\n",
      "[38,   400] loss: 2.313129\n",
      "[38,   600] loss: 2.313247\n",
      "[38,   800] loss: 2.310034\n",
      "[38,  1000] loss: 2.311260\n",
      "[38,  1200] loss: 2.311851\n",
      "[38,  1400] loss: 2.310335\n",
      "[38,  1600] loss: 2.311349\n",
      "[38,  1800] loss: 2.312557\n",
      "[38,  2000] loss: 2.311627\n",
      "[38,  2200] loss: 2.309445\n",
      "[38,  2400] loss: 2.310763\n",
      "[38,  2600] loss: 2.310739\n",
      "[38,  2800] loss: 2.313183\n",
      "[38,  3000] loss: 2.311584\n",
      "[38,  3200] loss: 2.311282\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[39,   200] loss: 2.311976\n",
      "[39,   400] loss: 2.310373\n",
      "[39,   600] loss: 2.311102\n",
      "[39,   800] loss: 2.311119\n",
      "[39,  1000] loss: 2.311664\n",
      "[39,  1200] loss: 2.310979\n",
      "[39,  1400] loss: 2.312190\n",
      "[39,  1600] loss: 2.311461\n",
      "[39,  1800] loss: 2.312460\n",
      "[39,  2000] loss: 2.311133\n",
      "[39,  2200] loss: 2.310278\n",
      "[39,  2400] loss: 2.311752\n",
      "[39,  2600] loss: 2.312855\n",
      "[39,  2800] loss: 2.311443\n",
      "[39,  3000] loss: 2.312673\n",
      "[39,  3200] loss: 2.312238\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[40,   200] loss: 2.311386\n",
      "[40,   400] loss: 2.310344\n",
      "[40,   600] loss: 2.312289\n",
      "[40,   800] loss: 2.311515\n",
      "[40,  1000] loss: 2.311332\n",
      "[40,  1200] loss: 2.312089\n",
      "[40,  1400] loss: 2.312849\n",
      "[40,  1600] loss: 2.310803\n",
      "[40,  1800] loss: 2.311934\n",
      "[40,  2000] loss: 2.311674\n",
      "[40,  2200] loss: 2.310026\n",
      "[40,  2400] loss: 2.312351\n",
      "[40,  2600] loss: 2.311127\n",
      "[40,  2800] loss: 2.312164\n",
      "[40,  3000] loss: 2.313083\n",
      "[40,  3200] loss: 2.312156\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[41,   200] loss: 2.312182\n",
      "[41,   400] loss: 2.311274\n",
      "[41,   600] loss: 2.310310\n",
      "[41,   800] loss: 2.310450\n",
      "[41,  1000] loss: 2.311165\n",
      "[41,  1200] loss: 2.310151\n",
      "[41,  1400] loss: 2.312471\n",
      "[41,  1600] loss: 2.311211\n",
      "[41,  1800] loss: 2.312763\n",
      "[41,  2000] loss: 2.311237\n",
      "[41,  2200] loss: 2.312576\n",
      "[41,  2400] loss: 2.310737\n",
      "[41,  2600] loss: 2.310753\n",
      "[41,  2800] loss: 2.310950\n",
      "[41,  3000] loss: 2.311619\n",
      "[41,  3200] loss: 2.312303\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[42,   200] loss: 2.308311\n",
      "[42,   400] loss: 2.311489\n",
      "[42,   600] loss: 2.312592\n",
      "[42,   800] loss: 2.311048\n",
      "[42,  1000] loss: 2.311764\n",
      "[42,  1200] loss: 2.310681\n",
      "[42,  1400] loss: 2.311182\n",
      "[42,  1600] loss: 2.313126\n",
      "[42,  1800] loss: 2.311747\n",
      "[42,  2000] loss: 2.310747\n",
      "[42,  2200] loss: 2.310775\n",
      "[42,  2400] loss: 2.310682\n",
      "[42,  2600] loss: 2.310034\n",
      "[42,  2800] loss: 2.312474\n",
      "[42,  3000] loss: 2.311667\n",
      "[42,  3200] loss: 2.312080\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[43,   200] loss: 2.310870\n",
      "[43,   400] loss: 2.310439\n",
      "[43,   600] loss: 2.312315\n",
      "[43,   800] loss: 2.311710\n",
      "[43,  1000] loss: 2.310545\n",
      "[43,  1200] loss: 2.310820\n",
      "[43,  1400] loss: 2.312630\n",
      "[43,  1600] loss: 2.313409\n",
      "[43,  1800] loss: 2.311452\n",
      "[43,  2000] loss: 2.311250\n",
      "[43,  2200] loss: 2.311196\n",
      "[43,  2400] loss: 2.309599\n",
      "[43,  2600] loss: 2.311373\n",
      "[43,  2800] loss: 2.312587\n",
      "[43,  3000] loss: 2.310993\n",
      "[43,  3200] loss: 2.312395\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[44,   200] loss: 2.311324\n",
      "[44,   400] loss: 2.312862\n",
      "[44,   600] loss: 2.308538\n",
      "[44,   800] loss: 2.312848\n",
      "[44,  1000] loss: 2.312067\n",
      "[44,  1200] loss: 2.311853\n",
      "[44,  1400] loss: 2.312276\n",
      "[44,  1600] loss: 2.311902\n",
      "[44,  1800] loss: 2.310815\n",
      "[44,  2000] loss: 2.312372\n",
      "[44,  2200] loss: 2.309500\n",
      "[44,  2400] loss: 2.310790\n",
      "[44,  2600] loss: 2.308664\n",
      "[44,  2800] loss: 2.310618\n",
      "[44,  3000] loss: 2.313013\n",
      "[44,  3200] loss: 2.310912\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[45,   200] loss: 2.312140\n",
      "[45,   400] loss: 2.311443\n",
      "[45,   600] loss: 2.311325\n",
      "[45,   800] loss: 2.310677\n",
      "[45,  1000] loss: 2.311244\n",
      "[45,  1200] loss: 2.310658\n",
      "[45,  1400] loss: 2.312360\n",
      "[45,  1600] loss: 2.312327\n",
      "[45,  1800] loss: 2.312165\n",
      "[45,  2000] loss: 2.310813\n",
      "[45,  2200] loss: 2.312192\n",
      "[45,  2400] loss: 2.311075\n",
      "[45,  2600] loss: 2.313841\n",
      "[45,  2800] loss: 2.311114\n",
      "[45,  3000] loss: 2.309880\n",
      "[45,  3200] loss: 2.311480\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[46,   200] loss: 2.310357\n",
      "[46,   400] loss: 2.311616\n",
      "[46,   600] loss: 2.311444\n",
      "[46,   800] loss: 2.314777\n",
      "[46,  1000] loss: 2.313093\n",
      "[46,  1200] loss: 2.314525\n",
      "[46,  1400] loss: 2.310983\n",
      "[46,  1600] loss: 2.312486\n",
      "[46,  1800] loss: 2.313130\n",
      "[46,  2000] loss: 2.311822\n",
      "[46,  2200] loss: 2.310242\n",
      "[46,  2400] loss: 2.309474\n",
      "[46,  2600] loss: 2.313319\n",
      "[46,  2800] loss: 2.312759\n",
      "[46,  3000] loss: 2.311988\n",
      "[46,  3200] loss: 2.311702\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[47,   200] loss: 2.315239\n",
      "[47,   400] loss: 2.311519\n",
      "[47,   600] loss: 2.311665\n",
      "[47,   800] loss: 2.311380\n",
      "[47,  1000] loss: 2.311197\n",
      "[47,  1200] loss: 2.311051\n",
      "[47,  1400] loss: 2.310284\n",
      "[47,  1600] loss: 2.309589\n",
      "[47,  1800] loss: 2.309869\n",
      "[47,  2000] loss: 2.311489\n",
      "[47,  2200] loss: 2.311750\n",
      "[47,  2400] loss: 2.310729\n",
      "[47,  2600] loss: 2.311879\n",
      "[47,  2800] loss: 2.311180\n",
      "[47,  3000] loss: 2.311018\n",
      "[47,  3200] loss: 2.312896\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[48,   200] loss: 2.310779\n",
      "[48,   400] loss: 2.313304\n",
      "[48,   600] loss: 2.312517\n",
      "[48,   800] loss: 2.311459\n",
      "[48,  1000] loss: 2.309439\n",
      "[48,  1200] loss: 2.311346\n",
      "[48,  1400] loss: 2.309393\n",
      "[48,  1600] loss: 2.310915\n",
      "[48,  1800] loss: 2.310579\n",
      "[48,  2000] loss: 2.310315\n",
      "[48,  2200] loss: 2.311534\n",
      "[48,  2400] loss: 2.312331\n",
      "[48,  2600] loss: 2.312221\n",
      "[48,  2800] loss: 2.309806\n",
      "[48,  3000] loss: 2.313087\n",
      "[48,  3200] loss: 2.311357\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[49,   200] loss: 2.310838\n",
      "[49,   400] loss: 2.314423\n",
      "[49,   600] loss: 2.311373\n",
      "[49,   800] loss: 2.313598\n",
      "[49,  1000] loss: 2.312518\n",
      "[49,  1200] loss: 2.311438\n",
      "[49,  1400] loss: 2.310646\n",
      "[49,  1600] loss: 2.311301\n",
      "[49,  1800] loss: 2.310889\n",
      "[49,  2000] loss: 2.311623\n",
      "[49,  2200] loss: 2.311478\n",
      "[49,  2400] loss: 2.313173\n",
      "[49,  2600] loss: 2.311663\n",
      "[49,  2800] loss: 2.309431\n",
      "[49,  3000] loss: 2.311691\n",
      "[49,  3200] loss: 2.312357\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[50,   200] loss: 2.312654\n",
      "[50,   400] loss: 2.310884\n",
      "[50,   600] loss: 2.310722\n",
      "[50,   800] loss: 2.312419\n",
      "[50,  1000] loss: 2.311793\n",
      "[50,  1200] loss: 2.314175\n",
      "[50,  1400] loss: 2.312095\n",
      "[50,  1600] loss: 2.310414\n",
      "[50,  1800] loss: 2.311493\n",
      "[50,  2000] loss: 2.310726\n",
      "[50,  2200] loss: 2.310100\n",
      "[50,  2400] loss: 2.312579\n",
      "[50,  2600] loss: 2.310905\n",
      "[50,  2800] loss: 2.311294\n",
      "[50,  3000] loss: 2.313028\n",
      "[50,  3200] loss: 2.311739\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[51,   200] loss: 2.311605\n",
      "[51,   400] loss: 2.311031\n",
      "[51,   600] loss: 2.315046\n",
      "[51,   800] loss: 2.311957\n",
      "[51,  1000] loss: 2.309650\n",
      "[51,  1200] loss: 2.310393\n",
      "[51,  1400] loss: 2.312222\n",
      "[51,  1600] loss: 2.310679\n",
      "[51,  1800] loss: 2.311667\n",
      "[51,  2000] loss: 2.309835\n",
      "[51,  2200] loss: 2.311829\n",
      "[51,  2400] loss: 2.312956\n",
      "[51,  2600] loss: 2.309747\n",
      "[51,  2800] loss: 2.310737\n",
      "[51,  3000] loss: 2.311054\n",
      "[51,  3200] loss: 2.311531\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[52,   200] loss: 2.311833\n",
      "[52,   400] loss: 2.313019\n",
      "[52,   600] loss: 2.310005\n",
      "[52,   800] loss: 2.311107\n",
      "[52,  1000] loss: 2.310975\n",
      "[52,  1200] loss: 2.312904\n",
      "[52,  1400] loss: 2.310040\n",
      "[52,  1600] loss: 2.311062\n",
      "[52,  1800] loss: 2.309988\n",
      "[52,  2000] loss: 2.311195\n",
      "[52,  2200] loss: 2.310623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52,  2400] loss: 2.312695\n",
      "[52,  2600] loss: 2.310323\n",
      "[52,  2800] loss: 2.309768\n",
      "[52,  3000] loss: 2.311223\n",
      "[52,  3200] loss: 2.310973\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[53,   200] loss: 2.311825\n",
      "[53,   400] loss: 2.311336\n",
      "[53,   600] loss: 2.310544\n",
      "[53,   800] loss: 2.309525\n",
      "[53,  1000] loss: 2.311913\n",
      "[53,  1200] loss: 2.309991\n",
      "[53,  1400] loss: 2.313221\n",
      "[53,  1600] loss: 2.313593\n",
      "[53,  1800] loss: 2.311208\n",
      "[53,  2000] loss: 2.311633\n",
      "[53,  2200] loss: 2.311018\n",
      "[53,  2400] loss: 2.313619\n",
      "[53,  2600] loss: 2.311154\n",
      "[53,  2800] loss: 2.311620\n",
      "[53,  3000] loss: 2.313873\n",
      "[53,  3200] loss: 2.310491\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[54,   200] loss: 2.312114\n",
      "[54,   400] loss: 2.310561\n",
      "[54,   600] loss: 2.309510\n",
      "[54,   800] loss: 2.311190\n",
      "[54,  1000] loss: 2.311455\n",
      "[54,  1200] loss: 2.311307\n",
      "[54,  1400] loss: 2.312211\n",
      "[54,  1600] loss: 2.311361\n",
      "[54,  1800] loss: 2.310934\n",
      "[54,  2000] loss: 2.311429\n",
      "[54,  2200] loss: 2.311116\n",
      "[54,  2400] loss: 2.311459\n",
      "[54,  2600] loss: 2.312284\n",
      "[54,  2800] loss: 2.311822\n",
      "[54,  3000] loss: 2.311341\n",
      "[54,  3200] loss: 2.311170\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[55,   200] loss: 2.313117\n",
      "[55,   400] loss: 2.313679\n",
      "[55,   600] loss: 2.311476\n",
      "[55,   800] loss: 2.311386\n",
      "[55,  1000] loss: 2.310764\n",
      "[55,  1200] loss: 2.310755\n",
      "[55,  1400] loss: 2.311557\n",
      "[55,  1600] loss: 2.311426\n",
      "[55,  1800] loss: 2.311815\n",
      "[55,  2000] loss: 2.312192\n",
      "[55,  2200] loss: 2.310589\n",
      "[55,  2400] loss: 2.312123\n",
      "[55,  2600] loss: 2.312670\n",
      "[55,  2800] loss: 2.311753\n",
      "[55,  3000] loss: 2.310566\n",
      "[55,  3200] loss: 2.311524\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[56,   200] loss: 2.312845\n",
      "[56,   400] loss: 2.309816\n",
      "[56,   600] loss: 2.311911\n",
      "[56,   800] loss: 2.311841\n",
      "[56,  1000] loss: 2.312093\n",
      "[56,  1200] loss: 2.310049\n",
      "[56,  1400] loss: 2.310962\n",
      "[56,  1600] loss: 2.311218\n",
      "[56,  1800] loss: 2.311667\n",
      "[56,  2000] loss: 2.310230\n",
      "[56,  2200] loss: 2.309121\n",
      "[56,  2400] loss: 2.310112\n",
      "[56,  2600] loss: 2.310855\n",
      "[56,  2800] loss: 2.311534\n",
      "[56,  3000] loss: 2.312179\n",
      "[56,  3200] loss: 2.310559\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[57,   200] loss: 2.313346\n",
      "[57,   400] loss: 2.311598\n",
      "[57,   600] loss: 2.311865\n",
      "[57,   800] loss: 2.309972\n",
      "[57,  1000] loss: 2.312372\n",
      "[57,  1200] loss: 2.311416\n",
      "[57,  1400] loss: 2.312449\n",
      "[57,  1600] loss: 2.309447\n",
      "[57,  1800] loss: 2.310270\n",
      "[57,  2000] loss: 2.312172\n",
      "[57,  2200] loss: 2.310543\n",
      "[57,  2400] loss: 2.310031\n",
      "[57,  2600] loss: 2.311265\n",
      "[57,  2800] loss: 2.311539\n",
      "[57,  3000] loss: 2.312567\n",
      "[57,  3200] loss: 2.311602\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[58,   200] loss: 2.311802\n",
      "[58,   400] loss: 2.311583\n",
      "[58,   600] loss: 2.314099\n",
      "[58,   800] loss: 2.313673\n",
      "[58,  1000] loss: 2.310547\n",
      "[58,  1200] loss: 2.311575\n",
      "[58,  1400] loss: 2.309529\n",
      "[58,  1600] loss: 2.311549\n",
      "[58,  1800] loss: 2.311179\n",
      "[58,  2000] loss: 2.311464\n",
      "[58,  2200] loss: 2.313066\n",
      "[58,  2400] loss: 2.313418\n",
      "[58,  2600] loss: 2.312498\n",
      "[58,  2800] loss: 2.310849\n",
      "[58,  3000] loss: 2.312071\n",
      "[58,  3200] loss: 2.312473\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[59,   200] loss: 2.311217\n",
      "[59,   400] loss: 2.312824\n",
      "[59,   600] loss: 2.313209\n",
      "[59,   800] loss: 2.309467\n",
      "[59,  1000] loss: 2.313514\n",
      "[59,  1200] loss: 2.311091\n",
      "[59,  1400] loss: 2.310629\n",
      "[59,  1600] loss: 2.311230\n",
      "[59,  1800] loss: 2.312237\n",
      "[59,  2000] loss: 2.311655\n",
      "[59,  2200] loss: 2.310555\n",
      "[59,  2400] loss: 2.314058\n",
      "[59,  2600] loss: 2.313160\n",
      "[59,  2800] loss: 2.310705\n",
      "[59,  3000] loss: 2.311661\n",
      "[59,  3200] loss: 2.311685\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[60,   200] loss: 2.310677\n",
      "[60,   400] loss: 2.310977\n",
      "[60,   600] loss: 2.312011\n",
      "[60,   800] loss: 2.311732\n",
      "[60,  1000] loss: 2.310908\n",
      "[60,  1200] loss: 2.310828\n",
      "[60,  1400] loss: 2.312157\n",
      "[60,  1600] loss: 2.311609\n",
      "[60,  1800] loss: 2.311058\n",
      "[60,  2000] loss: 2.310457\n",
      "[60,  2200] loss: 2.311989\n",
      "[60,  2400] loss: 2.309239\n",
      "[60,  2600] loss: 2.311770\n",
      "[60,  2800] loss: 2.313017\n",
      "[60,  3000] loss: 2.309257\n",
      "[60,  3200] loss: 2.310807\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[61,   200] loss: 2.312092\n",
      "[61,   400] loss: 2.310044\n",
      "[61,   600] loss: 2.312669\n",
      "[61,   800] loss: 2.311323\n",
      "[61,  1000] loss: 2.311729\n",
      "[61,  1200] loss: 2.312278\n",
      "[61,  1400] loss: 2.312908\n",
      "[61,  1600] loss: 2.312552\n",
      "[61,  1800] loss: 2.312472\n",
      "[61,  2000] loss: 2.312576\n",
      "[61,  2200] loss: 2.310846\n",
      "[61,  2400] loss: 2.308540\n",
      "[61,  2600] loss: 2.309800\n",
      "[61,  2800] loss: 2.312806\n",
      "[61,  3000] loss: 2.313208\n",
      "[61,  3200] loss: 2.311019\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[62,   200] loss: 2.311554\n",
      "[62,   400] loss: 2.310947\n",
      "[62,   600] loss: 2.312335\n",
      "[62,   800] loss: 2.311924\n",
      "[62,  1000] loss: 2.311705\n",
      "[62,  1200] loss: 2.311696\n",
      "[62,  1400] loss: 2.312387\n",
      "[62,  1600] loss: 2.312089\n",
      "[62,  1800] loss: 2.311754\n",
      "[62,  2000] loss: 2.309758\n",
      "[62,  2200] loss: 2.311198\n",
      "[62,  2400] loss: 2.310352\n",
      "[62,  2600] loss: 2.311517\n",
      "[62,  2800] loss: 2.310950\n",
      "[62,  3000] loss: 2.311698\n",
      "[62,  3200] loss: 2.312396\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[63,   200] loss: 2.313103\n",
      "[63,   400] loss: 2.311826\n",
      "[63,   600] loss: 2.312272\n",
      "[63,   800] loss: 2.309985\n",
      "[63,  1000] loss: 2.311521\n",
      "[63,  1200] loss: 2.310162\n",
      "[63,  1400] loss: 2.313747\n",
      "[63,  1600] loss: 2.312326\n",
      "[63,  1800] loss: 2.311116\n",
      "[63,  2000] loss: 2.313923\n",
      "[63,  2200] loss: 2.310882\n",
      "[63,  2400] loss: 2.310651\n",
      "[63,  2600] loss: 2.313591\n",
      "[63,  2800] loss: 2.312974\n",
      "[63,  3000] loss: 2.310158\n",
      "[63,  3200] loss: 2.311577\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[64,   200] loss: 2.310776\n",
      "[64,   400] loss: 2.312993\n",
      "[64,   600] loss: 2.312304\n",
      "[64,   800] loss: 2.310498\n",
      "[64,  1000] loss: 2.311975\n",
      "[64,  1200] loss: 2.309711\n",
      "[64,  1400] loss: 2.313195\n",
      "[64,  1600] loss: 2.312539\n",
      "[64,  1800] loss: 2.312557\n",
      "[64,  2000] loss: 2.310808\n",
      "[64,  2200] loss: 2.312786\n",
      "[64,  2400] loss: 2.309749\n",
      "[64,  2600] loss: 2.311838\n",
      "[64,  2800] loss: 2.310491\n",
      "[64,  3000] loss: 2.312110\n",
      "[64,  3200] loss: 2.311093\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[65,   200] loss: 2.311536\n",
      "[65,   400] loss: 2.313428\n",
      "[65,   600] loss: 2.312301\n",
      "[65,   800] loss: 2.311721\n",
      "[65,  1000] loss: 2.311208\n",
      "[65,  1200] loss: 2.311664\n",
      "[65,  1400] loss: 2.311161\n",
      "[65,  1600] loss: 2.311244\n",
      "[65,  1800] loss: 2.312416\n",
      "[65,  2000] loss: 2.310270\n",
      "[65,  2200] loss: 2.311605\n",
      "[65,  2400] loss: 2.310211\n",
      "[65,  2600] loss: 2.313733\n",
      "[65,  2800] loss: 2.312503\n",
      "[65,  3000] loss: 2.311696\n",
      "[65,  3200] loss: 2.311922\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[66,   200] loss: 2.309203\n",
      "[66,   400] loss: 2.312140\n",
      "[66,   600] loss: 2.311204\n",
      "[66,   800] loss: 2.311933\n",
      "[66,  1000] loss: 2.312515\n",
      "[66,  1200] loss: 2.311528\n",
      "[66,  1400] loss: 2.311443\n",
      "[66,  1600] loss: 2.310916\n",
      "[66,  1800] loss: 2.313572\n",
      "[66,  2000] loss: 2.311669\n",
      "[66,  2200] loss: 2.309400\n",
      "[66,  2400] loss: 2.310500\n",
      "[66,  2600] loss: 2.310820\n",
      "[66,  2800] loss: 2.312421\n",
      "[66,  3000] loss: 2.309117\n",
      "[66,  3200] loss: 2.312857\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[67,   200] loss: 2.311463\n",
      "[67,   400] loss: 2.310673\n",
      "[67,   600] loss: 2.312363\n",
      "[67,   800] loss: 2.312536\n",
      "[67,  1000] loss: 2.311412\n",
      "[67,  1200] loss: 2.309935\n",
      "[67,  1400] loss: 2.312131\n",
      "[67,  1600] loss: 2.312352\n",
      "[67,  1800] loss: 2.310319\n",
      "[67,  2000] loss: 2.310838\n",
      "[67,  2200] loss: 2.311314\n",
      "[67,  2400] loss: 2.310825\n",
      "[67,  2600] loss: 2.312521\n",
      "[67,  2800] loss: 2.311819\n",
      "[67,  3000] loss: 2.311781\n",
      "[67,  3200] loss: 2.310988\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[68,   200] loss: 2.312304\n",
      "[68,   400] loss: 2.311324\n",
      "[68,   600] loss: 2.311281\n",
      "[68,   800] loss: 2.311974\n",
      "[68,  1000] loss: 2.311036\n",
      "[68,  1200] loss: 2.309637\n",
      "[68,  1400] loss: 2.311296\n",
      "[68,  1600] loss: 2.311399\n",
      "[68,  1800] loss: 2.310287\n",
      "[68,  2000] loss: 2.309151\n",
      "[68,  2200] loss: 2.314035\n",
      "[68,  2400] loss: 2.312744\n",
      "[68,  2600] loss: 2.312033\n",
      "[68,  2800] loss: 2.310676\n",
      "[68,  3000] loss: 2.311818\n",
      "[68,  3200] loss: 2.311147\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[69,   200] loss: 2.312538\n",
      "[69,   400] loss: 2.311422\n",
      "[69,   600] loss: 2.311863\n",
      "[69,   800] loss: 2.311570\n",
      "[69,  1000] loss: 2.312279\n",
      "[69,  1200] loss: 2.311087\n",
      "[69,  1400] loss: 2.309294\n",
      "[69,  1600] loss: 2.311876\n",
      "[69,  1800] loss: 2.312923\n",
      "[69,  2000] loss: 2.314560\n",
      "[69,  2200] loss: 2.310473\n",
      "[69,  2400] loss: 2.312871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69,  2600] loss: 2.310511\n",
      "[69,  2800] loss: 2.310627\n",
      "[69,  3000] loss: 2.311836\n",
      "[69,  3200] loss: 2.311838\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[70,   200] loss: 2.310951\n",
      "[70,   400] loss: 2.310847\n",
      "[70,   600] loss: 2.312044\n",
      "[70,   800] loss: 2.312566\n",
      "[70,  1000] loss: 2.312048\n",
      "[70,  1200] loss: 2.311775\n",
      "[70,  1400] loss: 2.312615\n",
      "[70,  1600] loss: 2.309928\n",
      "[70,  1800] loss: 2.311802\n",
      "[70,  2000] loss: 2.312640\n",
      "[70,  2200] loss: 2.309727\n",
      "[70,  2400] loss: 2.312718\n",
      "[70,  2600] loss: 2.308928\n",
      "[70,  2800] loss: 2.310528\n",
      "[70,  3000] loss: 2.309946\n",
      "[70,  3200] loss: 2.313124\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[71,   200] loss: 2.312111\n",
      "[71,   400] loss: 2.311940\n",
      "[71,   600] loss: 2.309792\n",
      "[71,   800] loss: 2.311912\n",
      "[71,  1000] loss: 2.311758\n",
      "[71,  1200] loss: 2.311084\n",
      "[71,  1400] loss: 2.310927\n",
      "[71,  1600] loss: 2.312252\n",
      "[71,  1800] loss: 2.310862\n",
      "[71,  2000] loss: 2.309436\n",
      "[71,  2200] loss: 2.310454\n",
      "[71,  2400] loss: 2.312092\n",
      "[71,  2600] loss: 2.311517\n",
      "[71,  2800] loss: 2.310462\n",
      "[71,  3000] loss: 2.311732\n",
      "[71,  3200] loss: 2.310035\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[72,   200] loss: 2.312659\n",
      "[72,   400] loss: 2.311787\n",
      "[72,   600] loss: 2.311917\n",
      "[72,   800] loss: 2.311017\n",
      "[72,  1000] loss: 2.310633\n",
      "[72,  1200] loss: 2.311496\n",
      "[72,  1400] loss: 2.312284\n",
      "[72,  1600] loss: 2.311979\n",
      "[72,  1800] loss: 2.310484\n",
      "[72,  2000] loss: 2.312350\n",
      "[72,  2200] loss: 2.311100\n",
      "[72,  2400] loss: 2.310735\n",
      "[72,  2600] loss: 2.312935\n",
      "[72,  2800] loss: 2.310901\n",
      "[72,  3000] loss: 2.311859\n",
      "[72,  3200] loss: 2.311570\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[73,   200] loss: 2.310105\n",
      "[73,   400] loss: 2.310147\n",
      "[73,   600] loss: 2.312474\n",
      "[73,   800] loss: 2.311495\n",
      "[73,  1000] loss: 2.312451\n",
      "[73,  1200] loss: 2.309851\n",
      "[73,  1400] loss: 2.313100\n",
      "[73,  1600] loss: 2.310568\n",
      "[73,  1800] loss: 2.310472\n",
      "[73,  2000] loss: 2.311820\n",
      "[73,  2200] loss: 2.309944\n",
      "[73,  2400] loss: 2.312726\n",
      "[73,  2600] loss: 2.310042\n",
      "[73,  2800] loss: 2.312670\n",
      "[73,  3000] loss: 2.313184\n",
      "[73,  3200] loss: 2.311569\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[74,   200] loss: 2.310381\n",
      "[74,   400] loss: 2.312631\n",
      "[74,   600] loss: 2.310266\n",
      "[74,   800] loss: 2.311180\n",
      "[74,  1000] loss: 2.311278\n",
      "[74,  1200] loss: 2.312340\n",
      "[74,  1400] loss: 2.311076\n",
      "[74,  1600] loss: 2.312148\n",
      "[74,  1800] loss: 2.310474\n",
      "[74,  2000] loss: 2.311115\n",
      "[74,  2200] loss: 2.312434\n",
      "[74,  2400] loss: 2.310793\n",
      "[74,  2600] loss: 2.311017\n",
      "[74,  2800] loss: 2.313129\n",
      "[74,  3000] loss: 2.311568\n",
      "[74,  3200] loss: 2.313070\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[75,   200] loss: 2.310948\n",
      "[75,   400] loss: 2.311520\n",
      "[75,   600] loss: 2.311285\n",
      "[75,   800] loss: 2.311421\n",
      "[75,  1000] loss: 2.310656\n",
      "[75,  1200] loss: 2.310416\n",
      "[75,  1400] loss: 2.311360\n",
      "[75,  1600] loss: 2.311611\n",
      "[75,  1800] loss: 2.310846\n",
      "[75,  2000] loss: 2.310399\n",
      "[75,  2200] loss: 2.310816\n",
      "[75,  2400] loss: 2.311483\n",
      "[75,  2600] loss: 2.310237\n",
      "[75,  2800] loss: 2.311642\n",
      "[75,  3000] loss: 2.311592\n",
      "[75,  3200] loss: 2.311412\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[76,   200] loss: 2.312408\n",
      "[76,   400] loss: 2.312051\n",
      "[76,   600] loss: 2.311865\n",
      "[76,   800] loss: 2.311262\n",
      "[76,  1000] loss: 2.309674\n",
      "[76,  1200] loss: 2.310885\n",
      "[76,  1400] loss: 2.312010\n",
      "[76,  1600] loss: 2.310520\n",
      "[76,  1800] loss: 2.312147\n",
      "[76,  2000] loss: 2.310599\n",
      "[76,  2200] loss: 2.311458\n",
      "[76,  2400] loss: 2.314064\n",
      "[76,  2600] loss: 2.312022\n",
      "[76,  2800] loss: 2.310860\n",
      "[76,  3000] loss: 2.311673\n",
      "[76,  3200] loss: 2.311035\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[77,   200] loss: 2.309480\n",
      "[77,   400] loss: 2.312662\n",
      "[77,   600] loss: 2.313486\n",
      "[77,   800] loss: 2.311427\n",
      "[77,  1000] loss: 2.311711\n",
      "[77,  1200] loss: 2.311002\n",
      "[77,  1400] loss: 2.309093\n",
      "[77,  1600] loss: 2.311581\n",
      "[77,  1800] loss: 2.311292\n",
      "[77,  2000] loss: 2.313039\n",
      "[77,  2200] loss: 2.312297\n",
      "[77,  2400] loss: 2.311341\n",
      "[77,  2600] loss: 2.309193\n",
      "[77,  2800] loss: 2.313029\n",
      "[77,  3000] loss: 2.312501\n",
      "[77,  3200] loss: 2.312733\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[78,   200] loss: 2.312211\n",
      "[78,   400] loss: 2.309954\n",
      "[78,   600] loss: 2.311743\n",
      "[78,   800] loss: 2.311997\n",
      "[78,  1000] loss: 2.311104\n",
      "[78,  1200] loss: 2.310026\n",
      "[78,  1400] loss: 2.312151\n",
      "[78,  1600] loss: 2.310468\n",
      "[78,  1800] loss: 2.311315\n",
      "[78,  2000] loss: 2.313422\n",
      "[78,  2200] loss: 2.311384\n",
      "[78,  2400] loss: 2.312255\n",
      "[78,  2600] loss: 2.311883\n",
      "[78,  2800] loss: 2.311651\n",
      "[78,  3000] loss: 2.311501\n",
      "[78,  3200] loss: 2.311893\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[79,   200] loss: 2.311893\n",
      "[79,   400] loss: 2.309712\n",
      "[79,   600] loss: 2.309711\n",
      "[79,   800] loss: 2.312638\n",
      "[79,  1000] loss: 2.313471\n",
      "[79,  1200] loss: 2.310109\n",
      "[79,  1400] loss: 2.309322\n",
      "[79,  1600] loss: 2.310591\n",
      "[79,  1800] loss: 2.311091\n",
      "[79,  2000] loss: 2.311885\n",
      "[79,  2200] loss: 2.312029\n",
      "[79,  2400] loss: 2.309918\n",
      "[79,  2600] loss: 2.311152\n",
      "[79,  2800] loss: 2.311375\n",
      "[79,  3000] loss: 2.310704\n",
      "[79,  3200] loss: 2.311725\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[80,   200] loss: 2.311229\n",
      "[80,   400] loss: 2.311451\n",
      "[80,   600] loss: 2.310092\n",
      "[80,   800] loss: 2.312388\n",
      "[80,  1000] loss: 2.310298\n",
      "[80,  1200] loss: 2.310661\n",
      "[80,  1400] loss: 2.311672\n",
      "[80,  1600] loss: 2.312612\n",
      "[80,  1800] loss: 2.308717\n",
      "[80,  2000] loss: 2.312565\n",
      "[80,  2200] loss: 2.312024\n",
      "[80,  2400] loss: 2.311155\n",
      "[80,  2600] loss: 2.314132\n",
      "[80,  2800] loss: 2.312494\n",
      "[80,  3000] loss: 2.312700\n",
      "[80,  3200] loss: 2.309701\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[81,   200] loss: 2.311038\n",
      "[81,   400] loss: 2.311191\n",
      "[81,   600] loss: 2.311193\n",
      "[81,   800] loss: 2.312662\n",
      "[81,  1000] loss: 2.310165\n",
      "[81,  1200] loss: 2.312967\n",
      "[81,  1400] loss: 2.310970\n",
      "[81,  1600] loss: 2.314091\n",
      "[81,  1800] loss: 2.310442\n",
      "[81,  2000] loss: 2.312721\n",
      "[81,  2200] loss: 2.312038\n",
      "[81,  2400] loss: 2.311919\n",
      "[81,  2600] loss: 2.312576\n",
      "[81,  2800] loss: 2.311513\n",
      "[81,  3000] loss: 2.313009\n",
      "[81,  3200] loss: 2.311190\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[82,   200] loss: 2.312185\n",
      "[82,   400] loss: 2.311207\n",
      "[82,   600] loss: 2.310997\n",
      "[82,   800] loss: 2.311939\n",
      "[82,  1000] loss: 2.312281\n",
      "[82,  1200] loss: 2.310452\n",
      "[82,  1400] loss: 2.312186\n",
      "[82,  1600] loss: 2.313051\n",
      "[82,  1800] loss: 2.310911\n",
      "[82,  2000] loss: 2.310110\n",
      "[82,  2200] loss: 2.311838\n",
      "[82,  2400] loss: 2.312472\n",
      "[82,  2600] loss: 2.311246\n",
      "[82,  2800] loss: 2.311911\n",
      "[82,  3000] loss: 2.312572\n",
      "[82,  3200] loss: 2.311963\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[83,   200] loss: 2.312542\n",
      "[83,   400] loss: 2.310751\n",
      "[83,   600] loss: 2.312963\n",
      "[83,   800] loss: 2.312589\n",
      "[83,  1000] loss: 2.311448\n",
      "[83,  1200] loss: 2.311470\n",
      "[83,  1400] loss: 2.311503\n",
      "[83,  1600] loss: 2.312370\n",
      "[83,  1800] loss: 2.310514\n",
      "[83,  2000] loss: 2.311137\n",
      "[83,  2200] loss: 2.314089\n",
      "[83,  2400] loss: 2.312664\n",
      "[83,  2600] loss: 2.310312\n",
      "[83,  2800] loss: 2.309856\n",
      "[83,  3000] loss: 2.308923\n",
      "[83,  3200] loss: 2.311932\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[84,   200] loss: 2.310876\n",
      "[84,   400] loss: 2.312468\n",
      "[84,   600] loss: 2.311760\n",
      "[84,   800] loss: 2.310935\n",
      "[84,  1000] loss: 2.311855\n",
      "[84,  1200] loss: 2.311150\n",
      "[84,  1400] loss: 2.310235\n",
      "[84,  1600] loss: 2.310936\n",
      "[84,  1800] loss: 2.310213\n",
      "[84,  2000] loss: 2.311766\n",
      "[84,  2200] loss: 2.311478\n",
      "[84,  2400] loss: 2.313177\n",
      "[84,  2600] loss: 2.310720\n",
      "[84,  2800] loss: 2.310492\n",
      "[84,  3000] loss: 2.310876\n",
      "[84,  3200] loss: 2.310237\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[85,   200] loss: 2.311157\n",
      "[85,   400] loss: 2.312926\n",
      "[85,   600] loss: 2.312222\n",
      "[85,   800] loss: 2.312455\n",
      "[85,  1000] loss: 2.311523\n",
      "[85,  1200] loss: 2.310303\n",
      "[85,  1400] loss: 2.311640\n",
      "[85,  1600] loss: 2.312534\n",
      "[85,  1800] loss: 2.313058\n",
      "[85,  2000] loss: 2.312741\n",
      "[85,  2200] loss: 2.312026\n",
      "[85,  2400] loss: 2.312257\n",
      "[85,  2600] loss: 2.309553\n",
      "[85,  2800] loss: 2.310067\n",
      "[85,  3000] loss: 2.310786\n",
      "[85,  3200] loss: 2.310412\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[86,   200] loss: 2.311037\n",
      "[86,   400] loss: 2.310381\n",
      "[86,   600] loss: 2.311342\n",
      "[86,   800] loss: 2.311764\n",
      "[86,  1000] loss: 2.312387\n",
      "[86,  1200] loss: 2.310444\n",
      "[86,  1400] loss: 2.310782\n",
      "[86,  1600] loss: 2.310427\n",
      "[86,  1800] loss: 2.312590\n",
      "[86,  2000] loss: 2.309293\n",
      "[86,  2200] loss: 2.312290\n",
      "[86,  2400] loss: 2.309678\n",
      "[86,  2600] loss: 2.312573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86,  2800] loss: 2.312652\n",
      "[86,  3000] loss: 2.312305\n",
      "[86,  3200] loss: 2.310496\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[87,   200] loss: 2.310429\n",
      "[87,   400] loss: 2.310653\n",
      "[87,   600] loss: 2.309974\n",
      "[87,   800] loss: 2.310341\n",
      "[87,  1000] loss: 2.309815\n",
      "[87,  1200] loss: 2.311516\n",
      "[87,  1400] loss: 2.311253\n",
      "[87,  1600] loss: 2.311838\n",
      "[87,  1800] loss: 2.310307\n",
      "[87,  2000] loss: 2.311744\n",
      "[87,  2200] loss: 2.311496\n",
      "[87,  2400] loss: 2.312677\n",
      "[87,  2600] loss: 2.312061\n",
      "[87,  2800] loss: 2.311351\n",
      "[87,  3000] loss: 2.312584\n",
      "[87,  3200] loss: 2.312781\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[88,   200] loss: 2.312582\n",
      "[88,   400] loss: 2.312225\n",
      "[88,   600] loss: 2.313684\n",
      "[88,   800] loss: 2.309051\n",
      "[88,  1000] loss: 2.310198\n",
      "[88,  1200] loss: 2.310538\n",
      "[88,  1400] loss: 2.311176\n",
      "[88,  1600] loss: 2.310836\n",
      "[88,  1800] loss: 2.310352\n",
      "[88,  2000] loss: 2.312028\n",
      "[88,  2200] loss: 2.310377\n",
      "[88,  2400] loss: 2.312465\n",
      "[88,  2600] loss: 2.311339\n",
      "[88,  2800] loss: 2.312059\n",
      "[88,  3000] loss: 2.311262\n",
      "[88,  3200] loss: 2.311350\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[89,   200] loss: 2.310791\n",
      "[89,   400] loss: 2.310739\n",
      "[89,   600] loss: 2.312650\n",
      "[89,   800] loss: 2.310517\n",
      "[89,  1000] loss: 2.310979\n",
      "[89,  1200] loss: 2.311241\n",
      "[89,  1400] loss: 2.310363\n",
      "[89,  1600] loss: 2.310914\n",
      "[89,  1800] loss: 2.312460\n",
      "[89,  2000] loss: 2.310664\n",
      "[89,  2200] loss: 2.309798\n",
      "[89,  2400] loss: 2.312736\n",
      "[89,  2600] loss: 2.311961\n",
      "[89,  2800] loss: 2.310382\n",
      "[89,  3000] loss: 2.312333\n",
      "[89,  3200] loss: 2.311056\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[90,   200] loss: 2.311796\n",
      "[90,   400] loss: 2.310961\n",
      "[90,   600] loss: 2.310382\n",
      "[90,   800] loss: 2.310992\n",
      "[90,  1000] loss: 2.313899\n",
      "[90,  1200] loss: 2.312851\n",
      "[90,  1400] loss: 2.310663\n",
      "[90,  1600] loss: 2.310612\n",
      "[90,  1800] loss: 2.311910\n",
      "[90,  2000] loss: 2.312892\n",
      "[90,  2200] loss: 2.313253\n",
      "[90,  2400] loss: 2.312394\n",
      "[90,  2600] loss: 2.311126\n",
      "[90,  2800] loss: 2.314530\n",
      "[90,  3000] loss: 2.311735\n",
      "[90,  3200] loss: 2.311795\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[91,   200] loss: 2.310628\n",
      "[91,   400] loss: 2.310295\n",
      "[91,   600] loss: 2.311658\n",
      "[91,   800] loss: 2.314019\n",
      "[91,  1000] loss: 2.310901\n",
      "[91,  1200] loss: 2.311168\n",
      "[91,  1400] loss: 2.310997\n",
      "[91,  1600] loss: 2.309165\n",
      "[91,  1800] loss: 2.312140\n",
      "[91,  2000] loss: 2.310353\n",
      "[91,  2200] loss: 2.310332\n",
      "[91,  2400] loss: 2.312745\n",
      "[91,  2600] loss: 2.311218\n",
      "[91,  2800] loss: 2.310361\n",
      "[91,  3000] loss: 2.311675\n",
      "[91,  3200] loss: 2.312338\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[92,   200] loss: 2.310462\n",
      "[92,   400] loss: 2.312093\n",
      "[92,   600] loss: 2.310475\n",
      "[92,   800] loss: 2.313702\n",
      "[92,  1000] loss: 2.309663\n",
      "[92,  1200] loss: 2.310689\n",
      "[92,  1400] loss: 2.312472\n",
      "[92,  1600] loss: 2.312011\n",
      "[92,  1800] loss: 2.312654\n",
      "[92,  2000] loss: 2.313306\n",
      "[92,  2200] loss: 2.312443\n",
      "[92,  2400] loss: 2.309719\n",
      "[92,  2600] loss: 2.311580\n",
      "[92,  2800] loss: 2.309666\n",
      "[92,  3000] loss: 2.311593\n",
      "[92,  3200] loss: 2.311435\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[93,   200] loss: 2.309347\n",
      "[93,   400] loss: 2.312042\n",
      "[93,   600] loss: 2.311466\n",
      "[93,   800] loss: 2.311826\n",
      "[93,  1000] loss: 2.310344\n",
      "[93,  1200] loss: 2.313790\n",
      "[93,  1400] loss: 2.310572\n",
      "[93,  1600] loss: 2.312939\n",
      "[93,  1800] loss: 2.311186\n",
      "[93,  2000] loss: 2.310028\n",
      "[93,  2200] loss: 2.313067\n",
      "[93,  2400] loss: 2.309337\n",
      "[93,  2600] loss: 2.311659\n",
      "[93,  2800] loss: 2.309653\n",
      "[93,  3000] loss: 2.309628\n",
      "[93,  3200] loss: 2.314037\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[94,   200] loss: 2.310866\n",
      "[94,   400] loss: 2.310722\n",
      "[94,   600] loss: 2.312672\n",
      "[94,   800] loss: 2.311356\n",
      "[94,  1000] loss: 2.311604\n",
      "[94,  1200] loss: 2.312520\n",
      "[94,  1400] loss: 2.310773\n",
      "[94,  1600] loss: 2.309750\n",
      "[94,  1800] loss: 2.313778\n",
      "[94,  2000] loss: 2.312482\n",
      "[94,  2200] loss: 2.313804\n",
      "[94,  2400] loss: 2.312851\n",
      "[94,  2600] loss: 2.310236\n",
      "[94,  2800] loss: 2.311153\n",
      "[94,  3000] loss: 2.313202\n",
      "[94,  3200] loss: 2.310871\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[95,   200] loss: 2.311209\n",
      "[95,   400] loss: 2.309479\n",
      "[95,   600] loss: 2.312070\n",
      "[95,   800] loss: 2.311297\n",
      "[95,  1000] loss: 2.313904\n",
      "[95,  1200] loss: 2.312500\n",
      "[95,  1400] loss: 2.311609\n",
      "[95,  1600] loss: 2.310266\n",
      "[95,  1800] loss: 2.311723\n",
      "[95,  2000] loss: 2.311917\n",
      "[95,  2200] loss: 2.312370\n",
      "[95,  2400] loss: 2.312355\n",
      "[95,  2600] loss: 2.311765\n",
      "[95,  2800] loss: 2.312001\n",
      "[95,  3000] loss: 2.313230\n",
      "[95,  3200] loss: 2.312250\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[96,   200] loss: 2.311763\n",
      "[96,   400] loss: 2.312741\n",
      "[96,   600] loss: 2.311074\n",
      "[96,   800] loss: 2.310901\n",
      "[96,  1000] loss: 2.311104\n",
      "[96,  1200] loss: 2.310349\n",
      "[96,  1400] loss: 2.314055\n",
      "[96,  1600] loss: 2.310775\n",
      "[96,  1800] loss: 2.310885\n",
      "[96,  2000] loss: 2.311915\n",
      "[96,  2200] loss: 2.311247\n",
      "[96,  2400] loss: 2.309754\n",
      "[96,  2600] loss: 2.312844\n",
      "[96,  2800] loss: 2.310847\n",
      "[96,  3000] loss: 2.311522\n",
      "[96,  3200] loss: 2.311447\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[97,   200] loss: 2.312017\n",
      "[97,   400] loss: 2.310407\n",
      "[97,   600] loss: 2.312155\n",
      "[97,   800] loss: 2.311790\n",
      "[97,  1000] loss: 2.309262\n",
      "[97,  1200] loss: 2.309750\n",
      "[97,  1400] loss: 2.312278\n",
      "[97,  1600] loss: 2.312082\n",
      "[97,  1800] loss: 2.309832\n",
      "[97,  2000] loss: 2.311747\n",
      "[97,  2200] loss: 2.311408\n",
      "[97,  2400] loss: 2.312576\n",
      "[97,  2600] loss: 2.311370\n",
      "[97,  2800] loss: 2.311875\n",
      "[97,  3000] loss: 2.309167\n",
      "[97,  3200] loss: 2.311575\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[98,   200] loss: 2.310417\n",
      "[98,   400] loss: 2.314211\n",
      "[98,   600] loss: 2.310205\n",
      "[98,   800] loss: 2.313100\n",
      "[98,  1000] loss: 2.310631\n",
      "[98,  1200] loss: 2.311086\n",
      "[98,  1400] loss: 2.311569\n",
      "[98,  1600] loss: 2.313878\n",
      "[98,  1800] loss: 2.312109\n",
      "[98,  2000] loss: 2.311855\n",
      "[98,  2200] loss: 2.312960\n",
      "[98,  2400] loss: 2.311376\n",
      "[98,  2600] loss: 2.311971\n",
      "[98,  2800] loss: 2.312363\n",
      "[98,  3000] loss: 2.310427\n",
      "[98,  3200] loss: 2.311690\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[99,   200] loss: 2.311450\n",
      "[99,   400] loss: 2.312415\n",
      "[99,   600] loss: 2.310419\n",
      "[99,   800] loss: 2.312968\n",
      "[99,  1000] loss: 2.313212\n",
      "[99,  1200] loss: 2.311382\n",
      "[99,  1400] loss: 2.312628\n",
      "[99,  1600] loss: 2.309866\n",
      "[99,  1800] loss: 2.311348\n",
      "[99,  2000] loss: 2.310217\n",
      "[99,  2200] loss: 2.311149\n",
      "[99,  2400] loss: 2.313062\n",
      "[99,  2600] loss: 2.311405\n",
      "[99,  2800] loss: 2.311370\n",
      "[99,  3000] loss: 2.312411\n",
      "[99,  3200] loss: 2.308932\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[100,   200] loss: 2.312669\n",
      "[100,   400] loss: 2.312555\n",
      "[100,   600] loss: 2.311177\n",
      "[100,   800] loss: 2.310698\n",
      "[100,  1000] loss: 2.311174\n",
      "[100,  1200] loss: 2.311300\n",
      "[100,  1400] loss: 2.311030\n",
      "[100,  1600] loss: 2.311683\n",
      "[100,  1800] loss: 2.310742\n",
      "[100,  2000] loss: 2.310811\n",
      "[100,  2200] loss: 2.313114\n",
      "[100,  2400] loss: 2.313444\n",
      "[100,  2600] loss: 2.312353\n",
      "[100,  2800] loss: 2.310594\n",
      "[100,  3000] loss: 2.312854\n",
      "[100,  3200] loss: 2.312551\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[101,   200] loss: 2.311904\n",
      "[101,   400] loss: 2.312978\n",
      "[101,   600] loss: 2.311561\n",
      "[101,   800] loss: 2.310311\n",
      "[101,  1000] loss: 2.312968\n",
      "[101,  1200] loss: 2.312629\n",
      "[101,  1400] loss: 2.311267\n",
      "[101,  1600] loss: 2.311381\n",
      "[101,  1800] loss: 2.312977\n",
      "[101,  2000] loss: 2.311982\n",
      "[101,  2200] loss: 2.311365\n",
      "[101,  2400] loss: 2.311207\n",
      "[101,  2600] loss: 2.309296\n",
      "[101,  2800] loss: 2.310845\n",
      "[101,  3000] loss: 2.311072\n",
      "[101,  3200] loss: 2.313082\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[102,   200] loss: 2.311692\n",
      "[102,   400] loss: 2.310267\n",
      "[102,   600] loss: 2.311687\n",
      "[102,   800] loss: 2.313188\n",
      "[102,  1000] loss: 2.312300\n",
      "[102,  1200] loss: 2.310276\n",
      "[102,  1400] loss: 2.311070\n",
      "[102,  1600] loss: 2.309778\n",
      "[102,  1800] loss: 2.312607\n",
      "[102,  2000] loss: 2.311469\n",
      "[102,  2200] loss: 2.310860\n",
      "[102,  2400] loss: 2.311653\n",
      "[102,  2600] loss: 2.313610\n",
      "[102,  2800] loss: 2.310836\n",
      "[102,  3000] loss: 2.311378\n",
      "[102,  3200] loss: 2.312145\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[103,   200] loss: 2.311297\n",
      "[103,   400] loss: 2.312205\n",
      "[103,   600] loss: 2.310631\n",
      "[103,   800] loss: 2.310946\n",
      "[103,  1000] loss: 2.309965\n",
      "[103,  1200] loss: 2.310418\n",
      "[103,  1400] loss: 2.310640\n",
      "[103,  1600] loss: 2.312578\n",
      "[103,  1800] loss: 2.312375\n",
      "[103,  2000] loss: 2.311689\n",
      "[103,  2200] loss: 2.311471\n",
      "[103,  2400] loss: 2.311495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103,  2600] loss: 2.314721\n",
      "[103,  2800] loss: 2.311208\n",
      "[103,  3000] loss: 2.310406\n",
      "[103,  3200] loss: 2.312061\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[104,   200] loss: 2.312191\n",
      "[104,   400] loss: 2.311530\n",
      "[104,   600] loss: 2.310798\n",
      "[104,   800] loss: 2.310535\n",
      "[104,  1000] loss: 2.310854\n",
      "[104,  1200] loss: 2.311179\n",
      "[104,  1400] loss: 2.311394\n",
      "[104,  1600] loss: 2.311883\n",
      "[104,  1800] loss: 2.312374\n",
      "[104,  2000] loss: 2.311958\n",
      "[104,  2200] loss: 2.311916\n",
      "[104,  2400] loss: 2.310236\n",
      "[104,  2600] loss: 2.313175\n",
      "[104,  2800] loss: 2.312369\n",
      "[104,  3000] loss: 2.310132\n",
      "[104,  3200] loss: 2.312553\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[105,   200] loss: 2.310999\n",
      "[105,   400] loss: 2.312371\n",
      "[105,   600] loss: 2.311562\n",
      "[105,   800] loss: 2.312007\n",
      "[105,  1000] loss: 2.311386\n",
      "[105,  1200] loss: 2.313078\n",
      "[105,  1400] loss: 2.309506\n",
      "[105,  1600] loss: 2.311235\n",
      "[105,  1800] loss: 2.311201\n",
      "[105,  2000] loss: 2.311526\n",
      "[105,  2200] loss: 2.311744\n",
      "[105,  2400] loss: 2.312571\n",
      "[105,  2600] loss: 2.310395\n",
      "[105,  2800] loss: 2.311685\n",
      "[105,  3000] loss: 2.311032\n",
      "[105,  3200] loss: 2.311166\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[106,   200] loss: 2.310220\n",
      "[106,   400] loss: 2.312699\n",
      "[106,   600] loss: 2.311421\n",
      "[106,   800] loss: 2.312432\n",
      "[106,  1000] loss: 2.311546\n",
      "[106,  1200] loss: 2.310421\n",
      "[106,  1400] loss: 2.311139\n",
      "[106,  1600] loss: 2.313996\n",
      "[106,  1800] loss: 2.311352\n",
      "[106,  2000] loss: 2.310910\n",
      "[106,  2200] loss: 2.309954\n",
      "[106,  2400] loss: 2.310248\n",
      "[106,  2600] loss: 2.311510\n",
      "[106,  2800] loss: 2.310994\n",
      "[106,  3000] loss: 2.313355\n",
      "[106,  3200] loss: 2.311865\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[107,   200] loss: 2.312970\n",
      "[107,   400] loss: 2.313072\n",
      "[107,   600] loss: 2.313892\n",
      "[107,   800] loss: 2.311795\n",
      "[107,  1000] loss: 2.311216\n",
      "[107,  1200] loss: 2.312234\n",
      "[107,  1400] loss: 2.310641\n",
      "[107,  1600] loss: 2.312663\n",
      "[107,  1800] loss: 2.312125\n",
      "[107,  2000] loss: 2.312245\n",
      "[107,  2200] loss: 2.312367\n",
      "[107,  2400] loss: 2.311043\n",
      "[107,  2600] loss: 2.311713\n",
      "[107,  2800] loss: 2.310389\n",
      "[107,  3000] loss: 2.309806\n",
      "[107,  3200] loss: 2.311650\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[108,   200] loss: 2.312854\n",
      "[108,   400] loss: 2.310400\n",
      "[108,   600] loss: 2.311058\n",
      "[108,   800] loss: 2.311241\n",
      "[108,  1000] loss: 2.312680\n",
      "[108,  1200] loss: 2.311091\n",
      "[108,  1400] loss: 2.311580\n",
      "[108,  1600] loss: 2.311724\n",
      "[108,  1800] loss: 2.312102\n",
      "[108,  2000] loss: 2.311634\n",
      "[108,  2200] loss: 2.311315\n",
      "[108,  2400] loss: 2.310485\n",
      "[108,  2600] loss: 2.310293\n",
      "[108,  2800] loss: 2.311032\n",
      "[108,  3000] loss: 2.311427\n",
      "[108,  3200] loss: 2.310474\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[109,   200] loss: 2.312692\n",
      "[109,   400] loss: 2.310929\n",
      "[109,   600] loss: 2.312176\n",
      "[109,   800] loss: 2.310865\n",
      "[109,  1000] loss: 2.313657\n",
      "[109,  1200] loss: 2.312277\n",
      "[109,  1400] loss: 2.310269\n",
      "[109,  1600] loss: 2.311562\n",
      "[109,  1800] loss: 2.311561\n",
      "[109,  2000] loss: 2.310664\n",
      "[109,  2200] loss: 2.312010\n",
      "[109,  2400] loss: 2.311618\n",
      "[109,  2600] loss: 2.310006\n",
      "[109,  2800] loss: 2.311709\n",
      "[109,  3000] loss: 2.310559\n",
      "[109,  3200] loss: 2.309404\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[110,   200] loss: 2.312066\n",
      "[110,   400] loss: 2.310050\n",
      "[110,   600] loss: 2.310348\n",
      "[110,   800] loss: 2.311286\n",
      "[110,  1000] loss: 2.311697\n",
      "[110,  1200] loss: 2.310385\n",
      "[110,  1400] loss: 2.311378\n",
      "[110,  1600] loss: 2.311417\n",
      "[110,  1800] loss: 2.311261\n",
      "[110,  2000] loss: 2.311858\n",
      "[110,  2200] loss: 2.315152\n",
      "[110,  2400] loss: 2.311537\n",
      "[110,  2600] loss: 2.310654\n",
      "[110,  2800] loss: 2.309478\n",
      "[110,  3000] loss: 2.311753\n",
      "[110,  3200] loss: 2.311540\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[111,   200] loss: 2.312619\n",
      "[111,   400] loss: 2.311901\n",
      "[111,   600] loss: 2.312025\n",
      "[111,   800] loss: 2.310435\n",
      "[111,  1000] loss: 2.309667\n",
      "[111,  1200] loss: 2.311941\n",
      "[111,  1400] loss: 2.311792\n",
      "[111,  1600] loss: 2.311496\n",
      "[111,  1800] loss: 2.314447\n",
      "[111,  2000] loss: 2.312389\n",
      "[111,  2200] loss: 2.312738\n",
      "[111,  2400] loss: 2.311661\n",
      "[111,  2600] loss: 2.311243\n",
      "[111,  2800] loss: 2.313060\n",
      "[111,  3000] loss: 2.309678\n",
      "[111,  3200] loss: 2.310917\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[112,   200] loss: 2.310789\n",
      "[112,   400] loss: 2.310764\n",
      "[112,   600] loss: 2.310646\n",
      "[112,   800] loss: 2.310016\n",
      "[112,  1000] loss: 2.311000\n",
      "[112,  1200] loss: 2.310848\n",
      "[112,  1400] loss: 2.311753\n",
      "[112,  1600] loss: 2.310705\n",
      "[112,  1800] loss: 2.309547\n",
      "[112,  2000] loss: 2.311484\n",
      "[112,  2200] loss: 2.312825\n",
      "[112,  2400] loss: 2.313217\n",
      "[112,  2600] loss: 2.313460\n",
      "[112,  2800] loss: 2.311241\n",
      "[112,  3000] loss: 2.311594\n",
      "[112,  3200] loss: 2.310637\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[113,   200] loss: 2.311549\n",
      "[113,   400] loss: 2.312192\n",
      "[113,   600] loss: 2.312587\n",
      "[113,   800] loss: 2.311568\n",
      "[113,  1000] loss: 2.311841\n",
      "[113,  1200] loss: 2.310912\n",
      "[113,  1400] loss: 2.310534\n",
      "[113,  1600] loss: 2.311854\n",
      "[113,  1800] loss: 2.311549\n",
      "[113,  2000] loss: 2.310739\n",
      "[113,  2200] loss: 2.310565\n",
      "[113,  2400] loss: 2.310648\n",
      "[113,  2600] loss: 2.312664\n",
      "[113,  2800] loss: 2.314016\n",
      "[113,  3000] loss: 2.312433\n",
      "[113,  3200] loss: 2.312515\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[114,   200] loss: 2.310849\n",
      "[114,   400] loss: 2.312848\n",
      "[114,   600] loss: 2.312577\n",
      "[114,   800] loss: 2.313498\n",
      "[114,  1000] loss: 2.309608\n",
      "[114,  1200] loss: 2.310710\n",
      "[114,  1400] loss: 2.311577\n",
      "[114,  1600] loss: 2.310026\n",
      "[114,  1800] loss: 2.311379\n",
      "[114,  2000] loss: 2.311519\n",
      "[114,  2200] loss: 2.310982\n",
      "[114,  2400] loss: 2.312898\n",
      "[114,  2600] loss: 2.313374\n",
      "[114,  2800] loss: 2.310780\n",
      "[114,  3000] loss: 2.310178\n",
      "[114,  3200] loss: 2.312395\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[115,   200] loss: 2.312744\n",
      "[115,   400] loss: 2.310071\n",
      "[115,   600] loss: 2.314042\n",
      "[115,   800] loss: 2.311747\n",
      "[115,  1000] loss: 2.311234\n",
      "[115,  1200] loss: 2.311359\n",
      "[115,  1400] loss: 2.312900\n",
      "[115,  1600] loss: 2.310672\n",
      "[115,  1800] loss: 2.315519\n",
      "[115,  2000] loss: 2.310817\n",
      "[115,  2200] loss: 2.310545\n",
      "[115,  2400] loss: 2.311885\n",
      "[115,  2600] loss: 2.309870\n",
      "[115,  2800] loss: 2.312251\n",
      "[115,  3000] loss: 2.313925\n",
      "[115,  3200] loss: 2.312988\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[116,   200] loss: 2.311986\n",
      "[116,   400] loss: 2.312591\n",
      "[116,   600] loss: 2.311967\n",
      "[116,   800] loss: 2.312383\n",
      "[116,  1000] loss: 2.310041\n",
      "[116,  1200] loss: 2.311062\n",
      "[116,  1400] loss: 2.311257\n",
      "[116,  1600] loss: 2.310392\n",
      "[116,  1800] loss: 2.310751\n",
      "[116,  2000] loss: 2.311607\n",
      "[116,  2200] loss: 2.312686\n",
      "[116,  2400] loss: 2.311935\n",
      "[116,  2600] loss: 2.311494\n",
      "[116,  2800] loss: 2.310813\n",
      "[116,  3000] loss: 2.309862\n",
      "[116,  3200] loss: 2.312855\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[117,   200] loss: 2.311727\n",
      "[117,   400] loss: 2.310417\n",
      "[117,   600] loss: 2.311624\n",
      "[117,   800] loss: 2.311280\n",
      "[117,  1000] loss: 2.312084\n",
      "[117,  1200] loss: 2.311180\n",
      "[117,  1400] loss: 2.311587\n",
      "[117,  1600] loss: 2.312208\n",
      "[117,  1800] loss: 2.310672\n",
      "[117,  2000] loss: 2.309995\n",
      "[117,  2200] loss: 2.313116\n",
      "[117,  2400] loss: 2.314325\n",
      "[117,  2600] loss: 2.310653\n",
      "[117,  2800] loss: 2.312571\n",
      "[117,  3000] loss: 2.311420\n",
      "[117,  3200] loss: 2.311895\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[118,   200] loss: 2.312491\n",
      "[118,   400] loss: 2.311381\n",
      "[118,   600] loss: 2.311964\n",
      "[118,   800] loss: 2.311338\n",
      "[118,  1000] loss: 2.312796\n",
      "[118,  1200] loss: 2.311279\n",
      "[118,  1400] loss: 2.310739\n",
      "[118,  1600] loss: 2.311854\n",
      "[118,  1800] loss: 2.311105\n",
      "[118,  2000] loss: 2.311204\n",
      "[118,  2200] loss: 2.311661\n",
      "[118,  2400] loss: 2.315026\n",
      "[118,  2600] loss: 2.311980\n",
      "[118,  2800] loss: 2.312378\n",
      "[118,  3000] loss: 2.313056\n",
      "[118,  3200] loss: 2.313168\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[119,   200] loss: 2.310859\n",
      "[119,   400] loss: 2.310828\n",
      "[119,   600] loss: 2.310680\n",
      "[119,   800] loss: 2.313509\n",
      "[119,  1000] loss: 2.309520\n",
      "[119,  1200] loss: 2.311504\n",
      "[119,  1400] loss: 2.311351\n",
      "[119,  1600] loss: 2.311915\n",
      "[119,  1800] loss: 2.310931\n",
      "[119,  2000] loss: 2.312582\n",
      "[119,  2200] loss: 2.310324\n",
      "[119,  2400] loss: 2.311486\n",
      "[119,  2600] loss: 2.311342\n",
      "[119,  2800] loss: 2.311030\n",
      "[119,  3000] loss: 2.312176\n",
      "[119,  3200] loss: 2.311192\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[120,   200] loss: 2.310169\n",
      "[120,   400] loss: 2.310247\n",
      "[120,   600] loss: 2.311857\n",
      "[120,   800] loss: 2.310477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120,  1000] loss: 2.309741\n",
      "[120,  1200] loss: 2.311725\n",
      "[120,  1400] loss: 2.310497\n",
      "[120,  1600] loss: 2.312148\n",
      "[120,  1800] loss: 2.309974\n",
      "[120,  2000] loss: 2.311808\n",
      "[120,  2200] loss: 2.311366\n",
      "[120,  2400] loss: 2.311512\n",
      "[120,  2600] loss: 2.311542\n",
      "[120,  2800] loss: 2.311102\n",
      "[120,  3000] loss: 2.311060\n",
      "[120,  3200] loss: 2.308859\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[121,   200] loss: 2.310555\n",
      "[121,   400] loss: 2.310347\n",
      "[121,   600] loss: 2.309998\n",
      "[121,   800] loss: 2.310800\n",
      "[121,  1000] loss: 2.312860\n",
      "[121,  1200] loss: 2.310216\n",
      "[121,  1400] loss: 2.311115\n",
      "[121,  1600] loss: 2.310348\n",
      "[121,  1800] loss: 2.310404\n",
      "[121,  2000] loss: 2.312776\n",
      "[121,  2200] loss: 2.310430\n",
      "[121,  2400] loss: 2.311221\n",
      "[121,  2600] loss: 2.311749\n",
      "[121,  2800] loss: 2.310736\n",
      "[121,  3000] loss: 2.312220\n",
      "[121,  3200] loss: 2.312523\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[122,   200] loss: 2.312894\n",
      "[122,   400] loss: 2.310970\n",
      "[122,   600] loss: 2.312460\n",
      "[122,   800] loss: 2.312577\n",
      "[122,  1000] loss: 2.312788\n",
      "[122,  1200] loss: 2.311412\n",
      "[122,  1400] loss: 2.310991\n",
      "[122,  1600] loss: 2.310904\n",
      "[122,  1800] loss: 2.309687\n",
      "[122,  2000] loss: 2.311053\n",
      "[122,  2200] loss: 2.312330\n",
      "[122,  2400] loss: 2.312380\n",
      "[122,  2600] loss: 2.311916\n",
      "[122,  2800] loss: 2.312047\n",
      "[122,  3000] loss: 2.311631\n",
      "[122,  3200] loss: 2.311060\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[123,   200] loss: 2.311341\n",
      "[123,   400] loss: 2.309869\n",
      "[123,   600] loss: 2.313517\n",
      "[123,   800] loss: 2.310726\n",
      "[123,  1000] loss: 2.313020\n",
      "[123,  1200] loss: 2.313721\n",
      "[123,  1400] loss: 2.311418\n",
      "[123,  1600] loss: 2.313741\n",
      "[123,  1800] loss: 2.312473\n",
      "[123,  2000] loss: 2.310985\n",
      "[123,  2200] loss: 2.311393\n",
      "[123,  2400] loss: 2.311947\n",
      "[123,  2600] loss: 2.310553\n",
      "[123,  2800] loss: 2.312655\n",
      "[123,  3000] loss: 2.312762\n",
      "[123,  3200] loss: 2.312245\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[124,   200] loss: 2.310828\n",
      "[124,   400] loss: 2.312184\n",
      "[124,   600] loss: 2.311135\n",
      "[124,   800] loss: 2.310635\n",
      "[124,  1000] loss: 2.312235\n",
      "[124,  1200] loss: 2.310798\n",
      "[124,  1400] loss: 2.311676\n",
      "[124,  1600] loss: 2.311645\n",
      "[124,  1800] loss: 2.311791\n",
      "[124,  2000] loss: 2.310172\n",
      "[124,  2200] loss: 2.312852\n",
      "[124,  2400] loss: 2.311561\n",
      "[124,  2600] loss: 2.311334\n",
      "[124,  2800] loss: 2.312172\n",
      "[124,  3000] loss: 2.310930\n",
      "[124,  3200] loss: 2.311446\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[125,   200] loss: 2.309123\n",
      "[125,   400] loss: 2.313143\n",
      "[125,   600] loss: 2.310805\n",
      "[125,   800] loss: 2.311076\n",
      "[125,  1000] loss: 2.311371\n",
      "[125,  1200] loss: 2.310273\n",
      "[125,  1400] loss: 2.309717\n",
      "[125,  1600] loss: 2.311819\n",
      "[125,  1800] loss: 2.312564\n",
      "[125,  2000] loss: 2.312032\n",
      "[125,  2200] loss: 2.311729\n",
      "[125,  2400] loss: 2.311389\n",
      "[125,  2600] loss: 2.311880\n",
      "[125,  2800] loss: 2.310421\n",
      "[125,  3000] loss: 2.312796\n",
      "[125,  3200] loss: 2.309763\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[126,   200] loss: 2.310653\n",
      "[126,   400] loss: 2.310215\n",
      "[126,   600] loss: 2.312562\n",
      "[126,   800] loss: 2.311081\n",
      "[126,  1000] loss: 2.310822\n",
      "[126,  1200] loss: 2.311320\n",
      "[126,  1400] loss: 2.310761\n",
      "[126,  1600] loss: 2.312171\n",
      "[126,  1800] loss: 2.312846\n",
      "[126,  2000] loss: 2.312335\n",
      "[126,  2200] loss: 2.310177\n",
      "[126,  2400] loss: 2.310325\n",
      "[126,  2600] loss: 2.311154\n",
      "[126,  2800] loss: 2.311848\n",
      "[126,  3000] loss: 2.310947\n",
      "[126,  3200] loss: 2.313283\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[127,   200] loss: 2.312278\n",
      "[127,   400] loss: 2.313793\n",
      "[127,   600] loss: 2.311382\n",
      "[127,   800] loss: 2.313446\n",
      "[127,  1000] loss: 2.311746\n",
      "[127,  1200] loss: 2.311446\n",
      "[127,  1400] loss: 2.309644\n",
      "[127,  1600] loss: 2.311680\n",
      "[127,  1800] loss: 2.310415\n",
      "[127,  2000] loss: 2.310177\n",
      "[127,  2200] loss: 2.309282\n",
      "[127,  2400] loss: 2.311679\n",
      "[127,  2600] loss: 2.311450\n",
      "[127,  2800] loss: 2.313396\n",
      "[127,  3000] loss: 2.311742\n",
      "[127,  3200] loss: 2.311798\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[128,   200] loss: 2.312298\n",
      "[128,   400] loss: 2.313191\n",
      "[128,   600] loss: 2.312394\n",
      "[128,   800] loss: 2.309164\n",
      "[128,  1000] loss: 2.311379\n",
      "[128,  1200] loss: 2.311301\n",
      "[128,  1400] loss: 2.312166\n",
      "[128,  1600] loss: 2.310358\n",
      "[128,  1800] loss: 2.311689\n",
      "[128,  2000] loss: 2.309962\n",
      "[128,  2200] loss: 2.311524\n",
      "[128,  2400] loss: 2.312145\n",
      "[128,  2600] loss: 2.311081\n",
      "[128,  2800] loss: 2.312488\n",
      "[128,  3000] loss: 2.309976\n",
      "[128,  3200] loss: 2.311901\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[129,   200] loss: 2.310533\n",
      "[129,   400] loss: 2.312722\n",
      "[129,   600] loss: 2.309752\n",
      "[129,   800] loss: 2.310683\n",
      "[129,  1000] loss: 2.310498\n",
      "[129,  1200] loss: 2.310637\n",
      "[129,  1400] loss: 2.311860\n",
      "[129,  1600] loss: 2.311817\n",
      "[129,  1800] loss: 2.311093\n",
      "[129,  2000] loss: 2.312062\n",
      "[129,  2200] loss: 2.313014\n",
      "[129,  2400] loss: 2.313541\n",
      "[129,  2600] loss: 2.311929\n",
      "[129,  2800] loss: 2.310652\n",
      "[129,  3000] loss: 2.309650\n",
      "[129,  3200] loss: 2.312465\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[130,   200] loss: 2.311866\n",
      "[130,   400] loss: 2.311399\n",
      "[130,   600] loss: 2.311755\n",
      "[130,   800] loss: 2.312011\n",
      "[130,  1000] loss: 2.311062\n",
      "[130,  1200] loss: 2.310217\n",
      "[130,  1400] loss: 2.311376\n",
      "[130,  1600] loss: 2.311452\n",
      "[130,  1800] loss: 2.311942\n",
      "[130,  2000] loss: 2.310850\n",
      "[130,  2200] loss: 2.311613\n",
      "[130,  2400] loss: 2.310413\n",
      "[130,  2600] loss: 2.310258\n",
      "[130,  2800] loss: 2.310955\n",
      "[130,  3000] loss: 2.310873\n",
      "[130,  3200] loss: 2.311578\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[131,   200] loss: 2.312324\n",
      "[131,   400] loss: 2.311629\n",
      "[131,   600] loss: 2.311503\n",
      "[131,   800] loss: 2.312275\n",
      "[131,  1000] loss: 2.310655\n",
      "[131,  1200] loss: 2.309873\n",
      "[131,  1400] loss: 2.310754\n",
      "[131,  1600] loss: 2.311653\n",
      "[131,  1800] loss: 2.309924\n",
      "[131,  2000] loss: 2.311519\n",
      "[131,  2200] loss: 2.310953\n",
      "[131,  2400] loss: 2.312233\n",
      "[131,  2600] loss: 2.311812\n",
      "[131,  2800] loss: 2.314733\n",
      "[131,  3000] loss: 2.313445\n",
      "[131,  3200] loss: 2.311810\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[132,   200] loss: 2.310413\n",
      "[132,   400] loss: 2.310304\n",
      "[132,   600] loss: 2.313222\n",
      "[132,   800] loss: 2.310223\n",
      "[132,  1000] loss: 2.311596\n",
      "[132,  1200] loss: 2.310861\n",
      "[132,  1400] loss: 2.311444\n",
      "[132,  1600] loss: 2.312320\n",
      "[132,  1800] loss: 2.310634\n",
      "[132,  2000] loss: 2.311070\n",
      "[132,  2200] loss: 2.311522\n",
      "[132,  2400] loss: 2.309885\n",
      "[132,  2600] loss: 2.311788\n",
      "[132,  2800] loss: 2.312044\n",
      "[132,  3000] loss: 2.311248\n",
      "[132,  3200] loss: 2.311282\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[133,   200] loss: 2.310295\n",
      "[133,   400] loss: 2.312446\n",
      "[133,   600] loss: 2.310323\n",
      "[133,   800] loss: 2.309974\n",
      "[133,  1000] loss: 2.310529\n",
      "[133,  1200] loss: 2.309962\n",
      "[133,  1400] loss: 2.312428\n",
      "[133,  1600] loss: 2.312688\n",
      "[133,  1800] loss: 2.310417\n",
      "[133,  2000] loss: 2.312020\n",
      "[133,  2200] loss: 2.312680\n",
      "[133,  2400] loss: 2.310197\n",
      "[133,  2600] loss: 2.310885\n",
      "[133,  2800] loss: 2.311663\n",
      "[133,  3000] loss: 2.310925\n",
      "[133,  3200] loss: 2.310730\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[134,   200] loss: 2.311302\n",
      "[134,   400] loss: 2.309472\n",
      "[134,   600] loss: 2.313148\n",
      "[134,   800] loss: 2.312584\n",
      "[134,  1000] loss: 2.311960\n",
      "[134,  1200] loss: 2.310491\n",
      "[134,  1400] loss: 2.310839\n",
      "[134,  1600] loss: 2.310721\n",
      "[134,  1800] loss: 2.311819\n",
      "[134,  2000] loss: 2.309892\n",
      "[134,  2200] loss: 2.312308\n",
      "[134,  2400] loss: 2.309437\n",
      "[134,  2600] loss: 2.311969\n",
      "[134,  2800] loss: 2.310812\n",
      "[134,  3000] loss: 2.311085\n",
      "[134,  3200] loss: 2.312374\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[135,   200] loss: 2.311706\n",
      "[135,   400] loss: 2.311730\n",
      "[135,   600] loss: 2.311668\n",
      "[135,   800] loss: 2.310342\n",
      "[135,  1000] loss: 2.311291\n",
      "[135,  1200] loss: 2.311183\n",
      "[135,  1400] loss: 2.310463\n",
      "[135,  1600] loss: 2.310410\n",
      "[135,  1800] loss: 2.312263\n",
      "[135,  2000] loss: 2.309672\n",
      "[135,  2200] loss: 2.311449\n",
      "[135,  2400] loss: 2.312607\n",
      "[135,  2600] loss: 2.310905\n",
      "[135,  2800] loss: 2.310629\n",
      "[135,  3000] loss: 2.310528\n",
      "[135,  3200] loss: 2.312476\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[136,   200] loss: 2.310776\n",
      "[136,   400] loss: 2.312637\n",
      "[136,   600] loss: 2.312637\n",
      "[136,   800] loss: 2.312218\n",
      "[136,  1000] loss: 2.310342\n",
      "[136,  1200] loss: 2.309605\n",
      "[136,  1400] loss: 2.311006\n",
      "[136,  1600] loss: 2.313225\n",
      "[136,  1800] loss: 2.310475\n",
      "[136,  2000] loss: 2.310170\n",
      "[136,  2200] loss: 2.311131\n",
      "[136,  2400] loss: 2.312948\n",
      "[136,  2600] loss: 2.310677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136,  2800] loss: 2.311000\n",
      "[136,  3000] loss: 2.313384\n",
      "[136,  3200] loss: 2.310585\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[137,   200] loss: 2.311494\n",
      "[137,   400] loss: 2.312349\n",
      "[137,   600] loss: 2.311114\n",
      "[137,   800] loss: 2.310939\n",
      "[137,  1000] loss: 2.313192\n",
      "[137,  1200] loss: 2.312501\n",
      "[137,  1400] loss: 2.310375\n",
      "[137,  1600] loss: 2.311111\n",
      "[137,  1800] loss: 2.311508\n",
      "[137,  2000] loss: 2.311672\n",
      "[137,  2200] loss: 2.310621\n",
      "[137,  2400] loss: 2.313951\n",
      "[137,  2600] loss: 2.314656\n",
      "[137,  2800] loss: 2.313247\n",
      "[137,  3000] loss: 2.313004\n",
      "[137,  3200] loss: 2.310663\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[138,   200] loss: 2.311782\n",
      "[138,   400] loss: 2.310156\n",
      "[138,   600] loss: 2.310750\n",
      "[138,   800] loss: 2.312976\n",
      "[138,  1000] loss: 2.311587\n",
      "[138,  1200] loss: 2.311051\n",
      "[138,  1400] loss: 2.311865\n",
      "[138,  1600] loss: 2.311746\n",
      "[138,  1800] loss: 2.311635\n",
      "[138,  2000] loss: 2.311023\n",
      "[138,  2200] loss: 2.316046\n",
      "[138,  2400] loss: 2.312977\n",
      "[138,  2600] loss: 2.310209\n",
      "[138,  2800] loss: 2.310874\n",
      "[138,  3000] loss: 2.310761\n",
      "[138,  3200] loss: 2.313202\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[139,   200] loss: 2.312002\n",
      "[139,   400] loss: 2.310328\n",
      "[139,   600] loss: 2.311367\n",
      "[139,   800] loss: 2.312559\n",
      "[139,  1000] loss: 2.312973\n",
      "[139,  1200] loss: 2.310657\n",
      "[139,  1400] loss: 2.309254\n",
      "[139,  1600] loss: 2.310475\n",
      "[139,  1800] loss: 2.311948\n",
      "[139,  2000] loss: 2.311185\n",
      "[139,  2200] loss: 2.313307\n",
      "[139,  2400] loss: 2.313551\n",
      "[139,  2600] loss: 2.311853\n",
      "[139,  2800] loss: 2.309790\n",
      "[139,  3000] loss: 2.310633\n",
      "[139,  3200] loss: 2.311110\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[140,   200] loss: 2.311836\n",
      "[140,   400] loss: 2.311711\n",
      "[140,   600] loss: 2.312066\n",
      "[140,   800] loss: 2.311939\n",
      "[140,  1000] loss: 2.310600\n",
      "[140,  1200] loss: 2.312099\n",
      "[140,  1400] loss: 2.312899\n",
      "[140,  1600] loss: 2.310982\n",
      "[140,  1800] loss: 2.313164\n",
      "[140,  2000] loss: 2.310447\n",
      "[140,  2200] loss: 2.311072\n",
      "[140,  2400] loss: 2.311778\n",
      "[140,  2600] loss: 2.313048\n",
      "[140,  2800] loss: 2.312603\n",
      "[140,  3000] loss: 2.310922\n",
      "[140,  3200] loss: 2.311241\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[141,   200] loss: 2.312349\n",
      "[141,   400] loss: 2.310814\n",
      "[141,   600] loss: 2.312665\n",
      "[141,   800] loss: 2.312040\n",
      "[141,  1000] loss: 2.311320\n",
      "[141,  1200] loss: 2.309716\n",
      "[141,  1400] loss: 2.311153\n",
      "[141,  1600] loss: 2.310794\n",
      "[141,  1800] loss: 2.312028\n",
      "[141,  2000] loss: 2.311777\n",
      "[141,  2200] loss: 2.311163\n",
      "[141,  2400] loss: 2.312096\n",
      "[141,  2600] loss: 2.311279\n",
      "[141,  2800] loss: 2.313004\n",
      "[141,  3000] loss: 2.312126\n",
      "[141,  3200] loss: 2.311557\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[142,   200] loss: 2.312834\n",
      "[142,   400] loss: 2.310508\n",
      "[142,   600] loss: 2.312073\n",
      "[142,   800] loss: 2.311898\n",
      "[142,  1000] loss: 2.313723\n",
      "[142,  1200] loss: 2.312526\n",
      "[142,  1400] loss: 2.312500\n",
      "[142,  1600] loss: 2.310640\n",
      "[142,  1800] loss: 2.315121\n",
      "[142,  2000] loss: 2.310943\n",
      "[142,  2200] loss: 2.312253\n",
      "[142,  2400] loss: 2.310903\n",
      "[142,  2600] loss: 2.312310\n",
      "[142,  2800] loss: 2.309446\n",
      "[142,  3000] loss: 2.311245\n",
      "[142,  3200] loss: 2.313099\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[143,   200] loss: 2.311409\n",
      "[143,   400] loss: 2.311952\n",
      "[143,   600] loss: 2.311759\n",
      "[143,   800] loss: 2.310537\n",
      "[143,  1000] loss: 2.309036\n",
      "[143,  1200] loss: 2.312576\n",
      "[143,  1400] loss: 2.310758\n",
      "[143,  1600] loss: 2.310912\n",
      "[143,  1800] loss: 2.312305\n",
      "[143,  2000] loss: 2.310254\n",
      "[143,  2200] loss: 2.311785\n",
      "[143,  2400] loss: 2.312699\n",
      "[143,  2600] loss: 2.310661\n",
      "[143,  2800] loss: 2.312586\n",
      "[143,  3000] loss: 2.312617\n",
      "[143,  3200] loss: 2.311123\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[144,   200] loss: 2.311649\n",
      "[144,   400] loss: 2.310175\n",
      "[144,   600] loss: 2.309677\n",
      "[144,   800] loss: 2.312189\n",
      "[144,  1000] loss: 2.310327\n",
      "[144,  1200] loss: 2.311300\n",
      "[144,  1400] loss: 2.311605\n",
      "[144,  1600] loss: 2.312018\n",
      "[144,  1800] loss: 2.310729\n",
      "[144,  2000] loss: 2.311430\n",
      "[144,  2200] loss: 2.313225\n",
      "[144,  2400] loss: 2.312999\n",
      "[144,  2600] loss: 2.312372\n",
      "[144,  2800] loss: 2.310434\n",
      "[144,  3000] loss: 2.311264\n",
      "[144,  3200] loss: 2.310811\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[145,   200] loss: 2.312283\n",
      "[145,   400] loss: 2.310448\n",
      "[145,   600] loss: 2.310851\n",
      "[145,   800] loss: 2.312044\n",
      "[145,  1000] loss: 2.311270\n",
      "[145,  1200] loss: 2.309984\n",
      "[145,  1400] loss: 2.310137\n",
      "[145,  1600] loss: 2.312979\n",
      "[145,  1800] loss: 2.312950\n",
      "[145,  2000] loss: 2.309424\n",
      "[145,  2200] loss: 2.311065\n",
      "[145,  2400] loss: 2.312747\n",
      "[145,  2600] loss: 2.309671\n",
      "[145,  2800] loss: 2.311115\n",
      "[145,  3000] loss: 2.312572\n",
      "[145,  3200] loss: 2.312765\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[146,   200] loss: 2.311047\n",
      "[146,   400] loss: 2.312768\n",
      "[146,   600] loss: 2.310510\n",
      "[146,   800] loss: 2.311452\n",
      "[146,  1000] loss: 2.309766\n",
      "[146,  1200] loss: 2.311586\n",
      "[146,  1400] loss: 2.313830\n",
      "[146,  1600] loss: 2.312339\n",
      "[146,  1800] loss: 2.310581\n",
      "[146,  2000] loss: 2.310222\n",
      "[146,  2200] loss: 2.311505\n",
      "[146,  2400] loss: 2.311856\n",
      "[146,  2600] loss: 2.311779\n",
      "[146,  2800] loss: 2.311892\n",
      "[146,  3000] loss: 2.314401\n",
      "[146,  3200] loss: 2.311466\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[147,   200] loss: 2.313023\n",
      "[147,   400] loss: 2.311126\n",
      "[147,   600] loss: 2.312506\n",
      "[147,   800] loss: 2.311027\n",
      "[147,  1000] loss: 2.310301\n",
      "[147,  1200] loss: 2.311487\n",
      "[147,  1400] loss: 2.312307\n",
      "[147,  1600] loss: 2.310064\n",
      "[147,  1800] loss: 2.310960\n",
      "[147,  2000] loss: 2.312187\n",
      "[147,  2200] loss: 2.309758\n",
      "[147,  2400] loss: 2.312309\n",
      "[147,  2600] loss: 2.313246\n",
      "[147,  2800] loss: 2.310433\n",
      "[147,  3000] loss: 2.308982\n",
      "[147,  3200] loss: 2.312306\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[148,   200] loss: 2.310647\n",
      "[148,   400] loss: 2.311249\n",
      "[148,   600] loss: 2.310872\n",
      "[148,   800] loss: 2.312438\n",
      "[148,  1000] loss: 2.311484\n",
      "[148,  1200] loss: 2.311425\n",
      "[148,  1400] loss: 2.313890\n",
      "[148,  1600] loss: 2.310521\n",
      "[148,  1800] loss: 2.311792\n",
      "[148,  2000] loss: 2.311434\n",
      "[148,  2200] loss: 2.312959\n",
      "[148,  2400] loss: 2.311107\n",
      "[148,  2600] loss: 2.312302\n",
      "[148,  2800] loss: 2.310215\n",
      "[148,  3000] loss: 2.309675\n",
      "[148,  3200] loss: 2.313915\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[149,   200] loss: 2.312173\n",
      "[149,   400] loss: 2.313293\n",
      "[149,   600] loss: 2.312643\n",
      "[149,   800] loss: 2.310201\n",
      "[149,  1000] loss: 2.310133\n",
      "[149,  1200] loss: 2.311817\n",
      "[149,  1400] loss: 2.311289\n",
      "[149,  1600] loss: 2.312685\n",
      "[149,  1800] loss: 2.312764\n",
      "[149,  2000] loss: 2.310366\n",
      "[149,  2200] loss: 2.312269\n",
      "[149,  2400] loss: 2.311395\n",
      "[149,  2600] loss: 2.311058\n",
      "[149,  2800] loss: 2.309811\n",
      "[149,  3000] loss: 2.313668\n",
      "[149,  3200] loss: 2.310254\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[150,   200] loss: 2.314728\n",
      "[150,   400] loss: 2.311385\n",
      "[150,   600] loss: 2.312438\n",
      "[150,   800] loss: 2.309056\n",
      "[150,  1000] loss: 2.310625\n",
      "[150,  1200] loss: 2.309707\n",
      "[150,  1400] loss: 2.314514\n",
      "[150,  1600] loss: 2.309451\n",
      "[150,  1800] loss: 2.309605\n",
      "[150,  2000] loss: 2.310490\n",
      "[150,  2200] loss: 2.312705\n",
      "[150,  2400] loss: 2.313222\n",
      "[150,  2600] loss: 2.311533\n",
      "[150,  2800] loss: 2.312746\n",
      "[150,  3000] loss: 2.310531\n",
      "[150,  3200] loss: 2.311727\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[151,   200] loss: 2.305093\n",
      "[151,   400] loss: 2.303459\n",
      "[151,   600] loss: 2.302993\n",
      "[151,   800] loss: 2.303297\n",
      "[151,  1000] loss: 2.303459\n",
      "[151,  1200] loss: 2.303110\n",
      "[151,  1400] loss: 2.303068\n",
      "[151,  1600] loss: 2.303379\n",
      "[151,  1800] loss: 2.303615\n",
      "[151,  2000] loss: 2.303765\n",
      "[151,  2200] loss: 2.303659\n",
      "[151,  2400] loss: 2.303460\n",
      "[151,  2600] loss: 2.303596\n",
      "[151,  2800] loss: 2.303464\n",
      "[151,  3000] loss: 2.303210\n",
      "[151,  3200] loss: 2.303962\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[152,   200] loss: 2.303724\n",
      "[152,   400] loss: 2.303580\n",
      "[152,   600] loss: 2.303425\n",
      "[152,   800] loss: 2.303448\n",
      "[152,  1000] loss: 2.303447\n",
      "[152,  1200] loss: 2.303233\n",
      "[152,  1400] loss: 2.303400\n",
      "[152,  1600] loss: 2.303549\n",
      "[152,  1800] loss: 2.303035\n",
      "[152,  2000] loss: 2.303377\n",
      "[152,  2200] loss: 2.303553\n",
      "[152,  2400] loss: 2.303531\n",
      "[152,  2600] loss: 2.303045\n",
      "[152,  2800] loss: 2.303605\n",
      "[152,  3000] loss: 2.303417\n",
      "[152,  3200] loss: 2.303368\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[153,   200] loss: 2.303638\n",
      "[153,   400] loss: 2.303423\n",
      "[153,   600] loss: 2.303529\n",
      "[153,   800] loss: 2.302695\n",
      "[153,  1000] loss: 2.303697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153,  1200] loss: 2.303473\n",
      "[153,  1400] loss: 2.303438\n",
      "[153,  1600] loss: 2.303713\n",
      "[153,  1800] loss: 2.303228\n",
      "[153,  2000] loss: 2.303526\n",
      "[153,  2200] loss: 2.303622\n",
      "[153,  2400] loss: 2.303749\n",
      "[153,  2600] loss: 2.303283\n",
      "[153,  2800] loss: 2.303376\n",
      "[153,  3000] loss: 2.303074\n",
      "[153,  3200] loss: 2.303448\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[154,   200] loss: 2.303739\n",
      "[154,   400] loss: 2.303464\n",
      "[154,   600] loss: 2.303707\n",
      "[154,   800] loss: 2.303904\n",
      "[154,  1000] loss: 2.303325\n",
      "[154,  1200] loss: 2.303546\n",
      "[154,  1400] loss: 2.303731\n",
      "[154,  1600] loss: 2.303554\n",
      "[154,  1800] loss: 2.303333\n",
      "[154,  2000] loss: 2.303491\n",
      "[154,  2200] loss: 2.303032\n",
      "[154,  2400] loss: 2.303364\n",
      "[154,  2600] loss: 2.303497\n",
      "[154,  2800] loss: 2.303537\n",
      "[154,  3000] loss: 2.303588\n",
      "[154,  3200] loss: 2.303326\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[155,   200] loss: 2.303807\n",
      "[155,   400] loss: 2.303502\n",
      "[155,   600] loss: 2.303320\n",
      "[155,   800] loss: 2.303925\n",
      "[155,  1000] loss: 2.303464\n",
      "[155,  1200] loss: 2.303479\n",
      "[155,  1400] loss: 2.303039\n",
      "[155,  1600] loss: 2.303701\n",
      "[155,  1800] loss: 2.303361\n",
      "[155,  2000] loss: 2.303229\n",
      "[155,  2200] loss: 2.303235\n",
      "[155,  2400] loss: 2.303326\n",
      "[155,  2600] loss: 2.303514\n",
      "[155,  2800] loss: 2.303197\n",
      "[155,  3000] loss: 2.303675\n",
      "[155,  3200] loss: 2.303768\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[156,   200] loss: 2.303609\n",
      "[156,   400] loss: 2.303703\n",
      "[156,   600] loss: 2.303841\n",
      "[156,   800] loss: 2.303782\n",
      "[156,  1000] loss: 2.303452\n",
      "[156,  1200] loss: 2.303495\n",
      "[156,  1400] loss: 2.303644\n",
      "[156,  1600] loss: 2.303442\n",
      "[156,  1800] loss: 2.303345\n",
      "[156,  2000] loss: 2.303466\n",
      "[156,  2200] loss: 2.303475\n",
      "[156,  2400] loss: 2.303423\n",
      "[156,  2600] loss: 2.303179\n",
      "[156,  2800] loss: 2.303525\n",
      "[156,  3000] loss: 2.303525\n",
      "[156,  3200] loss: 2.303328\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[157,   200] loss: 2.303288\n",
      "[157,   400] loss: 2.303448\n",
      "[157,   600] loss: 2.303087\n",
      "[157,   800] loss: 2.303625\n",
      "[157,  1000] loss: 2.303686\n",
      "[157,  1200] loss: 2.303722\n",
      "[157,  1400] loss: 2.303346\n",
      "[157,  1600] loss: 2.303513\n",
      "[157,  1800] loss: 2.303584\n",
      "[157,  2000] loss: 2.303373\n",
      "[157,  2200] loss: 2.303422\n",
      "[157,  2400] loss: 2.303381\n",
      "[157,  2600] loss: 2.303668\n",
      "[157,  2800] loss: 2.303440\n",
      "[157,  3000] loss: 2.303473\n",
      "[157,  3200] loss: 2.303641\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[158,   200] loss: 2.303067\n",
      "[158,   400] loss: 2.303536\n",
      "[158,   600] loss: 2.303401\n",
      "[158,   800] loss: 2.303250\n",
      "[158,  1000] loss: 2.303318\n",
      "[158,  1200] loss: 2.303929\n",
      "[158,  1400] loss: 2.303695\n",
      "[158,  1600] loss: 2.303491\n",
      "[158,  1800] loss: 2.303492\n",
      "[158,  2000] loss: 2.303523\n",
      "[158,  2200] loss: 2.303293\n",
      "[158,  2400] loss: 2.303610\n",
      "[158,  2600] loss: 2.303541\n",
      "[158,  2800] loss: 2.303335\n",
      "[158,  3000] loss: 2.303734\n",
      "[158,  3200] loss: 2.302975\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[159,   200] loss: 2.303312\n",
      "[159,   400] loss: 2.303153\n",
      "[159,   600] loss: 2.303602\n",
      "[159,   800] loss: 2.303099\n",
      "[159,  1000] loss: 2.303801\n",
      "[159,  1200] loss: 2.303505\n",
      "[159,  1400] loss: 2.303758\n",
      "[159,  1600] loss: 2.303382\n",
      "[159,  1800] loss: 2.303797\n",
      "[159,  2000] loss: 2.303008\n",
      "[159,  2200] loss: 2.303478\n",
      "[159,  2400] loss: 2.303507\n",
      "[159,  2600] loss: 2.302969\n",
      "[159,  2800] loss: 2.303183\n",
      "[159,  3000] loss: 2.303518\n",
      "[159,  3200] loss: 2.303488\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[160,   200] loss: 2.303502\n",
      "[160,   400] loss: 2.303260\n",
      "[160,   600] loss: 2.303257\n",
      "[160,   800] loss: 2.304183\n",
      "[160,  1000] loss: 2.303465\n",
      "[160,  1200] loss: 2.303246\n",
      "[160,  1400] loss: 2.303374\n",
      "[160,  1600] loss: 2.303553\n",
      "[160,  1800] loss: 2.303329\n",
      "[160,  2000] loss: 2.303701\n",
      "[160,  2200] loss: 2.303670\n",
      "[160,  2400] loss: 2.303662\n",
      "[160,  2600] loss: 2.303426\n",
      "[160,  2800] loss: 2.303387\n",
      "[160,  3000] loss: 2.303402\n",
      "[160,  3200] loss: 2.303595\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[161,   200] loss: 2.303644\n",
      "[161,   400] loss: 2.303824\n",
      "[161,   600] loss: 2.303486\n",
      "[161,   800] loss: 2.303893\n",
      "[161,  1000] loss: 2.303703\n",
      "[161,  1200] loss: 2.303262\n",
      "[161,  1400] loss: 2.303477\n",
      "[161,  1600] loss: 2.303521\n",
      "[161,  1800] loss: 2.303410\n",
      "[161,  2000] loss: 2.303661\n",
      "[161,  2200] loss: 2.303589\n",
      "[161,  2400] loss: 2.303369\n",
      "[161,  2600] loss: 2.303304\n",
      "[161,  2800] loss: 2.303969\n",
      "[161,  3000] loss: 2.303099\n",
      "[161,  3200] loss: 2.303355\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[162,   200] loss: 2.303382\n",
      "[162,   400] loss: 2.303264\n",
      "[162,   600] loss: 2.303626\n",
      "[162,   800] loss: 2.303900\n",
      "[162,  1000] loss: 2.303635\n",
      "[162,  1200] loss: 2.303645\n",
      "[162,  1400] loss: 2.303841\n",
      "[162,  1600] loss: 2.303423\n",
      "[162,  1800] loss: 2.303438\n",
      "[162,  2000] loss: 2.303691\n",
      "[162,  2200] loss: 2.303585\n",
      "[162,  2400] loss: 2.303574\n",
      "[162,  2600] loss: 2.303496\n",
      "[162,  2800] loss: 2.303495\n",
      "[162,  3000] loss: 2.303595\n",
      "[162,  3200] loss: 2.303398\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[163,   200] loss: 2.303383\n",
      "[163,   400] loss: 2.303374\n",
      "[163,   600] loss: 2.303474\n",
      "[163,   800] loss: 2.303453\n",
      "[163,  1000] loss: 2.303304\n",
      "[163,  1200] loss: 2.303352\n",
      "[163,  1400] loss: 2.303352\n",
      "[163,  1600] loss: 2.303669\n",
      "[163,  1800] loss: 2.303403\n",
      "[163,  2000] loss: 2.303149\n",
      "[163,  2200] loss: 2.303633\n",
      "[163,  2400] loss: 2.303722\n",
      "[163,  2600] loss: 2.303551\n",
      "[163,  2800] loss: 2.303704\n",
      "[163,  3000] loss: 2.303462\n",
      "[163,  3200] loss: 2.303408\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[164,   200] loss: 2.303401\n",
      "[164,   400] loss: 2.303713\n",
      "[164,   600] loss: 2.303242\n",
      "[164,   800] loss: 2.303339\n",
      "[164,  1000] loss: 2.303765\n",
      "[164,  1200] loss: 2.303174\n",
      "[164,  1400] loss: 2.303558\n",
      "[164,  1600] loss: 2.303425\n",
      "[164,  1800] loss: 2.303655\n",
      "[164,  2000] loss: 2.303085\n",
      "[164,  2200] loss: 2.303596\n",
      "[164,  2400] loss: 2.303399\n",
      "[164,  2600] loss: 2.303940\n",
      "[164,  2800] loss: 2.303376\n",
      "[164,  3000] loss: 2.303716\n",
      "[164,  3200] loss: 2.303142\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[165,   200] loss: 2.303423\n",
      "[165,   400] loss: 2.302952\n",
      "[165,   600] loss: 2.303751\n",
      "[165,   800] loss: 2.303843\n",
      "[165,  1000] loss: 2.303557\n",
      "[165,  1200] loss: 2.303475\n",
      "[165,  1400] loss: 2.303478\n",
      "[165,  1600] loss: 2.303064\n",
      "[165,  1800] loss: 2.302806\n",
      "[165,  2000] loss: 2.303659\n",
      "[165,  2200] loss: 2.303152\n",
      "[165,  2400] loss: 2.303146\n",
      "[165,  2600] loss: 2.303493\n",
      "[165,  2800] loss: 2.303460\n",
      "[165,  3000] loss: 2.303373\n",
      "[165,  3200] loss: 2.303732\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[166,   200] loss: 2.303768\n",
      "[166,   400] loss: 2.303743\n",
      "[166,   600] loss: 2.303362\n",
      "[166,   800] loss: 2.303581\n",
      "[166,  1000] loss: 2.303170\n",
      "[166,  1200] loss: 2.303963\n",
      "[166,  1400] loss: 2.303361\n",
      "[166,  1600] loss: 2.303356\n",
      "[166,  1800] loss: 2.303540\n",
      "[166,  2000] loss: 2.303590\n",
      "[166,  2200] loss: 2.303287\n",
      "[166,  2400] loss: 2.303271\n",
      "[166,  2600] loss: 2.303623\n",
      "[166,  2800] loss: 2.304047\n",
      "[166,  3000] loss: 2.303918\n",
      "[166,  3200] loss: 2.303494\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[167,   200] loss: 2.303724\n",
      "[167,   400] loss: 2.303331\n",
      "[167,   600] loss: 2.304262\n",
      "[167,   800] loss: 2.303506\n",
      "[167,  1000] loss: 2.303406\n",
      "[167,  1200] loss: 2.303524\n",
      "[167,  1400] loss: 2.303653\n",
      "[167,  1600] loss: 2.303787\n",
      "[167,  1800] loss: 2.303166\n",
      "[167,  2000] loss: 2.303679\n",
      "[167,  2200] loss: 2.303617\n",
      "[167,  2400] loss: 2.303551\n",
      "[167,  2600] loss: 2.303211\n",
      "[167,  2800] loss: 2.303625\n",
      "[167,  3000] loss: 2.303854\n",
      "[167,  3200] loss: 2.303723\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[168,   200] loss: 2.303193\n",
      "[168,   400] loss: 2.303720\n",
      "[168,   600] loss: 2.303291\n",
      "[168,   800] loss: 2.303550\n",
      "[168,  1000] loss: 2.303633\n",
      "[168,  1200] loss: 2.303718\n",
      "[168,  1400] loss: 2.303696\n",
      "[168,  1600] loss: 2.303865\n",
      "[168,  1800] loss: 2.303679\n",
      "[168,  2000] loss: 2.302842\n",
      "[168,  2200] loss: 2.303640\n",
      "[168,  2400] loss: 2.303046\n",
      "[168,  2600] loss: 2.302912\n",
      "[168,  2800] loss: 2.303678\n",
      "[168,  3000] loss: 2.303541\n",
      "[168,  3200] loss: 2.303216\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[169,   200] loss: 2.303301\n",
      "[169,   400] loss: 2.303438\n",
      "[169,   600] loss: 2.303671\n",
      "[169,   800] loss: 2.303334\n",
      "[169,  1000] loss: 2.303514\n",
      "[169,  1200] loss: 2.303535\n",
      "[169,  1400] loss: 2.303367\n",
      "[169,  1600] loss: 2.303687\n",
      "[169,  1800] loss: 2.303572\n",
      "[169,  2000] loss: 2.303574\n",
      "[169,  2200] loss: 2.303581\n",
      "[169,  2400] loss: 2.303419\n",
      "[169,  2600] loss: 2.303531\n",
      "[169,  2800] loss: 2.303836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169,  3000] loss: 2.303941\n",
      "[169,  3200] loss: 2.303790\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[170,   200] loss: 2.303494\n",
      "[170,   400] loss: 2.303506\n",
      "[170,   600] loss: 2.303743\n",
      "[170,   800] loss: 2.302877\n",
      "[170,  1000] loss: 2.303602\n",
      "[170,  1200] loss: 2.303794\n",
      "[170,  1400] loss: 2.303455\n",
      "[170,  1600] loss: 2.303153\n",
      "[170,  1800] loss: 2.303666\n",
      "[170,  2000] loss: 2.303210\n",
      "[170,  2200] loss: 2.303847\n",
      "[170,  2400] loss: 2.303379\n",
      "[170,  2600] loss: 2.303935\n",
      "[170,  2800] loss: 2.303585\n",
      "[170,  3000] loss: 2.303710\n",
      "[170,  3200] loss: 2.303482\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[171,   200] loss: 2.303694\n",
      "[171,   400] loss: 2.303214\n",
      "[171,   600] loss: 2.303712\n",
      "[171,   800] loss: 2.303923\n",
      "[171,  1000] loss: 2.303660\n",
      "[171,  1200] loss: 2.303781\n",
      "[171,  1400] loss: 2.303977\n",
      "[171,  1600] loss: 2.303069\n",
      "[171,  1800] loss: 2.303271\n",
      "[171,  2000] loss: 2.303450\n",
      "[171,  2200] loss: 2.303200\n",
      "[171,  2400] loss: 2.303931\n",
      "[171,  2600] loss: 2.303530\n",
      "[171,  2800] loss: 2.303755\n",
      "[171,  3000] loss: 2.303365\n",
      "[171,  3200] loss: 2.303270\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[172,   200] loss: 2.303901\n",
      "[172,   400] loss: 2.303599\n",
      "[172,   600] loss: 2.303152\n",
      "[172,   800] loss: 2.303529\n",
      "[172,  1000] loss: 2.303194\n",
      "[172,  1200] loss: 2.303214\n",
      "[172,  1400] loss: 2.303416\n",
      "[172,  1600] loss: 2.303450\n",
      "[172,  1800] loss: 2.303761\n",
      "[172,  2000] loss: 2.303212\n",
      "[172,  2200] loss: 2.303030\n",
      "[172,  2400] loss: 2.303410\n",
      "[172,  2600] loss: 2.303395\n",
      "[172,  2800] loss: 2.303387\n",
      "[172,  3000] loss: 2.303381\n",
      "[172,  3200] loss: 2.303540\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[173,   200] loss: 2.303661\n",
      "[173,   400] loss: 2.303955\n",
      "[173,   600] loss: 2.303296\n",
      "[173,   800] loss: 2.303153\n",
      "[173,  1000] loss: 2.303771\n",
      "[173,  1200] loss: 2.303584\n",
      "[173,  1400] loss: 2.303483\n",
      "[173,  1600] loss: 2.303493\n",
      "[173,  1800] loss: 2.303636\n",
      "[173,  2000] loss: 2.303283\n",
      "[173,  2200] loss: 2.303164\n",
      "[173,  2400] loss: 2.303787\n",
      "[173,  2600] loss: 2.303567\n",
      "[173,  2800] loss: 2.303418\n",
      "[173,  3000] loss: 2.303227\n",
      "[173,  3200] loss: 2.303717\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[174,   200] loss: 2.303358\n",
      "[174,   400] loss: 2.303536\n",
      "[174,   600] loss: 2.303181\n",
      "[174,   800] loss: 2.303535\n",
      "[174,  1000] loss: 2.303834\n",
      "[174,  1200] loss: 2.303254\n",
      "[174,  1400] loss: 2.303744\n",
      "[174,  1600] loss: 2.303371\n",
      "[174,  1800] loss: 2.303356\n",
      "[174,  2000] loss: 2.303773\n",
      "[174,  2200] loss: 2.303034\n",
      "[174,  2400] loss: 2.303269\n",
      "[174,  2600] loss: 2.303655\n",
      "[174,  2800] loss: 2.303826\n",
      "[174,  3000] loss: 2.303107\n",
      "[174,  3200] loss: 2.303728\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[175,   200] loss: 2.303597\n",
      "[175,   400] loss: 2.303525\n",
      "[175,   600] loss: 2.303623\n",
      "[175,   800] loss: 2.303295\n",
      "[175,  1000] loss: 2.303319\n",
      "[175,  1200] loss: 2.303634\n",
      "[175,  1400] loss: 2.303370\n",
      "[175,  1600] loss: 2.303956\n",
      "[175,  1800] loss: 2.303185\n",
      "[175,  2000] loss: 2.303405\n",
      "[175,  2200] loss: 2.303473\n",
      "[175,  2400] loss: 2.303148\n",
      "[175,  2600] loss: 2.303748\n",
      "[175,  2800] loss: 2.303709\n",
      "[175,  3000] loss: 2.303704\n",
      "[175,  3200] loss: 2.303497\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[176,   200] loss: 2.303486\n",
      "[176,   400] loss: 2.303332\n",
      "[176,   600] loss: 2.303758\n",
      "[176,   800] loss: 2.303589\n",
      "[176,  1000] loss: 2.303505\n",
      "[176,  1200] loss: 2.303526\n",
      "[176,  1400] loss: 2.303365\n",
      "[176,  1600] loss: 2.303320\n",
      "[176,  1800] loss: 2.303606\n",
      "[176,  2000] loss: 2.302469\n",
      "[176,  2200] loss: 2.303147\n",
      "[176,  2400] loss: 2.303305\n",
      "[176,  2600] loss: 2.303501\n",
      "[176,  2800] loss: 2.303412\n",
      "[176,  3000] loss: 2.303516\n",
      "[176,  3200] loss: 2.303934\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[177,   200] loss: 2.303459\n",
      "[177,   400] loss: 2.303396\n",
      "[177,   600] loss: 2.303818\n",
      "[177,   800] loss: 2.303847\n",
      "[177,  1000] loss: 2.303814\n",
      "[177,  1200] loss: 2.303199\n",
      "[177,  1400] loss: 2.303619\n",
      "[177,  1600] loss: 2.303471\n",
      "[177,  1800] loss: 2.303608\n",
      "[177,  2000] loss: 2.303930\n",
      "[177,  2200] loss: 2.303636\n",
      "[177,  2400] loss: 2.303854\n",
      "[177,  2600] loss: 2.303117\n",
      "[177,  2800] loss: 2.303336\n",
      "[177,  3000] loss: 2.303482\n",
      "[177,  3200] loss: 2.303348\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[178,   200] loss: 2.303382\n",
      "[178,   400] loss: 2.303377\n",
      "[178,   600] loss: 2.303720\n",
      "[178,   800] loss: 2.303531\n",
      "[178,  1000] loss: 2.303556\n",
      "[178,  1200] loss: 2.303426\n",
      "[178,  1400] loss: 2.303721\n",
      "[178,  1600] loss: 2.302974\n",
      "[178,  1800] loss: 2.303619\n",
      "[178,  2000] loss: 2.303613\n",
      "[178,  2200] loss: 2.303106\n",
      "[178,  2400] loss: 2.303793\n",
      "[178,  2600] loss: 2.303649\n",
      "[178,  2800] loss: 2.303459\n",
      "[178,  3000] loss: 2.302920\n",
      "[178,  3200] loss: 2.303564\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[179,   200] loss: 2.302989\n",
      "[179,   400] loss: 2.303375\n",
      "[179,   600] loss: 2.303497\n",
      "[179,   800] loss: 2.303820\n",
      "[179,  1000] loss: 2.303032\n",
      "[179,  1200] loss: 2.303724\n",
      "[179,  1400] loss: 2.303920\n",
      "[179,  1600] loss: 2.303664\n",
      "[179,  1800] loss: 2.303444\n",
      "[179,  2000] loss: 2.303947\n",
      "[179,  2200] loss: 2.303203\n",
      "[179,  2400] loss: 2.303712\n",
      "[179,  2600] loss: 2.303490\n",
      "[179,  2800] loss: 2.303536\n",
      "[179,  3000] loss: 2.303356\n",
      "[179,  3200] loss: 2.303959\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[180,   200] loss: 2.303317\n",
      "[180,   400] loss: 2.303895\n",
      "[180,   600] loss: 2.303572\n",
      "[180,   800] loss: 2.303029\n",
      "[180,  1000] loss: 2.303771\n",
      "[180,  1200] loss: 2.303802\n",
      "[180,  1400] loss: 2.303265\n",
      "[180,  1600] loss: 2.303589\n",
      "[180,  1800] loss: 2.303473\n",
      "[180,  2000] loss: 2.303676\n",
      "[180,  2200] loss: 2.303834\n",
      "[180,  2400] loss: 2.303270\n",
      "[180,  2600] loss: 2.303411\n",
      "[180,  2800] loss: 2.303829\n",
      "[180,  3000] loss: 2.303421\n",
      "[180,  3200] loss: 2.303853\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[181,   200] loss: 2.303637\n",
      "[181,   400] loss: 2.303666\n",
      "[181,   600] loss: 2.303772\n",
      "[181,   800] loss: 2.303132\n",
      "[181,  1000] loss: 2.303274\n",
      "[181,  1200] loss: 2.303263\n",
      "[181,  1400] loss: 2.303748\n",
      "[181,  1600] loss: 2.304086\n",
      "[181,  1800] loss: 2.303638\n",
      "[181,  2000] loss: 2.303542\n",
      "[181,  2200] loss: 2.303387\n",
      "[181,  2400] loss: 2.303478\n",
      "[181,  2600] loss: 2.303463\n",
      "[181,  2800] loss: 2.303532\n",
      "[181,  3000] loss: 2.303353\n",
      "[181,  3200] loss: 2.303312\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[182,   200] loss: 2.303683\n",
      "[182,   400] loss: 2.303550\n",
      "[182,   600] loss: 2.303524\n",
      "[182,   800] loss: 2.303739\n",
      "[182,  1000] loss: 2.303213\n",
      "[182,  1200] loss: 2.303306\n",
      "[182,  1400] loss: 2.302988\n",
      "[182,  1600] loss: 2.303712\n",
      "[182,  1800] loss: 2.303525\n",
      "[182,  2000] loss: 2.303284\n",
      "[182,  2200] loss: 2.303181\n",
      "[182,  2400] loss: 2.303639\n",
      "[182,  2600] loss: 2.303600\n",
      "[182,  2800] loss: 2.303280\n",
      "[182,  3000] loss: 2.303342\n",
      "[182,  3200] loss: 2.303655\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[183,   200] loss: 2.303777\n",
      "[183,   400] loss: 2.303281\n",
      "[183,   600] loss: 2.303406\n",
      "[183,   800] loss: 2.303355\n",
      "[183,  1000] loss: 2.303515\n",
      "[183,  1200] loss: 2.303641\n",
      "[183,  1400] loss: 2.303201\n",
      "[183,  1600] loss: 2.303393\n",
      "[183,  1800] loss: 2.303411\n",
      "[183,  2000] loss: 2.303914\n",
      "[183,  2200] loss: 2.303621\n",
      "[183,  2400] loss: 2.303546\n",
      "[183,  2600] loss: 2.303099\n",
      "[183,  2800] loss: 2.303431\n",
      "[183,  3000] loss: 2.303413\n",
      "[183,  3200] loss: 2.303365\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[184,   200] loss: 2.303921\n",
      "[184,   400] loss: 2.303484\n",
      "[184,   600] loss: 2.303627\n",
      "[184,   800] loss: 2.303108\n",
      "[184,  1000] loss: 2.303956\n",
      "[184,  1200] loss: 2.303822\n",
      "[184,  1400] loss: 2.303345\n",
      "[184,  1600] loss: 2.303608\n",
      "[184,  1800] loss: 2.303251\n",
      "[184,  2000] loss: 2.303536\n",
      "[184,  2200] loss: 2.303037\n",
      "[184,  2400] loss: 2.303302\n",
      "[184,  2600] loss: 2.303259\n",
      "[184,  2800] loss: 2.303421\n",
      "[184,  3000] loss: 2.303463\n",
      "[184,  3200] loss: 2.303494\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[185,   200] loss: 2.303567\n",
      "[185,   400] loss: 2.303891\n",
      "[185,   600] loss: 2.303139\n",
      "[185,   800] loss: 2.303658\n",
      "[185,  1000] loss: 2.303685\n",
      "[185,  1200] loss: 2.303084\n",
      "[185,  1400] loss: 2.303636\n",
      "[185,  1600] loss: 2.303540\n",
      "[185,  1800] loss: 2.303684\n",
      "[185,  2000] loss: 2.303559\n",
      "[185,  2200] loss: 2.303404\n",
      "[185,  2400] loss: 2.303376\n",
      "[185,  2600] loss: 2.303435\n",
      "[185,  2800] loss: 2.303510\n",
      "[185,  3000] loss: 2.303422\n",
      "[185,  3200] loss: 2.303351\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[186,   200] loss: 2.303703\n",
      "[186,   400] loss: 2.303550\n",
      "[186,   600] loss: 2.303067\n",
      "[186,   800] loss: 2.303800\n",
      "[186,  1000] loss: 2.303365\n",
      "[186,  1200] loss: 2.303781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186,  1400] loss: 2.303471\n",
      "[186,  1600] loss: 2.303051\n",
      "[186,  1800] loss: 2.303543\n",
      "[186,  2000] loss: 2.302904\n",
      "[186,  2200] loss: 2.303148\n",
      "[186,  2400] loss: 2.303775\n",
      "[186,  2600] loss: 2.303447\n",
      "[186,  2800] loss: 2.303465\n",
      "[186,  3000] loss: 2.303517\n",
      "[186,  3200] loss: 2.303550\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[187,   200] loss: 2.303638\n",
      "[187,   400] loss: 2.303374\n",
      "[187,   600] loss: 2.303296\n",
      "[187,   800] loss: 2.303643\n",
      "[187,  1000] loss: 2.303561\n",
      "[187,  1200] loss: 2.303169\n",
      "[187,  1400] loss: 2.303860\n",
      "[187,  1600] loss: 2.303662\n",
      "[187,  1800] loss: 2.303162\n",
      "[187,  2000] loss: 2.303727\n",
      "[187,  2200] loss: 2.303355\n",
      "[187,  2400] loss: 2.303631\n",
      "[187,  2600] loss: 2.303683\n",
      "[187,  2800] loss: 2.303028\n",
      "[187,  3000] loss: 2.303519\n",
      "[187,  3200] loss: 2.303433\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[188,   200] loss: 2.303549\n",
      "[188,   400] loss: 2.303359\n",
      "[188,   600] loss: 2.303475\n",
      "[188,   800] loss: 2.303528\n",
      "[188,  1000] loss: 2.303695\n",
      "[188,  1200] loss: 2.303743\n",
      "[188,  1400] loss: 2.303169\n",
      "[188,  1600] loss: 2.303698\n",
      "[188,  1800] loss: 2.303780\n",
      "[188,  2000] loss: 2.303362\n",
      "[188,  2200] loss: 2.303493\n",
      "[188,  2400] loss: 2.303430\n",
      "[188,  2600] loss: 2.303461\n",
      "[188,  2800] loss: 2.303533\n",
      "[188,  3000] loss: 2.303321\n",
      "[188,  3200] loss: 2.303403\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[189,   200] loss: 2.303657\n",
      "[189,   400] loss: 2.304010\n",
      "[189,   600] loss: 2.303381\n",
      "[189,   800] loss: 2.303637\n",
      "[189,  1000] loss: 2.303466\n",
      "[189,  1200] loss: 2.303480\n",
      "[189,  1400] loss: 2.303383\n",
      "[189,  1600] loss: 2.303230\n",
      "[189,  1800] loss: 2.303493\n",
      "[189,  2000] loss: 2.303378\n",
      "[189,  2200] loss: 2.303391\n",
      "[189,  2400] loss: 2.303943\n",
      "[189,  2600] loss: 2.303517\n",
      "[189,  2800] loss: 2.303547\n",
      "[189,  3000] loss: 2.303375\n",
      "[189,  3200] loss: 2.303618\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[190,   200] loss: 2.303498\n",
      "[190,   400] loss: 2.303694\n",
      "[190,   600] loss: 2.302975\n",
      "[190,   800] loss: 2.303404\n",
      "[190,  1000] loss: 2.303513\n",
      "[190,  1200] loss: 2.303761\n",
      "[190,  1400] loss: 2.303104\n",
      "[190,  1600] loss: 2.302792\n",
      "[190,  1800] loss: 2.303192\n",
      "[190,  2000] loss: 2.303537\n",
      "[190,  2200] loss: 2.303131\n",
      "[190,  2400] loss: 2.303462\n",
      "[190,  2600] loss: 2.303371\n",
      "[190,  2800] loss: 2.303568\n",
      "[190,  3000] loss: 2.303293\n",
      "[190,  3200] loss: 2.304019\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[191,   200] loss: 2.303325\n",
      "[191,   400] loss: 2.303806\n",
      "[191,   600] loss: 2.303846\n",
      "[191,   800] loss: 2.303233\n",
      "[191,  1000] loss: 2.303632\n",
      "[191,  1200] loss: 2.303937\n",
      "[191,  1400] loss: 2.303738\n",
      "[191,  1600] loss: 2.303479\n",
      "[191,  1800] loss: 2.303417\n",
      "[191,  2000] loss: 2.303447\n",
      "[191,  2200] loss: 2.303384\n",
      "[191,  2400] loss: 2.303454\n",
      "[191,  2600] loss: 2.303128\n",
      "[191,  2800] loss: 2.303225\n",
      "[191,  3000] loss: 2.303224\n",
      "[191,  3200] loss: 2.303724\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[192,   200] loss: 2.303309\n",
      "[192,   400] loss: 2.303413\n",
      "[192,   600] loss: 2.303213\n",
      "[192,   800] loss: 2.303084\n",
      "[192,  1000] loss: 2.303426\n",
      "[192,  1200] loss: 2.303720\n",
      "[192,  1400] loss: 2.303735\n",
      "[192,  1600] loss: 2.303768\n",
      "[192,  1800] loss: 2.303524\n",
      "[192,  2000] loss: 2.303936\n",
      "[192,  2200] loss: 2.303334\n",
      "[192,  2400] loss: 2.303688\n",
      "[192,  2600] loss: 2.303762\n",
      "[192,  2800] loss: 2.303388\n",
      "[192,  3000] loss: 2.303382\n",
      "[192,  3200] loss: 2.303957\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[193,   200] loss: 2.303521\n",
      "[193,   400] loss: 2.303466\n",
      "[193,   600] loss: 2.303100\n",
      "[193,   800] loss: 2.303676\n",
      "[193,  1000] loss: 2.303347\n",
      "[193,  1200] loss: 2.303105\n",
      "[193,  1400] loss: 2.303717\n",
      "[193,  1600] loss: 2.303278\n",
      "[193,  1800] loss: 2.303778\n",
      "[193,  2000] loss: 2.303664\n",
      "[193,  2200] loss: 2.303504\n",
      "[193,  2400] loss: 2.303287\n",
      "[193,  2600] loss: 2.303273\n",
      "[193,  2800] loss: 2.303166\n",
      "[193,  3000] loss: 2.303395\n",
      "[193,  3200] loss: 2.303375\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[194,   200] loss: 2.303552\n",
      "[194,   400] loss: 2.303277\n",
      "[194,   600] loss: 2.303668\n",
      "[194,   800] loss: 2.303644\n",
      "[194,  1000] loss: 2.303272\n",
      "[194,  1200] loss: 2.303699\n",
      "[194,  1400] loss: 2.303699\n",
      "[194,  1600] loss: 2.303611\n",
      "[194,  1800] loss: 2.302810\n",
      "[194,  2000] loss: 2.303799\n",
      "[194,  2200] loss: 2.303827\n",
      "[194,  2400] loss: 2.303660\n",
      "[194,  2600] loss: 2.302948\n",
      "[194,  2800] loss: 2.303188\n",
      "[194,  3000] loss: 2.303584\n",
      "[194,  3200] loss: 2.303222\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[195,   200] loss: 2.303414\n",
      "[195,   400] loss: 2.303460\n",
      "[195,   600] loss: 2.303349\n",
      "[195,   800] loss: 2.303586\n",
      "[195,  1000] loss: 2.303730\n",
      "[195,  1200] loss: 2.303618\n",
      "[195,  1400] loss: 2.303348\n",
      "[195,  1600] loss: 2.302934\n",
      "[195,  1800] loss: 2.304308\n",
      "[195,  2000] loss: 2.303844\n",
      "[195,  2200] loss: 2.303371\n",
      "[195,  2400] loss: 2.303474\n",
      "[195,  2600] loss: 2.303297\n",
      "[195,  2800] loss: 2.303568\n",
      "[195,  3000] loss: 2.303507\n",
      "[195,  3200] loss: 2.303751\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[196,   200] loss: 2.303083\n",
      "[196,   400] loss: 2.303394\n",
      "[196,   600] loss: 2.303613\n",
      "[196,   800] loss: 2.303007\n",
      "[196,  1000] loss: 2.303382\n",
      "[196,  1200] loss: 2.303256\n",
      "[196,  1400] loss: 2.303448\n",
      "[196,  1600] loss: 2.303749\n",
      "[196,  1800] loss: 2.302842\n",
      "[196,  2000] loss: 2.303715\n",
      "[196,  2200] loss: 2.303633\n",
      "[196,  2400] loss: 2.303290\n",
      "[196,  2600] loss: 2.303453\n",
      "[196,  2800] loss: 2.303355\n",
      "[196,  3000] loss: 2.303492\n",
      "[196,  3200] loss: 2.303429\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[197,   200] loss: 2.303519\n",
      "[197,   400] loss: 2.303544\n",
      "[197,   600] loss: 2.303915\n",
      "[197,   800] loss: 2.303151\n",
      "[197,  1000] loss: 2.303268\n",
      "[197,  1200] loss: 2.303557\n",
      "[197,  1400] loss: 2.303822\n",
      "[197,  1600] loss: 2.303795\n",
      "[197,  1800] loss: 2.303470\n",
      "[197,  2000] loss: 2.302993\n",
      "[197,  2200] loss: 2.303566\n",
      "[197,  2400] loss: 2.303487\n",
      "[197,  2600] loss: 2.303339\n",
      "[197,  2800] loss: 2.303769\n",
      "[197,  3000] loss: 2.303500\n",
      "[197,  3200] loss: 2.303738\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[198,   200] loss: 2.303112\n",
      "[198,   400] loss: 2.303548\n",
      "[198,   600] loss: 2.303505\n",
      "[198,   800] loss: 2.303517\n",
      "[198,  1000] loss: 2.303496\n",
      "[198,  1200] loss: 2.303276\n",
      "[198,  1400] loss: 2.303728\n",
      "[198,  1600] loss: 2.303058\n",
      "[198,  1800] loss: 2.303610\n",
      "[198,  2000] loss: 2.303336\n",
      "[198,  2200] loss: 2.303782\n",
      "[198,  2400] loss: 2.303210\n",
      "[198,  2600] loss: 2.303381\n",
      "[198,  2800] loss: 2.303462\n",
      "[198,  3000] loss: 2.303557\n",
      "[198,  3200] loss: 2.303824\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[199,   200] loss: 2.303676\n",
      "[199,   400] loss: 2.303567\n",
      "[199,   600] loss: 2.303329\n",
      "[199,   800] loss: 2.302956\n",
      "[199,  1000] loss: 2.303586\n",
      "[199,  1200] loss: 2.303246\n",
      "[199,  1400] loss: 2.303308\n",
      "[199,  1600] loss: 2.303255\n",
      "[199,  1800] loss: 2.303417\n",
      "[199,  2000] loss: 2.303711\n",
      "[199,  2200] loss: 2.303407\n",
      "[199,  2400] loss: 2.303654\n",
      "[199,  2600] loss: 2.303459\n",
      "[199,  2800] loss: 2.302722\n",
      "[199,  3000] loss: 2.303734\n",
      "[199,  3200] loss: 2.303833\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[200,   200] loss: 2.303677\n",
      "[200,   400] loss: 2.303575\n",
      "[200,   600] loss: 2.303067\n",
      "[200,   800] loss: 2.303267\n",
      "[200,  1000] loss: 2.303575\n",
      "[200,  1200] loss: 2.303595\n",
      "[200,  1400] loss: 2.303284\n",
      "[200,  1600] loss: 2.303875\n",
      "[200,  1800] loss: 2.303670\n",
      "[200,  2000] loss: 2.303585\n",
      "[200,  2200] loss: 2.302897\n",
      "[200,  2400] loss: 2.303408\n",
      "[200,  2600] loss: 2.303574\n",
      "[200,  2800] loss: 2.303593\n",
      "[200,  3000] loss: 2.303503\n",
      "[200,  3200] loss: 2.303824\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[201,   200] loss: 2.303240\n",
      "[201,   400] loss: 2.303684\n",
      "[201,   600] loss: 2.303627\n",
      "[201,   800] loss: 2.303536\n",
      "[201,  1000] loss: 2.303733\n",
      "[201,  1200] loss: 2.303495\n",
      "[201,  1400] loss: 2.303383\n",
      "[201,  1600] loss: 2.303182\n",
      "[201,  1800] loss: 2.303656\n",
      "[201,  2000] loss: 2.303037\n",
      "[201,  2200] loss: 2.303677\n",
      "[201,  2400] loss: 2.303275\n",
      "[201,  2600] loss: 2.303246\n",
      "[201,  2800] loss: 2.303493\n",
      "[201,  3000] loss: 2.303432\n",
      "[201,  3200] loss: 2.303621\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[202,   200] loss: 2.303264\n",
      "[202,   400] loss: 2.303559\n",
      "[202,   600] loss: 2.303003\n",
      "[202,   800] loss: 2.303621\n",
      "[202,  1000] loss: 2.303169\n",
      "[202,  1200] loss: 2.303020\n",
      "[202,  1400] loss: 2.303871\n",
      "[202,  1600] loss: 2.303291\n",
      "[202,  1800] loss: 2.303667\n",
      "[202,  2000] loss: 2.303315\n",
      "[202,  2200] loss: 2.303267\n",
      "[202,  2400] loss: 2.303436\n",
      "[202,  2600] loss: 2.303514\n",
      "[202,  2800] loss: 2.303475\n",
      "[202,  3000] loss: 2.303928\n",
      "[202,  3200] loss: 2.303562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[203,   200] loss: 2.303769\n",
      "[203,   400] loss: 2.303484\n",
      "[203,   600] loss: 2.303529\n",
      "[203,   800] loss: 2.303795\n",
      "[203,  1000] loss: 2.303367\n",
      "[203,  1200] loss: 2.303502\n",
      "[203,  1400] loss: 2.303485\n",
      "[203,  1600] loss: 2.302925\n",
      "[203,  1800] loss: 2.303633\n",
      "[203,  2000] loss: 2.304004\n",
      "[203,  2200] loss: 2.303870\n",
      "[203,  2400] loss: 2.303513\n",
      "[203,  2600] loss: 2.303818\n",
      "[203,  2800] loss: 2.303576\n",
      "[203,  3000] loss: 2.303061\n",
      "[203,  3200] loss: 2.303819\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[204,   200] loss: 2.302805\n",
      "[204,   400] loss: 2.303123\n",
      "[204,   600] loss: 2.303716\n",
      "[204,   800] loss: 2.303739\n",
      "[204,  1000] loss: 2.303510\n",
      "[204,  1200] loss: 2.303311\n",
      "[204,  1400] loss: 2.303553\n",
      "[204,  1600] loss: 2.303155\n",
      "[204,  1800] loss: 2.303272\n",
      "[204,  2000] loss: 2.303852\n",
      "[204,  2200] loss: 2.303537\n",
      "[204,  2400] loss: 2.303517\n",
      "[204,  2600] loss: 2.303758\n",
      "[204,  2800] loss: 2.303605\n",
      "[204,  3000] loss: 2.303427\n",
      "[204,  3200] loss: 2.303738\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[205,   200] loss: 2.303771\n",
      "[205,   400] loss: 2.303264\n",
      "[205,   600] loss: 2.303640\n",
      "[205,   800] loss: 2.303788\n",
      "[205,  1000] loss: 2.303713\n",
      "[205,  1200] loss: 2.303600\n",
      "[205,  1400] loss: 2.303612\n",
      "[205,  1600] loss: 2.303708\n",
      "[205,  1800] loss: 2.303448\n",
      "[205,  2000] loss: 2.303340\n",
      "[205,  2200] loss: 2.302917\n",
      "[205,  2400] loss: 2.304171\n",
      "[205,  2600] loss: 2.303710\n",
      "[205,  2800] loss: 2.303215\n",
      "[205,  3000] loss: 2.303696\n",
      "[205,  3200] loss: 2.303351\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[206,   200] loss: 2.303624\n",
      "[206,   400] loss: 2.303375\n",
      "[206,   600] loss: 2.303173\n",
      "[206,   800] loss: 2.303371\n",
      "[206,  1000] loss: 2.303668\n",
      "[206,  1200] loss: 2.303491\n",
      "[206,  1400] loss: 2.303600\n",
      "[206,  1600] loss: 2.304019\n",
      "[206,  1800] loss: 2.303284\n",
      "[206,  2000] loss: 2.303368\n",
      "[206,  2200] loss: 2.303477\n",
      "[206,  2400] loss: 2.303230\n",
      "[206,  2600] loss: 2.303646\n",
      "[206,  2800] loss: 2.303548\n",
      "[206,  3000] loss: 2.303669\n",
      "[206,  3200] loss: 2.303501\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[207,   200] loss: 2.303942\n",
      "[207,   400] loss: 2.303690\n",
      "[207,   600] loss: 2.303648\n",
      "[207,   800] loss: 2.303067\n",
      "[207,  1000] loss: 2.303384\n",
      "[207,  1200] loss: 2.303612\n",
      "[207,  1400] loss: 2.303833\n",
      "[207,  1600] loss: 2.303740\n",
      "[207,  1800] loss: 2.303363\n",
      "[207,  2000] loss: 2.302982\n",
      "[207,  2200] loss: 2.303781\n",
      "[207,  2400] loss: 2.303587\n",
      "[207,  2600] loss: 2.303331\n",
      "[207,  2800] loss: 2.303535\n",
      "[207,  3000] loss: 2.303565\n",
      "[207,  3200] loss: 2.303192\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[208,   200] loss: 2.303686\n",
      "[208,   400] loss: 2.303313\n",
      "[208,   600] loss: 2.303539\n",
      "[208,   800] loss: 2.303209\n",
      "[208,  1000] loss: 2.303875\n",
      "[208,  1200] loss: 2.303188\n",
      "[208,  1400] loss: 2.303175\n",
      "[208,  1600] loss: 2.303749\n",
      "[208,  1800] loss: 2.303110\n",
      "[208,  2000] loss: 2.303732\n",
      "[208,  2200] loss: 2.303206\n",
      "[208,  2400] loss: 2.302946\n",
      "[208,  2600] loss: 2.303494\n",
      "[208,  2800] loss: 2.303436\n",
      "[208,  3000] loss: 2.303215\n",
      "[208,  3200] loss: 2.303804\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[209,   200] loss: 2.303670\n",
      "[209,   400] loss: 2.303399\n",
      "[209,   600] loss: 2.303230\n",
      "[209,   800] loss: 2.303125\n",
      "[209,  1000] loss: 2.303517\n",
      "[209,  1200] loss: 2.303747\n",
      "[209,  1400] loss: 2.303040\n",
      "[209,  1600] loss: 2.303494\n",
      "[209,  1800] loss: 2.303311\n",
      "[209,  2000] loss: 2.303816\n",
      "[209,  2200] loss: 2.303022\n",
      "[209,  2400] loss: 2.303480\n",
      "[209,  2600] loss: 2.302937\n",
      "[209,  2800] loss: 2.303643\n",
      "[209,  3000] loss: 2.303534\n",
      "[209,  3200] loss: 2.303948\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[210,   200] loss: 2.303573\n",
      "[210,   400] loss: 2.303347\n",
      "[210,   600] loss: 2.303557\n",
      "[210,   800] loss: 2.303010\n",
      "[210,  1000] loss: 2.303337\n",
      "[210,  1200] loss: 2.303293\n",
      "[210,  1400] loss: 2.303643\n",
      "[210,  1600] loss: 2.303551\n",
      "[210,  1800] loss: 2.303265\n",
      "[210,  2000] loss: 2.303445\n",
      "[210,  2200] loss: 2.303691\n",
      "[210,  2400] loss: 2.303445\n",
      "[210,  2600] loss: 2.303118\n",
      "[210,  2800] loss: 2.303545\n",
      "[210,  3000] loss: 2.303258\n",
      "[210,  3200] loss: 2.303660\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[211,   200] loss: 2.303570\n",
      "[211,   400] loss: 2.303480\n",
      "[211,   600] loss: 2.303635\n",
      "[211,   800] loss: 2.303207\n",
      "[211,  1000] loss: 2.303681\n",
      "[211,  1200] loss: 2.303701\n",
      "[211,  1400] loss: 2.303269\n",
      "[211,  1600] loss: 2.303149\n",
      "[211,  1800] loss: 2.303706\n",
      "[211,  2000] loss: 2.303503\n",
      "[211,  2200] loss: 2.303579\n",
      "[211,  2400] loss: 2.303942\n",
      "[211,  2600] loss: 2.303502\n",
      "[211,  2800] loss: 2.302925\n",
      "[211,  3000] loss: 2.303661\n",
      "[211,  3200] loss: 2.303645\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[212,   200] loss: 2.303481\n",
      "[212,   400] loss: 2.303709\n",
      "[212,   600] loss: 2.303390\n",
      "[212,   800] loss: 2.304104\n",
      "[212,  1000] loss: 2.303158\n",
      "[212,  1200] loss: 2.303261\n",
      "[212,  1400] loss: 2.303430\n",
      "[212,  1600] loss: 2.303486\n",
      "[212,  1800] loss: 2.303700\n",
      "[212,  2000] loss: 2.303443\n",
      "[212,  2200] loss: 2.303353\n",
      "[212,  2400] loss: 2.303080\n",
      "[212,  2600] loss: 2.303443\n",
      "[212,  2800] loss: 2.304018\n",
      "[212,  3000] loss: 2.303715\n",
      "[212,  3200] loss: 2.303633\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[213,   200] loss: 2.302996\n",
      "[213,   400] loss: 2.303588\n",
      "[213,   600] loss: 2.303556\n",
      "[213,   800] loss: 2.303526\n",
      "[213,  1000] loss: 2.303537\n",
      "[213,  1200] loss: 2.303788\n",
      "[213,  1400] loss: 2.303600\n",
      "[213,  1600] loss: 2.302607\n",
      "[213,  1800] loss: 2.302822\n",
      "[213,  2000] loss: 2.303537\n",
      "[213,  2200] loss: 2.303914\n",
      "[213,  2400] loss: 2.303473\n",
      "[213,  2600] loss: 2.303430\n",
      "[213,  2800] loss: 2.303767\n",
      "[213,  3000] loss: 2.303505\n",
      "[213,  3200] loss: 2.303572\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[214,   200] loss: 2.303760\n",
      "[214,   400] loss: 2.303421\n",
      "[214,   600] loss: 2.303199\n",
      "[214,   800] loss: 2.303627\n",
      "[214,  1000] loss: 2.303989\n",
      "[214,  1200] loss: 2.303402\n",
      "[214,  1400] loss: 2.303604\n",
      "[214,  1600] loss: 2.303791\n",
      "[214,  1800] loss: 2.303382\n",
      "[214,  2000] loss: 2.302943\n",
      "[214,  2200] loss: 2.303164\n",
      "[214,  2400] loss: 2.303613\n",
      "[214,  2600] loss: 2.303280\n",
      "[214,  2800] loss: 2.303511\n",
      "[214,  3000] loss: 2.303592\n",
      "[214,  3200] loss: 2.302997\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[215,   200] loss: 2.303334\n",
      "[215,   400] loss: 2.303610\n",
      "[215,   600] loss: 2.303747\n",
      "[215,   800] loss: 2.303519\n",
      "[215,  1000] loss: 2.303355\n",
      "[215,  1200] loss: 2.303865\n",
      "[215,  1400] loss: 2.303373\n",
      "[215,  1600] loss: 2.303209\n",
      "[215,  1800] loss: 2.303489\n",
      "[215,  2000] loss: 2.303310\n",
      "[215,  2200] loss: 2.302978\n",
      "[215,  2400] loss: 2.303232\n",
      "[215,  2600] loss: 2.303653\n",
      "[215,  2800] loss: 2.303315\n",
      "[215,  3000] loss: 2.303484\n",
      "[215,  3200] loss: 2.303417\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[216,   200] loss: 2.303727\n",
      "[216,   400] loss: 2.303311\n",
      "[216,   600] loss: 2.303104\n",
      "[216,   800] loss: 2.303228\n",
      "[216,  1000] loss: 2.303548\n",
      "[216,  1200] loss: 2.303386\n",
      "[216,  1400] loss: 2.303355\n",
      "[216,  1600] loss: 2.303258\n",
      "[216,  1800] loss: 2.303678\n",
      "[216,  2000] loss: 2.303637\n",
      "[216,  2200] loss: 2.303296\n",
      "[216,  2400] loss: 2.303663\n",
      "[216,  2600] loss: 2.303409\n",
      "[216,  2800] loss: 2.303604\n",
      "[216,  3000] loss: 2.303477\n",
      "[216,  3200] loss: 2.303409\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[217,   200] loss: 2.303641\n",
      "[217,   400] loss: 2.303520\n",
      "[217,   600] loss: 2.303681\n",
      "[217,   800] loss: 2.303725\n",
      "[217,  1000] loss: 2.303183\n",
      "[217,  1200] loss: 2.303380\n",
      "[217,  1400] loss: 2.303298\n",
      "[217,  1600] loss: 2.303260\n",
      "[217,  1800] loss: 2.303545\n",
      "[217,  2000] loss: 2.303508\n",
      "[217,  2200] loss: 2.303572\n",
      "[217,  2400] loss: 2.303799\n",
      "[217,  2600] loss: 2.303738\n",
      "[217,  2800] loss: 2.303334\n",
      "[217,  3000] loss: 2.303440\n",
      "[217,  3200] loss: 2.303178\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[218,   200] loss: 2.303251\n",
      "[218,   400] loss: 2.303153\n",
      "[218,   600] loss: 2.303427\n",
      "[218,   800] loss: 2.303409\n",
      "[218,  1000] loss: 2.303480\n",
      "[218,  1200] loss: 2.303362\n",
      "[218,  1400] loss: 2.303170\n",
      "[218,  1600] loss: 2.303147\n",
      "[218,  1800] loss: 2.303610\n",
      "[218,  2000] loss: 2.303380\n",
      "[218,  2200] loss: 2.303263\n",
      "[218,  2400] loss: 2.303665\n",
      "[218,  2600] loss: 2.303535\n",
      "[218,  2800] loss: 2.303352\n",
      "[218,  3000] loss: 2.303462\n",
      "[218,  3200] loss: 2.303430\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[219,   200] loss: 2.303648\n",
      "[219,   400] loss: 2.303747\n",
      "[219,   600] loss: 2.303044\n",
      "[219,   800] loss: 2.303790\n",
      "[219,  1000] loss: 2.303191\n",
      "[219,  1200] loss: 2.303498\n",
      "[219,  1400] loss: 2.303285\n",
      "[219,  1600] loss: 2.303387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[219,  1800] loss: 2.303652\n",
      "[219,  2000] loss: 2.303481\n",
      "[219,  2200] loss: 2.303520\n",
      "[219,  2400] loss: 2.303648\n",
      "[219,  2600] loss: 2.303666\n",
      "[219,  2800] loss: 2.303749\n",
      "[219,  3000] loss: 2.303559\n",
      "[219,  3200] loss: 2.303213\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[220,   200] loss: 2.303142\n",
      "[220,   400] loss: 2.303675\n",
      "[220,   600] loss: 2.303613\n",
      "[220,   800] loss: 2.303441\n",
      "[220,  1000] loss: 2.303522\n",
      "[220,  1200] loss: 2.303813\n",
      "[220,  1400] loss: 2.303506\n",
      "[220,  1600] loss: 2.303342\n",
      "[220,  1800] loss: 2.303763\n",
      "[220,  2000] loss: 2.303531\n",
      "[220,  2200] loss: 2.303376\n",
      "[220,  2400] loss: 2.303545\n",
      "[220,  2600] loss: 2.303577\n",
      "[220,  2800] loss: 2.303676\n",
      "[220,  3000] loss: 2.303341\n",
      "[220,  3200] loss: 2.303500\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[221,   200] loss: 2.303342\n",
      "[221,   400] loss: 2.303381\n",
      "[221,   600] loss: 2.303459\n",
      "[221,   800] loss: 2.303556\n",
      "[221,  1000] loss: 2.303412\n",
      "[221,  1200] loss: 2.303886\n",
      "[221,  1400] loss: 2.303258\n",
      "[221,  1600] loss: 2.303679\n",
      "[221,  1800] loss: 2.303222\n",
      "[221,  2000] loss: 2.303523\n",
      "[221,  2200] loss: 2.303501\n",
      "[221,  2400] loss: 2.303730\n",
      "[221,  2600] loss: 2.303079\n",
      "[221,  2800] loss: 2.303506\n",
      "[221,  3000] loss: 2.303669\n",
      "[221,  3200] loss: 2.303785\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[222,   200] loss: 2.303561\n",
      "[222,   400] loss: 2.303168\n",
      "[222,   600] loss: 2.303390\n",
      "[222,   800] loss: 2.303712\n",
      "[222,  1000] loss: 2.303648\n",
      "[222,  1200] loss: 2.303530\n",
      "[222,  1400] loss: 2.303341\n",
      "[222,  1600] loss: 2.303588\n",
      "[222,  1800] loss: 2.303256\n",
      "[222,  2000] loss: 2.303621\n",
      "[222,  2200] loss: 2.303801\n",
      "[222,  2400] loss: 2.303273\n",
      "[222,  2600] loss: 2.303762\n",
      "[222,  2800] loss: 2.303418\n",
      "[222,  3000] loss: 2.303932\n",
      "[222,  3200] loss: 2.303334\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[223,   200] loss: 2.303411\n",
      "[223,   400] loss: 2.303786\n",
      "[223,   600] loss: 2.302864\n",
      "[223,   800] loss: 2.303721\n",
      "[223,  1000] loss: 2.303374\n",
      "[223,  1200] loss: 2.303800\n",
      "[223,  1400] loss: 2.303502\n",
      "[223,  1600] loss: 2.303510\n",
      "[223,  1800] loss: 2.303751\n",
      "[223,  2000] loss: 2.303574\n",
      "[223,  2200] loss: 2.303446\n",
      "[223,  2400] loss: 2.303658\n",
      "[223,  2600] loss: 2.303714\n",
      "[223,  2800] loss: 2.303248\n",
      "[223,  3000] loss: 2.303384\n",
      "[223,  3200] loss: 2.303742\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[224,   200] loss: 2.303555\n",
      "[224,   400] loss: 2.303156\n",
      "[224,   600] loss: 2.303431\n",
      "[224,   800] loss: 2.303552\n",
      "[224,  1000] loss: 2.303436\n",
      "[224,  1200] loss: 2.303703\n",
      "[224,  1400] loss: 2.303814\n",
      "[224,  1600] loss: 2.303463\n",
      "[224,  1800] loss: 2.303247\n",
      "[224,  2000] loss: 2.303712\n",
      "[224,  2200] loss: 2.303459\n",
      "[224,  2400] loss: 2.303449\n",
      "[224,  2600] loss: 2.303357\n",
      "[224,  2800] loss: 2.303697\n",
      "[224,  3000] loss: 2.303835\n",
      "[224,  3200] loss: 2.303286\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[225,   200] loss: 2.303355\n",
      "[225,   400] loss: 2.303649\n",
      "[225,   600] loss: 2.303246\n",
      "[225,   800] loss: 2.303669\n",
      "[225,  1000] loss: 2.303300\n",
      "[225,  1200] loss: 2.303261\n",
      "[225,  1400] loss: 2.303585\n",
      "[225,  1600] loss: 2.303651\n",
      "[225,  1800] loss: 2.303470\n",
      "[225,  2000] loss: 2.302895\n",
      "[225,  2200] loss: 2.303710\n",
      "[225,  2400] loss: 2.303741\n",
      "[225,  2600] loss: 2.302911\n",
      "[225,  2800] loss: 2.303457\n",
      "[225,  3000] loss: 2.303531\n",
      "[225,  3200] loss: 2.303342\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[226,   200] loss: 2.303507\n",
      "[226,   400] loss: 2.303390\n",
      "[226,   600] loss: 2.302888\n",
      "[226,   800] loss: 2.303144\n",
      "[226,  1000] loss: 2.303643\n",
      "[226,  1200] loss: 2.303510\n",
      "[226,  1400] loss: 2.303113\n",
      "[226,  1600] loss: 2.303503\n",
      "[226,  1800] loss: 2.303571\n",
      "[226,  2000] loss: 2.303061\n",
      "[226,  2200] loss: 2.303254\n",
      "[226,  2400] loss: 2.303502\n",
      "[226,  2600] loss: 2.303311\n",
      "[226,  2800] loss: 2.303915\n",
      "[226,  3000] loss: 2.303385\n",
      "[226,  3200] loss: 2.303100\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[227,   200] loss: 2.303464\n",
      "[227,   400] loss: 2.303645\n",
      "[227,   600] loss: 2.303336\n",
      "[227,   800] loss: 2.302912\n",
      "[227,  1000] loss: 2.303394\n",
      "[227,  1200] loss: 2.303670\n",
      "[227,  1400] loss: 2.303159\n",
      "[227,  1600] loss: 2.303307\n",
      "[227,  1800] loss: 2.303163\n",
      "[227,  2000] loss: 2.303840\n",
      "[227,  2200] loss: 2.303552\n",
      "[227,  2400] loss: 2.303146\n",
      "[227,  2600] loss: 2.303207\n",
      "[227,  2800] loss: 2.303524\n",
      "[227,  3000] loss: 2.303854\n",
      "[227,  3200] loss: 2.303349\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[228,   200] loss: 2.303689\n",
      "[228,   400] loss: 2.303480\n",
      "[228,   600] loss: 2.303619\n",
      "[228,   800] loss: 2.303695\n",
      "[228,  1000] loss: 2.303411\n",
      "[228,  1200] loss: 2.303510\n",
      "[228,  1400] loss: 2.303002\n",
      "[228,  1600] loss: 2.303727\n",
      "[228,  1800] loss: 2.303441\n",
      "[228,  2000] loss: 2.303656\n",
      "[228,  2200] loss: 2.303150\n",
      "[228,  2400] loss: 2.303439\n",
      "[228,  2600] loss: 2.303659\n",
      "[228,  2800] loss: 2.303224\n",
      "[228,  3000] loss: 2.303142\n",
      "[228,  3200] loss: 2.303117\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[229,   200] loss: 2.303647\n",
      "[229,   400] loss: 2.303331\n",
      "[229,   600] loss: 2.303536\n",
      "[229,   800] loss: 2.303675\n",
      "[229,  1000] loss: 2.303145\n",
      "[229,  1200] loss: 2.303074\n",
      "[229,  1400] loss: 2.303430\n",
      "[229,  1600] loss: 2.303828\n",
      "[229,  1800] loss: 2.303323\n",
      "[229,  2000] loss: 2.303536\n",
      "[229,  2200] loss: 2.303784\n",
      "[229,  2400] loss: 2.303675\n",
      "[229,  2600] loss: 2.303778\n",
      "[229,  2800] loss: 2.303410\n",
      "[229,  3000] loss: 2.303484\n",
      "[229,  3200] loss: 2.303409\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[230,   200] loss: 2.303273\n",
      "[230,   400] loss: 2.303701\n",
      "[230,   600] loss: 2.303478\n",
      "[230,   800] loss: 2.303037\n",
      "[230,  1000] loss: 2.303639\n",
      "[230,  1200] loss: 2.303484\n",
      "[230,  1400] loss: 2.303457\n",
      "[230,  1600] loss: 2.303869\n",
      "[230,  1800] loss: 2.303668\n",
      "[230,  2000] loss: 2.303677\n",
      "[230,  2200] loss: 2.303834\n",
      "[230,  2400] loss: 2.303491\n",
      "[230,  2600] loss: 2.303799\n",
      "[230,  2800] loss: 2.303333\n",
      "[230,  3000] loss: 2.303684\n",
      "[230,  3200] loss: 2.303385\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[231,   200] loss: 2.303592\n",
      "[231,   400] loss: 2.303141\n",
      "[231,   600] loss: 2.303985\n",
      "[231,   800] loss: 2.303281\n",
      "[231,  1000] loss: 2.303358\n",
      "[231,  1200] loss: 2.303403\n",
      "[231,  1400] loss: 2.303300\n",
      "[231,  1600] loss: 2.303588\n",
      "[231,  1800] loss: 2.303991\n",
      "[231,  2000] loss: 2.303347\n",
      "[231,  2200] loss: 2.303398\n",
      "[231,  2400] loss: 2.303074\n",
      "[231,  2600] loss: 2.303755\n",
      "[231,  2800] loss: 2.303149\n",
      "[231,  3000] loss: 2.303707\n",
      "[231,  3200] loss: 2.303204\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[232,   200] loss: 2.303806\n",
      "[232,   400] loss: 2.303297\n",
      "[232,   600] loss: 2.302937\n",
      "[232,   800] loss: 2.303953\n",
      "[232,  1000] loss: 2.303428\n",
      "[232,  1200] loss: 2.303158\n",
      "[232,  1400] loss: 2.303555\n",
      "[232,  1600] loss: 2.303180\n",
      "[232,  1800] loss: 2.303762\n",
      "[232,  2000] loss: 2.303649\n",
      "[232,  2200] loss: 2.303713\n",
      "[232,  2400] loss: 2.303557\n",
      "[232,  2600] loss: 2.303461\n",
      "[232,  2800] loss: 2.303475\n",
      "[232,  3000] loss: 2.303605\n",
      "[232,  3200] loss: 2.303488\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[233,   200] loss: 2.303454\n",
      "[233,   400] loss: 2.302988\n",
      "[233,   600] loss: 2.304033\n",
      "[233,   800] loss: 2.303326\n",
      "[233,  1000] loss: 2.303724\n",
      "[233,  1200] loss: 2.303524\n",
      "[233,  1400] loss: 2.303771\n",
      "[233,  1600] loss: 2.303936\n",
      "[233,  1800] loss: 2.303290\n",
      "[233,  2000] loss: 2.303237\n",
      "[233,  2200] loss: 2.303218\n",
      "[233,  2400] loss: 2.303770\n",
      "[233,  2600] loss: 2.303506\n",
      "[233,  2800] loss: 2.303520\n",
      "[233,  3000] loss: 2.303290\n",
      "[233,  3200] loss: 2.303526\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[234,   200] loss: 2.302931\n",
      "[234,   400] loss: 2.303369\n",
      "[234,   600] loss: 2.303123\n",
      "[234,   800] loss: 2.303416\n",
      "[234,  1000] loss: 2.303407\n",
      "[234,  1200] loss: 2.303323\n",
      "[234,  1400] loss: 2.303668\n",
      "[234,  1600] loss: 2.303557\n",
      "[234,  1800] loss: 2.303428\n",
      "[234,  2000] loss: 2.304005\n",
      "[234,  2200] loss: 2.303330\n",
      "[234,  2400] loss: 2.303312\n",
      "[234,  2600] loss: 2.303572\n",
      "[234,  2800] loss: 2.303427\n",
      "[234,  3000] loss: 2.303338\n",
      "[234,  3200] loss: 2.303613\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[235,   200] loss: 2.303388\n",
      "[235,   400] loss: 2.303480\n",
      "[235,   600] loss: 2.303210\n",
      "[235,   800] loss: 2.303744\n",
      "[235,  1000] loss: 2.303277\n",
      "[235,  1200] loss: 2.303610\n",
      "[235,  1400] loss: 2.303284\n",
      "[235,  1600] loss: 2.303417\n",
      "[235,  1800] loss: 2.303508\n",
      "[235,  2000] loss: 2.303549\n",
      "[235,  2200] loss: 2.303506\n",
      "[235,  2400] loss: 2.303721\n",
      "[235,  2600] loss: 2.303601\n",
      "[235,  2800] loss: 2.303385\n",
      "[235,  3000] loss: 2.303596\n",
      "[235,  3200] loss: 2.303952\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236,   200] loss: 2.303328\n",
      "[236,   400] loss: 2.303647\n",
      "[236,   600] loss: 2.303582\n",
      "[236,   800] loss: 2.303153\n",
      "[236,  1000] loss: 2.303830\n",
      "[236,  1200] loss: 2.303360\n",
      "[236,  1400] loss: 2.303660\n",
      "[236,  1600] loss: 2.303228\n",
      "[236,  1800] loss: 2.303763\n",
      "[236,  2000] loss: 2.303595\n",
      "[236,  2200] loss: 2.303418\n",
      "[236,  2400] loss: 2.303593\n",
      "[236,  2600] loss: 2.303428\n",
      "[236,  2800] loss: 2.303264\n",
      "[236,  3000] loss: 2.303345\n",
      "[236,  3200] loss: 2.303967\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[237,   200] loss: 2.303532\n",
      "[237,   400] loss: 2.303551\n",
      "[237,   600] loss: 2.303612\n",
      "[237,   800] loss: 2.303473\n",
      "[237,  1000] loss: 2.303468\n",
      "[237,  1200] loss: 2.303472\n",
      "[237,  1400] loss: 2.303419\n",
      "[237,  1600] loss: 2.303518\n",
      "[237,  1800] loss: 2.303319\n",
      "[237,  2000] loss: 2.302970\n",
      "[237,  2200] loss: 2.303600\n",
      "[237,  2400] loss: 2.303958\n",
      "[237,  2600] loss: 2.303622\n",
      "[237,  2800] loss: 2.303309\n",
      "[237,  3000] loss: 2.303816\n",
      "[237,  3200] loss: 2.303352\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[238,   200] loss: 2.303217\n",
      "[238,   400] loss: 2.303430\n",
      "[238,   600] loss: 2.303587\n",
      "[238,   800] loss: 2.303326\n",
      "[238,  1000] loss: 2.303306\n",
      "[238,  1200] loss: 2.303341\n",
      "[238,  1400] loss: 2.303259\n",
      "[238,  1600] loss: 2.303616\n",
      "[238,  1800] loss: 2.302793\n",
      "[238,  2000] loss: 2.303843\n",
      "[238,  2200] loss: 2.303333\n",
      "[238,  2400] loss: 2.303377\n",
      "[238,  2600] loss: 2.303542\n",
      "[238,  2800] loss: 2.303780\n",
      "[238,  3000] loss: 2.303229\n",
      "[238,  3200] loss: 2.303267\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[239,   200] loss: 2.303281\n",
      "[239,   400] loss: 2.303190\n",
      "[239,   600] loss: 2.304148\n",
      "[239,   800] loss: 2.303450\n",
      "[239,  1000] loss: 2.303572\n",
      "[239,  1200] loss: 2.303360\n",
      "[239,  1400] loss: 2.303650\n",
      "[239,  1600] loss: 2.303863\n",
      "[239,  1800] loss: 2.303398\n",
      "[239,  2000] loss: 2.303342\n",
      "[239,  2200] loss: 2.303760\n",
      "[239,  2400] loss: 2.303482\n",
      "[239,  2600] loss: 2.303335\n",
      "[239,  2800] loss: 2.303699\n",
      "[239,  3000] loss: 2.303401\n",
      "[239,  3200] loss: 2.303332\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[240,   200] loss: 2.303681\n",
      "[240,   400] loss: 2.303538\n",
      "[240,   600] loss: 2.303199\n",
      "[240,   800] loss: 2.303910\n",
      "[240,  1000] loss: 2.303518\n",
      "[240,  1200] loss: 2.303669\n",
      "[240,  1400] loss: 2.303565\n",
      "[240,  1600] loss: 2.303303\n",
      "[240,  1800] loss: 2.303731\n",
      "[240,  2000] loss: 2.303416\n",
      "[240,  2200] loss: 2.303370\n",
      "[240,  2400] loss: 2.303688\n",
      "[240,  2600] loss: 2.303562\n",
      "[240,  2800] loss: 2.303209\n",
      "[240,  3000] loss: 2.303633\n",
      "[240,  3200] loss: 2.303031\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[241,   200] loss: 2.303788\n",
      "[241,   400] loss: 2.303350\n",
      "[241,   600] loss: 2.303584\n",
      "[241,   800] loss: 2.303082\n",
      "[241,  1000] loss: 2.303501\n",
      "[241,  1200] loss: 2.303121\n",
      "[241,  1400] loss: 2.303361\n",
      "[241,  1600] loss: 2.303538\n",
      "[241,  1800] loss: 2.303490\n",
      "[241,  2000] loss: 2.303489\n",
      "[241,  2200] loss: 2.303578\n",
      "[241,  2400] loss: 2.302927\n",
      "[241,  2600] loss: 2.302879\n",
      "[241,  2800] loss: 2.303843\n",
      "[241,  3000] loss: 2.303006\n",
      "[241,  3200] loss: 2.303625\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[242,   200] loss: 2.303497\n",
      "[242,   400] loss: 2.303200\n",
      "[242,   600] loss: 2.303435\n",
      "[242,   800] loss: 2.303672\n",
      "[242,  1000] loss: 2.303617\n",
      "[242,  1200] loss: 2.303445\n",
      "[242,  1400] loss: 2.303718\n",
      "[242,  1600] loss: 2.303427\n",
      "[242,  1800] loss: 2.303657\n",
      "[242,  2000] loss: 2.303262\n",
      "[242,  2200] loss: 2.303611\n",
      "[242,  2400] loss: 2.303480\n",
      "[242,  2600] loss: 2.303755\n",
      "[242,  2800] loss: 2.303616\n",
      "[242,  3000] loss: 2.303458\n",
      "[242,  3200] loss: 2.303154\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[243,   200] loss: 2.303475\n",
      "[243,   400] loss: 2.303299\n",
      "[243,   600] loss: 2.303242\n",
      "[243,   800] loss: 2.303531\n",
      "[243,  1000] loss: 2.303359\n",
      "[243,  1200] loss: 2.303084\n",
      "[243,  1400] loss: 2.303662\n",
      "[243,  1600] loss: 2.303588\n",
      "[243,  1800] loss: 2.303848\n",
      "[243,  2000] loss: 2.303837\n",
      "[243,  2200] loss: 2.303201\n",
      "[243,  2400] loss: 2.303711\n",
      "[243,  2600] loss: 2.303543\n",
      "[243,  2800] loss: 2.303933\n",
      "[243,  3000] loss: 2.303953\n",
      "[243,  3200] loss: 2.303272\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[244,   200] loss: 2.303942\n",
      "[244,   400] loss: 2.303838\n",
      "[244,   600] loss: 2.303718\n",
      "[244,   800] loss: 2.303736\n",
      "[244,  1000] loss: 2.303574\n",
      "[244,  1200] loss: 2.303567\n",
      "[244,  1400] loss: 2.302816\n",
      "[244,  1600] loss: 2.303353\n",
      "[244,  1800] loss: 2.303483\n",
      "[244,  2000] loss: 2.303281\n",
      "[244,  2200] loss: 2.303007\n",
      "[244,  2400] loss: 2.303779\n",
      "[244,  2600] loss: 2.303354\n",
      "[244,  2800] loss: 2.303468\n",
      "[244,  3000] loss: 2.303724\n",
      "[244,  3200] loss: 2.303451\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[245,   200] loss: 2.303291\n",
      "[245,   400] loss: 2.303665\n",
      "[245,   600] loss: 2.304054\n",
      "[245,   800] loss: 2.303274\n",
      "[245,  1000] loss: 2.303673\n",
      "[245,  1200] loss: 2.303451\n",
      "[245,  1400] loss: 2.303317\n",
      "[245,  1600] loss: 2.302882\n",
      "[245,  1800] loss: 2.303074\n",
      "[245,  2000] loss: 2.303646\n",
      "[245,  2200] loss: 2.303534\n",
      "[245,  2400] loss: 2.303419\n",
      "[245,  2600] loss: 2.303485\n",
      "[245,  2800] loss: 2.303427\n",
      "[245,  3000] loss: 2.302722\n",
      "[245,  3200] loss: 2.303377\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[246,   200] loss: 2.303041\n",
      "[246,   400] loss: 2.303961\n",
      "[246,   600] loss: 2.303651\n",
      "[246,   800] loss: 2.303576\n",
      "[246,  1000] loss: 2.303118\n",
      "[246,  1200] loss: 2.303731\n",
      "[246,  1400] loss: 2.303594\n",
      "[246,  1600] loss: 2.303493\n",
      "[246,  1800] loss: 2.303480\n",
      "[246,  2000] loss: 2.303506\n",
      "[246,  2200] loss: 2.303668\n",
      "[246,  2400] loss: 2.303569\n",
      "[246,  2600] loss: 2.303466\n",
      "[246,  2800] loss: 2.303697\n",
      "[246,  3000] loss: 2.303702\n",
      "[246,  3200] loss: 2.303821\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[247,   200] loss: 2.303321\n",
      "[247,   400] loss: 2.303418\n",
      "[247,   600] loss: 2.303719\n",
      "[247,   800] loss: 2.303585\n",
      "[247,  1000] loss: 2.303957\n",
      "[247,  1200] loss: 2.303372\n",
      "[247,  1400] loss: 2.303258\n",
      "[247,  1600] loss: 2.303494\n",
      "[247,  1800] loss: 2.304018\n",
      "[247,  2000] loss: 2.303225\n",
      "[247,  2200] loss: 2.303342\n",
      "[247,  2400] loss: 2.303600\n",
      "[247,  2600] loss: 2.303705\n",
      "[247,  2800] loss: 2.303326\n",
      "[247,  3000] loss: 2.303327\n",
      "[247,  3200] loss: 2.303569\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[248,   200] loss: 2.303616\n",
      "[248,   400] loss: 2.303800\n",
      "[248,   600] loss: 2.304023\n",
      "[248,   800] loss: 2.303652\n",
      "[248,  1000] loss: 2.303512\n",
      "[248,  1200] loss: 2.303126\n",
      "[248,  1400] loss: 2.303033\n",
      "[248,  1600] loss: 2.303752\n",
      "[248,  1800] loss: 2.303431\n",
      "[248,  2000] loss: 2.303548\n",
      "[248,  2200] loss: 2.303418\n",
      "[248,  2400] loss: 2.303598\n",
      "[248,  2600] loss: 2.303691\n",
      "[248,  2800] loss: 2.303261\n",
      "[248,  3000] loss: 2.303189\n",
      "[248,  3200] loss: 2.303149\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[249,   200] loss: 2.303786\n",
      "[249,   400] loss: 2.303646\n",
      "[249,   600] loss: 2.303704\n",
      "[249,   800] loss: 2.303296\n",
      "[249,  1000] loss: 2.303528\n",
      "[249,  1200] loss: 2.303721\n",
      "[249,  1400] loss: 2.303512\n",
      "[249,  1600] loss: 2.303529\n",
      "[249,  1800] loss: 2.302929\n",
      "[249,  2000] loss: 2.303271\n",
      "[249,  2200] loss: 2.303428\n",
      "[249,  2400] loss: 2.303387\n",
      "[249,  2600] loss: 2.303591\n",
      "[249,  2800] loss: 2.303203\n",
      "[249,  3000] loss: 2.303648\n",
      "[249,  3200] loss: 2.303707\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[250,   200] loss: 2.303316\n",
      "[250,   400] loss: 2.303546\n",
      "[250,   600] loss: 2.303828\n",
      "[250,   800] loss: 2.303490\n",
      "[250,  1000] loss: 2.303820\n",
      "[250,  1200] loss: 2.303136\n",
      "[250,  1400] loss: 2.303561\n",
      "[250,  1600] loss: 2.303431\n",
      "[250,  1800] loss: 2.303715\n",
      "[250,  2000] loss: 2.303162\n",
      "[250,  2200] loss: 2.303774\n",
      "[250,  2400] loss: 2.303576\n",
      "[250,  2600] loss: 2.303154\n",
      "[250,  2800] loss: 2.303333\n",
      "[250,  3000] loss: 2.303113\n",
      "[250,  3200] loss: 2.303822\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[251,   200] loss: 2.303166\n",
      "[251,   400] loss: 2.302924\n",
      "[251,   600] loss: 2.302683\n",
      "[251,   800] loss: 2.302701\n",
      "[251,  1000] loss: 2.302799\n",
      "[251,  1200] loss: 2.302606\n",
      "[251,  1400] loss: 2.302461\n",
      "[251,  1600] loss: 2.302712\n",
      "[251,  1800] loss: 2.302834\n",
      "[251,  2000] loss: 2.302608\n",
      "[251,  2200] loss: 2.302570\n",
      "[251,  2400] loss: 2.302632\n",
      "[251,  2600] loss: 2.302622\n",
      "[251,  2800] loss: 2.302558\n",
      "[251,  3000] loss: 2.302686\n",
      "[251,  3200] loss: 2.302469\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[252,   200] loss: 2.302725\n",
      "[252,   400] loss: 2.302597\n",
      "[252,   600] loss: 2.302766\n",
      "[252,   800] loss: 2.302826\n",
      "[252,  1000] loss: 2.302604\n",
      "[252,  1200] loss: 2.302397\n",
      "[252,  1400] loss: 2.302753\n",
      "[252,  1600] loss: 2.302675\n",
      "[252,  1800] loss: 2.302533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[252,  2000] loss: 2.302625\n",
      "[252,  2200] loss: 2.302594\n",
      "[252,  2400] loss: 2.302686\n",
      "[252,  2600] loss: 2.302627\n",
      "[252,  2800] loss: 2.302490\n",
      "[252,  3000] loss: 2.302837\n",
      "[252,  3200] loss: 2.302584\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[253,   200] loss: 2.302549\n",
      "[253,   400] loss: 2.302604\n",
      "[253,   600] loss: 2.302815\n",
      "[253,   800] loss: 2.302456\n",
      "[253,  1000] loss: 2.302405\n",
      "[253,  1200] loss: 2.302904\n",
      "[253,  1400] loss: 2.302695\n",
      "[253,  1600] loss: 2.302557\n",
      "[253,  1800] loss: 2.302703\n",
      "[253,  2000] loss: 2.302747\n",
      "[253,  2200] loss: 2.302502\n",
      "[253,  2400] loss: 2.302935\n",
      "[253,  2600] loss: 2.302538\n",
      "[253,  2800] loss: 2.302686\n",
      "[253,  3000] loss: 2.302475\n",
      "[253,  3200] loss: 2.302584\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[254,   200] loss: 2.302567\n",
      "[254,   400] loss: 2.302733\n",
      "[254,   600] loss: 2.302611\n",
      "[254,   800] loss: 2.302673\n",
      "[254,  1000] loss: 2.302621\n",
      "[254,  1200] loss: 2.302632\n",
      "[254,  1400] loss: 2.302672\n",
      "[254,  1600] loss: 2.302694\n",
      "[254,  1800] loss: 2.302535\n",
      "[254,  2000] loss: 2.302770\n",
      "[254,  2200] loss: 2.302567\n",
      "[254,  2400] loss: 2.302710\n",
      "[254,  2600] loss: 2.302625\n",
      "[254,  2800] loss: 2.302675\n",
      "[254,  3000] loss: 2.302777\n",
      "[254,  3200] loss: 2.302458\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[255,   200] loss: 2.302535\n",
      "[255,   400] loss: 2.302736\n",
      "[255,   600] loss: 2.302493\n",
      "[255,   800] loss: 2.302655\n",
      "[255,  1000] loss: 2.302658\n",
      "[255,  1200] loss: 2.302649\n",
      "[255,  1400] loss: 2.302445\n",
      "[255,  1600] loss: 2.302740\n",
      "[255,  1800] loss: 2.302702\n",
      "[255,  2000] loss: 2.302541\n",
      "[255,  2200] loss: 2.302615\n",
      "[255,  2400] loss: 2.302754\n",
      "[255,  2600] loss: 2.302717\n",
      "[255,  2800] loss: 2.302627\n",
      "[255,  3000] loss: 2.302796\n",
      "[255,  3200] loss: 2.302656\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[256,   200] loss: 2.302706\n",
      "[256,   400] loss: 2.302472\n",
      "[256,   600] loss: 2.302808\n",
      "[256,   800] loss: 2.302638\n",
      "[256,  1000] loss: 2.302610\n",
      "[256,  1200] loss: 2.302647\n",
      "[256,  1400] loss: 2.302593\n",
      "[256,  1600] loss: 2.302793\n",
      "[256,  1800] loss: 2.302428\n",
      "[256,  2000] loss: 2.302917\n",
      "[256,  2200] loss: 2.302642\n",
      "[256,  2400] loss: 2.302560\n",
      "[256,  2600] loss: 2.302666\n",
      "[256,  2800] loss: 2.302679\n",
      "[256,  3000] loss: 2.302811\n",
      "[256,  3200] loss: 2.302551\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[257,   200] loss: 2.302435\n",
      "[257,   400] loss: 2.302942\n",
      "[257,   600] loss: 2.302500\n",
      "[257,   800] loss: 2.302755\n",
      "[257,  1000] loss: 2.302788\n",
      "[257,  1200] loss: 2.302715\n",
      "[257,  1400] loss: 2.302650\n",
      "[257,  1600] loss: 2.302642\n",
      "[257,  1800] loss: 2.302520\n",
      "[257,  2000] loss: 2.302684\n",
      "[257,  2200] loss: 2.302728\n",
      "[257,  2400] loss: 2.302627\n",
      "[257,  2600] loss: 2.302713\n",
      "[257,  2800] loss: 2.302582\n",
      "[257,  3000] loss: 2.302654\n",
      "[257,  3200] loss: 2.302537\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[258,   200] loss: 2.302581\n",
      "[258,   400] loss: 2.302766\n",
      "[258,   600] loss: 2.302584\n",
      "[258,   800] loss: 2.302770\n",
      "[258,  1000] loss: 2.302582\n",
      "[258,  1200] loss: 2.302571\n",
      "[258,  1400] loss: 2.302440\n",
      "[258,  1600] loss: 2.302920\n",
      "[258,  1800] loss: 2.302538\n",
      "[258,  2000] loss: 2.302705\n",
      "[258,  2200] loss: 2.302460\n",
      "[258,  2400] loss: 2.302668\n",
      "[258,  2600] loss: 2.302626\n",
      "[258,  2800] loss: 2.302776\n",
      "[258,  3000] loss: 2.302786\n",
      "[258,  3200] loss: 2.302634\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[259,   200] loss: 2.302719\n",
      "[259,   400] loss: 2.302697\n",
      "[259,   600] loss: 2.302705\n",
      "[259,   800] loss: 2.302444\n",
      "[259,  1000] loss: 2.302747\n",
      "[259,  1200] loss: 2.302771\n",
      "[259,  1400] loss: 2.302529\n",
      "[259,  1600] loss: 2.302535\n",
      "[259,  1800] loss: 2.302741\n",
      "[259,  2000] loss: 2.302698\n",
      "[259,  2200] loss: 2.302682\n",
      "[259,  2400] loss: 2.302586\n",
      "[259,  2600] loss: 2.302634\n",
      "[259,  2800] loss: 2.302763\n",
      "[259,  3000] loss: 2.302567\n",
      "[259,  3200] loss: 2.302792\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[260,   200] loss: 2.302659\n",
      "[260,   400] loss: 2.302560\n",
      "[260,   600] loss: 2.302623\n",
      "[260,   800] loss: 2.302627\n",
      "[260,  1000] loss: 2.302460\n",
      "[260,  1200] loss: 2.302582\n",
      "[260,  1400] loss: 2.302743\n",
      "[260,  1600] loss: 2.302661\n",
      "[260,  1800] loss: 2.302776\n",
      "[260,  2000] loss: 2.302742\n",
      "[260,  2200] loss: 2.302622\n",
      "[260,  2400] loss: 2.302657\n",
      "[260,  2600] loss: 2.302801\n",
      "[260,  2800] loss: 2.302682\n",
      "[260,  3000] loss: 2.302705\n",
      "[260,  3200] loss: 2.302592\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[261,   200] loss: 2.302614\n",
      "[261,   400] loss: 2.302711\n",
      "[261,   600] loss: 2.302751\n",
      "[261,   800] loss: 2.302399\n",
      "[261,  1000] loss: 2.302739\n",
      "[261,  1200] loss: 2.302590\n",
      "[261,  1400] loss: 2.302715\n",
      "[261,  1600] loss: 2.302596\n",
      "[261,  1800] loss: 2.302833\n",
      "[261,  2000] loss: 2.302737\n",
      "[261,  2200] loss: 2.302605\n",
      "[261,  2400] loss: 2.302744\n",
      "[261,  2600] loss: 2.302646\n",
      "[261,  2800] loss: 2.302640\n",
      "[261,  3000] loss: 2.302586\n",
      "[261,  3200] loss: 2.302681\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[262,   200] loss: 2.302612\n",
      "[262,   400] loss: 2.302632\n",
      "[262,   600] loss: 2.302525\n",
      "[262,   800] loss: 2.302765\n",
      "[262,  1000] loss: 2.302590\n",
      "[262,  1200] loss: 2.302677\n",
      "[262,  1400] loss: 2.302604\n",
      "[262,  1600] loss: 2.302649\n",
      "[262,  1800] loss: 2.302735\n",
      "[262,  2000] loss: 2.302743\n",
      "[262,  2200] loss: 2.302657\n",
      "[262,  2400] loss: 2.302828\n",
      "[262,  2600] loss: 2.302597\n",
      "[262,  2800] loss: 2.302585\n",
      "[262,  3000] loss: 2.302769\n",
      "[262,  3200] loss: 2.302723\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[263,   200] loss: 2.302162\n",
      "[263,   400] loss: 2.301999\n",
      "[263,   600] loss: 2.303116\n",
      "[263,   800] loss: 2.302863\n",
      "[263,  1000] loss: 2.302568\n",
      "[263,  1200] loss: 2.302888\n",
      "[263,  1400] loss: 2.302544\n",
      "[263,  1600] loss: 2.302846\n",
      "[263,  1800] loss: 2.302637\n",
      "[263,  2000] loss: 2.302643\n",
      "[263,  2200] loss: 2.302584\n",
      "[263,  2400] loss: 2.302542\n",
      "[263,  2600] loss: 2.302712\n",
      "[263,  2800] loss: 2.302728\n",
      "[263,  3000] loss: 2.302668\n",
      "[263,  3200] loss: 2.302745\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[264,   200] loss: 2.302527\n",
      "[264,   400] loss: 2.302933\n",
      "[264,   600] loss: 2.302693\n",
      "[264,   800] loss: 2.302514\n",
      "[264,  1000] loss: 2.302645\n",
      "[264,  1200] loss: 2.302383\n",
      "[264,  1400] loss: 2.302841\n",
      "[264,  1600] loss: 2.302615\n",
      "[264,  1800] loss: 2.302576\n",
      "[264,  2000] loss: 2.302651\n",
      "[264,  2200] loss: 2.302606\n",
      "[264,  2400] loss: 2.302587\n",
      "[264,  2600] loss: 2.302581\n",
      "[264,  2800] loss: 2.302846\n",
      "[264,  3000] loss: 2.302594\n",
      "[264,  3200] loss: 2.302620\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[265,   200] loss: 2.302630\n",
      "[265,   400] loss: 2.302771\n",
      "[265,   600] loss: 2.302688\n",
      "[265,   800] loss: 2.302497\n",
      "[265,  1000] loss: 2.302568\n",
      "[265,  1200] loss: 2.302881\n",
      "[265,  1400] loss: 2.302516\n",
      "[265,  1600] loss: 2.302696\n",
      "[265,  1800] loss: 2.302778\n",
      "[265,  2000] loss: 2.302310\n",
      "[265,  2200] loss: 2.302726\n",
      "[265,  2400] loss: 2.302596\n",
      "[265,  2600] loss: 2.302776\n",
      "[265,  2800] loss: 2.302390\n",
      "[265,  3000] loss: 2.302759\n",
      "[265,  3200] loss: 2.302739\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[266,   200] loss: 2.302635\n",
      "[266,   400] loss: 2.302797\n",
      "[266,   600] loss: 2.302650\n",
      "[266,   800] loss: 2.302611\n",
      "[266,  1000] loss: 2.302684\n",
      "[266,  1200] loss: 2.302593\n",
      "[266,  1400] loss: 2.302644\n",
      "[266,  1600] loss: 2.302672\n",
      "[266,  1800] loss: 2.302627\n",
      "[266,  2000] loss: 2.302700\n",
      "[266,  2200] loss: 2.302540\n",
      "[266,  2400] loss: 2.302625\n",
      "[266,  2600] loss: 2.302678\n",
      "[266,  2800] loss: 2.302399\n",
      "[266,  3000] loss: 2.302599\n",
      "[266,  3200] loss: 2.302899\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[267,   200] loss: 2.302620\n",
      "[267,   400] loss: 2.302589\n",
      "[267,   600] loss: 2.302519\n",
      "[267,   800] loss: 2.302450\n",
      "[267,  1000] loss: 2.302551\n",
      "[267,  1200] loss: 2.302700\n",
      "[267,  1400] loss: 2.302696\n",
      "[267,  1600] loss: 2.302726\n",
      "[267,  1800] loss: 2.302780\n",
      "[267,  2000] loss: 2.302600\n",
      "[267,  2200] loss: 2.302601\n",
      "[267,  2400] loss: 2.302602\n",
      "[267,  2600] loss: 2.302632\n",
      "[267,  2800] loss: 2.302884\n",
      "[267,  3000] loss: 2.302636\n",
      "[267,  3200] loss: 2.302800\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[268,   200] loss: 2.302538\n",
      "[268,   400] loss: 2.302615\n",
      "[268,   600] loss: 2.302749\n",
      "[268,   800] loss: 2.302599\n",
      "[268,  1000] loss: 2.302755\n",
      "[268,  1200] loss: 2.302613\n",
      "[268,  1400] loss: 2.302806\n",
      "[268,  1600] loss: 2.302610\n",
      "[268,  1800] loss: 2.302513\n",
      "[268,  2000] loss: 2.302836\n",
      "[268,  2200] loss: 2.302573\n",
      "[268,  2400] loss: 2.302715\n",
      "[268,  2600] loss: 2.302700\n",
      "[268,  2800] loss: 2.302508\n",
      "[268,  3000] loss: 2.302499\n",
      "[268,  3200] loss: 2.302597\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[269,   200] loss: 2.302765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269,   400] loss: 2.302542\n",
      "[269,   600] loss: 2.302677\n",
      "[269,   800] loss: 2.302646\n",
      "[269,  1000] loss: 2.302498\n",
      "[269,  1200] loss: 2.302570\n",
      "[269,  1400] loss: 2.302439\n",
      "[269,  1600] loss: 2.302720\n",
      "[269,  1800] loss: 2.302713\n",
      "[269,  2000] loss: 2.302784\n",
      "[269,  2200] loss: 2.302572\n",
      "[269,  2400] loss: 2.302583\n",
      "[269,  2600] loss: 2.302755\n",
      "[269,  2800] loss: 2.302682\n",
      "[269,  3000] loss: 2.302559\n",
      "[269,  3200] loss: 2.302875\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[270,   200] loss: 2.302512\n",
      "[270,   400] loss: 2.302493\n",
      "[270,   600] loss: 2.302546\n",
      "[270,   800] loss: 2.302816\n",
      "[270,  1000] loss: 2.302756\n",
      "[270,  1200] loss: 2.302784\n",
      "[270,  1400] loss: 2.302541\n",
      "[270,  1600] loss: 2.302526\n",
      "[270,  1800] loss: 2.302477\n",
      "[270,  2000] loss: 2.302741\n",
      "[270,  2200] loss: 2.302449\n",
      "[270,  2400] loss: 2.302532\n",
      "[270,  2600] loss: 2.302781\n",
      "[270,  2800] loss: 2.302657\n",
      "[270,  3000] loss: 2.302860\n",
      "[270,  3200] loss: 2.302612\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[271,   200] loss: 2.302570\n",
      "[271,   400] loss: 2.302713\n",
      "[271,   600] loss: 2.302810\n",
      "[271,   800] loss: 2.302483\n",
      "[271,  1000] loss: 2.302818\n",
      "[271,  1200] loss: 2.302665\n",
      "[271,  1400] loss: 2.302719\n",
      "[271,  1600] loss: 2.302655\n",
      "[271,  1800] loss: 2.302723\n",
      "[271,  2000] loss: 2.302645\n",
      "[271,  2200] loss: 2.302669\n",
      "[271,  2400] loss: 2.302570\n",
      "[271,  2600] loss: 2.302758\n",
      "[271,  2800] loss: 2.302720\n",
      "[271,  3000] loss: 2.302649\n",
      "[271,  3200] loss: 2.302504\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[272,   200] loss: 2.302712\n",
      "[272,   400] loss: 2.302800\n",
      "[272,   600] loss: 2.302536\n",
      "[272,   800] loss: 2.302674\n",
      "[272,  1000] loss: 2.302692\n",
      "[272,  1200] loss: 2.302793\n",
      "[272,  1400] loss: 2.302633\n",
      "[272,  1600] loss: 2.302746\n",
      "[272,  1800] loss: 2.302606\n",
      "[272,  2000] loss: 2.302537\n",
      "[272,  2200] loss: 2.302671\n",
      "[272,  2400] loss: 2.302664\n",
      "[272,  2600] loss: 2.302678\n",
      "[272,  2800] loss: 2.302606\n",
      "[272,  3000] loss: 2.302742\n",
      "[272,  3200] loss: 2.302705\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[273,   200] loss: 2.302717\n",
      "[273,   400] loss: 2.302481\n",
      "[273,   600] loss: 2.302849\n",
      "[273,   800] loss: 2.302423\n",
      "[273,  1000] loss: 2.302654\n",
      "[273,  1200] loss: 2.302677\n",
      "[273,  1400] loss: 2.302768\n",
      "[273,  1600] loss: 2.302816\n",
      "[273,  1800] loss: 2.302545\n",
      "[273,  2000] loss: 2.302812\n",
      "[273,  2200] loss: 2.302602\n",
      "[273,  2400] loss: 2.302432\n",
      "[273,  2600] loss: 2.302766\n",
      "[273,  2800] loss: 2.302536\n",
      "[273,  3000] loss: 2.302586\n",
      "[273,  3200] loss: 2.302454\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[274,   200] loss: 2.302502\n",
      "[274,   400] loss: 2.302741\n",
      "[274,   600] loss: 2.302864\n",
      "[274,   800] loss: 2.302718\n",
      "[274,  1000] loss: 2.302791\n",
      "[274,  1200] loss: 2.302561\n",
      "[274,  1400] loss: 2.302571\n",
      "[274,  1600] loss: 2.302612\n",
      "[274,  1800] loss: 2.302603\n",
      "[274,  2000] loss: 2.302667\n",
      "[274,  2200] loss: 2.302383\n",
      "[274,  2400] loss: 2.302862\n",
      "[274,  2600] loss: 2.302664\n",
      "[274,  2800] loss: 2.302578\n",
      "[274,  3000] loss: 2.302552\n",
      "[274,  3200] loss: 2.302685\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[275,   200] loss: 2.302889\n",
      "[275,   400] loss: 2.302579\n",
      "[275,   600] loss: 2.302749\n",
      "[275,   800] loss: 2.302760\n",
      "[275,  1000] loss: 2.302489\n",
      "[275,  1200] loss: 2.302676\n",
      "[275,  1400] loss: 2.302552\n",
      "[275,  1600] loss: 2.302374\n",
      "[275,  1800] loss: 2.302195\n",
      "[275,  2000] loss: 2.303043\n",
      "[275,  2200] loss: 2.302762\n",
      "[275,  2400] loss: 2.302610\n",
      "[275,  2600] loss: 2.302653\n",
      "[275,  2800] loss: 2.302659\n",
      "[275,  3000] loss: 2.302538\n",
      "[275,  3200] loss: 2.302705\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[276,   200] loss: 2.302545\n",
      "[276,   400] loss: 2.302459\n",
      "[276,   600] loss: 2.302395\n",
      "[276,   800] loss: 2.302723\n",
      "[276,  1000] loss: 2.302788\n",
      "[276,  1200] loss: 2.302737\n",
      "[276,  1400] loss: 2.302723\n",
      "[276,  1600] loss: 2.302612\n",
      "[276,  1800] loss: 2.302687\n",
      "[276,  2000] loss: 2.302703\n",
      "[276,  2200] loss: 2.302752\n",
      "[276,  2400] loss: 2.302598\n",
      "[276,  2600] loss: 2.302815\n",
      "[276,  2800] loss: 2.302727\n",
      "[276,  3000] loss: 2.302646\n",
      "[276,  3200] loss: 2.302622\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[277,   200] loss: 2.302542\n",
      "[277,   400] loss: 2.302705\n",
      "[277,   600] loss: 2.302625\n",
      "[277,   800] loss: 2.302790\n",
      "[277,  1000] loss: 2.302557\n",
      "[277,  1200] loss: 2.302577\n",
      "[277,  1400] loss: 2.302576\n",
      "[277,  1600] loss: 2.302732\n",
      "[277,  1800] loss: 2.302773\n",
      "[277,  2000] loss: 2.302703\n",
      "[277,  2200] loss: 2.302702\n",
      "[277,  2400] loss: 2.302607\n",
      "[277,  2600] loss: 2.302530\n",
      "[277,  2800] loss: 2.302464\n",
      "[277,  3000] loss: 2.302778\n",
      "[277,  3200] loss: 2.302599\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[278,   200] loss: 2.302609\n",
      "[278,   400] loss: 2.302840\n",
      "[278,   600] loss: 2.302509\n",
      "[278,   800] loss: 2.302558\n",
      "[278,  1000] loss: 2.302800\n",
      "[278,  1200] loss: 2.302581\n",
      "[278,  1400] loss: 2.302754\n",
      "[278,  1600] loss: 2.302488\n",
      "[278,  1800] loss: 2.302423\n",
      "[278,  2000] loss: 2.302586\n",
      "[278,  2200] loss: 2.302787\n",
      "[278,  2400] loss: 2.302866\n",
      "[278,  2600] loss: 2.302606\n",
      "[278,  2800] loss: 2.302629\n",
      "[278,  3000] loss: 2.302473\n",
      "[278,  3200] loss: 2.302781\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[279,   200] loss: 2.302704\n",
      "[279,   400] loss: 2.302621\n",
      "[279,   600] loss: 2.302636\n",
      "[279,   800] loss: 2.302589\n",
      "[279,  1000] loss: 2.302537\n",
      "[279,  1200] loss: 2.302853\n",
      "[279,  1400] loss: 2.302751\n",
      "[279,  1600] loss: 2.302631\n",
      "[279,  1800] loss: 2.302510\n",
      "[279,  2000] loss: 2.302521\n",
      "[279,  2200] loss: 2.302626\n",
      "[279,  2400] loss: 2.302860\n",
      "[279,  2600] loss: 2.302660\n",
      "[279,  2800] loss: 2.302673\n",
      "[279,  3000] loss: 2.302717\n",
      "[279,  3200] loss: 2.302536\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[280,   200] loss: 2.302549\n",
      "[280,   400] loss: 2.302708\n",
      "[280,   600] loss: 2.302697\n",
      "[280,   800] loss: 2.302654\n",
      "[280,  1000] loss: 2.302770\n",
      "[280,  1200] loss: 2.302514\n",
      "[280,  1400] loss: 2.302716\n",
      "[280,  1600] loss: 2.302780\n",
      "[280,  1800] loss: 2.302660\n",
      "[280,  2000] loss: 2.302546\n",
      "[280,  2200] loss: 2.302594\n",
      "[280,  2400] loss: 2.302552\n",
      "[280,  2600] loss: 2.302691\n",
      "[280,  2800] loss: 2.302520\n",
      "[280,  3000] loss: 2.302806\n",
      "[280,  3200] loss: 2.302603\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[281,   200] loss: 2.302744\n",
      "[281,   400] loss: 2.302645\n",
      "[281,   600] loss: 2.302612\n",
      "[281,   800] loss: 2.302613\n",
      "[281,  1000] loss: 2.302559\n",
      "[281,  1200] loss: 2.302631\n",
      "[281,  1400] loss: 2.302639\n",
      "[281,  1600] loss: 2.302834\n",
      "[281,  1800] loss: 2.302597\n",
      "[281,  2000] loss: 2.302811\n",
      "[281,  2200] loss: 2.302542\n",
      "[281,  2400] loss: 2.302702\n",
      "[281,  2600] loss: 2.302692\n",
      "[281,  2800] loss: 2.302706\n",
      "[281,  3000] loss: 2.302654\n",
      "[281,  3200] loss: 2.302559\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[282,   200] loss: 2.302777\n",
      "[282,   400] loss: 2.302469\n",
      "[282,   600] loss: 2.302879\n",
      "[282,   800] loss: 2.302631\n",
      "[282,  1000] loss: 2.302625\n",
      "[282,  1200] loss: 2.302553\n",
      "[282,  1400] loss: 2.302537\n",
      "[282,  1600] loss: 2.302750\n",
      "[282,  1800] loss: 2.302491\n",
      "[282,  2000] loss: 2.302620\n",
      "[282,  2200] loss: 2.302551\n",
      "[282,  2400] loss: 2.302765\n",
      "[282,  2600] loss: 2.302730\n",
      "[282,  2800] loss: 2.302523\n",
      "[282,  3000] loss: 2.302343\n",
      "[282,  3200] loss: 2.302771\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[283,   200] loss: 2.302457\n",
      "[283,   400] loss: 2.302845\n",
      "[283,   600] loss: 2.302538\n",
      "[283,   800] loss: 2.302647\n",
      "[283,  1000] loss: 2.302599\n",
      "[283,  1200] loss: 2.302737\n",
      "[283,  1400] loss: 2.302648\n",
      "[283,  1600] loss: 2.302695\n",
      "[283,  1800] loss: 2.302556\n",
      "[283,  2000] loss: 2.302575\n",
      "[283,  2200] loss: 2.302337\n",
      "[283,  2400] loss: 2.302695\n",
      "[283,  2600] loss: 2.302727\n",
      "[283,  2800] loss: 2.302667\n",
      "[283,  3000] loss: 2.302545\n",
      "[283,  3200] loss: 2.302622\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[284,   200] loss: 2.302553\n",
      "[284,   400] loss: 2.302540\n",
      "[284,   600] loss: 2.302488\n",
      "[284,   800] loss: 2.302721\n",
      "[284,  1000] loss: 2.302903\n",
      "[284,  1200] loss: 2.302652\n",
      "[284,  1400] loss: 2.302703\n",
      "[284,  1600] loss: 2.302541\n",
      "[284,  1800] loss: 2.302552\n",
      "[284,  2000] loss: 2.302763\n",
      "[284,  2200] loss: 2.302704\n",
      "[284,  2400] loss: 2.302589\n",
      "[284,  2600] loss: 2.302618\n",
      "[284,  2800] loss: 2.302635\n",
      "[284,  3000] loss: 2.302640\n",
      "[284,  3200] loss: 2.302567\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[285,   200] loss: 2.302507\n",
      "[285,   400] loss: 2.302653\n",
      "[285,   600] loss: 2.302596\n",
      "[285,   800] loss: 2.302744\n",
      "[285,  1000] loss: 2.302769\n",
      "[285,  1200] loss: 2.302542\n",
      "[285,  1400] loss: 2.302641\n",
      "[285,  1600] loss: 2.302787\n",
      "[285,  1800] loss: 2.302758\n",
      "[285,  2000] loss: 2.302662\n",
      "[285,  2200] loss: 2.302710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285,  2400] loss: 2.302663\n",
      "[285,  2600] loss: 2.302799\n",
      "[285,  2800] loss: 2.302688\n",
      "[285,  3000] loss: 2.302507\n",
      "[285,  3200] loss: 2.302641\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[286,   200] loss: 2.302672\n",
      "[286,   400] loss: 2.302614\n",
      "[286,   600] loss: 2.302744\n",
      "[286,   800] loss: 2.302628\n",
      "[286,  1000] loss: 2.302597\n",
      "[286,  1200] loss: 2.302590\n",
      "[286,  1400] loss: 2.302519\n",
      "[286,  1600] loss: 2.302791\n",
      "[286,  1800] loss: 2.302566\n",
      "[286,  2000] loss: 2.302498\n",
      "[286,  2200] loss: 2.302483\n",
      "[286,  2400] loss: 2.302935\n",
      "[286,  2600] loss: 2.302842\n",
      "[286,  2800] loss: 2.302618\n",
      "[286,  3000] loss: 2.302573\n",
      "[286,  3200] loss: 2.302637\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[287,   200] loss: 2.302575\n",
      "[287,   400] loss: 2.302479\n",
      "[287,   600] loss: 2.302836\n",
      "[287,   800] loss: 2.302811\n",
      "[287,  1000] loss: 2.302547\n",
      "[287,  1200] loss: 2.302746\n",
      "[287,  1400] loss: 2.302746\n",
      "[287,  1600] loss: 2.302697\n",
      "[287,  1800] loss: 2.302345\n",
      "[287,  2000] loss: 2.302906\n",
      "[287,  2200] loss: 2.302665\n",
      "[287,  2400] loss: 2.302664\n",
      "[287,  2600] loss: 2.302461\n",
      "[287,  2800] loss: 2.302448\n",
      "[287,  3000] loss: 2.302734\n",
      "[287,  3200] loss: 2.302721\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[288,   200] loss: 2.302749\n",
      "[288,   400] loss: 2.302745\n",
      "[288,   600] loss: 2.302605\n",
      "[288,   800] loss: 2.302706\n",
      "[288,  1000] loss: 2.302674\n",
      "[288,  1200] loss: 2.302696\n",
      "[288,  1400] loss: 2.302432\n",
      "[288,  1600] loss: 2.302492\n",
      "[288,  1800] loss: 2.302617\n",
      "[288,  2000] loss: 2.302775\n",
      "[288,  2200] loss: 2.302563\n",
      "[288,  2400] loss: 2.302730\n",
      "[288,  2600] loss: 2.302609\n",
      "[288,  2800] loss: 2.302766\n",
      "[288,  3000] loss: 2.302667\n",
      "[288,  3200] loss: 2.302603\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[289,   200] loss: 2.302847\n",
      "[289,   400] loss: 2.302764\n",
      "[289,   600] loss: 2.302651\n",
      "[289,   800] loss: 2.302459\n",
      "[289,  1000] loss: 2.302631\n",
      "[289,  1200] loss: 2.302768\n",
      "[289,  1400] loss: 2.302468\n",
      "[289,  1600] loss: 2.302769\n",
      "[289,  1800] loss: 2.302711\n",
      "[289,  2000] loss: 2.302483\n",
      "[289,  2200] loss: 2.302394\n",
      "[289,  2400] loss: 2.302676\n",
      "[289,  2600] loss: 2.302933\n",
      "[289,  2800] loss: 2.302568\n",
      "[289,  3000] loss: 2.302583\n",
      "[289,  3200] loss: 2.302568\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[290,   200] loss: 2.302677\n",
      "[290,   400] loss: 2.302577\n",
      "[290,   600] loss: 2.302821\n",
      "[290,   800] loss: 2.302742\n",
      "[290,  1000] loss: 2.302527\n",
      "[290,  1200] loss: 2.302886\n",
      "[290,  1400] loss: 2.302737\n",
      "[290,  1600] loss: 2.302623\n",
      "[290,  1800] loss: 2.302605\n",
      "[290,  2000] loss: 2.302432\n",
      "[290,  2200] loss: 2.302691\n",
      "[290,  2400] loss: 2.302667\n",
      "[290,  2600] loss: 2.302582\n",
      "[290,  2800] loss: 2.302671\n",
      "[290,  3000] loss: 2.302536\n",
      "[290,  3200] loss: 2.302546\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[291,   200] loss: 2.302533\n",
      "[291,   400] loss: 2.302616\n",
      "[291,   600] loss: 2.302450\n",
      "[291,   800] loss: 2.302536\n",
      "[291,  1000] loss: 2.302783\n",
      "[291,  1200] loss: 2.302811\n",
      "[291,  1400] loss: 2.302630\n",
      "[291,  1600] loss: 2.302657\n",
      "[291,  1800] loss: 2.302419\n",
      "[291,  2000] loss: 2.302325\n",
      "[291,  2200] loss: 2.302686\n",
      "[291,  2400] loss: 2.302828\n",
      "[291,  2600] loss: 2.302638\n",
      "[291,  2800] loss: 2.302785\n",
      "[291,  3000] loss: 2.302720\n",
      "[291,  3200] loss: 2.302651\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[292,   200] loss: 2.302552\n",
      "[292,   400] loss: 2.302692\n",
      "[292,   600] loss: 2.302490\n",
      "[292,   800] loss: 2.302549\n",
      "[292,  1000] loss: 2.302493\n",
      "[292,  1200] loss: 2.302771\n",
      "[292,  1400] loss: 2.302555\n",
      "[292,  1600] loss: 2.302889\n",
      "[292,  1800] loss: 2.302671\n",
      "[292,  2000] loss: 2.302298\n",
      "[292,  2200] loss: 2.302852\n",
      "[292,  2400] loss: 2.302763\n",
      "[292,  2600] loss: 2.302698\n",
      "[292,  2800] loss: 2.302535\n",
      "[292,  3000] loss: 2.302585\n",
      "[292,  3200] loss: 2.302768\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[293,   200] loss: 2.302707\n",
      "[293,   400] loss: 2.302633\n",
      "[293,   600] loss: 2.302557\n",
      "[293,   800] loss: 2.302633\n",
      "[293,  1000] loss: 2.302655\n",
      "[293,  1200] loss: 2.302847\n",
      "[293,  1400] loss: 2.302810\n",
      "[293,  1600] loss: 2.302607\n",
      "[293,  1800] loss: 2.302734\n",
      "[293,  2000] loss: 2.302494\n",
      "[293,  2200] loss: 2.302797\n",
      "[293,  2400] loss: 2.302748\n",
      "[293,  2600] loss: 2.302585\n",
      "[293,  2800] loss: 2.302674\n",
      "[293,  3000] loss: 2.302576\n",
      "[293,  3200] loss: 2.302597\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[294,   200] loss: 2.302605\n",
      "[294,   400] loss: 2.302743\n",
      "[294,   600] loss: 2.302771\n",
      "[294,   800] loss: 2.302513\n",
      "[294,  1000] loss: 2.302616\n",
      "[294,  1200] loss: 2.302517\n",
      "[294,  1400] loss: 2.302715\n",
      "[294,  1600] loss: 2.302796\n",
      "[294,  1800] loss: 2.302350\n",
      "[294,  2000] loss: 2.302824\n",
      "[294,  2200] loss: 2.302524\n",
      "[294,  2400] loss: 2.302776\n",
      "[294,  2600] loss: 2.302553\n",
      "[294,  2800] loss: 2.302566\n",
      "[294,  3000] loss: 2.302742\n",
      "[294,  3200] loss: 2.302737\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[295,   200] loss: 2.302558\n",
      "[295,   400] loss: 2.302659\n",
      "[295,   600] loss: 2.302708\n",
      "[295,   800] loss: 2.302529\n",
      "[295,  1000] loss: 2.302564\n",
      "[295,  1200] loss: 2.302741\n",
      "[295,  1400] loss: 2.302498\n",
      "[295,  1600] loss: 2.302796\n",
      "[295,  1800] loss: 2.302626\n",
      "[295,  2000] loss: 2.302694\n",
      "[295,  2200] loss: 2.302629\n",
      "[295,  2400] loss: 2.302432\n",
      "[295,  2600] loss: 2.302875\n",
      "[295,  2800] loss: 2.302527\n",
      "[295,  3000] loss: 2.302733\n",
      "[295,  3200] loss: 2.302667\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[296,   200] loss: 2.302729\n",
      "[296,   400] loss: 2.302602\n",
      "[296,   600] loss: 2.302734\n",
      "[296,   800] loss: 2.302516\n",
      "[296,  1000] loss: 2.302945\n",
      "[296,  1200] loss: 2.302668\n",
      "[296,  1400] loss: 2.302670\n",
      "[296,  1600] loss: 2.302648\n",
      "[296,  1800] loss: 2.302704\n",
      "[296,  2000] loss: 2.302444\n",
      "[296,  2200] loss: 2.302686\n",
      "[296,  2400] loss: 2.302414\n",
      "[296,  2600] loss: 2.302607\n",
      "[296,  2800] loss: 2.302544\n",
      "[296,  3000] loss: 2.302432\n",
      "[296,  3200] loss: 2.302798\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[297,   200] loss: 2.302531\n",
      "[297,   400] loss: 2.302675\n",
      "[297,   600] loss: 2.302647\n",
      "[297,   800] loss: 2.302745\n",
      "[297,  1000] loss: 2.302443\n",
      "[297,  1200] loss: 2.302491\n",
      "[297,  1400] loss: 2.302647\n",
      "[297,  1600] loss: 2.302849\n",
      "[297,  1800] loss: 2.302344\n",
      "[297,  2000] loss: 2.302704\n",
      "[297,  2200] loss: 2.302761\n",
      "[297,  2400] loss: 2.302626\n",
      "[297,  2600] loss: 2.302762\n",
      "[297,  2800] loss: 2.302744\n",
      "[297,  3000] loss: 2.302654\n",
      "[297,  3200] loss: 2.302360\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[298,   200] loss: 2.302633\n",
      "[298,   400] loss: 2.302599\n",
      "[298,   600] loss: 2.302799\n",
      "[298,   800] loss: 2.302499\n",
      "[298,  1000] loss: 2.302822\n",
      "[298,  1200] loss: 2.302675\n",
      "[298,  1400] loss: 2.302719\n",
      "[298,  1600] loss: 2.302826\n",
      "[298,  1800] loss: 2.302518\n",
      "[298,  2000] loss: 2.302471\n",
      "[298,  2200] loss: 2.302751\n",
      "[298,  2400] loss: 2.302733\n",
      "[298,  2600] loss: 2.302693\n",
      "[298,  2800] loss: 2.302547\n",
      "[298,  3000] loss: 2.302523\n",
      "[298,  3200] loss: 2.302881\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[299,   200] loss: 2.302467\n",
      "[299,   400] loss: 2.302690\n",
      "[299,   600] loss: 2.302453\n",
      "[299,   800] loss: 2.302602\n",
      "[299,  1000] loss: 2.302794\n",
      "[299,  1200] loss: 2.302395\n",
      "[299,  1400] loss: 2.302558\n",
      "[299,  1600] loss: 2.302775\n",
      "[299,  1800] loss: 2.302785\n",
      "[299,  2000] loss: 2.302581\n",
      "[299,  2200] loss: 2.302557\n",
      "[299,  2400] loss: 2.302637\n",
      "[299,  2600] loss: 2.302838\n",
      "[299,  2800] loss: 2.302663\n",
      "[299,  3000] loss: 2.302831\n",
      "[299,  3200] loss: 2.302559\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[300,   200] loss: 2.302486\n",
      "[300,   400] loss: 2.302888\n",
      "[300,   600] loss: 2.302604\n",
      "[300,   800] loss: 2.302806\n",
      "[300,  1000] loss: 2.302768\n",
      "[300,  1200] loss: 2.302501\n",
      "[300,  1400] loss: 2.302663\n",
      "[300,  1600] loss: 2.302790\n",
      "[300,  1800] loss: 2.302594\n",
      "[300,  2000] loss: 2.302627\n",
      "[300,  2200] loss: 2.302332\n",
      "[300,  2400] loss: 2.302761\n",
      "[300,  2600] loss: 2.302618\n",
      "[300,  2800] loss: 2.302518\n",
      "[300,  3000] loss: 2.302357\n",
      "[300,  3200] loss: 2.302747\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[301,   200] loss: 2.302515\n",
      "[301,   400] loss: 2.302503\n",
      "[301,   600] loss: 2.302568\n",
      "[301,   800] loss: 2.302817\n",
      "[301,  1000] loss: 2.302454\n",
      "[301,  1200] loss: 2.302514\n",
      "[301,  1400] loss: 2.302784\n",
      "[301,  1600] loss: 2.302529\n",
      "[301,  1800] loss: 2.302842\n",
      "[301,  2000] loss: 2.302693\n",
      "[301,  2200] loss: 2.302547\n",
      "[301,  2400] loss: 2.302698\n",
      "[301,  2600] loss: 2.302833\n",
      "[301,  2800] loss: 2.302622\n",
      "[301,  3000] loss: 2.302732\n",
      "[301,  3200] loss: 2.302641\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[302,   200] loss: 2.302733\n",
      "[302,   400] loss: 2.302695\n",
      "[302,   600] loss: 2.302627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302,   800] loss: 2.302585\n",
      "[302,  1000] loss: 2.302481\n",
      "[302,  1200] loss: 2.302268\n",
      "[302,  1400] loss: 2.302948\n",
      "[302,  1600] loss: 2.302789\n",
      "[302,  1800] loss: 2.302599\n",
      "[302,  2000] loss: 2.302659\n",
      "[302,  2200] loss: 2.302766\n",
      "[302,  2400] loss: 2.302305\n",
      "[302,  2600] loss: 2.302729\n",
      "[302,  2800] loss: 2.302858\n",
      "[302,  3000] loss: 2.302675\n",
      "[302,  3200] loss: 2.302603\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[303,   200] loss: 2.302636\n",
      "[303,   400] loss: 2.302558\n",
      "[303,   600] loss: 2.302691\n",
      "[303,   800] loss: 2.302599\n",
      "[303,  1000] loss: 2.302887\n",
      "[303,  1200] loss: 2.302563\n",
      "[303,  1400] loss: 2.302751\n",
      "[303,  1600] loss: 2.302653\n",
      "[303,  1800] loss: 2.302752\n",
      "[303,  2000] loss: 2.302631\n",
      "[303,  2200] loss: 2.302800\n",
      "[303,  2400] loss: 2.302602\n",
      "[303,  2600] loss: 2.302746\n",
      "[303,  2800] loss: 2.302506\n",
      "[303,  3000] loss: 2.302591\n",
      "[303,  3200] loss: 2.302524\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[304,   200] loss: 2.302713\n",
      "[304,   400] loss: 2.302751\n",
      "[304,   600] loss: 2.302580\n",
      "[304,   800] loss: 2.302396\n",
      "[304,  1000] loss: 2.302832\n",
      "[304,  1200] loss: 2.302718\n",
      "[304,  1400] loss: 2.302498\n",
      "[304,  1600] loss: 2.302599\n",
      "[304,  1800] loss: 2.302617\n",
      "[304,  2000] loss: 2.302661\n",
      "[304,  2200] loss: 2.302629\n",
      "[304,  2400] loss: 2.302637\n",
      "[304,  2600] loss: 2.302314\n",
      "[304,  2800] loss: 2.302751\n",
      "[304,  3000] loss: 2.302737\n",
      "[304,  3200] loss: 2.302597\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[305,   200] loss: 2.302328\n",
      "[305,   400] loss: 2.302824\n",
      "[305,   600] loss: 2.302579\n",
      "[305,   800] loss: 2.302807\n",
      "[305,  1000] loss: 2.302676\n",
      "[305,  1200] loss: 2.302435\n",
      "[305,  1400] loss: 2.302725\n",
      "[305,  1600] loss: 2.302754\n",
      "[305,  1800] loss: 2.302598\n",
      "[305,  2000] loss: 2.302646\n",
      "[305,  2200] loss: 2.302782\n",
      "[305,  2400] loss: 2.302585\n",
      "[305,  2600] loss: 2.302553\n",
      "[305,  2800] loss: 2.302611\n",
      "[305,  3000] loss: 2.302695\n",
      "[305,  3200] loss: 2.302826\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[306,   200] loss: 2.302576\n",
      "[306,   400] loss: 2.302818\n",
      "[306,   600] loss: 2.302573\n",
      "[306,   800] loss: 2.302657\n",
      "[306,  1000] loss: 2.302488\n",
      "[306,  1200] loss: 2.302777\n",
      "[306,  1400] loss: 2.302727\n",
      "[306,  1600] loss: 2.302603\n",
      "[306,  1800] loss: 2.302633\n",
      "[306,  2000] loss: 2.302478\n",
      "[306,  2200] loss: 2.302717\n",
      "[306,  2400] loss: 2.302581\n",
      "[306,  2600] loss: 2.302568\n",
      "[306,  2800] loss: 2.302633\n",
      "[306,  3000] loss: 2.302656\n",
      "[306,  3200] loss: 2.302621\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[307,   200] loss: 2.302458\n",
      "[307,   400] loss: 2.302819\n",
      "[307,   600] loss: 2.302700\n",
      "[307,   800] loss: 2.302543\n",
      "[307,  1000] loss: 2.302587\n",
      "[307,  1200] loss: 2.302570\n",
      "[307,  1400] loss: 2.302692\n",
      "[307,  1600] loss: 2.302570\n",
      "[307,  1800] loss: 2.302774\n",
      "[307,  2000] loss: 2.302628\n",
      "[307,  2200] loss: 2.302701\n",
      "[307,  2400] loss: 2.302413\n",
      "[307,  2600] loss: 2.302589\n",
      "[307,  2800] loss: 2.302759\n",
      "[307,  3000] loss: 2.302517\n",
      "[307,  3200] loss: 2.302751\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[308,   200] loss: 2.302621\n",
      "[308,   400] loss: 2.302796\n",
      "[308,   600] loss: 2.302725\n",
      "[308,   800] loss: 2.302546\n",
      "[308,  1000] loss: 2.302638\n",
      "[308,  1200] loss: 2.302663\n",
      "[308,  1400] loss: 2.302476\n",
      "[308,  1600] loss: 2.302834\n",
      "[308,  1800] loss: 2.302672\n",
      "[308,  2000] loss: 2.302565\n",
      "[308,  2200] loss: 2.302657\n",
      "[308,  2400] loss: 2.302510\n",
      "[308,  2600] loss: 2.302581\n",
      "[308,  2800] loss: 2.302678\n",
      "[308,  3000] loss: 2.302685\n",
      "[308,  3200] loss: 2.302566\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[309,   200] loss: 2.302640\n",
      "[309,   400] loss: 2.302719\n",
      "[309,   600] loss: 2.302652\n",
      "[309,   800] loss: 2.302765\n",
      "[309,  1000] loss: 2.302688\n",
      "[309,  1200] loss: 2.302624\n",
      "[309,  1400] loss: 2.302753\n",
      "[309,  1600] loss: 2.302587\n",
      "[309,  1800] loss: 2.302608\n",
      "[309,  2000] loss: 2.302741\n",
      "[309,  2200] loss: 2.302608\n",
      "[309,  2400] loss: 2.302526\n",
      "[309,  2600] loss: 2.302514\n",
      "[309,  2800] loss: 2.302573\n",
      "[309,  3000] loss: 2.302727\n",
      "[309,  3200] loss: 2.302840\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[310,   200] loss: 2.302611\n",
      "[310,   400] loss: 2.302380\n",
      "[310,   600] loss: 2.302560\n",
      "[310,   800] loss: 2.302553\n",
      "[310,  1000] loss: 2.302677\n",
      "[310,  1200] loss: 2.302512\n",
      "[310,  1400] loss: 2.302807\n",
      "[310,  1600] loss: 2.302787\n",
      "[310,  1800] loss: 2.302324\n",
      "[310,  2000] loss: 2.302839\n",
      "[310,  2200] loss: 2.302725\n",
      "[310,  2400] loss: 2.302660\n",
      "[310,  2600] loss: 2.302717\n",
      "[310,  2800] loss: 2.302811\n",
      "[310,  3000] loss: 2.302448\n",
      "[310,  3200] loss: 2.302720\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[311,   200] loss: 2.302597\n",
      "[311,   400] loss: 2.302792\n",
      "[311,   600] loss: 2.302722\n",
      "[311,   800] loss: 2.302659\n",
      "[311,  1000] loss: 2.302490\n",
      "[311,  1200] loss: 2.302815\n",
      "[311,  1400] loss: 2.302701\n",
      "[311,  1600] loss: 2.302587\n",
      "[311,  1800] loss: 2.302673\n",
      "[311,  2000] loss: 2.302721\n",
      "[311,  2200] loss: 2.302752\n",
      "[311,  2400] loss: 2.302613\n",
      "[311,  2600] loss: 2.302505\n",
      "[311,  2800] loss: 2.302839\n",
      "[311,  3000] loss: 2.302637\n",
      "[311,  3200] loss: 2.302664\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[312,   200] loss: 2.302691\n",
      "[312,   400] loss: 2.302635\n",
      "[312,   600] loss: 2.302658\n",
      "[312,   800] loss: 2.302612\n",
      "[312,  1000] loss: 2.302632\n",
      "[312,  1200] loss: 2.302593\n",
      "[312,  1400] loss: 2.302782\n",
      "[312,  1600] loss: 2.302675\n",
      "[312,  1800] loss: 2.302627\n",
      "[312,  2000] loss: 2.302819\n",
      "[312,  2200] loss: 2.302518\n",
      "[312,  2400] loss: 2.302667\n",
      "[312,  2600] loss: 2.302713\n",
      "[312,  2800] loss: 2.302514\n",
      "[312,  3000] loss: 2.302647\n",
      "[312,  3200] loss: 2.302716\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[313,   200] loss: 2.302499\n",
      "[313,   400] loss: 2.302335\n",
      "[313,   600] loss: 2.302691\n",
      "[313,   800] loss: 2.302215\n",
      "[313,  1000] loss: 2.302894\n",
      "[313,  1200] loss: 2.302837\n",
      "[313,  1400] loss: 2.302530\n",
      "[313,  1600] loss: 2.302646\n",
      "[313,  1800] loss: 2.302739\n",
      "[313,  2000] loss: 2.302691\n",
      "[313,  2200] loss: 2.302670\n",
      "[313,  2400] loss: 2.302642\n",
      "[313,  2600] loss: 2.302793\n",
      "[313,  2800] loss: 2.302752\n",
      "[313,  3000] loss: 2.302668\n",
      "[313,  3200] loss: 2.302438\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[314,   200] loss: 2.302556\n",
      "[314,   400] loss: 2.302531\n",
      "[314,   600] loss: 2.302628\n",
      "[314,   800] loss: 2.302330\n",
      "[314,  1000] loss: 2.302406\n",
      "[314,  1200] loss: 2.302690\n",
      "[314,  1400] loss: 2.302557\n",
      "[314,  1600] loss: 2.302912\n",
      "[314,  1800] loss: 2.302719\n",
      "[314,  2000] loss: 2.302750\n",
      "[314,  2200] loss: 2.302731\n",
      "[314,  2400] loss: 2.302695\n",
      "[314,  2600] loss: 2.302799\n",
      "[314,  2800] loss: 2.302579\n",
      "[314,  3000] loss: 2.302689\n",
      "[314,  3200] loss: 2.302706\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[315,   200] loss: 2.302240\n",
      "[315,   400] loss: 2.302751\n",
      "[315,   600] loss: 2.302580\n",
      "[315,   800] loss: 2.302913\n",
      "[315,  1000] loss: 2.302725\n",
      "[315,  1200] loss: 2.302698\n",
      "[315,  1400] loss: 2.302623\n",
      "[315,  1600] loss: 2.302738\n",
      "[315,  1800] loss: 2.302677\n",
      "[315,  2000] loss: 2.302712\n",
      "[315,  2200] loss: 2.302763\n",
      "[315,  2400] loss: 2.302366\n",
      "[315,  2600] loss: 2.302539\n",
      "[315,  2800] loss: 2.302666\n",
      "[315,  3000] loss: 2.302809\n",
      "[315,  3200] loss: 2.302627\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[316,   200] loss: 2.302783\n",
      "[316,   400] loss: 2.302569\n",
      "[316,   600] loss: 2.302450\n",
      "[316,   800] loss: 2.303002\n",
      "[316,  1000] loss: 2.302592\n",
      "[316,  1200] loss: 2.302593\n",
      "[316,  1400] loss: 2.302501\n",
      "[316,  1600] loss: 2.302672\n",
      "[316,  1800] loss: 2.302557\n",
      "[316,  2000] loss: 2.302704\n",
      "[316,  2200] loss: 2.302686\n",
      "[316,  2400] loss: 2.302699\n",
      "[316,  2600] loss: 2.302689\n",
      "[316,  2800] loss: 2.302748\n",
      "[316,  3000] loss: 2.302712\n",
      "[316,  3200] loss: 2.302603\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[317,   200] loss: 2.302603\n",
      "[317,   400] loss: 2.302684\n",
      "[317,   600] loss: 2.302862\n",
      "[317,   800] loss: 2.302466\n",
      "[317,  1000] loss: 2.302801\n",
      "[317,  1200] loss: 2.302385\n",
      "[317,  1400] loss: 2.302561\n",
      "[317,  1600] loss: 2.302545\n",
      "[317,  1800] loss: 2.302439\n",
      "[317,  2000] loss: 2.302504\n",
      "[317,  2200] loss: 2.302751\n",
      "[317,  2400] loss: 2.302746\n",
      "[317,  2600] loss: 2.302675\n",
      "[317,  2800] loss: 2.302655\n",
      "[317,  3000] loss: 2.302551\n",
      "[317,  3200] loss: 2.302813\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[318,   200] loss: 2.302592\n",
      "[318,   400] loss: 2.302717\n",
      "[318,   600] loss: 2.302740\n",
      "[318,   800] loss: 2.302594\n",
      "[318,  1000] loss: 2.302691\n",
      "[318,  1200] loss: 2.302658\n",
      "[318,  1400] loss: 2.302544\n",
      "[318,  1600] loss: 2.302446\n",
      "[318,  1800] loss: 2.302825\n",
      "[318,  2000] loss: 2.302606\n",
      "[318,  2200] loss: 2.302783\n",
      "[318,  2400] loss: 2.302469\n",
      "[318,  2600] loss: 2.302606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318,  2800] loss: 2.302641\n",
      "[318,  3000] loss: 2.302708\n",
      "[318,  3200] loss: 2.302801\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[319,   200] loss: 2.302436\n",
      "[319,   400] loss: 2.302539\n",
      "[319,   600] loss: 2.302680\n",
      "[319,   800] loss: 2.302575\n",
      "[319,  1000] loss: 2.302483\n",
      "[319,  1200] loss: 2.302821\n",
      "[319,  1400] loss: 2.302551\n",
      "[319,  1600] loss: 2.302351\n",
      "[319,  1800] loss: 2.302642\n",
      "[319,  2000] loss: 2.302412\n",
      "[319,  2200] loss: 2.302797\n",
      "[319,  2400] loss: 2.302869\n",
      "[319,  2600] loss: 2.302531\n",
      "[319,  2800] loss: 2.302745\n",
      "[319,  3000] loss: 2.302693\n",
      "[319,  3200] loss: 2.302732\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[320,   200] loss: 2.302741\n",
      "[320,   400] loss: 2.302604\n",
      "[320,   600] loss: 2.302678\n",
      "[320,   800] loss: 2.302731\n",
      "[320,  1000] loss: 2.302711\n",
      "[320,  1200] loss: 2.302786\n",
      "[320,  1400] loss: 2.302569\n",
      "[320,  1600] loss: 2.302720\n",
      "[320,  1800] loss: 2.302567\n",
      "[320,  2000] loss: 2.302331\n",
      "[320,  2200] loss: 2.302524\n",
      "[320,  2400] loss: 2.302555\n",
      "[320,  2600] loss: 2.302734\n",
      "[320,  2800] loss: 2.302707\n",
      "[320,  3000] loss: 2.302814\n",
      "[320,  3200] loss: 2.302776\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[321,   200] loss: 2.302745\n",
      "[321,   400] loss: 2.302739\n",
      "[321,   600] loss: 2.302391\n",
      "[321,   800] loss: 2.302718\n",
      "[321,  1000] loss: 2.302622\n",
      "[321,  1200] loss: 2.302694\n",
      "[321,  1400] loss: 2.302754\n",
      "[321,  1600] loss: 2.302645\n",
      "[321,  1800] loss: 2.302733\n",
      "[321,  2000] loss: 2.302681\n",
      "[321,  2200] loss: 2.302794\n",
      "[321,  2400] loss: 2.302727\n",
      "[321,  2600] loss: 2.302663\n",
      "[321,  2800] loss: 2.302523\n",
      "[321,  3000] loss: 2.302675\n",
      "[321,  3200] loss: 2.302543\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[322,   200] loss: 2.302796\n",
      "[322,   400] loss: 2.302531\n",
      "[322,   600] loss: 2.302723\n",
      "[322,   800] loss: 2.302799\n",
      "[322,  1000] loss: 2.302570\n",
      "[322,  1200] loss: 2.302477\n",
      "[322,  1400] loss: 2.302771\n",
      "[322,  1600] loss: 2.302636\n",
      "[322,  1800] loss: 2.302396\n",
      "[322,  2000] loss: 2.302705\n",
      "[322,  2200] loss: 2.302323\n",
      "[322,  2400] loss: 2.302877\n",
      "[322,  2600] loss: 2.302844\n",
      "[322,  2800] loss: 2.302516\n",
      "[322,  3000] loss: 2.302795\n",
      "[322,  3200] loss: 2.302736\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[323,   200] loss: 2.302752\n",
      "[323,   400] loss: 2.302619\n",
      "[323,   600] loss: 2.302476\n",
      "[323,   800] loss: 2.302556\n",
      "[323,  1000] loss: 2.302879\n",
      "[323,  1200] loss: 2.302746\n",
      "[323,  1400] loss: 2.302755\n",
      "[323,  1600] loss: 2.302570\n",
      "[323,  1800] loss: 2.302626\n",
      "[323,  2000] loss: 2.302618\n",
      "[323,  2200] loss: 2.302499\n",
      "[323,  2400] loss: 2.302518\n",
      "[323,  2600] loss: 2.302711\n",
      "[323,  2800] loss: 2.302704\n",
      "[323,  3000] loss: 2.302717\n",
      "[323,  3200] loss: 2.302674\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[324,   200] loss: 2.302826\n",
      "[324,   400] loss: 2.302625\n",
      "[324,   600] loss: 2.302718\n",
      "[324,   800] loss: 2.302611\n",
      "[324,  1000] loss: 2.302672\n",
      "[324,  1200] loss: 2.302439\n",
      "[324,  1400] loss: 2.302658\n",
      "[324,  1600] loss: 2.302551\n",
      "[324,  1800] loss: 2.302629\n",
      "[324,  2000] loss: 2.302544\n",
      "[324,  2200] loss: 2.302765\n",
      "[324,  2400] loss: 2.302519\n",
      "[324,  2600] loss: 2.302711\n",
      "[324,  2800] loss: 2.302598\n",
      "[324,  3000] loss: 2.302775\n",
      "[324,  3200] loss: 2.302707\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[325,   200] loss: 2.302535\n",
      "[325,   400] loss: 2.302635\n",
      "[325,   600] loss: 2.302819\n",
      "[325,   800] loss: 2.302570\n",
      "[325,  1000] loss: 2.302628\n",
      "[325,  1200] loss: 2.302722\n",
      "[325,  1400] loss: 2.302743\n",
      "[325,  1600] loss: 2.302679\n",
      "[325,  1800] loss: 2.302488\n",
      "[325,  2000] loss: 2.302637\n",
      "[325,  2200] loss: 2.302531\n",
      "[325,  2400] loss: 2.302685\n",
      "[325,  2600] loss: 2.302475\n",
      "[325,  2800] loss: 2.302610\n",
      "[325,  3000] loss: 2.302608\n",
      "[325,  3200] loss: 2.302729\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[326,   200] loss: 2.302872\n",
      "[326,   400] loss: 2.302633\n",
      "[326,   600] loss: 2.302687\n",
      "[326,   800] loss: 2.302491\n",
      "[326,  1000] loss: 2.302583\n",
      "[326,  1200] loss: 2.302794\n",
      "[326,  1400] loss: 2.302712\n",
      "[326,  1600] loss: 2.302694\n",
      "[326,  1800] loss: 2.302594\n",
      "[326,  2000] loss: 2.302633\n",
      "[326,  2200] loss: 2.302739\n",
      "[326,  2400] loss: 2.302768\n",
      "[326,  2600] loss: 2.302490\n",
      "[326,  2800] loss: 2.302604\n",
      "[326,  3000] loss: 2.302434\n",
      "[326,  3200] loss: 2.302729\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[327,   200] loss: 2.302822\n",
      "[327,   400] loss: 2.302574\n",
      "[327,   600] loss: 2.302726\n",
      "[327,   800] loss: 2.302754\n",
      "[327,  1000] loss: 2.302598\n",
      "[327,  1200] loss: 2.302707\n",
      "[327,  1400] loss: 2.302623\n",
      "[327,  1600] loss: 2.302694\n",
      "[327,  1800] loss: 2.302725\n",
      "[327,  2000] loss: 2.302481\n",
      "[327,  2200] loss: 2.302504\n",
      "[327,  2400] loss: 2.302388\n",
      "[327,  2600] loss: 2.302630\n",
      "[327,  2800] loss: 2.302738\n",
      "[327,  3000] loss: 2.302720\n",
      "[327,  3200] loss: 2.302467\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[328,   200] loss: 2.302850\n",
      "[328,   400] loss: 2.302654\n",
      "[328,   600] loss: 2.302676\n",
      "[328,   800] loss: 2.302549\n",
      "[328,  1000] loss: 2.302810\n",
      "[328,  1200] loss: 2.302617\n",
      "[328,  1400] loss: 2.302641\n",
      "[328,  1600] loss: 2.302569\n",
      "[328,  1800] loss: 2.302617\n",
      "[328,  2000] loss: 2.302462\n",
      "[328,  2200] loss: 2.302750\n",
      "[328,  2400] loss: 2.302666\n",
      "[328,  2600] loss: 2.302625\n",
      "[328,  2800] loss: 2.302758\n",
      "[328,  3000] loss: 2.302690\n",
      "[328,  3200] loss: 2.302639\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[329,   200] loss: 2.302635\n",
      "[329,   400] loss: 2.302657\n",
      "[329,   600] loss: 2.302602\n",
      "[329,   800] loss: 2.302556\n",
      "[329,  1000] loss: 2.302615\n",
      "[329,  1200] loss: 2.302572\n",
      "[329,  1400] loss: 2.302607\n",
      "[329,  1600] loss: 2.302882\n",
      "[329,  1800] loss: 2.302671\n",
      "[329,  2000] loss: 2.302729\n",
      "[329,  2200] loss: 2.302671\n",
      "[329,  2400] loss: 2.302609\n",
      "[329,  2600] loss: 2.302808\n",
      "[329,  2800] loss: 2.302452\n",
      "[329,  3000] loss: 2.302820\n",
      "[329,  3200] loss: 2.302661\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[330,   200] loss: 2.302622\n",
      "[330,   400] loss: 2.302505\n",
      "[330,   600] loss: 2.302611\n",
      "[330,   800] loss: 2.302761\n",
      "[330,  1000] loss: 2.302656\n",
      "[330,  1200] loss: 2.302830\n",
      "[330,  1400] loss: 2.302610\n",
      "[330,  1600] loss: 2.302573\n",
      "[330,  1800] loss: 2.302504\n",
      "[330,  2000] loss: 2.302617\n",
      "[330,  2200] loss: 2.302703\n",
      "[330,  2400] loss: 2.302680\n",
      "[330,  2600] loss: 2.302551\n",
      "[330,  2800] loss: 2.302777\n",
      "[330,  3000] loss: 2.302677\n",
      "[330,  3200] loss: 2.302619\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[331,   200] loss: 2.302784\n",
      "[331,   400] loss: 2.302471\n",
      "[331,   600] loss: 2.302583\n",
      "[331,   800] loss: 2.302419\n",
      "[331,  1000] loss: 2.302777\n",
      "[331,  1200] loss: 2.302801\n",
      "[331,  1400] loss: 2.302691\n",
      "[331,  1600] loss: 2.302650\n",
      "[331,  1800] loss: 2.302570\n",
      "[331,  2000] loss: 2.302742\n",
      "[331,  2200] loss: 2.302524\n",
      "[331,  2400] loss: 2.302827\n",
      "[331,  2600] loss: 2.302729\n",
      "[331,  2800] loss: 2.302748\n",
      "[331,  3000] loss: 2.302608\n",
      "[331,  3200] loss: 2.302464\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[332,   200] loss: 2.302558\n",
      "[332,   400] loss: 2.302590\n",
      "[332,   600] loss: 2.302739\n",
      "[332,   800] loss: 2.302638\n",
      "[332,  1000] loss: 2.302317\n",
      "[332,  1200] loss: 2.302721\n",
      "[332,  1400] loss: 2.302476\n",
      "[332,  1600] loss: 2.302816\n",
      "[332,  1800] loss: 2.302535\n",
      "[332,  2000] loss: 2.302626\n",
      "[332,  2200] loss: 2.302663\n",
      "[332,  2400] loss: 2.302464\n",
      "[332,  2600] loss: 2.302768\n",
      "[332,  2800] loss: 2.302657\n",
      "[332,  3000] loss: 2.302749\n",
      "[332,  3200] loss: 2.302646\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[333,   200] loss: 2.302730\n",
      "[333,   400] loss: 2.302725\n",
      "[333,   600] loss: 2.302519\n",
      "[333,   800] loss: 2.302748\n",
      "[333,  1000] loss: 2.302757\n",
      "[333,  1200] loss: 2.302802\n",
      "[333,  1400] loss: 2.302677\n",
      "[333,  1600] loss: 2.302520\n",
      "[333,  1800] loss: 2.302374\n",
      "[333,  2000] loss: 2.302600\n",
      "[333,  2200] loss: 2.302574\n",
      "[333,  2400] loss: 2.302914\n",
      "[333,  2600] loss: 2.302403\n",
      "[333,  2800] loss: 2.302572\n",
      "[333,  3000] loss: 2.302791\n",
      "[333,  3200] loss: 2.302745\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[334,   200] loss: 2.302275\n",
      "[334,   400] loss: 2.302934\n",
      "[334,   600] loss: 2.302474\n",
      "[334,   800] loss: 2.302740\n",
      "[334,  1000] loss: 2.302507\n",
      "[334,  1200] loss: 2.302768\n",
      "[334,  1400] loss: 2.302627\n",
      "[334,  1600] loss: 2.302719\n",
      "[334,  1800] loss: 2.302793\n",
      "[334,  2000] loss: 2.302540\n",
      "[334,  2200] loss: 2.302410\n",
      "[334,  2400] loss: 2.302768\n",
      "[334,  2600] loss: 2.302567\n",
      "[334,  2800] loss: 2.302698\n",
      "[334,  3000] loss: 2.302763\n",
      "[334,  3200] loss: 2.302759\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[335,   200] loss: 2.302541\n",
      "[335,   400] loss: 2.302498\n",
      "[335,   600] loss: 2.302704\n",
      "[335,   800] loss: 2.302748\n",
      "[335,  1000] loss: 2.302834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335,  1200] loss: 2.302523\n",
      "[335,  1400] loss: 2.302840\n",
      "[335,  1600] loss: 2.302607\n",
      "[335,  1800] loss: 2.302641\n",
      "[335,  2000] loss: 2.302515\n",
      "[335,  2200] loss: 2.302505\n",
      "[335,  2400] loss: 2.302645\n",
      "[335,  2600] loss: 2.302530\n",
      "[335,  2800] loss: 2.302721\n",
      "[335,  3000] loss: 2.302624\n",
      "[335,  3200] loss: 2.302799\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[336,   200] loss: 2.302661\n",
      "[336,   400] loss: 2.302483\n",
      "[336,   600] loss: 2.302789\n",
      "[336,   800] loss: 2.302586\n",
      "[336,  1000] loss: 2.302579\n",
      "[336,  1200] loss: 2.302779\n",
      "[336,  1400] loss: 2.302698\n",
      "[336,  1600] loss: 2.302411\n",
      "[336,  1800] loss: 2.302652\n",
      "[336,  2000] loss: 2.302496\n",
      "[336,  2200] loss: 2.302846\n",
      "[336,  2400] loss: 2.302697\n",
      "[336,  2600] loss: 2.302663\n",
      "[336,  2800] loss: 2.302810\n",
      "[336,  3000] loss: 2.302630\n",
      "[336,  3200] loss: 2.302612\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[337,   200] loss: 2.302758\n",
      "[337,   400] loss: 2.302723\n",
      "[337,   600] loss: 2.302631\n",
      "[337,   800] loss: 2.302560\n",
      "[337,  1000] loss: 2.302548\n",
      "[337,  1200] loss: 2.302521\n",
      "[337,  1400] loss: 2.302751\n",
      "[337,  1600] loss: 2.302549\n",
      "[337,  1800] loss: 2.302697\n",
      "[337,  2000] loss: 2.302803\n",
      "[337,  2200] loss: 2.302577\n",
      "[337,  2400] loss: 2.302807\n",
      "[337,  2600] loss: 2.302666\n",
      "[337,  2800] loss: 2.302456\n",
      "[337,  3000] loss: 2.302406\n",
      "[337,  3200] loss: 2.302384\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[338,   200] loss: 2.302359\n",
      "[338,   400] loss: 2.302583\n",
      "[338,   600] loss: 2.302836\n",
      "[338,   800] loss: 2.302591\n",
      "[338,  1000] loss: 2.302775\n",
      "[338,  1200] loss: 2.302731\n",
      "[338,  1400] loss: 2.302607\n",
      "[338,  1600] loss: 2.302813\n",
      "[338,  1800] loss: 2.302555\n",
      "[338,  2000] loss: 2.302639\n",
      "[338,  2200] loss: 2.302576\n",
      "[338,  2400] loss: 2.302850\n",
      "[338,  2600] loss: 2.302589\n",
      "[338,  2800] loss: 2.302791\n",
      "[338,  3000] loss: 2.302719\n",
      "[338,  3200] loss: 2.302540\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[339,   200] loss: 2.302486\n",
      "[339,   400] loss: 2.302649\n",
      "[339,   600] loss: 2.302554\n",
      "[339,   800] loss: 2.302708\n",
      "[339,  1000] loss: 2.302722\n",
      "[339,  1200] loss: 2.302503\n",
      "[339,  1400] loss: 2.302625\n",
      "[339,  1600] loss: 2.302750\n",
      "[339,  1800] loss: 2.302598\n",
      "[339,  2000] loss: 2.302538\n",
      "[339,  2200] loss: 2.302755\n",
      "[339,  2400] loss: 2.302781\n",
      "[339,  2600] loss: 2.302546\n",
      "[339,  2800] loss: 2.302612\n",
      "[339,  3000] loss: 2.302843\n",
      "[339,  3200] loss: 2.302665\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[340,   200] loss: 2.302641\n",
      "[340,   400] loss: 2.302661\n",
      "[340,   600] loss: 2.302620\n",
      "[340,   800] loss: 2.302652\n",
      "[340,  1000] loss: 2.302649\n",
      "[340,  1200] loss: 2.302786\n",
      "[340,  1400] loss: 2.302695\n",
      "[340,  1600] loss: 2.302662\n",
      "[340,  1800] loss: 2.302550\n",
      "[340,  2000] loss: 2.302799\n",
      "[340,  2200] loss: 2.302649\n",
      "[340,  2400] loss: 2.302545\n",
      "[340,  2600] loss: 2.302735\n",
      "[340,  2800] loss: 2.302455\n",
      "[340,  3000] loss: 2.302645\n",
      "[340,  3200] loss: 2.302496\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[341,   200] loss: 2.302677\n",
      "[341,   400] loss: 2.302670\n",
      "[341,   600] loss: 2.302845\n",
      "[341,   800] loss: 2.302615\n",
      "[341,  1000] loss: 2.302681\n",
      "[341,  1200] loss: 2.302402\n",
      "[341,  1400] loss: 2.302671\n",
      "[341,  1600] loss: 2.302578\n",
      "[341,  1800] loss: 2.302227\n",
      "[341,  2000] loss: 2.302698\n",
      "[341,  2200] loss: 2.302897\n",
      "[341,  2400] loss: 2.302401\n",
      "[341,  2600] loss: 2.302838\n",
      "[341,  2800] loss: 2.302716\n",
      "[341,  3000] loss: 2.302794\n",
      "[341,  3200] loss: 2.302529\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[342,   200] loss: 2.302740\n",
      "[342,   400] loss: 2.302628\n",
      "[342,   600] loss: 2.302745\n",
      "[342,   800] loss: 2.302642\n",
      "[342,  1000] loss: 2.302738\n",
      "[342,  1200] loss: 2.302691\n",
      "[342,  1400] loss: 2.302476\n",
      "[342,  1600] loss: 2.302552\n",
      "[342,  1800] loss: 2.302604\n",
      "[342,  2000] loss: 2.302710\n",
      "[342,  2200] loss: 2.302553\n",
      "[342,  2400] loss: 2.302597\n",
      "[342,  2600] loss: 2.302716\n",
      "[342,  2800] loss: 2.302589\n",
      "[342,  3000] loss: 2.302444\n",
      "[342,  3200] loss: 2.302712\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[343,   200] loss: 2.302700\n",
      "[343,   400] loss: 2.302628\n",
      "[343,   600] loss: 2.302762\n",
      "[343,   800] loss: 2.302674\n",
      "[343,  1000] loss: 2.302718\n",
      "[343,  1200] loss: 2.302753\n",
      "[343,  1400] loss: 2.302550\n",
      "[343,  1600] loss: 2.302645\n",
      "[343,  1800] loss: 2.302670\n",
      "[343,  2000] loss: 2.302488\n",
      "[343,  2200] loss: 2.302501\n",
      "[343,  2400] loss: 2.302705\n",
      "[343,  2600] loss: 2.302784\n",
      "[343,  2800] loss: 2.302541\n",
      "[343,  3000] loss: 2.302504\n",
      "[343,  3200] loss: 2.302538\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[344,   200] loss: 2.302534\n",
      "[344,   400] loss: 2.302672\n",
      "[344,   600] loss: 2.302424\n",
      "[344,   800] loss: 2.302550\n",
      "[344,  1000] loss: 2.302757\n",
      "[344,  1200] loss: 2.302456\n",
      "[344,  1400] loss: 2.302798\n",
      "[344,  1600] loss: 2.302657\n",
      "[344,  1800] loss: 2.302556\n",
      "[344,  2000] loss: 2.302591\n",
      "[344,  2200] loss: 2.302759\n",
      "[344,  2400] loss: 2.302747\n",
      "[344,  2600] loss: 2.302786\n",
      "[344,  2800] loss: 2.302642\n",
      "[344,  3000] loss: 2.302753\n",
      "[344,  3200] loss: 2.302679\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[345,   200] loss: 2.302630\n",
      "[345,   400] loss: 2.302618\n",
      "[345,   600] loss: 2.302753\n",
      "[345,   800] loss: 2.302636\n",
      "[345,  1000] loss: 2.302299\n",
      "[345,  1200] loss: 2.302652\n",
      "[345,  1400] loss: 2.302560\n",
      "[345,  1600] loss: 2.302775\n",
      "[345,  1800] loss: 2.302620\n",
      "[345,  2000] loss: 2.302665\n",
      "[345,  2200] loss: 2.302779\n",
      "[345,  2400] loss: 2.302561\n",
      "[345,  2600] loss: 2.302897\n",
      "[345,  2800] loss: 2.302671\n",
      "[345,  3000] loss: 2.302649\n",
      "[345,  3200] loss: 2.302598\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[346,   200] loss: 2.302420\n",
      "[346,   400] loss: 2.302476\n",
      "[346,   600] loss: 2.302798\n",
      "[346,   800] loss: 2.302430\n",
      "[346,  1000] loss: 2.302598\n",
      "[346,  1200] loss: 2.302772\n",
      "[346,  1400] loss: 2.302741\n",
      "[346,  1600] loss: 2.302725\n",
      "[346,  1800] loss: 2.302655\n",
      "[346,  2000] loss: 2.302834\n",
      "[346,  2200] loss: 2.302585\n",
      "[346,  2400] loss: 2.302703\n",
      "[346,  2600] loss: 2.302575\n",
      "[346,  2800] loss: 2.302694\n",
      "[346,  3000] loss: 2.302632\n",
      "[346,  3200] loss: 2.302797\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[347,   200] loss: 2.302565\n",
      "[347,   400] loss: 2.302463\n",
      "[347,   600] loss: 2.302894\n",
      "[347,   800] loss: 2.302754\n",
      "[347,  1000] loss: 2.302632\n",
      "[347,  1200] loss: 2.302759\n",
      "[347,  1400] loss: 2.302757\n",
      "[347,  1600] loss: 2.302571\n",
      "[347,  1800] loss: 2.302661\n",
      "[347,  2000] loss: 2.302538\n",
      "[347,  2200] loss: 2.302500\n",
      "[347,  2400] loss: 2.302771\n",
      "[347,  2600] loss: 2.302788\n",
      "[347,  2800] loss: 2.302658\n",
      "[347,  3000] loss: 2.302543\n",
      "[347,  3200] loss: 2.302650\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[348,   200] loss: 2.302641\n",
      "[348,   400] loss: 2.302641\n",
      "[348,   600] loss: 2.302838\n",
      "[348,   800] loss: 2.302681\n",
      "[348,  1000] loss: 2.302549\n",
      "[348,  1200] loss: 2.302777\n",
      "[348,  1400] loss: 2.302630\n",
      "[348,  1600] loss: 2.302581\n",
      "[348,  1800] loss: 2.302750\n",
      "[348,  2000] loss: 2.302355\n",
      "[348,  2200] loss: 2.302766\n",
      "[348,  2400] loss: 2.302633\n",
      "[348,  2600] loss: 2.302544\n",
      "[348,  2800] loss: 2.302799\n",
      "[348,  3000] loss: 2.302769\n",
      "[348,  3200] loss: 2.302622\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[349,   200] loss: 2.302724\n",
      "[349,   400] loss: 2.302499\n",
      "[349,   600] loss: 2.302378\n",
      "[349,   800] loss: 2.302956\n",
      "[349,  1000] loss: 2.302449\n",
      "[349,  1200] loss: 2.302679\n",
      "[349,  1400] loss: 2.302750\n",
      "[349,  1600] loss: 2.302734\n",
      "[349,  1800] loss: 2.302609\n",
      "[349,  2000] loss: 2.302772\n",
      "[349,  2200] loss: 2.302601\n",
      "[349,  2400] loss: 2.302633\n",
      "[349,  2600] loss: 2.302624\n",
      "[349,  2800] loss: 2.302688\n",
      "[349,  3000] loss: 2.302654\n",
      "[349,  3200] loss: 2.302679\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[350,   200] loss: 2.302568\n",
      "[350,   400] loss: 2.302671\n",
      "[350,   600] loss: 2.302838\n",
      "[350,   800] loss: 2.302522\n",
      "[350,  1000] loss: 2.302558\n",
      "[350,  1200] loss: 2.302483\n",
      "[350,  1400] loss: 2.302644\n",
      "[350,  1600] loss: 2.302642\n",
      "[350,  1800] loss: 2.302563\n",
      "[350,  2000] loss: 2.302614\n",
      "[350,  2200] loss: 2.302623\n",
      "[350,  2400] loss: 2.302621\n",
      "[350,  2600] loss: 2.302635\n",
      "[350,  2800] loss: 2.302824\n",
      "[350,  3000] loss: 2.302700\n",
      "[350,  3200] loss: 2.302789\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# define and train the network\n",
    "red_stylised_model_path = './cifar32_style_red_model.pth'\n",
    "red_stylised_model = ResNet50()\n",
    "lr=0.1\n",
    "red_stylised_optimizer = optim.Adam(red_stylised_model.parameters(), lr=lr)\n",
    "red_stylised_lr_scheduler = optim.lr_scheduler.MultiStepLR(red_stylised_optimizer, milestones=[150, 250])\n",
    "train_part(red_stylised_model, data_loader_train_style_red, data_loader_val_style_red, red_stylised_model_path, red_stylised_optimizer, red_stylised_lr_scheduler, epochs = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Reduced Stylised ResNet50 Model on Normal CIFAR10 Dataset\n",
    "\n",
    "The below code tests the reduced stylised ResNet50 model on the normal CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1000 / 10000 correct, accuracy of the dataset is: 10.000 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "red_stylised_model = ResNet50()\n",
    "red_stylised_model.load_state_dict(torch.load('./cifar32_style_red_model.pth'))\n",
    "red_stylised_model.to(device)\n",
    "check_accuracy(data_loader_test, red_stylised_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Reduced Stylised ResNet50 Model on Reduced Stylised CIFAR10 Dataset\n",
    "\n",
    "The below code tests the reduced stylised ResNet50 model on the reduced stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1000 / 10000 correct, accuracy of the dataset is: 10.000 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "red_stylised_model = ResNet50()\n",
    "red_stylised_model.load_state_dict(torch.load('./cifar32_style_red_model.pth'))\n",
    "red_stylised_model.to(device)\n",
    "check_accuracy(data_loader_test_style_red, red_stylised_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
