{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Contextural Bias of ResNet50 on CIFAR10 Dataset - Stylised ResNet50 Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook trains and tests a stylised ResNet50 torch model with the CIFAR10 dataset. It includes functions for loading the dataset, turning them into tensors, model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Conv2d, AvgPool2d\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following cell provides a class that loads the CIFAR dataset given the relevant path, processes it into a dictionary format of class labels and content then processes the images into tensors. The class also has helper functions to extract information about the dataset needed for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        super(CifarDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.num_classes = 0\n",
    "        self.classes = []\n",
    "        \n",
    "        classes_list = []\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            classes_list.append(class_name)\n",
    "        classes_list.sort()\n",
    "        self.classes = [dict(class_idx = k, class_name = v) for k, v in enumerate(classes_list)]\n",
    "        \n",
    "\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "        tr = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        img = tr(img)\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_datasets(data_path):\n",
    "    dataset = CifarDataset(data_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being used for this experiment are normal CIFAR10 dataset, stylised CIFAR10 dataset and stylised CIFAR10 dataset created by using reduced style images where stylisation was done by AdaIN style transfer.\n",
    "\n",
    "The following cells call the function created above to load the training, validation and testing datasets of the normal, stylised and reduced stylised CIFAR10 datasets and transform them into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 36000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load normal CIFAR10\n",
    "data_path_train = \"../../CIFAR/cifar32/training\"\n",
    "dataset_train = get_cifar_datasets(data_path_train)\n",
    "\n",
    "data_path_val = \"../../CIFAR/cifar32/validation/\"\n",
    "dataset_val = get_cifar_datasets(data_path_val)\n",
    "\n",
    "data_path_test = \"../../CIFAR/cifar32/testing/\"\n",
    "dataset_test = get_cifar_datasets(data_path_test)\n",
    "\n",
    "print(f\"Number of train samples {dataset_train.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train.get_class_names()))\n",
    "\n",
    "print(f\"Number of val samples {dataset_val.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val.get_class_names()))\n",
    "\n",
    "print(f\"Number of test samples {dataset_test.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val = DataLoader(dataset_val, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with original kaggle images\n",
    "data_path_train_style = \"../../CIFAR/cifar32_style/training\"\n",
    "dataset_train_style = get_cifar_datasets(data_path_train_style)\n",
    "\n",
    "data_path_val_style = \"../../CIFAR/cifar32_style/validation/\"\n",
    "dataset_val_style = get_cifar_datasets(data_path_val_style)\n",
    "\n",
    "data_path_test_style = \"../../CIFAR/cifar32_style/testing/\"\n",
    "dataset_test_style = get_cifar_datasets(data_path_test_style)\n",
    "\n",
    "print(f\"Number of stylised train samples {dataset_train_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised val samples {dataset_val_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style.get_class_names()))\n",
    "\n",
    "print(f\"Number of stylised test samples {dataset_test_style.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style = DataLoader(dataset_train_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style = DataLoader(dataset_val_style, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style = DataLoader(dataset_test_style, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reduced stylised train samples 216000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised val samples 4000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n",
      "Number of reduced stylised test samples 10000\n",
      "Class names are: ['0000000001', '0000000010', '0000000100', '0000001000', '0000010000', '0000100000', '0001000000', '0010000000', '0100000000', '1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Load stylised CIFAR10 with reduced kaggle images\n",
    "data_path_train_style_red = \"../../CIFAR/cifar32_style_red/training\"\n",
    "dataset_train_style_red = get_cifar_datasets(data_path_train_style_red)\n",
    "\n",
    "data_path_val_style_red = \"../../CIFAR/cifar32_style_red/validation/\"\n",
    "dataset_val_style_red = get_cifar_datasets(data_path_val_style_red)\n",
    "\n",
    "data_path_test_style_red = \"../../CIFAR/cifar32_style_red/testing/\"\n",
    "dataset_test_style_red = get_cifar_datasets(data_path_test_style_red)\n",
    "\n",
    "print(f\"Number of reduced stylised train samples {dataset_train_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_train_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised val samples {dataset_val_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_val_style_red.get_class_names()))\n",
    "\n",
    "print(f\"Number of reduced stylised test samples {dataset_test_style_red.__len__()}\")\n",
    "print(\"Class names are: \" + str(dataset_test_style_red.get_class_names()))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_loader_train_style_red = DataLoader(dataset_train_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_val_style_red = DataLoader(dataset_val_style_red, BATCH_SIZE, shuffle = True)\n",
    "data_loader_test_style_red = DataLoader(dataset_test_style_red, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch['image'] torch.Size([64, 3, 32, 32])\n",
      "Shape of batch['cls'] torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the data was loaded correctly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4,4, figsize=(8, 8))\n",
    "\n",
    "for batch in data_loader_test_style:\n",
    "\n",
    "    print(f\"Shape of batch['image'] {batch['image'].shape}\")\n",
    "    print(f\"Shape of batch['cls'] {batch['cls'].shape}\")\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        col = i % 4\n",
    "        row = i // 4\n",
    "\n",
    "        img = batch['image'][i].numpy()\n",
    "\n",
    "        axes[row,col].set_axis_off()\n",
    "        axes[row,col].set_title(batch['class_name'][i])\n",
    "        axes[row,col].imshow(np.transpose(img,(1,2,0)))\n",
    "                         \n",
    "        if i >= 15:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the training, validation and testing functions for the experiment. train_part() function trains and updates gradients on each batch of the training set and once that is done, it tests its accuracy on the validation set. Every time the validation test returns a better accuracy than the current maximum, the model is saved and carries on with the next epoch. Then it checks with the learning reate scheduler for any changes in learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "    \n",
    "print_every = 100\n",
    "def check_accuracy(loader, model):\n",
    "    # function for test accuracy on validation and test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "            scores = model(inputs)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct, accuracy of the dataset is: %.3f %%' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_part(model, train_data, val_data, model_path, optimizer, lr_scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "    val_acc = 0\n",
    "    num_epoch = 2\n",
    "    # Main Loop\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        val_loss = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # Training Loop\n",
    "        for i, batch in enumerate(train_data, 0):\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['cls'].to(device)\n",
    "\n",
    "            # get outputs from the input data and calculate the cross entropy loss\n",
    "            scores = model(inputs)\n",
    "            loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "            # zero and update the gradients and optimise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.6f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            for i, batch in enumerate(val_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['cls'].to(device)\n",
    "\n",
    "                # get the outputs from the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # compute accuracy based on the outputs\n",
    "                _, preds = outputs.max(1)\n",
    "                num_correct += (preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "            if acc > val_acc:\n",
    "                print('saving model')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                val_acc = acc\n",
    "            else:\n",
    "                print('skip model saving')\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylised ResNet50 Training\n",
    "\n",
    "The model used in this experiment is ResNet50 from torchvision library trained for 350 epochs with Adam optimiser, learning rate scheduler and default settings. The dataset used to train the model is stylised CIFAR10. Learning rate starts at 0.1 and changes every 150, 250th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 4.814861\n",
      "[1,   400] loss: 2.330386\n",
      "[1,   600] loss: 2.325220\n",
      "[1,   800] loss: 2.320634\n",
      "[1,  1000] loss: 2.314374\n",
      "[1,  1200] loss: 2.314757\n",
      "[1,  1400] loss: 2.310814\n",
      "[1,  1600] loss: 2.329358\n",
      "[1,  1800] loss: 2.311765\n",
      "[1,  2000] loss: 2.311805\n",
      "[1,  2200] loss: 2.312730\n",
      "[1,  2400] loss: 2.313262\n",
      "[1,  2600] loss: 2.310711\n",
      "[1,  2800] loss: 2.312876\n",
      "[1,  3000] loss: 2.309694\n",
      "[1,  3200] loss: 2.311206\n",
      "Got 381 / 4000 correct (9.53)\n",
      "saving model\n",
      "[2,   200] loss: 2.310493\n",
      "[2,   400] loss: 2.311781\n",
      "[2,   600] loss: 2.311385\n",
      "[2,   800] loss: 2.313027\n",
      "[2,  1000] loss: 2.311976\n",
      "[2,  1200] loss: 2.311547\n",
      "[2,  1400] loss: 2.310302\n",
      "[2,  1600] loss: 2.311423\n",
      "[2,  1800] loss: 2.311325\n",
      "[2,  2000] loss: 2.311794\n",
      "[2,  2200] loss: 4.386050\n",
      "[2,  2400] loss: 2.348172\n",
      "[2,  2600] loss: 2.310305\n",
      "[2,  2800] loss: 2.311673\n",
      "[2,  3000] loss: 2.311817\n",
      "[2,  3200] loss: 2.309990\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[3,   200] loss: 2.312606\n",
      "[3,   400] loss: 2.310862\n",
      "[3,   600] loss: 2.311252\n",
      "[3,   800] loss: 2.310502\n",
      "[3,  1000] loss: 2.311859\n",
      "[3,  1200] loss: 2.311920\n",
      "[3,  1400] loss: 2.309447\n",
      "[3,  1600] loss: 2.311791\n",
      "[3,  1800] loss: 2.312472\n",
      "[3,  2000] loss: 2.310619\n",
      "[3,  2200] loss: 2.311625\n",
      "[3,  2400] loss: 2.311122\n",
      "[3,  2600] loss: 2.312970\n",
      "[3,  2800] loss: 2.313118\n",
      "[3,  3000] loss: 2.311171\n",
      "[3,  3200] loss: 2.310680\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[4,   200] loss: 2.312078\n",
      "[4,   400] loss: 2.310848\n",
      "[4,   600] loss: 2.311300\n",
      "[4,   800] loss: 2.311380\n",
      "[4,  1000] loss: 2.310512\n",
      "[4,  1200] loss: 2.309827\n",
      "[4,  1400] loss: 2.311084\n",
      "[4,  1600] loss: 2.312424\n",
      "[4,  1800] loss: 2.310951\n",
      "[4,  2000] loss: 2.312672\n",
      "[4,  2200] loss: 2.311020\n",
      "[4,  2400] loss: 2.309479\n",
      "[4,  2600] loss: 2.311084\n",
      "[4,  2800] loss: 2.310418\n",
      "[4,  3000] loss: 2.313259\n",
      "[4,  3200] loss: 2.310612\n",
      "Got 383 / 4000 correct (9.57)\n",
      "saving model\n",
      "[5,   200] loss: 2.311775\n",
      "[5,   400] loss: 2.312684\n",
      "[5,   600] loss: 2.311341\n",
      "[5,   800] loss: 2.311181\n",
      "[5,  1000] loss: 2.312352\n",
      "[5,  1200] loss: 2.311137\n",
      "[5,  1400] loss: 2.311096\n",
      "[5,  1600] loss: 2.313521\n",
      "[5,  1800] loss: 2.311624\n",
      "[5,  2000] loss: 2.309879\n",
      "[5,  2200] loss: 2.310762\n",
      "[5,  2400] loss: 2.311181\n",
      "[5,  2600] loss: 2.309673\n",
      "[5,  2800] loss: 2.310753\n",
      "[5,  3000] loss: 2.312583\n",
      "[5,  3200] loss: 2.311768\n",
      "Got 401 / 4000 correct (10.03)\n",
      "saving model\n",
      "[6,   200] loss: 2.311498\n",
      "[6,   400] loss: 2.312868\n",
      "[6,   600] loss: 2.311904\n",
      "[6,   800] loss: 2.310829\n",
      "[6,  1000] loss: 2.312013\n",
      "[6,  1200] loss: 2.311071\n",
      "[6,  1400] loss: 2.310343\n",
      "[6,  1600] loss: 2.312059\n",
      "[6,  1800] loss: 2.311237\n",
      "[6,  2000] loss: 2.313523\n",
      "[6,  2200] loss: 2.312150\n",
      "[6,  2400] loss: 2.311700\n",
      "[6,  2600] loss: 2.309985\n",
      "[6,  2800] loss: 2.310477\n",
      "[6,  3000] loss: 2.313558\n",
      "[6,  3200] loss: 2.312476\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[7,   200] loss: 2.312049\n",
      "[7,   400] loss: 2.312440\n",
      "[7,   600] loss: 2.311118\n",
      "[7,   800] loss: 2.313025\n",
      "[7,  1000] loss: 2.312151\n",
      "[7,  1200] loss: 2.311843\n",
      "[7,  1400] loss: 2.312562\n",
      "[7,  1600] loss: 2.312306\n",
      "[7,  1800] loss: 2.309749\n",
      "[7,  2000] loss: 2.311168\n",
      "[7,  2200] loss: 2.312571\n",
      "[7,  2400] loss: 2.310697\n",
      "[7,  2600] loss: 2.312159\n",
      "[7,  2800] loss: 2.313434\n",
      "[7,  3000] loss: 2.311291\n",
      "[7,  3200] loss: 2.311453\n",
      "Got 422 / 4000 correct (10.55)\n",
      "saving model\n",
      "[8,   200] loss: 2.311954\n",
      "[8,   400] loss: 2.311479\n",
      "[8,   600] loss: 2.310465\n",
      "[8,   800] loss: 2.311620\n",
      "[8,  1000] loss: 2.311736\n",
      "[8,  1200] loss: 2.309950\n",
      "[8,  1400] loss: 2.311189\n",
      "[8,  1600] loss: 2.311101\n",
      "[8,  1800] loss: 2.311199\n",
      "[8,  2000] loss: 2.312025\n",
      "[8,  2200] loss: 2.312325\n",
      "[8,  2400] loss: 2.311699\n",
      "[8,  2600] loss: 2.312434\n",
      "[8,  2800] loss: 2.310797\n",
      "[8,  3000] loss: 2.311694\n",
      "[8,  3200] loss: 2.310337\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[9,   200] loss: 2.311885\n",
      "[9,   400] loss: 2.311904\n",
      "[9,   600] loss: 2.310755\n",
      "[9,   800] loss: 2.313180\n",
      "[9,  1000] loss: 2.311293\n",
      "[9,  1200] loss: 2.310500\n",
      "[9,  1400] loss: 2.313695\n",
      "[9,  1600] loss: 2.310411\n",
      "[9,  1800] loss: 2.310300\n",
      "[9,  2000] loss: 2.308949\n",
      "[9,  2200] loss: 2.312476\n",
      "[9,  2400] loss: 2.313545\n",
      "[9,  2600] loss: 2.311076\n",
      "[9,  2800] loss: 2.312133\n",
      "[9,  3000] loss: 2.310062\n",
      "[9,  3200] loss: 2.313215\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[10,   200] loss: 2.310526\n",
      "[10,   400] loss: 2.312373\n",
      "[10,   600] loss: 2.311474\n",
      "[10,   800] loss: 2.311738\n",
      "[10,  1000] loss: 2.311309\n",
      "[10,  1200] loss: 2.309875\n",
      "[10,  1400] loss: 2.312233\n",
      "[10,  1600] loss: 2.311494\n",
      "[10,  1800] loss: 2.309900\n",
      "[10,  2000] loss: 2.310052\n",
      "[10,  2200] loss: 2.310684\n",
      "[10,  2400] loss: 2.313143\n",
      "[10,  2600] loss: 2.310890\n",
      "[10,  2800] loss: 2.310565\n",
      "[10,  3000] loss: 2.311965\n",
      "[10,  3200] loss: 2.310597\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[11,   200] loss: 2.311981\n",
      "[11,   400] loss: 2.312918\n",
      "[11,   600] loss: 2.312873\n",
      "[11,   800] loss: 2.310768\n",
      "[11,  1000] loss: 2.311839\n",
      "[11,  1200] loss: 2.310606\n",
      "[11,  1400] loss: 2.310802\n",
      "[11,  1600] loss: 2.311240\n",
      "[11,  1800] loss: 2.311922\n",
      "[11,  2000] loss: 2.310284\n",
      "[11,  2200] loss: 2.310825\n",
      "[11,  2400] loss: 2.310532\n",
      "[11,  2600] loss: 2.312762\n",
      "[11,  2800] loss: 2.309751\n",
      "[11,  3000] loss: 2.311055\n",
      "[11,  3200] loss: 2.311321\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[12,   200] loss: 2.309765\n",
      "[12,   400] loss: 2.311059\n",
      "[12,   600] loss: 2.310556\n",
      "[12,   800] loss: 2.309775\n",
      "[12,  1000] loss: 2.311801\n",
      "[12,  1200] loss: 2.312904\n",
      "[12,  1400] loss: 2.309885\n",
      "[12,  1600] loss: 2.313582\n",
      "[12,  1800] loss: 2.314150\n",
      "[12,  2000] loss: 2.308737\n",
      "[12,  2200] loss: 2.311673\n",
      "[12,  2400] loss: 2.312310\n",
      "[12,  2600] loss: 2.309882\n",
      "[12,  2800] loss: 2.313020\n",
      "[12,  3000] loss: 2.311619\n",
      "[12,  3200] loss: 2.313361\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[13,   200] loss: 2.309598\n",
      "[13,   400] loss: 2.311455\n",
      "[13,   600] loss: 2.310773\n",
      "[13,   800] loss: 2.310705\n",
      "[13,  1000] loss: 2.309541\n",
      "[13,  1200] loss: 2.310980\n",
      "[13,  1400] loss: 2.311057\n",
      "[13,  1600] loss: 2.309607\n",
      "[13,  1800] loss: 2.311156\n",
      "[13,  2000] loss: 2.310812\n",
      "[13,  2200] loss: 2.310496\n",
      "[13,  2400] loss: 2.311939\n",
      "[13,  2600] loss: 2.312260\n",
      "[13,  2800] loss: 2.310767\n",
      "[13,  3000] loss: 2.308973\n",
      "[13,  3200] loss: 2.311281\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[14,   200] loss: 2.311599\n",
      "[14,   400] loss: 2.312164\n",
      "[14,   600] loss: 2.311634\n",
      "[14,   800] loss: 2.310812\n",
      "[14,  1000] loss: 2.309751\n",
      "[14,  1200] loss: 2.312492\n",
      "[14,  1400] loss: 2.312272\n",
      "[14,  1600] loss: 2.310035\n",
      "[14,  1800] loss: 2.313419\n",
      "[14,  2000] loss: 2.312611\n",
      "[14,  2200] loss: 2.311709\n",
      "[14,  2400] loss: 2.309162\n",
      "[14,  2600] loss: 2.312541\n",
      "[14,  2800] loss: 2.311075\n",
      "[14,  3000] loss: 2.311154\n",
      "[14,  3200] loss: 2.312048\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[15,   200] loss: 2.312345\n",
      "[15,   400] loss: 2.308967\n",
      "[15,   600] loss: 2.310572\n",
      "[15,   800] loss: 2.311701\n",
      "[15,  1000] loss: 2.312515\n",
      "[15,  1200] loss: 2.311533\n",
      "[15,  1400] loss: 2.312993\n",
      "[15,  1600] loss: 2.312088\n",
      "[15,  1800] loss: 2.310729\n",
      "[15,  2000] loss: 2.311966\n",
      "[15,  2200] loss: 2.311467\n",
      "[15,  2400] loss: 2.311161\n",
      "[15,  2600] loss: 2.310843\n",
      "[15,  2800] loss: 2.310897\n",
      "[15,  3000] loss: 2.311838\n",
      "[15,  3200] loss: 2.312402\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[16,   200] loss: 2.312945\n",
      "[16,   400] loss: 2.312866\n",
      "[16,   600] loss: 2.313182\n",
      "[16,   800] loss: 2.309908\n",
      "[16,  1000] loss: 2.312502\n",
      "[16,  1200] loss: 2.313821\n",
      "[16,  1400] loss: 2.310700\n",
      "[16,  1600] loss: 2.310573\n",
      "[16,  1800] loss: 2.310866\n",
      "[16,  2000] loss: 2.311626\n",
      "[16,  2200] loss: 2.311942\n",
      "[16,  2400] loss: 2.313397\n",
      "[16,  2600] loss: 2.311297\n",
      "[16,  2800] loss: 2.313031\n",
      "[16,  3000] loss: 2.313231\n",
      "[16,  3200] loss: 2.310443\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[17,   200] loss: 2.312238\n",
      "[17,   400] loss: 2.309721\n",
      "[17,   600] loss: 2.311775\n",
      "[17,   800] loss: 2.310480\n",
      "[17,  1000] loss: 2.310532\n",
      "[17,  1200] loss: 2.313541\n",
      "[17,  1400] loss: 2.312811\n",
      "[17,  1600] loss: 2.310937\n",
      "[17,  1800] loss: 2.310872\n",
      "[17,  2000] loss: 2.309781\n",
      "[17,  2200] loss: 2.309741\n",
      "[17,  2400] loss: 2.310893\n",
      "[17,  2600] loss: 2.313818\n",
      "[17,  2800] loss: 2.312091\n",
      "[17,  3000] loss: 2.311157\n",
      "[17,  3200] loss: 2.311913\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[18,   200] loss: 2.311707\n",
      "[18,   400] loss: 2.310042\n",
      "[18,   600] loss: 2.310244\n",
      "[18,   800] loss: 2.309891\n",
      "[18,  1000] loss: 2.312291\n",
      "[18,  1200] loss: 2.310427\n",
      "[18,  1400] loss: 2.311335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,  1600] loss: 2.311134\n",
      "[18,  1800] loss: 2.309463\n",
      "[18,  2000] loss: 2.310559\n",
      "[18,  2200] loss: 2.312627\n",
      "[18,  2400] loss: 2.312621\n",
      "[18,  2600] loss: 2.311939\n",
      "[18,  2800] loss: 2.310307\n",
      "[18,  3000] loss: 2.312118\n",
      "[18,  3200] loss: 2.310979\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[19,   200] loss: 2.310983\n",
      "[19,   400] loss: 2.313033\n",
      "[19,   600] loss: 2.311456\n",
      "[19,   800] loss: 2.311632\n",
      "[19,  1000] loss: 2.312604\n",
      "[19,  1200] loss: 2.309319\n",
      "[19,  1400] loss: 2.309513\n",
      "[19,  1600] loss: 2.311836\n",
      "[19,  1800] loss: 2.313165\n",
      "[19,  2000] loss: 2.311213\n",
      "[19,  2200] loss: 2.311403\n",
      "[19,  2400] loss: 2.312746\n",
      "[19,  2600] loss: 2.310331\n",
      "[19,  2800] loss: 2.311961\n",
      "[19,  3000] loss: 2.312265\n",
      "[19,  3200] loss: 2.310400\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[20,   200] loss: 2.309914\n",
      "[20,   400] loss: 2.311953\n",
      "[20,   600] loss: 2.315150\n",
      "[20,   800] loss: 2.313603\n",
      "[20,  1000] loss: 2.313026\n",
      "[20,  1200] loss: 2.313578\n",
      "[20,  1400] loss: 2.312315\n",
      "[20,  1600] loss: 2.312913\n",
      "[20,  1800] loss: 2.313685\n",
      "[20,  2000] loss: 2.310409\n",
      "[20,  2200] loss: 2.311628\n",
      "[20,  2400] loss: 2.314684\n",
      "[20,  2600] loss: 2.310584\n",
      "[20,  2800] loss: 2.311072\n",
      "[20,  3000] loss: 2.310919\n",
      "[20,  3200] loss: 2.311620\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[21,   200] loss: 2.309346\n",
      "[21,   400] loss: 2.311499\n",
      "[21,   600] loss: 2.311089\n",
      "[21,   800] loss: 2.311693\n",
      "[21,  1000] loss: 2.309395\n",
      "[21,  1200] loss: 2.312124\n",
      "[21,  1400] loss: 2.312273\n",
      "[21,  1600] loss: 2.311877\n",
      "[21,  1800] loss: 2.310859\n",
      "[21,  2000] loss: 2.312021\n",
      "[21,  2200] loss: 2.311115\n",
      "[21,  2400] loss: 2.311093\n",
      "[21,  2600] loss: 2.312057\n",
      "[21,  2800] loss: 2.312230\n",
      "[21,  3000] loss: 2.310736\n",
      "[21,  3200] loss: 2.311318\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[22,   200] loss: 2.311992\n",
      "[22,   400] loss: 2.312854\n",
      "[22,   600] loss: 2.311105\n",
      "[22,   800] loss: 2.312988\n",
      "[22,  1000] loss: 2.311980\n",
      "[22,  1200] loss: 2.311316\n",
      "[22,  1400] loss: 2.311703\n",
      "[22,  1600] loss: 2.309666\n",
      "[22,  1800] loss: 2.312249\n",
      "[22,  2000] loss: 2.310719\n",
      "[22,  2200] loss: 2.311579\n",
      "[22,  2400] loss: 2.313605\n",
      "[22,  2600] loss: 2.311707\n",
      "[22,  2800] loss: 2.310455\n",
      "[22,  3000] loss: 2.311922\n",
      "[22,  3200] loss: 2.311042\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[23,   200] loss: 2.312518\n",
      "[23,   400] loss: 2.310350\n",
      "[23,   600] loss: 2.310130\n",
      "[23,   800] loss: 2.312045\n",
      "[23,  1000] loss: 2.311728\n",
      "[23,  1200] loss: 2.312466\n",
      "[23,  1400] loss: 2.311264\n",
      "[23,  1600] loss: 2.313823\n",
      "[23,  1800] loss: 2.309735\n",
      "[23,  2000] loss: 2.311939\n",
      "[23,  2200] loss: 2.312247\n",
      "[23,  2400] loss: 2.311475\n",
      "[23,  2600] loss: 2.310526\n",
      "[23,  2800] loss: 2.309620\n",
      "[23,  3000] loss: 2.310253\n",
      "[23,  3200] loss: 2.311434\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[24,   200] loss: 2.311510\n",
      "[24,   400] loss: 2.313184\n",
      "[24,   600] loss: 2.311812\n",
      "[24,   800] loss: 2.312410\n",
      "[24,  1000] loss: 2.312230\n",
      "[24,  1200] loss: 2.312293\n",
      "[24,  1400] loss: 2.311940\n",
      "[24,  1600] loss: 2.312090\n",
      "[24,  1800] loss: 2.310509\n",
      "[24,  2000] loss: 2.310835\n",
      "[24,  2200] loss: 2.311419\n",
      "[24,  2400] loss: 2.313948\n",
      "[24,  2600] loss: 2.310287\n",
      "[24,  2800] loss: 2.311159\n",
      "[24,  3000] loss: 2.311870\n",
      "[24,  3200] loss: 2.313448\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[25,   200] loss: 2.310732\n",
      "[25,   400] loss: 2.312240\n",
      "[25,   600] loss: 2.311766\n",
      "[25,   800] loss: 2.310525\n",
      "[25,  1000] loss: 2.310230\n",
      "[25,  1200] loss: 2.311177\n",
      "[25,  1400] loss: 2.310801\n",
      "[25,  1600] loss: 2.311388\n",
      "[25,  1800] loss: 2.313056\n",
      "[25,  2000] loss: 2.310963\n",
      "[25,  2200] loss: 2.310294\n",
      "[25,  2400] loss: 2.312601\n",
      "[25,  2600] loss: 2.313144\n",
      "[25,  2800] loss: 2.311337\n",
      "[25,  3000] loss: 2.312463\n",
      "[25,  3200] loss: 2.311969\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[26,   200] loss: 2.309522\n",
      "[26,   400] loss: 2.312973\n",
      "[26,   600] loss: 2.311218\n",
      "[26,   800] loss: 2.314099\n",
      "[26,  1000] loss: 2.311764\n",
      "[26,  1200] loss: 2.312412\n",
      "[26,  1400] loss: 2.309796\n",
      "[26,  1600] loss: 2.313930\n",
      "[26,  1800] loss: 2.314017\n",
      "[26,  2000] loss: 2.311238\n",
      "[26,  2200] loss: 2.313704\n",
      "[26,  2400] loss: 2.309112\n",
      "[26,  2600] loss: 2.311672\n",
      "[26,  2800] loss: 2.311289\n",
      "[26,  3000] loss: 2.310670\n",
      "[26,  3200] loss: 2.311244\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[27,   200] loss: 2.310807\n",
      "[27,   400] loss: 2.311239\n",
      "[27,   600] loss: 2.311173\n",
      "[27,   800] loss: 2.311325\n",
      "[27,  1000] loss: 2.311919\n",
      "[27,  1200] loss: 2.309299\n",
      "[27,  1400] loss: 2.311040\n",
      "[27,  1600] loss: 2.311716\n",
      "[27,  1800] loss: 2.315250\n",
      "[27,  2000] loss: 2.311301\n",
      "[27,  2200] loss: 2.312311\n",
      "[27,  2400] loss: 2.309718\n",
      "[27,  2600] loss: 2.310707\n",
      "[27,  2800] loss: 2.311270\n",
      "[27,  3000] loss: 2.310711\n",
      "[27,  3200] loss: 2.310777\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[28,   200] loss: 2.313307\n",
      "[28,   400] loss: 2.311717\n",
      "[28,   600] loss: 2.311743\n",
      "[28,   800] loss: 2.310588\n",
      "[28,  1000] loss: 2.312623\n",
      "[28,  1200] loss: 2.310806\n",
      "[28,  1400] loss: 2.311951\n",
      "[28,  1600] loss: 2.309617\n",
      "[28,  1800] loss: 2.309723\n",
      "[28,  2000] loss: 2.311073\n",
      "[28,  2200] loss: 2.309841\n",
      "[28,  2400] loss: 2.310332\n",
      "[28,  2600] loss: 2.313303\n",
      "[28,  2800] loss: 2.312739\n",
      "[28,  3000] loss: 2.311788\n",
      "[28,  3200] loss: 2.310492\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[29,   200] loss: 2.309917\n",
      "[29,   400] loss: 2.311914\n",
      "[29,   600] loss: 2.311620\n",
      "[29,   800] loss: 2.313107\n",
      "[29,  1000] loss: 2.310920\n",
      "[29,  1200] loss: 2.312349\n",
      "[29,  1400] loss: 2.312268\n",
      "[29,  1600] loss: 2.311760\n",
      "[29,  1800] loss: 2.311569\n",
      "[29,  2000] loss: 2.313121\n",
      "[29,  2200] loss: 2.312956\n",
      "[29,  2400] loss: 2.312176\n",
      "[29,  2600] loss: 2.311162\n",
      "[29,  2800] loss: 2.311066\n",
      "[29,  3000] loss: 2.310331\n",
      "[29,  3200] loss: 2.314162\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[30,   200] loss: 2.311919\n",
      "[30,   400] loss: 2.312156\n",
      "[30,   600] loss: 2.313158\n",
      "[30,   800] loss: 2.312693\n",
      "[30,  1000] loss: 2.310854\n",
      "[30,  1200] loss: 2.311468\n",
      "[30,  1400] loss: 2.311902\n",
      "[30,  1600] loss: 2.310336\n",
      "[30,  1800] loss: 2.309626\n",
      "[30,  2000] loss: 2.310639\n",
      "[30,  2200] loss: 2.312632\n",
      "[30,  2400] loss: 2.310308\n",
      "[30,  2600] loss: 2.312784\n",
      "[30,  2800] loss: 2.311776\n",
      "[30,  3000] loss: 2.312464\n",
      "[30,  3200] loss: 2.312502\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[31,   200] loss: 2.313731\n",
      "[31,   400] loss: 2.312885\n",
      "[31,   600] loss: 2.312061\n",
      "[31,   800] loss: 2.312202\n",
      "[31,  1000] loss: 2.311920\n",
      "[31,  1200] loss: 2.312290\n",
      "[31,  1400] loss: 2.311163\n",
      "[31,  1600] loss: 2.311244\n",
      "[31,  1800] loss: 2.310708\n",
      "[31,  2000] loss: 2.312170\n",
      "[31,  2200] loss: 2.309342\n",
      "[31,  2400] loss: 2.312418\n",
      "[31,  2600] loss: 2.311957\n",
      "[31,  2800] loss: 2.311595\n",
      "[31,  3000] loss: 2.311039\n",
      "[31,  3200] loss: 2.311679\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[32,   200] loss: 2.309776\n",
      "[32,   400] loss: 2.311108\n",
      "[32,   600] loss: 2.312867\n",
      "[32,   800] loss: 2.312345\n",
      "[32,  1000] loss: 2.311071\n",
      "[32,  1200] loss: 2.311326\n",
      "[32,  1400] loss: 2.312524\n",
      "[32,  1600] loss: 2.311834\n",
      "[32,  1800] loss: 2.311308\n",
      "[32,  2000] loss: 2.311584\n",
      "[32,  2200] loss: 2.310354\n",
      "[32,  2400] loss: 2.312008\n",
      "[32,  2600] loss: 2.313292\n",
      "[32,  2800] loss: 2.311479\n",
      "[32,  3000] loss: 2.311441\n",
      "[32,  3200] loss: 2.311906\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[33,   200] loss: 2.311070\n",
      "[33,   400] loss: 2.309951\n",
      "[33,   600] loss: 2.311896\n",
      "[33,   800] loss: 2.310962\n",
      "[33,  1000] loss: 2.311674\n",
      "[33,  1200] loss: 2.309918\n",
      "[33,  1400] loss: 2.310832\n",
      "[33,  1600] loss: 2.311717\n",
      "[33,  1800] loss: 2.311961\n",
      "[33,  2000] loss: 2.311188\n",
      "[33,  2200] loss: 2.311548\n",
      "[33,  2400] loss: 2.313359\n",
      "[33,  2600] loss: 2.309825\n",
      "[33,  2800] loss: 2.310618\n",
      "[33,  3000] loss: 2.310998\n",
      "[33,  3200] loss: 2.312619\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[34,   200] loss: 2.310201\n",
      "[34,   400] loss: 2.310085\n",
      "[34,   600] loss: 2.310682\n",
      "[34,   800] loss: 2.311606\n",
      "[34,  1000] loss: 2.312982\n",
      "[34,  1200] loss: 2.311385\n",
      "[34,  1400] loss: 2.313605\n",
      "[34,  1600] loss: 2.311364\n",
      "[34,  1800] loss: 2.311768\n",
      "[34,  2000] loss: 2.311550\n",
      "[34,  2200] loss: 2.309896\n",
      "[34,  2400] loss: 2.312546\n",
      "[34,  2600] loss: 2.312838\n",
      "[34,  2800] loss: 2.311201\n",
      "[34,  3000] loss: 2.310835\n",
      "[34,  3200] loss: 2.309847\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[35,   200] loss: 2.310337\n",
      "[35,   400] loss: 2.311305\n",
      "[35,   600] loss: 2.310870\n",
      "[35,   800] loss: 2.310750\n",
      "[35,  1000] loss: 2.312512\n",
      "[35,  1200] loss: 2.312011\n",
      "[35,  1400] loss: 2.311148\n",
      "[35,  1600] loss: 2.309058\n",
      "[35,  1800] loss: 2.309780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,  2000] loss: 2.312904\n",
      "[35,  2200] loss: 2.313250\n",
      "[35,  2400] loss: 2.310845\n",
      "[35,  2600] loss: 2.312410\n",
      "[35,  2800] loss: 2.312473\n",
      "[35,  3000] loss: 2.309167\n",
      "[35,  3200] loss: 2.311417\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[36,   200] loss: 2.310518\n",
      "[36,   400] loss: 2.310985\n",
      "[36,   600] loss: 2.309825\n",
      "[36,   800] loss: 2.312776\n",
      "[36,  1000] loss: 2.311838\n",
      "[36,  1200] loss: 2.311513\n",
      "[36,  1400] loss: 2.309991\n",
      "[36,  1600] loss: 2.312109\n",
      "[36,  1800] loss: 2.308711\n",
      "[36,  2000] loss: 2.310723\n",
      "[36,  2200] loss: 2.311954\n",
      "[36,  2400] loss: 2.313037\n",
      "[36,  2600] loss: 2.310198\n",
      "[36,  2800] loss: 2.311495\n",
      "[36,  3000] loss: 2.310723\n",
      "[36,  3200] loss: 2.308546\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[37,   200] loss: 2.312666\n",
      "[37,   400] loss: 2.311650\n",
      "[37,   600] loss: 2.311663\n",
      "[37,   800] loss: 2.312645\n",
      "[37,  1000] loss: 2.310951\n",
      "[37,  1200] loss: 2.310392\n",
      "[37,  1400] loss: 2.312809\n",
      "[37,  1600] loss: 2.309977\n",
      "[37,  1800] loss: 2.309551\n",
      "[37,  2000] loss: 2.311298\n",
      "[37,  2200] loss: 2.312883\n",
      "[37,  2400] loss: 2.313220\n",
      "[37,  2600] loss: 2.312874\n",
      "[37,  2800] loss: 2.310882\n",
      "[37,  3000] loss: 2.313766\n",
      "[37,  3200] loss: 2.309961\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[38,   200] loss: 2.311227\n",
      "[38,   400] loss: 2.313498\n",
      "[38,   600] loss: 2.311310\n",
      "[38,   800] loss: 2.310477\n",
      "[38,  1000] loss: 2.311074\n",
      "[38,  1200] loss: 2.310792\n",
      "[38,  1400] loss: 2.311014\n",
      "[38,  1600] loss: 2.311454\n",
      "[38,  1800] loss: 2.309581\n",
      "[38,  2000] loss: 2.310913\n",
      "[38,  2200] loss: 2.310093\n",
      "[38,  2400] loss: 2.311630\n",
      "[38,  2600] loss: 2.312974\n",
      "[38,  2800] loss: 2.311528\n",
      "[38,  3000] loss: 2.312261\n",
      "[38,  3200] loss: 2.312062\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[39,   200] loss: 2.310133\n",
      "[39,   400] loss: 2.311532\n",
      "[39,   600] loss: 2.309499\n",
      "[39,   800] loss: 2.310644\n",
      "[39,  1000] loss: 2.310679\n",
      "[39,  1200] loss: 2.311896\n",
      "[39,  1400] loss: 2.310722\n",
      "[39,  1600] loss: 2.310771\n",
      "[39,  1800] loss: 2.311065\n",
      "[39,  2000] loss: 2.312580\n",
      "[39,  2200] loss: 2.310811\n",
      "[39,  2400] loss: 2.312021\n",
      "[39,  2600] loss: 2.309946\n",
      "[39,  2800] loss: 2.311652\n",
      "[39,  3000] loss: 2.311134\n",
      "[39,  3200] loss: 2.311575\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[40,   200] loss: 2.311531\n",
      "[40,   400] loss: 2.308926\n",
      "[40,   600] loss: 2.312141\n",
      "[40,   800] loss: 2.310636\n",
      "[40,  1000] loss: 2.312867\n",
      "[40,  1200] loss: 2.311374\n",
      "[40,  1400] loss: 2.312812\n",
      "[40,  1600] loss: 2.313036\n",
      "[40,  1800] loss: 2.309900\n",
      "[40,  2000] loss: 2.311860\n",
      "[40,  2200] loss: 2.311914\n",
      "[40,  2400] loss: 2.310801\n",
      "[40,  2600] loss: 2.310917\n",
      "[40,  2800] loss: 2.312252\n",
      "[40,  3000] loss: 2.311161\n",
      "[40,  3200] loss: 2.311054\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[41,   200] loss: 2.310550\n",
      "[41,   400] loss: 2.310327\n",
      "[41,   600] loss: 2.311761\n",
      "[41,   800] loss: 2.311059\n",
      "[41,  1000] loss: 2.309608\n",
      "[41,  1200] loss: 2.313287\n",
      "[41,  1400] loss: 2.311442\n",
      "[41,  1600] loss: 2.312517\n",
      "[41,  1800] loss: 2.311553\n",
      "[41,  2000] loss: 2.311974\n",
      "[41,  2200] loss: 2.311506\n",
      "[41,  2400] loss: 2.311164\n",
      "[41,  2600] loss: 2.310913\n",
      "[41,  2800] loss: 2.310663\n",
      "[41,  3000] loss: 2.312100\n",
      "[41,  3200] loss: 2.309484\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[42,   200] loss: 2.314523\n",
      "[42,   400] loss: 2.311126\n",
      "[42,   600] loss: 2.311480\n",
      "[42,   800] loss: 2.312081\n",
      "[42,  1000] loss: 2.311603\n",
      "[42,  1200] loss: 2.311286\n",
      "[42,  1400] loss: 2.312220\n",
      "[42,  1600] loss: 2.309412\n",
      "[42,  1800] loss: 2.310062\n",
      "[42,  2000] loss: 2.311603\n",
      "[42,  2200] loss: 2.309885\n",
      "[42,  2400] loss: 2.311154\n",
      "[42,  2600] loss: 2.311983\n",
      "[42,  2800] loss: 2.311013\n",
      "[42,  3000] loss: 2.311157\n",
      "[42,  3200] loss: 2.310354\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[43,   200] loss: 2.312232\n",
      "[43,   400] loss: 2.313850\n",
      "[43,   600] loss: 2.311219\n",
      "[43,   800] loss: 2.311160\n",
      "[43,  1000] loss: 2.311495\n",
      "[43,  1200] loss: 2.311496\n",
      "[43,  1400] loss: 2.310809\n",
      "[43,  1600] loss: 2.312513\n",
      "[43,  1800] loss: 2.312880\n",
      "[43,  2000] loss: 2.311345\n",
      "[43,  2200] loss: 2.310382\n",
      "[43,  2400] loss: 2.311713\n",
      "[43,  2600] loss: 2.311890\n",
      "[43,  2800] loss: 2.311207\n",
      "[43,  3000] loss: 2.312216\n",
      "[43,  3200] loss: 2.311226\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[44,   200] loss: 2.312804\n",
      "[44,   400] loss: 2.312394\n",
      "[44,   600] loss: 2.311823\n",
      "[44,   800] loss: 2.309949\n",
      "[44,  1000] loss: 2.311283\n",
      "[44,  1200] loss: 2.311405\n",
      "[44,  1400] loss: 2.312675\n",
      "[44,  1600] loss: 2.313047\n",
      "[44,  1800] loss: 2.314095\n",
      "[44,  2000] loss: 2.310285\n",
      "[44,  2200] loss: 2.310441\n",
      "[44,  2400] loss: 2.311585\n",
      "[44,  2600] loss: 2.310661\n",
      "[44,  2800] loss: 2.313714\n",
      "[44,  3000] loss: 2.313159\n",
      "[44,  3200] loss: 2.311437\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[45,   200] loss: 2.312449\n",
      "[45,   400] loss: 2.311739\n",
      "[45,   600] loss: 2.311910\n",
      "[45,   800] loss: 2.312344\n",
      "[45,  1000] loss: 2.311657\n",
      "[45,  1200] loss: 2.309771\n",
      "[45,  1400] loss: 2.310870\n",
      "[45,  1600] loss: 2.310636\n",
      "[45,  1800] loss: 2.310113\n",
      "[45,  2000] loss: 2.310021\n",
      "[45,  2200] loss: 2.310676\n",
      "[45,  2400] loss: 2.309757\n",
      "[45,  2600] loss: 2.311541\n",
      "[45,  2800] loss: 2.312021\n",
      "[45,  3000] loss: 2.310766\n",
      "[45,  3200] loss: 2.312209\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[46,   200] loss: 2.312022\n",
      "[46,   400] loss: 2.311820\n",
      "[46,   600] loss: 2.311115\n",
      "[46,   800] loss: 2.309887\n",
      "[46,  1000] loss: 2.309868\n",
      "[46,  1200] loss: 2.311697\n",
      "[46,  1400] loss: 2.312741\n",
      "[46,  1600] loss: 2.310245\n",
      "[46,  1800] loss: 2.310002\n",
      "[46,  2000] loss: 2.310049\n",
      "[46,  2200] loss: 2.311129\n",
      "[46,  2400] loss: 2.312235\n",
      "[46,  2600] loss: 2.312095\n",
      "[46,  2800] loss: 2.310920\n",
      "[46,  3000] loss: 2.312218\n",
      "[46,  3200] loss: 2.311272\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[47,   200] loss: 2.312955\n",
      "[47,   400] loss: 2.310621\n",
      "[47,   600] loss: 2.310628\n",
      "[47,   800] loss: 2.313833\n",
      "[47,  1000] loss: 2.309495\n",
      "[47,  1200] loss: 2.312961\n",
      "[47,  1400] loss: 2.310668\n",
      "[47,  1600] loss: 2.312618\n",
      "[47,  1800] loss: 2.309962\n",
      "[47,  2000] loss: 2.312289\n",
      "[47,  2200] loss: 2.313446\n",
      "[47,  2400] loss: 2.310692\n",
      "[47,  2600] loss: 2.312177\n",
      "[47,  2800] loss: 2.312172\n",
      "[47,  3000] loss: 2.308734\n",
      "[47,  3200] loss: 2.312188\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[48,   200] loss: 2.310865\n",
      "[48,   400] loss: 2.310005\n",
      "[48,   600] loss: 2.311148\n",
      "[48,   800] loss: 2.311092\n",
      "[48,  1000] loss: 2.313408\n",
      "[48,  1200] loss: 2.310633\n",
      "[48,  1400] loss: 2.310835\n",
      "[48,  1600] loss: 2.310927\n",
      "[48,  1800] loss: 2.313384\n",
      "[48,  2000] loss: 2.310632\n",
      "[48,  2200] loss: 2.311985\n",
      "[48,  2400] loss: 2.311243\n",
      "[48,  2600] loss: 2.312238\n",
      "[48,  2800] loss: 2.312190\n",
      "[48,  3000] loss: 2.310707\n",
      "[48,  3200] loss: 2.312295\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[49,   200] loss: 2.311162\n",
      "[49,   400] loss: 2.312597\n",
      "[49,   600] loss: 2.310278\n",
      "[49,   800] loss: 2.311771\n",
      "[49,  1000] loss: 2.312105\n",
      "[49,  1200] loss: 2.311590\n",
      "[49,  1400] loss: 2.311915\n",
      "[49,  1600] loss: 2.311059\n",
      "[49,  1800] loss: 2.310839\n",
      "[49,  2000] loss: 2.311382\n",
      "[49,  2200] loss: 2.309910\n",
      "[49,  2400] loss: 2.310777\n",
      "[49,  2600] loss: 2.312960\n",
      "[49,  2800] loss: 2.311950\n",
      "[49,  3000] loss: 2.312243\n",
      "[49,  3200] loss: 2.311422\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[50,   200] loss: 2.311014\n",
      "[50,   400] loss: 2.311060\n",
      "[50,   600] loss: 2.310167\n",
      "[50,   800] loss: 2.309028\n",
      "[50,  1000] loss: 2.310389\n",
      "[50,  1200] loss: 2.310648\n",
      "[50,  1400] loss: 2.311488\n",
      "[50,  1600] loss: 2.311634\n",
      "[50,  1800] loss: 2.310280\n",
      "[50,  2000] loss: 2.311086\n",
      "[50,  2200] loss: 2.309008\n",
      "[50,  2400] loss: 2.311064\n",
      "[50,  2600] loss: 2.310824\n",
      "[50,  2800] loss: 2.310697\n",
      "[50,  3000] loss: 2.310680\n",
      "[50,  3200] loss: 2.312094\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[51,   200] loss: 2.312462\n",
      "[51,   400] loss: 2.310830\n",
      "[51,   600] loss: 2.312765\n",
      "[51,   800] loss: 2.310884\n",
      "[51,  1000] loss: 2.313173\n",
      "[51,  1200] loss: 2.309955\n",
      "[51,  1400] loss: 2.311249\n",
      "[51,  1600] loss: 2.314170\n",
      "[51,  1800] loss: 2.310433\n",
      "[51,  2000] loss: 2.310082\n",
      "[51,  2200] loss: 2.313373\n",
      "[51,  2400] loss: 2.311737\n",
      "[51,  2600] loss: 2.310941\n",
      "[51,  2800] loss: 2.313111\n",
      "[51,  3000] loss: 2.312441\n",
      "[51,  3200] loss: 2.310372\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[52,   200] loss: 2.310524\n",
      "[52,   400] loss: 2.313244\n",
      "[52,   600] loss: 2.310328\n",
      "[52,   800] loss: 2.312163\n",
      "[52,  1000] loss: 2.309721\n",
      "[52,  1200] loss: 2.309880\n",
      "[52,  1400] loss: 2.311397\n",
      "[52,  1600] loss: 2.311952\n",
      "[52,  1800] loss: 2.311154\n",
      "[52,  2000] loss: 2.312947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52,  2200] loss: 2.313430\n",
      "[52,  2400] loss: 2.312406\n",
      "[52,  2600] loss: 2.312074\n",
      "[52,  2800] loss: 2.311504\n",
      "[52,  3000] loss: 2.311326\n",
      "[52,  3200] loss: 2.312881\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[53,   200] loss: 2.310500\n",
      "[53,   400] loss: 2.311214\n",
      "[53,   600] loss: 2.311482\n",
      "[53,   800] loss: 2.310261\n",
      "[53,  1000] loss: 2.310285\n",
      "[53,  1200] loss: 2.310164\n",
      "[53,  1400] loss: 2.311696\n",
      "[53,  1600] loss: 2.311278\n",
      "[53,  1800] loss: 2.310329\n",
      "[53,  2000] loss: 2.312387\n",
      "[53,  2200] loss: 2.309215\n",
      "[53,  2400] loss: 2.311149\n",
      "[53,  2600] loss: 2.311414\n",
      "[53,  2800] loss: 2.311098\n",
      "[53,  3000] loss: 2.311790\n",
      "[53,  3200] loss: 2.313049\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[54,   200] loss: 2.312472\n",
      "[54,   400] loss: 2.313333\n",
      "[54,   600] loss: 2.312274\n",
      "[54,   800] loss: 2.310973\n",
      "[54,  1000] loss: 2.310938\n",
      "[54,  1200] loss: 2.310665\n",
      "[54,  1400] loss: 2.310373\n",
      "[54,  1600] loss: 2.310903\n",
      "[54,  1800] loss: 2.312175\n",
      "[54,  2000] loss: 2.311982\n",
      "[54,  2200] loss: 2.309317\n",
      "[54,  2400] loss: 2.312290\n",
      "[54,  2600] loss: 2.313334\n",
      "[54,  2800] loss: 2.311188\n",
      "[54,  3000] loss: 2.311153\n",
      "[54,  3200] loss: 2.309795\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[55,   200] loss: 2.312194\n",
      "[55,   400] loss: 2.310028\n",
      "[55,   600] loss: 2.311443\n",
      "[55,   800] loss: 2.310770\n",
      "[55,  1000] loss: 2.312044\n",
      "[55,  1200] loss: 2.310969\n",
      "[55,  1400] loss: 2.310730\n",
      "[55,  1600] loss: 2.312413\n",
      "[55,  1800] loss: 2.310544\n",
      "[55,  2000] loss: 2.311913\n",
      "[55,  2200] loss: 2.312008\n",
      "[55,  2400] loss: 2.311221\n",
      "[55,  2600] loss: 2.309763\n",
      "[55,  2800] loss: 2.310825\n",
      "[55,  3000] loss: 2.311041\n",
      "[55,  3200] loss: 2.309912\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[56,   200] loss: 2.310036\n",
      "[56,   400] loss: 2.312895\n",
      "[56,   600] loss: 2.309764\n",
      "[56,   800] loss: 2.310797\n",
      "[56,  1000] loss: 2.311321\n",
      "[56,  1200] loss: 2.311344\n",
      "[56,  1400] loss: 2.311841\n",
      "[56,  1600] loss: 2.311010\n",
      "[56,  1800] loss: 2.311398\n",
      "[56,  2000] loss: 2.311450\n",
      "[56,  2200] loss: 2.311219\n",
      "[56,  2400] loss: 2.313035\n",
      "[56,  2600] loss: 2.313034\n",
      "[56,  2800] loss: 2.310006\n",
      "[56,  3000] loss: 2.310762\n",
      "[56,  3200] loss: 2.311466\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[57,   200] loss: 2.309753\n",
      "[57,   400] loss: 2.311259\n",
      "[57,   600] loss: 2.314337\n",
      "[57,   800] loss: 2.311515\n",
      "[57,  1000] loss: 2.310787\n",
      "[57,  1200] loss: 2.310323\n",
      "[57,  1400] loss: 2.312917\n",
      "[57,  1600] loss: 2.310984\n",
      "[57,  1800] loss: 2.312586\n",
      "[57,  2000] loss: 2.310892\n",
      "[57,  2200] loss: 2.311233\n",
      "[57,  2400] loss: 2.314096\n",
      "[57,  2600] loss: 2.310381\n",
      "[57,  2800] loss: 2.312290\n",
      "[57,  3000] loss: 2.309279\n",
      "[57,  3200] loss: 2.310334\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[58,   200] loss: 2.312009\n",
      "[58,   400] loss: 2.310360\n",
      "[58,   600] loss: 2.311973\n",
      "[58,   800] loss: 2.311872\n",
      "[58,  1000] loss: 2.311136\n",
      "[58,  1200] loss: 2.311257\n",
      "[58,  1400] loss: 2.313340\n",
      "[58,  1600] loss: 2.313758\n",
      "[58,  1800] loss: 2.311612\n",
      "[58,  2000] loss: 2.310349\n",
      "[58,  2200] loss: 2.312195\n",
      "[58,  2400] loss: 2.310474\n",
      "[58,  2600] loss: 2.309213\n",
      "[58,  2800] loss: 2.311702\n",
      "[58,  3000] loss: 2.311100\n",
      "[58,  3200] loss: 2.310258\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[59,   200] loss: 2.312615\n",
      "[59,   400] loss: 2.313810\n",
      "[59,   600] loss: 2.310568\n",
      "[59,   800] loss: 2.310514\n",
      "[59,  1000] loss: 2.309177\n",
      "[59,  1200] loss: 2.310446\n",
      "[59,  1400] loss: 2.311769\n",
      "[59,  1600] loss: 2.310459\n",
      "[59,  1800] loss: 2.311718\n",
      "[59,  2000] loss: 2.311947\n",
      "[59,  2200] loss: 2.311634\n",
      "[59,  2400] loss: 2.312962\n",
      "[59,  2600] loss: 2.312737\n",
      "[59,  2800] loss: 2.310646\n",
      "[59,  3000] loss: 2.313187\n",
      "[59,  3200] loss: 2.311980\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[60,   200] loss: 2.312178\n",
      "[60,   400] loss: 2.311745\n",
      "[60,   600] loss: 2.312323\n",
      "[60,   800] loss: 2.311796\n",
      "[60,  1000] loss: 2.311322\n",
      "[60,  1200] loss: 2.311247\n",
      "[60,  1400] loss: 2.312488\n",
      "[60,  1600] loss: 2.311964\n",
      "[60,  1800] loss: 2.312200\n",
      "[60,  2000] loss: 2.311323\n",
      "[60,  2200] loss: 2.312249\n",
      "[60,  2400] loss: 2.312116\n",
      "[60,  2600] loss: 2.309942\n",
      "[60,  2800] loss: 2.312051\n",
      "[60,  3000] loss: 2.311672\n",
      "[60,  3200] loss: 2.310947\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[61,   200] loss: 2.310354\n",
      "[61,   400] loss: 2.312257\n",
      "[61,   600] loss: 2.309904\n",
      "[61,   800] loss: 2.311033\n",
      "[61,  1000] loss: 2.311780\n",
      "[61,  1200] loss: 2.310580\n",
      "[61,  1400] loss: 2.311534\n",
      "[61,  1600] loss: 2.311401\n",
      "[61,  1800] loss: 2.310869\n",
      "[61,  2000] loss: 2.313546\n",
      "[61,  2200] loss: 2.310762\n",
      "[61,  2400] loss: 2.310609\n",
      "[61,  2600] loss: 2.313098\n",
      "[61,  2800] loss: 2.311536\n",
      "[61,  3000] loss: 2.309501\n",
      "[61,  3200] loss: 2.311874\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[62,   200] loss: 2.312276\n",
      "[62,   400] loss: 2.311896\n",
      "[62,   600] loss: 2.313217\n",
      "[62,   800] loss: 2.311579\n",
      "[62,  1000] loss: 2.312263\n",
      "[62,  1200] loss: 2.310757\n",
      "[62,  1400] loss: 2.313658\n",
      "[62,  1600] loss: 2.310442\n",
      "[62,  1800] loss: 2.311895\n",
      "[62,  2000] loss: 2.310770\n",
      "[62,  2200] loss: 2.310744\n",
      "[62,  2400] loss: 2.310353\n",
      "[62,  2600] loss: 2.310490\n",
      "[62,  2800] loss: 2.310673\n",
      "[62,  3000] loss: 2.313412\n",
      "[62,  3200] loss: 2.309690\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[63,   200] loss: 2.311627\n",
      "[63,   400] loss: 2.311870\n",
      "[63,   600] loss: 2.312886\n",
      "[63,   800] loss: 2.312371\n",
      "[63,  1000] loss: 2.311377\n",
      "[63,  1200] loss: 2.311844\n",
      "[63,  1400] loss: 2.309799\n",
      "[63,  1600] loss: 2.867677\n",
      "[63,  1800] loss: 4.169621\n",
      "[63,  2000] loss: 2.312282\n",
      "[63,  2200] loss: 2.315074\n",
      "[63,  2400] loss: 2.312472\n",
      "[63,  2600] loss: 2.311589\n",
      "[63,  2800] loss: 2.310968\n",
      "[63,  3000] loss: 2.311593\n",
      "[63,  3200] loss: 2.311760\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[64,   200] loss: 2.310742\n",
      "[64,   400] loss: 2.311356\n",
      "[64,   600] loss: 2.310503\n",
      "[64,   800] loss: 2.308587\n",
      "[64,  1000] loss: 2.312694\n",
      "[64,  1200] loss: 2.311103\n",
      "[64,  1400] loss: 2.310174\n",
      "[64,  1600] loss: 2.311889\n",
      "[64,  1800] loss: 2.311306\n",
      "[64,  2000] loss: 2.311958\n",
      "[64,  2200] loss: 2.310303\n",
      "[64,  2400] loss: 2.311926\n",
      "[64,  2600] loss: 2.310749\n",
      "[64,  2800] loss: 2.312246\n",
      "[64,  3000] loss: 2.312181\n",
      "[64,  3200] loss: 2.310393\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[65,   200] loss: 2.311747\n",
      "[65,   400] loss: 2.311282\n",
      "[65,   600] loss: 2.310012\n",
      "[65,   800] loss: 2.311406\n",
      "[65,  1000] loss: 2.311863\n",
      "[65,  1200] loss: 2.310452\n",
      "[65,  1400] loss: 2.312821\n",
      "[65,  1600] loss: 2.309719\n",
      "[65,  1800] loss: 2.312094\n",
      "[65,  2000] loss: 2.313073\n",
      "[65,  2200] loss: 2.310377\n",
      "[65,  2400] loss: 2.311555\n",
      "[65,  2600] loss: 2.311598\n",
      "[65,  2800] loss: 2.311642\n",
      "[65,  3000] loss: 2.311873\n",
      "[65,  3200] loss: 2.311852\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[66,   200] loss: 2.310970\n",
      "[66,   400] loss: 2.311615\n",
      "[66,   600] loss: 2.311077\n",
      "[66,   800] loss: 2.312092\n",
      "[66,  1000] loss: 2.311695\n",
      "[66,  1200] loss: 2.312033\n",
      "[66,  1400] loss: 2.310390\n",
      "[66,  1600] loss: 2.310051\n",
      "[66,  1800] loss: 2.311343\n",
      "[66,  2000] loss: 2.311097\n",
      "[66,  2200] loss: 2.310718\n",
      "[66,  2400] loss: 2.312566\n",
      "[66,  2600] loss: 2.311847\n",
      "[66,  2800] loss: 2.313480\n",
      "[66,  3000] loss: 2.313138\n",
      "[66,  3200] loss: 2.312749\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[67,   200] loss: 2.312205\n",
      "[67,   400] loss: 2.310161\n",
      "[67,   600] loss: 2.312265\n",
      "[67,   800] loss: 2.312306\n",
      "[67,  1000] loss: 2.310758\n",
      "[67,  1200] loss: 2.312112\n",
      "[67,  1400] loss: 2.313202\n",
      "[67,  1600] loss: 2.311591\n",
      "[67,  1800] loss: 2.312305\n",
      "[67,  2000] loss: 2.310084\n",
      "[67,  2200] loss: 2.311070\n",
      "[67,  2400] loss: 2.311439\n",
      "[67,  2600] loss: 2.311423\n",
      "[67,  2800] loss: 2.311130\n",
      "[67,  3000] loss: 2.311103\n",
      "[67,  3200] loss: 2.312194\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[68,   200] loss: 2.312954\n",
      "[68,   400] loss: 2.312907\n",
      "[68,   600] loss: 2.309962\n",
      "[68,   800] loss: 2.310593\n",
      "[68,  1000] loss: 2.308617\n",
      "[68,  1200] loss: 2.311797\n",
      "[68,  1400] loss: 2.311935\n",
      "[68,  1600] loss: 2.311724\n",
      "[68,  1800] loss: 2.313701\n",
      "[68,  2000] loss: 2.311456\n",
      "[68,  2200] loss: 2.313021\n",
      "[68,  2400] loss: 2.312145\n",
      "[68,  2600] loss: 2.311363\n",
      "[68,  2800] loss: 2.311664\n",
      "[68,  3000] loss: 2.311164\n",
      "[68,  3200] loss: 2.310685\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[69,   200] loss: 2.312809\n",
      "[69,   400] loss: 2.309047\n",
      "[69,   600] loss: 2.310896\n",
      "[69,   800] loss: 2.309987\n",
      "[69,  1000] loss: 2.312052\n",
      "[69,  1200] loss: 2.311283\n",
      "[69,  1400] loss: 2.311636\n",
      "[69,  1600] loss: 2.310732\n",
      "[69,  1800] loss: 2.311221\n",
      "[69,  2000] loss: 2.310651\n",
      "[69,  2200] loss: 2.310483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69,  2400] loss: 2.311424\n",
      "[69,  2600] loss: 2.310697\n",
      "[69,  2800] loss: 2.311557\n",
      "[69,  3000] loss: 2.311577\n",
      "[69,  3200] loss: 2.312351\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[70,   200] loss: 2.312113\n",
      "[70,   400] loss: 2.311466\n",
      "[70,   600] loss: 2.310278\n",
      "[70,   800] loss: 2.312297\n",
      "[70,  1000] loss: 2.312065\n",
      "[70,  1200] loss: 2.312499\n",
      "[70,  1400] loss: 2.311278\n",
      "[70,  1600] loss: 2.312684\n",
      "[70,  1800] loss: 2.310257\n",
      "[70,  2000] loss: 2.311331\n",
      "[70,  2200] loss: 2.312441\n",
      "[70,  2400] loss: 2.311067\n",
      "[70,  2600] loss: 2.311384\n",
      "[70,  2800] loss: 2.311586\n",
      "[70,  3000] loss: 2.312489\n",
      "[70,  3200] loss: 2.313166\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[71,   200] loss: 2.311777\n",
      "[71,   400] loss: 2.311305\n",
      "[71,   600] loss: 2.309190\n",
      "[71,   800] loss: 2.311067\n",
      "[71,  1000] loss: 2.314080\n",
      "[71,  1200] loss: 2.313217\n",
      "[71,  1400] loss: 2.314880\n",
      "[71,  1600] loss: 2.311985\n",
      "[71,  1800] loss: 2.310070\n",
      "[71,  2000] loss: 2.310055\n",
      "[71,  2200] loss: 2.311458\n",
      "[71,  2400] loss: 2.311414\n",
      "[71,  2600] loss: 2.310534\n",
      "[71,  2800] loss: 2.313251\n",
      "[71,  3000] loss: 2.312122\n",
      "[71,  3200] loss: 2.310905\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[72,   200] loss: 2.312053\n",
      "[72,   400] loss: 2.312497\n",
      "[72,   600] loss: 2.310920\n",
      "[72,   800] loss: 2.309722\n",
      "[72,  1000] loss: 2.312383\n",
      "[72,  1200] loss: 2.313771\n",
      "[72,  1400] loss: 2.310146\n",
      "[72,  1600] loss: 2.310962\n",
      "[72,  1800] loss: 2.312822\n",
      "[72,  2000] loss: 2.311275\n",
      "[72,  2200] loss: 2.311141\n",
      "[72,  2400] loss: 2.310547\n",
      "[72,  2600] loss: 2.309840\n",
      "[72,  2800] loss: 2.312181\n",
      "[72,  3000] loss: 2.310635\n",
      "[72,  3200] loss: 2.310400\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[73,   200] loss: 2.309145\n",
      "[73,   400] loss: 2.310553\n",
      "[73,   600] loss: 2.311773\n",
      "[73,   800] loss: 2.310594\n",
      "[73,  1000] loss: 2.311277\n",
      "[73,  1200] loss: 2.312304\n",
      "[73,  1400] loss: 2.312240\n",
      "[73,  1600] loss: 2.312799\n",
      "[73,  1800] loss: 2.311113\n",
      "[73,  2000] loss: 2.311038\n",
      "[73,  2200] loss: 2.311765\n",
      "[73,  2400] loss: 2.311166\n",
      "[73,  2600] loss: 2.309825\n",
      "[73,  2800] loss: 2.311760\n",
      "[73,  3000] loss: 2.311756\n",
      "[73,  3200] loss: 2.310134\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[74,   200] loss: 2.312384\n",
      "[74,   400] loss: 2.314205\n",
      "[74,   600] loss: 2.311643\n",
      "[74,   800] loss: 2.311539\n",
      "[74,  1000] loss: 2.311460\n",
      "[74,  1200] loss: 2.310908\n",
      "[74,  1400] loss: 2.311230\n",
      "[74,  1600] loss: 2.311383\n",
      "[74,  1800] loss: 2.312713\n",
      "[74,  2000] loss: 2.312337\n",
      "[74,  2200] loss: 2.310363\n",
      "[74,  2400] loss: 2.311717\n",
      "[74,  2600] loss: 2.309608\n",
      "[74,  2800] loss: 2.311296\n",
      "[74,  3000] loss: 2.309500\n",
      "[74,  3200] loss: 2.310138\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[75,   200] loss: 2.310787\n",
      "[75,   400] loss: 2.309606\n",
      "[75,   600] loss: 2.310475\n",
      "[75,   800] loss: 2.312057\n",
      "[75,  1000] loss: 2.311163\n",
      "[75,  1200] loss: 2.312633\n",
      "[75,  1400] loss: 2.308984\n",
      "[75,  1600] loss: 2.312486\n",
      "[75,  1800] loss: 2.312159\n",
      "[75,  2000] loss: 2.309975\n",
      "[75,  2200] loss: 2.309770\n",
      "[75,  2400] loss: 2.311477\n",
      "[75,  2600] loss: 2.312514\n",
      "[75,  2800] loss: 2.309990\n",
      "[75,  3000] loss: 2.311550\n",
      "[75,  3200] loss: 2.309568\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[76,   200] loss: 2.310985\n",
      "[76,   400] loss: 2.309460\n",
      "[76,   600] loss: 2.309384\n",
      "[76,   800] loss: 2.311333\n",
      "[76,  1000] loss: 2.310043\n",
      "[76,  1200] loss: 2.312658\n",
      "[76,  1400] loss: 2.310621\n",
      "[76,  1600] loss: 2.311386\n",
      "[76,  1800] loss: 2.309902\n",
      "[76,  2000] loss: 2.310422\n",
      "[76,  2200] loss: 2.311297\n",
      "[76,  2400] loss: 2.312280\n",
      "[76,  2600] loss: 2.311727\n",
      "[76,  2800] loss: 2.310818\n",
      "[76,  3000] loss: 2.311505\n",
      "[76,  3200] loss: 2.311853\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[77,   200] loss: 2.311346\n",
      "[77,   400] loss: 2.310974\n",
      "[77,   600] loss: 2.311085\n",
      "[77,   800] loss: 2.310544\n",
      "[77,  1000] loss: 2.309919\n",
      "[77,  1200] loss: 2.311556\n",
      "[77,  1400] loss: 2.309036\n",
      "[77,  1600] loss: 2.312824\n",
      "[77,  1800] loss: 2.310800\n",
      "[77,  2000] loss: 2.311085\n",
      "[77,  2200] loss: 2.311426\n",
      "[77,  2400] loss: 2.310973\n",
      "[77,  2600] loss: 2.311745\n",
      "[77,  2800] loss: 2.309234\n",
      "[77,  3000] loss: 2.312493\n",
      "[77,  3200] loss: 2.310955\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[78,   200] loss: 2.311373\n",
      "[78,   400] loss: 2.312314\n",
      "[78,   600] loss: 2.312635\n",
      "[78,   800] loss: 2.310840\n",
      "[78,  1000] loss: 2.312106\n",
      "[78,  1200] loss: 2.311843\n",
      "[78,  1400] loss: 2.312995\n",
      "[78,  1600] loss: 2.309753\n",
      "[78,  1800] loss: 2.312338\n",
      "[78,  2000] loss: 2.312027\n",
      "[78,  2200] loss: 2.310960\n",
      "[78,  2400] loss: 2.312654\n",
      "[78,  2600] loss: 2.309895\n",
      "[78,  2800] loss: 2.311115\n",
      "[78,  3000] loss: 2.311066\n",
      "[78,  3200] loss: 2.312052\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[79,   200] loss: 2.311416\n",
      "[79,   400] loss: 2.311169\n",
      "[79,   600] loss: 2.310784\n",
      "[79,   800] loss: 2.310099\n",
      "[79,  1000] loss: 2.311751\n",
      "[79,  1200] loss: 2.311342\n",
      "[79,  1400] loss: 2.311176\n",
      "[79,  1600] loss: 2.310563\n",
      "[79,  1800] loss: 2.313397\n",
      "[79,  2000] loss: 2.310353\n",
      "[79,  2200] loss: 2.310750\n",
      "[79,  2400] loss: 2.311238\n",
      "[79,  2600] loss: 2.311105\n",
      "[79,  2800] loss: 2.311538\n",
      "[79,  3000] loss: 2.310225\n",
      "[79,  3200] loss: 2.312339\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[80,   200] loss: 2.311758\n",
      "[80,   400] loss: 2.312487\n",
      "[80,   600] loss: 2.311511\n",
      "[80,   800] loss: 2.313246\n",
      "[80,  1000] loss: 2.312902\n",
      "[80,  1200] loss: 2.311553\n",
      "[80,  1400] loss: 2.311631\n",
      "[80,  1600] loss: 2.311614\n",
      "[80,  1800] loss: 2.310261\n",
      "[80,  2000] loss: 2.311318\n",
      "[80,  2200] loss: 2.311077\n",
      "[80,  2400] loss: 2.309888\n",
      "[80,  2600] loss: 2.311805\n",
      "[80,  2800] loss: 2.312744\n",
      "[80,  3000] loss: 2.312913\n",
      "[80,  3200] loss: 2.310731\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[81,   200] loss: 2.310272\n",
      "[81,   400] loss: 2.314455\n",
      "[81,   600] loss: 2.312338\n",
      "[81,   800] loss: 2.312076\n",
      "[81,  1000] loss: 2.310835\n",
      "[81,  1200] loss: 2.313479\n",
      "[81,  1400] loss: 2.310029\n",
      "[81,  1600] loss: 2.310090\n",
      "[81,  1800] loss: 2.312135\n",
      "[81,  2000] loss: 2.310584\n",
      "[81,  2200] loss: 2.311674\n",
      "[81,  2400] loss: 2.312628\n",
      "[81,  2600] loss: 2.311277\n",
      "[81,  2800] loss: 2.310233\n",
      "[81,  3000] loss: 2.311902\n",
      "[81,  3200] loss: 2.310819\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[82,   200] loss: 2.311093\n",
      "[82,   400] loss: 2.311747\n",
      "[82,   600] loss: 2.312427\n",
      "[82,   800] loss: 2.309817\n",
      "[82,  1000] loss: 2.310634\n",
      "[82,  1200] loss: 2.310379\n",
      "[82,  1400] loss: 2.310722\n",
      "[82,  1600] loss: 2.311885\n",
      "[82,  1800] loss: 2.310222\n",
      "[82,  2000] loss: 2.314092\n",
      "[82,  2200] loss: 2.312204\n",
      "[82,  2400] loss: 2.308268\n",
      "[82,  2600] loss: 2.308007\n",
      "[82,  2800] loss: 2.311616\n",
      "[82,  3000] loss: 2.313311\n",
      "[82,  3200] loss: 2.309690\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[83,   200] loss: 2.311792\n",
      "[83,   400] loss: 2.312122\n",
      "[83,   600] loss: 2.312237\n",
      "[83,   800] loss: 2.310708\n",
      "[83,  1000] loss: 2.312392\n",
      "[83,  1200] loss: 2.311919\n",
      "[83,  1400] loss: 2.312416\n",
      "[83,  1600] loss: 2.311633\n",
      "[83,  1800] loss: 2.311675\n",
      "[83,  2000] loss: 2.309831\n",
      "[83,  2200] loss: 2.312174\n",
      "[83,  2400] loss: 2.311511\n",
      "[83,  2600] loss: 2.309752\n",
      "[83,  2800] loss: 2.310013\n",
      "[83,  3000] loss: 2.309372\n",
      "[83,  3200] loss: 2.310846\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[84,   200] loss: 2.310570\n",
      "[84,   400] loss: 2.310725\n",
      "[84,   600] loss: 2.310816\n",
      "[84,   800] loss: 2.311779\n",
      "[84,  1000] loss: 2.310784\n",
      "[84,  1200] loss: 2.312143\n",
      "[84,  1400] loss: 2.311484\n",
      "[84,  1600] loss: 2.311975\n",
      "[84,  1800] loss: 2.312536\n",
      "[84,  2000] loss: 2.312159\n",
      "[84,  2200] loss: 2.312701\n",
      "[84,  2400] loss: 2.310227\n",
      "[84,  2600] loss: 2.310848\n",
      "[84,  2800] loss: 2.313369\n",
      "[84,  3000] loss: 2.311562\n",
      "[84,  3200] loss: 2.308347\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[85,   200] loss: 2.312504\n",
      "[85,   400] loss: 2.311427\n",
      "[85,   600] loss: 2.312185\n",
      "[85,   800] loss: 2.311694\n",
      "[85,  1000] loss: 2.309874\n",
      "[85,  1200] loss: 2.312359\n",
      "[85,  1400] loss: 2.310989\n",
      "[85,  1600] loss: 2.314246\n",
      "[85,  1800] loss: 2.312591\n",
      "[85,  2000] loss: 2.312600\n",
      "[85,  2200] loss: 2.311096\n",
      "[85,  2400] loss: 2.312488\n",
      "[85,  2600] loss: 2.311014\n",
      "[85,  2800] loss: 2.312572\n",
      "[85,  3000] loss: 2.309993\n",
      "[85,  3200] loss: 2.314433\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[86,   200] loss: 2.310910\n",
      "[86,   400] loss: 2.311627\n",
      "[86,   600] loss: 2.311815\n",
      "[86,   800] loss: 2.311982\n",
      "[86,  1000] loss: 2.312579\n",
      "[86,  1200] loss: 2.311829\n",
      "[86,  1400] loss: 2.311735\n",
      "[86,  1600] loss: 2.313568\n",
      "[86,  1800] loss: 2.311546\n",
      "[86,  2000] loss: 2.310673\n",
      "[86,  2200] loss: 2.311863\n",
      "[86,  2400] loss: 2.312274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86,  2600] loss: 2.310996\n",
      "[86,  2800] loss: 2.312543\n",
      "[86,  3000] loss: 2.310612\n",
      "[86,  3200] loss: 2.311645\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[87,   200] loss: 2.312844\n",
      "[87,   400] loss: 2.310613\n",
      "[87,   600] loss: 2.312361\n",
      "[87,   800] loss: 2.314859\n",
      "[87,  1000] loss: 2.311684\n",
      "[87,  1200] loss: 2.312574\n",
      "[87,  1400] loss: 2.310529\n",
      "[87,  1600] loss: 2.310686\n",
      "[87,  1800] loss: 2.312262\n",
      "[87,  2000] loss: 2.311329\n",
      "[87,  2200] loss: 2.310695\n",
      "[87,  2400] loss: 2.314100\n",
      "[87,  2600] loss: 2.311577\n",
      "[87,  2800] loss: 2.312209\n",
      "[87,  3000] loss: 2.312026\n",
      "[87,  3200] loss: 2.312240\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[88,   200] loss: 2.309882\n",
      "[88,   400] loss: 2.311690\n",
      "[88,   600] loss: 2.313722\n",
      "[88,   800] loss: 2.310530\n",
      "[88,  1000] loss: 2.311665\n",
      "[88,  1200] loss: 2.311346\n",
      "[88,  1400] loss: 2.311075\n",
      "[88,  1600] loss: 2.311024\n",
      "[88,  1800] loss: 2.311204\n",
      "[88,  2000] loss: 2.311687\n",
      "[88,  2200] loss: 2.310669\n",
      "[88,  2400] loss: 2.310756\n",
      "[88,  2600] loss: 2.311480\n",
      "[88,  2800] loss: 2.310467\n",
      "[88,  3000] loss: 2.311899\n",
      "[88,  3200] loss: 2.311660\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[89,   200] loss: 2.311675\n",
      "[89,   400] loss: 2.310618\n",
      "[89,   600] loss: 2.311709\n",
      "[89,   800] loss: 2.311166\n",
      "[89,  1000] loss: 2.309828\n",
      "[89,  1200] loss: 2.312253\n",
      "[89,  1400] loss: 2.312199\n",
      "[89,  1600] loss: 2.312166\n",
      "[89,  1800] loss: 2.311172\n",
      "[89,  2000] loss: 2.313285\n",
      "[89,  2200] loss: 2.311065\n",
      "[89,  2400] loss: 2.312728\n",
      "[89,  2600] loss: 2.313948\n",
      "[89,  2800] loss: 2.313209\n",
      "[89,  3000] loss: 2.310642\n",
      "[89,  3200] loss: 2.310506\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[90,   200] loss: 2.311135\n",
      "[90,   400] loss: 2.311835\n",
      "[90,   600] loss: 2.310463\n",
      "[90,   800] loss: 2.310557\n",
      "[90,  1000] loss: 2.310057\n",
      "[90,  1200] loss: 2.310567\n",
      "[90,  1400] loss: 2.311783\n",
      "[90,  1600] loss: 2.311396\n",
      "[90,  1800] loss: 2.311841\n",
      "[90,  2000] loss: 2.310126\n",
      "[90,  2200] loss: 2.309751\n",
      "[90,  2400] loss: 2.313576\n",
      "[90,  2600] loss: 2.313148\n",
      "[90,  2800] loss: 2.312916\n",
      "[90,  3000] loss: 2.310231\n",
      "[90,  3200] loss: 2.312060\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[91,   200] loss: 2.310882\n",
      "[91,   400] loss: 2.311855\n",
      "[91,   600] loss: 2.310046\n",
      "[91,   800] loss: 2.309116\n",
      "[91,  1000] loss: 2.311230\n",
      "[91,  1200] loss: 2.313348\n",
      "[91,  1400] loss: 2.311117\n",
      "[91,  1600] loss: 2.312178\n",
      "[91,  1800] loss: 2.311604\n",
      "[91,  2000] loss: 2.313972\n",
      "[91,  2200] loss: 2.308904\n",
      "[91,  2400] loss: 2.313334\n",
      "[91,  2600] loss: 2.312312\n",
      "[91,  2800] loss: 2.312462\n",
      "[91,  3000] loss: 2.311437\n",
      "[91,  3200] loss: 2.312588\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[92,   200] loss: 2.312616\n",
      "[92,   400] loss: 2.312005\n",
      "[92,   600] loss: 2.311403\n",
      "[92,   800] loss: 2.310826\n",
      "[92,  1000] loss: 2.310659\n",
      "[92,  1200] loss: 2.311710\n",
      "[92,  1400] loss: 2.311218\n",
      "[92,  1600] loss: 2.310695\n",
      "[92,  1800] loss: 2.312235\n",
      "[92,  2000] loss: 2.309893\n",
      "[92,  2200] loss: 2.312432\n",
      "[92,  2400] loss: 2.310733\n",
      "[92,  2600] loss: 2.312155\n",
      "[92,  2800] loss: 2.311269\n",
      "[92,  3000] loss: 2.311507\n",
      "[92,  3200] loss: 2.311325\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[93,   200] loss: 2.309808\n",
      "[93,   400] loss: 2.311915\n",
      "[93,   600] loss: 2.309658\n",
      "[93,   800] loss: 2.311348\n",
      "[93,  1000] loss: 2.310642\n",
      "[93,  1200] loss: 2.310351\n",
      "[93,  1400] loss: 2.311053\n",
      "[93,  1600] loss: 2.313910\n",
      "[93,  1800] loss: 2.310850\n",
      "[93,  2000] loss: 2.310639\n",
      "[93,  2200] loss: 2.311207\n",
      "[93,  2400] loss: 2.311183\n",
      "[93,  2600] loss: 2.311622\n",
      "[93,  2800] loss: 2.311827\n",
      "[93,  3000] loss: 2.310413\n",
      "[93,  3200] loss: 2.312293\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[94,   200] loss: 2.310142\n",
      "[94,   400] loss: 2.311158\n",
      "[94,   600] loss: 2.311666\n",
      "[94,   800] loss: 2.311466\n",
      "[94,  1000] loss: 2.312652\n",
      "[94,  1200] loss: 2.312192\n",
      "[94,  1400] loss: 2.311867\n",
      "[94,  1600] loss: 2.311000\n",
      "[94,  1800] loss: 2.310237\n",
      "[94,  2000] loss: 2.311120\n",
      "[94,  2200] loss: 2.311558\n",
      "[94,  2400] loss: 2.310708\n",
      "[94,  2600] loss: 2.310493\n",
      "[94,  2800] loss: 2.311900\n",
      "[94,  3000] loss: 2.312097\n",
      "[94,  3200] loss: 2.312822\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[95,   200] loss: 2.310137\n",
      "[95,   400] loss: 2.310548\n",
      "[95,   600] loss: 2.312751\n",
      "[95,   800] loss: 2.312107\n",
      "[95,  1000] loss: 2.313511\n",
      "[95,  1200] loss: 2.310588\n",
      "[95,  1400] loss: 2.311598\n",
      "[95,  1600] loss: 2.311635\n",
      "[95,  1800] loss: 2.312437\n",
      "[95,  2000] loss: 2.311201\n",
      "[95,  2200] loss: 2.310484\n",
      "[95,  2400] loss: 2.310713\n",
      "[95,  2600] loss: 2.311481\n",
      "[95,  2800] loss: 2.312573\n",
      "[95,  3000] loss: 2.309324\n",
      "[95,  3200] loss: 2.312887\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[96,   200] loss: 2.311934\n",
      "[96,   400] loss: 2.312563\n",
      "[96,   600] loss: 2.312166\n",
      "[96,   800] loss: 2.313482\n",
      "[96,  1000] loss: 2.311236\n",
      "[96,  1200] loss: 2.310499\n",
      "[96,  1400] loss: 2.310697\n",
      "[96,  1600] loss: 2.308723\n",
      "[96,  1800] loss: 2.311744\n",
      "[96,  2000] loss: 2.311738\n",
      "[96,  2200] loss: 2.310811\n",
      "[96,  2400] loss: 2.310002\n",
      "[96,  2600] loss: 2.310087\n",
      "[96,  2800] loss: 2.311362\n",
      "[96,  3000] loss: 2.310410\n",
      "[96,  3200] loss: 2.311961\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[97,   200] loss: 2.311493\n",
      "[97,   400] loss: 2.310770\n",
      "[97,   600] loss: 2.312753\n",
      "[97,   800] loss: 2.312928\n",
      "[97,  1000] loss: 2.313058\n",
      "[97,  1200] loss: 2.310105\n",
      "[97,  1400] loss: 2.312108\n",
      "[97,  1600] loss: 2.311244\n",
      "[97,  1800] loss: 2.310914\n",
      "[97,  2000] loss: 2.312716\n",
      "[97,  2200] loss: 2.310628\n",
      "[97,  2400] loss: 2.311011\n",
      "[97,  2600] loss: 2.312377\n",
      "[97,  2800] loss: 2.311601\n",
      "[97,  3000] loss: 2.313434\n",
      "[97,  3200] loss: 2.311236\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[98,   200] loss: 2.312015\n",
      "[98,   400] loss: 2.310873\n",
      "[98,   600] loss: 2.311058\n",
      "[98,   800] loss: 2.312659\n",
      "[98,  1000] loss: 2.311542\n",
      "[98,  1200] loss: 2.312997\n",
      "[98,  1400] loss: 2.310416\n",
      "[98,  1600] loss: 2.311083\n",
      "[98,  1800] loss: 2.311119\n",
      "[98,  2000] loss: 2.310809\n",
      "[98,  2200] loss: 2.313348\n",
      "[98,  2400] loss: 2.311259\n",
      "[98,  2600] loss: 2.309869\n",
      "[98,  2800] loss: 2.311595\n",
      "[98,  3000] loss: 2.309743\n",
      "[98,  3200] loss: 2.310896\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[99,   200] loss: 2.309600\n",
      "[99,   400] loss: 2.313250\n",
      "[99,   600] loss: 2.311492\n",
      "[99,   800] loss: 2.310892\n",
      "[99,  1000] loss: 2.311971\n",
      "[99,  1200] loss: 2.311690\n",
      "[99,  1400] loss: 2.312142\n",
      "[99,  1600] loss: 2.313318\n",
      "[99,  1800] loss: 2.311534\n",
      "[99,  2000] loss: 2.311206\n",
      "[99,  2200] loss: 2.312702\n",
      "[99,  2400] loss: 2.311475\n",
      "[99,  2600] loss: 2.310354\n",
      "[99,  2800] loss: 2.312528\n",
      "[99,  3000] loss: 2.312148\n",
      "[99,  3200] loss: 2.313648\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[100,   200] loss: 2.311518\n",
      "[100,   400] loss: 2.310180\n",
      "[100,   600] loss: 2.312269\n",
      "[100,   800] loss: 2.311783\n",
      "[100,  1000] loss: 2.313500\n",
      "[100,  1200] loss: 2.310587\n",
      "[100,  1400] loss: 2.311739\n",
      "[100,  1600] loss: 2.312404\n",
      "[100,  1800] loss: 2.310838\n",
      "[100,  2000] loss: 2.310905\n",
      "[100,  2200] loss: 2.311567\n",
      "[100,  2400] loss: 2.310950\n",
      "[100,  2600] loss: 2.312014\n",
      "[100,  2800] loss: 2.310323\n",
      "[100,  3000] loss: 2.309632\n",
      "[100,  3200] loss: 2.311210\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[101,   200] loss: 2.311019\n",
      "[101,   400] loss: 2.312258\n",
      "[101,   600] loss: 2.311035\n",
      "[101,   800] loss: 2.309276\n",
      "[101,  1000] loss: 2.311973\n",
      "[101,  1200] loss: 2.310788\n",
      "[101,  1400] loss: 2.311967\n",
      "[101,  1600] loss: 2.310698\n",
      "[101,  1800] loss: 2.312483\n",
      "[101,  2000] loss: 2.310469\n",
      "[101,  2200] loss: 2.312320\n",
      "[101,  2400] loss: 2.310641\n",
      "[101,  2600] loss: 2.312240\n",
      "[101,  2800] loss: 2.311807\n",
      "[101,  3000] loss: 2.310656\n",
      "[101,  3200] loss: 2.312413\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[102,   200] loss: 2.314703\n",
      "[102,   400] loss: 2.311179\n",
      "[102,   600] loss: 2.310459\n",
      "[102,   800] loss: 2.310434\n",
      "[102,  1000] loss: 2.314113\n",
      "[102,  1200] loss: 2.311220\n",
      "[102,  1400] loss: 2.310912\n",
      "[102,  1600] loss: 2.310899\n",
      "[102,  1800] loss: 2.313534\n",
      "[102,  2000] loss: 2.311353\n",
      "[102,  2200] loss: 2.311451\n",
      "[102,  2400] loss: 2.310467\n",
      "[102,  2600] loss: 2.311326\n",
      "[102,  2800] loss: 2.311755\n",
      "[102,  3000] loss: 2.314194\n",
      "[102,  3200] loss: 2.310182\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[103,   200] loss: 2.310133\n",
      "[103,   400] loss: 2.308879\n",
      "[103,   600] loss: 2.310721\n",
      "[103,   800] loss: 2.310969\n",
      "[103,  1000] loss: 2.310204\n",
      "[103,  1200] loss: 2.313227\n",
      "[103,  1400] loss: 2.311867\n",
      "[103,  1600] loss: 2.310832\n",
      "[103,  1800] loss: 2.309185\n",
      "[103,  2000] loss: 2.312295\n",
      "[103,  2200] loss: 2.310917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103,  2400] loss: 2.312629\n",
      "[103,  2600] loss: 2.310237\n",
      "[103,  2800] loss: 2.311601\n",
      "[103,  3000] loss: 2.311324\n",
      "[103,  3200] loss: 2.309444\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[104,   200] loss: 2.311893\n",
      "[104,   400] loss: 2.311682\n",
      "[104,   600] loss: 2.312182\n",
      "[104,   800] loss: 2.310641\n",
      "[104,  1000] loss: 2.312689\n",
      "[104,  1200] loss: 2.312724\n",
      "[104,  1400] loss: 2.309080\n",
      "[104,  1600] loss: 2.313481\n",
      "[104,  1800] loss: 2.313145\n",
      "[104,  2000] loss: 2.312971\n",
      "[104,  2200] loss: 2.311139\n",
      "[104,  2400] loss: 2.310586\n",
      "[104,  2600] loss: 2.313201\n",
      "[104,  2800] loss: 2.311226\n",
      "[104,  3000] loss: 2.311724\n",
      "[104,  3200] loss: 2.313551\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[105,   200] loss: 2.312522\n",
      "[105,   400] loss: 2.310305\n",
      "[105,   600] loss: 2.312295\n",
      "[105,   800] loss: 2.312836\n",
      "[105,  1000] loss: 2.311431\n",
      "[105,  1200] loss: 2.312090\n",
      "[105,  1400] loss: 2.310292\n",
      "[105,  1600] loss: 2.310914\n",
      "[105,  1800] loss: 2.311645\n",
      "[105,  2000] loss: 2.309536\n",
      "[105,  2200] loss: 2.311990\n",
      "[105,  2400] loss: 2.313552\n",
      "[105,  2600] loss: 2.308595\n",
      "[105,  2800] loss: 2.312708\n",
      "[105,  3000] loss: 2.309219\n",
      "[105,  3200] loss: 2.311082\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[106,   200] loss: 2.311606\n",
      "[106,   400] loss: 2.310579\n",
      "[106,   600] loss: 2.312999\n",
      "[106,   800] loss: 2.311989\n",
      "[106,  1000] loss: 2.310795\n",
      "[106,  1200] loss: 2.310906\n",
      "[106,  1400] loss: 2.309593\n",
      "[106,  1600] loss: 2.310480\n",
      "[106,  1800] loss: 2.312209\n",
      "[106,  2000] loss: 2.310327\n",
      "[106,  2200] loss: 2.310825\n",
      "[106,  2400] loss: 2.311637\n",
      "[106,  2600] loss: 2.311616\n",
      "[106,  2800] loss: 2.314129\n",
      "[106,  3000] loss: 2.312336\n",
      "[106,  3200] loss: 2.310810\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[107,   200] loss: 2.311987\n",
      "[107,   400] loss: 2.311430\n",
      "[107,   600] loss: 2.312210\n",
      "[107,   800] loss: 2.311438\n",
      "[107,  1000] loss: 2.312200\n",
      "[107,  1200] loss: 2.311461\n",
      "[107,  1400] loss: 2.311667\n",
      "[107,  1600] loss: 2.311273\n",
      "[107,  1800] loss: 2.310978\n",
      "[107,  2000] loss: 2.311159\n",
      "[107,  2200] loss: 2.311945\n",
      "[107,  2400] loss: 2.311582\n",
      "[107,  2600] loss: 2.311812\n",
      "[107,  2800] loss: 2.310688\n",
      "[107,  3000] loss: 2.312805\n",
      "[107,  3200] loss: 2.311884\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[108,   200] loss: 2.313442\n",
      "[108,   400] loss: 2.311437\n",
      "[108,   600] loss: 2.312333\n",
      "[108,   800] loss: 2.311116\n",
      "[108,  1000] loss: 2.311326\n",
      "[108,  1200] loss: 2.311333\n",
      "[108,  1400] loss: 2.310019\n",
      "[108,  1600] loss: 2.310323\n",
      "[108,  1800] loss: 2.310321\n",
      "[108,  2000] loss: 2.310965\n",
      "[108,  2200] loss: 2.311353\n",
      "[108,  2400] loss: 2.312278\n",
      "[108,  2600] loss: 2.309646\n",
      "[108,  2800] loss: 2.311933\n",
      "[108,  3000] loss: 2.310756\n",
      "[108,  3200] loss: 2.310493\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[109,   200] loss: 2.312602\n",
      "[109,   400] loss: 2.311141\n",
      "[109,   600] loss: 3.253008\n",
      "[109,   800] loss: 2.317570\n",
      "[109,  1000] loss: 2.313436\n",
      "[109,  1200] loss: 2.310622\n",
      "[109,  1400] loss: 2.312069\n",
      "[109,  1600] loss: 2.309562\n",
      "[109,  1800] loss: 2.311024\n",
      "[109,  2000] loss: 2.312319\n",
      "[109,  2200] loss: 2.310304\n",
      "[109,  2400] loss: 2.311462\n",
      "[109,  2600] loss: 2.311503\n",
      "[109,  2800] loss: 2.311603\n",
      "[109,  3000] loss: 2.311817\n",
      "[109,  3200] loss: 2.313866\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[110,   200] loss: 2.311856\n",
      "[110,   400] loss: 2.310608\n",
      "[110,   600] loss: 2.311498\n",
      "[110,   800] loss: 2.328974\n",
      "[110,  1000] loss: 2.698355\n",
      "[110,  1200] loss: 2.310978\n",
      "[110,  1400] loss: 2.312878\n",
      "[110,  1600] loss: 2.309667\n",
      "[110,  1800] loss: 2.312174\n",
      "[110,  2000] loss: 2.310065\n",
      "[110,  2200] loss: 2.309206\n",
      "[110,  2400] loss: 2.311076\n",
      "[110,  2600] loss: 2.310896\n",
      "[110,  2800] loss: 2.312572\n",
      "[110,  3000] loss: 2.312500\n",
      "[110,  3200] loss: 2.311355\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[111,   200] loss: 2.310808\n",
      "[111,   400] loss: 2.310987\n",
      "[111,   600] loss: 2.311795\n",
      "[111,   800] loss: 2.311742\n",
      "[111,  1000] loss: 2.312244\n",
      "[111,  1200] loss: 2.311535\n",
      "[111,  1400] loss: 2.312625\n",
      "[111,  1600] loss: 2.310870\n",
      "[111,  1800] loss: 2.311109\n",
      "[111,  2000] loss: 2.311882\n",
      "[111,  2200] loss: 2.310641\n",
      "[111,  2400] loss: 2.312101\n",
      "[111,  2600] loss: 2.311787\n",
      "[111,  2800] loss: 2.309486\n",
      "[111,  3000] loss: 2.311228\n",
      "[111,  3200] loss: 2.308810\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[112,   200] loss: 2.311849\n",
      "[112,   400] loss: 2.309697\n",
      "[112,   600] loss: 2.310800\n",
      "[112,   800] loss: 2.313266\n",
      "[112,  1000] loss: 2.312078\n",
      "[112,  1200] loss: 2.310731\n",
      "[112,  1400] loss: 2.310179\n",
      "[112,  1600] loss: 2.310875\n",
      "[112,  1800] loss: 2.311813\n",
      "[112,  2000] loss: 2.309657\n",
      "[112,  2200] loss: 2.310266\n",
      "[112,  2400] loss: 2.311646\n",
      "[112,  2600] loss: 2.313231\n",
      "[112,  2800] loss: 2.311914\n",
      "[112,  3000] loss: 2.312467\n",
      "[112,  3200] loss: 2.311785\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[113,   200] loss: 2.310648\n",
      "[113,   400] loss: 2.313782\n",
      "[113,   600] loss: 2.312890\n",
      "[113,   800] loss: 2.314766\n",
      "[113,  1000] loss: 2.310341\n",
      "[113,  1200] loss: 2.311262\n",
      "[113,  1400] loss: 2.312944\n",
      "[113,  1600] loss: 2.311427\n",
      "[113,  1800] loss: 2.310785\n",
      "[113,  2000] loss: 2.312463\n",
      "[113,  2200] loss: 2.311993\n",
      "[113,  2400] loss: 2.310894\n",
      "[113,  2600] loss: 2.311063\n",
      "[113,  2800] loss: 2.310739\n",
      "[113,  3000] loss: 2.311756\n",
      "[113,  3200] loss: 2.310407\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[114,   200] loss: 2.311550\n",
      "[114,   400] loss: 2.311603\n",
      "[114,   600] loss: 2.310951\n",
      "[114,   800] loss: 2.312688\n",
      "[114,  1000] loss: 2.310819\n",
      "[114,  1200] loss: 2.310168\n",
      "[114,  1400] loss: 2.311622\n",
      "[114,  1600] loss: 2.312266\n",
      "[114,  1800] loss: 2.312111\n",
      "[114,  2000] loss: 2.310170\n",
      "[114,  2200] loss: 2.311717\n",
      "[114,  2400] loss: 2.312657\n",
      "[114,  2600] loss: 2.311127\n",
      "[114,  2800] loss: 2.311898\n",
      "[114,  3000] loss: 2.309891\n",
      "[114,  3200] loss: 2.312452\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[115,   200] loss: 2.310803\n",
      "[115,   400] loss: 2.310636\n",
      "[115,   600] loss: 2.313151\n",
      "[115,   800] loss: 2.312657\n",
      "[115,  1000] loss: 2.309749\n",
      "[115,  1200] loss: 2.311132\n",
      "[115,  1400] loss: 2.310646\n",
      "[115,  1600] loss: 2.311580\n",
      "[115,  1800] loss: 2.313232\n",
      "[115,  2000] loss: 2.311411\n",
      "[115,  2200] loss: 2.309124\n",
      "[115,  2400] loss: 2.314004\n",
      "[115,  2600] loss: 2.309959\n",
      "[115,  2800] loss: 2.310873\n",
      "[115,  3000] loss: 2.310053\n",
      "[115,  3200] loss: 2.311954\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[116,   200] loss: 2.311573\n",
      "[116,   400] loss: 2.310334\n",
      "[116,   600] loss: 2.312194\n",
      "[116,   800] loss: 2.310064\n",
      "[116,  1000] loss: 2.308939\n",
      "[116,  1200] loss: 2.309901\n",
      "[116,  1400] loss: 2.309652\n",
      "[116,  1600] loss: 2.311614\n",
      "[116,  1800] loss: 2.310981\n",
      "[116,  2000] loss: 2.310531\n",
      "[116,  2200] loss: 2.311587\n",
      "[116,  2400] loss: 2.313328\n",
      "[116,  2600] loss: 2.310339\n",
      "[116,  2800] loss: 2.311591\n",
      "[116,  3000] loss: 2.311892\n",
      "[116,  3200] loss: 2.311514\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[117,   200] loss: 2.310891\n",
      "[117,   400] loss: 2.311034\n",
      "[117,   600] loss: 2.312507\n",
      "[117,   800] loss: 2.310836\n",
      "[117,  1000] loss: 2.311939\n",
      "[117,  1200] loss: 2.311644\n",
      "[117,  1400] loss: 2.310556\n",
      "[117,  1600] loss: 2.310386\n",
      "[117,  1800] loss: 2.309852\n",
      "[117,  2000] loss: 2.311283\n",
      "[117,  2200] loss: 2.311996\n",
      "[117,  2400] loss: 2.312974\n",
      "[117,  2600] loss: 2.310715\n",
      "[117,  2800] loss: 2.312548\n",
      "[117,  3000] loss: 2.311255\n",
      "[117,  3200] loss: 2.311253\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[118,   200] loss: 2.312236\n",
      "[118,   400] loss: 2.310744\n",
      "[118,   600] loss: 2.311664\n",
      "[118,   800] loss: 2.311067\n",
      "[118,  1000] loss: 2.311870\n",
      "[118,  1200] loss: 2.311029\n",
      "[118,  1400] loss: 2.311813\n",
      "[118,  1600] loss: 2.310248\n",
      "[118,  1800] loss: 2.312477\n",
      "[118,  2000] loss: 2.312064\n",
      "[118,  2200] loss: 2.313299\n",
      "[118,  2400] loss: 2.312691\n",
      "[118,  2600] loss: 2.311127\n",
      "[118,  2800] loss: 2.311372\n",
      "[118,  3000] loss: 2.314301\n",
      "[118,  3200] loss: 2.311738\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[119,   200] loss: 2.310993\n",
      "[119,   400] loss: 2.308408\n",
      "[119,   600] loss: 2.310628\n",
      "[119,   800] loss: 2.310341\n",
      "[119,  1000] loss: 2.312527\n",
      "[119,  1200] loss: 2.313326\n",
      "[119,  1400] loss: 2.310609\n",
      "[119,  1600] loss: 2.311895\n",
      "[119,  1800] loss: 2.311314\n",
      "[119,  2000] loss: 2.310273\n",
      "[119,  2200] loss: 2.310244\n",
      "[119,  2400] loss: 2.312269\n",
      "[119,  2600] loss: 2.310717\n",
      "[119,  2800] loss: 2.309758\n",
      "[119,  3000] loss: 2.309910\n",
      "[119,  3200] loss: 2.312563\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[120,   200] loss: 2.311419\n",
      "[120,   400] loss: 2.309711\n",
      "[120,   600] loss: 2.312041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120,   800] loss: 2.311413\n",
      "[120,  1000] loss: 2.312220\n",
      "[120,  1200] loss: 2.312268\n",
      "[120,  1400] loss: 2.311085\n",
      "[120,  1600] loss: 2.309850\n",
      "[120,  1800] loss: 2.311735\n",
      "[120,  2000] loss: 2.310835\n",
      "[120,  2200] loss: 2.310422\n",
      "[120,  2400] loss: 2.311717\n",
      "[120,  2600] loss: 2.310853\n",
      "[120,  2800] loss: 2.311171\n",
      "[120,  3000] loss: 2.312420\n",
      "[120,  3200] loss: 2.312994\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[121,   200] loss: 2.310614\n",
      "[121,   400] loss: 2.311397\n",
      "[121,   600] loss: 2.311874\n",
      "[121,   800] loss: 2.312253\n",
      "[121,  1000] loss: 2.310890\n",
      "[121,  1200] loss: 2.312440\n",
      "[121,  1400] loss: 2.311346\n",
      "[121,  1600] loss: 2.311909\n",
      "[121,  1800] loss: 2.311342\n",
      "[121,  2000] loss: 2.311919\n",
      "[121,  2200] loss: 2.313043\n",
      "[121,  2400] loss: 2.313490\n",
      "[121,  2600] loss: 2.310276\n",
      "[121,  2800] loss: 2.310202\n",
      "[121,  3000] loss: 2.311127\n",
      "[121,  3200] loss: 2.312370\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[122,   200] loss: 2.312161\n",
      "[122,   400] loss: 2.310650\n",
      "[122,   600] loss: 2.309964\n",
      "[122,   800] loss: 2.313059\n",
      "[122,  1000] loss: 2.312149\n",
      "[122,  1200] loss: 2.313117\n",
      "[122,  1400] loss: 2.309420\n",
      "[122,  1600] loss: 2.311402\n",
      "[122,  1800] loss: 2.309436\n",
      "[122,  2000] loss: 2.311092\n",
      "[122,  2200] loss: 2.314316\n",
      "[122,  2400] loss: 2.312032\n",
      "[122,  2600] loss: 2.309769\n",
      "[122,  2800] loss: 2.310949\n",
      "[122,  3000] loss: 2.312099\n",
      "[122,  3200] loss: 2.310402\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[123,   200] loss: 2.311730\n",
      "[123,   400] loss: 2.311572\n",
      "[123,   600] loss: 2.312291\n",
      "[123,   800] loss: 2.310415\n",
      "[123,  1000] loss: 2.310287\n",
      "[123,  1200] loss: 2.313253\n",
      "[123,  1400] loss: 2.311198\n",
      "[123,  1600] loss: 2.312813\n",
      "[123,  1800] loss: 2.312319\n",
      "[123,  2000] loss: 2.311214\n",
      "[123,  2200] loss: 2.310222\n",
      "[123,  2400] loss: 2.311078\n",
      "[123,  2600] loss: 2.311733\n",
      "[123,  2800] loss: 2.311566\n",
      "[123,  3000] loss: 2.309773\n",
      "[123,  3200] loss: 2.311296\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[124,   200] loss: 2.312096\n",
      "[124,   400] loss: 2.313407\n",
      "[124,   600] loss: 2.311763\n",
      "[124,   800] loss: 2.311404\n",
      "[124,  1000] loss: 2.311710\n",
      "[124,  1200] loss: 2.311480\n",
      "[124,  1400] loss: 2.311675\n",
      "[124,  1600] loss: 2.310112\n",
      "[124,  1800] loss: 2.309300\n",
      "[124,  2000] loss: 2.311911\n",
      "[124,  2200] loss: 2.312628\n",
      "[124,  2400] loss: 2.310871\n",
      "[124,  2600] loss: 2.311700\n",
      "[124,  2800] loss: 2.312254\n",
      "[124,  3000] loss: 2.311273\n",
      "[124,  3200] loss: 2.310260\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[125,   200] loss: 2.311510\n",
      "[125,   400] loss: 2.312360\n",
      "[125,   600] loss: 2.312676\n",
      "[125,   800] loss: 2.311352\n",
      "[125,  1000] loss: 2.311958\n",
      "[125,  1200] loss: 2.312762\n",
      "[125,  1400] loss: 2.313548\n",
      "[125,  1600] loss: 2.309909\n",
      "[125,  1800] loss: 2.310864\n",
      "[125,  2000] loss: 2.310932\n",
      "[125,  2200] loss: 2.312116\n",
      "[125,  2400] loss: 2.310839\n",
      "[125,  2600] loss: 2.313844\n",
      "[125,  2800] loss: 2.310742\n",
      "[125,  3000] loss: 2.310452\n",
      "[125,  3200] loss: 2.310223\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[126,   200] loss: 2.309737\n",
      "[126,   400] loss: 2.309925\n",
      "[126,   600] loss: 2.312932\n",
      "[126,   800] loss: 2.311052\n",
      "[126,  1000] loss: 2.313789\n",
      "[126,  1200] loss: 2.310389\n",
      "[126,  1400] loss: 2.313079\n",
      "[126,  1600] loss: 2.311504\n",
      "[126,  1800] loss: 2.310847\n",
      "[126,  2000] loss: 2.311384\n",
      "[126,  2200] loss: 2.310721\n",
      "[126,  2400] loss: 2.310834\n",
      "[126,  2600] loss: 2.310133\n",
      "[126,  2800] loss: 2.310255\n",
      "[126,  3000] loss: 2.310798\n",
      "[126,  3200] loss: 2.312135\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[127,   200] loss: 2.312079\n",
      "[127,   400] loss: 2.310950\n",
      "[127,   600] loss: 2.311593\n",
      "[127,   800] loss: 2.310169\n",
      "[127,  1000] loss: 2.309542\n",
      "[127,  1200] loss: 2.311457\n",
      "[127,  1400] loss: 2.312862\n",
      "[127,  1600] loss: 2.309834\n",
      "[127,  1800] loss: 2.311541\n",
      "[127,  2000] loss: 2.311221\n",
      "[127,  2200] loss: 2.310887\n",
      "[127,  2400] loss: 2.313536\n",
      "[127,  2600] loss: 2.310569\n",
      "[127,  2800] loss: 2.311235\n",
      "[127,  3000] loss: 2.311967\n",
      "[127,  3200] loss: 2.310271\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[128,   200] loss: 2.312351\n",
      "[128,   400] loss: 2.311092\n",
      "[128,   600] loss: 2.311180\n",
      "[128,   800] loss: 2.310770\n",
      "[128,  1000] loss: 2.312163\n",
      "[128,  1200] loss: 2.313357\n",
      "[128,  1400] loss: 2.310080\n",
      "[128,  1600] loss: 2.310938\n",
      "[128,  1800] loss: 2.312011\n",
      "[128,  2000] loss: 2.310584\n",
      "[128,  2200] loss: 2.311331\n",
      "[128,  2400] loss: 2.311006\n",
      "[128,  2600] loss: 2.311823\n",
      "[128,  2800] loss: 2.311002\n",
      "[128,  3000] loss: 2.310735\n",
      "[128,  3200] loss: 2.312271\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[129,   200] loss: 2.311501\n",
      "[129,   400] loss: 2.311894\n",
      "[129,   600] loss: 2.311502\n",
      "[129,   800] loss: 2.312046\n",
      "[129,  1000] loss: 2.311061\n",
      "[129,  1200] loss: 2.310241\n",
      "[129,  1400] loss: 2.309431\n",
      "[129,  1600] loss: 2.311533\n",
      "[129,  1800] loss: 2.310066\n",
      "[129,  2000] loss: 2.310972\n",
      "[129,  2200] loss: 2.312128\n",
      "[129,  2400] loss: 2.311207\n",
      "[129,  2600] loss: 2.311236\n",
      "[129,  2800] loss: 2.311786\n",
      "[129,  3000] loss: 2.313574\n",
      "[129,  3200] loss: 2.312088\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[130,   200] loss: 2.313194\n",
      "[130,   400] loss: 2.311270\n",
      "[130,   600] loss: 2.311111\n",
      "[130,   800] loss: 2.311735\n",
      "[130,  1000] loss: 2.311449\n",
      "[130,  1200] loss: 2.312516\n",
      "[130,  1400] loss: 2.312036\n",
      "[130,  1600] loss: 2.311625\n",
      "[130,  1800] loss: 2.311626\n",
      "[130,  2000] loss: 2.313874\n",
      "[130,  2200] loss: 2.311500\n",
      "[130,  2400] loss: 2.312121\n",
      "[130,  2600] loss: 2.311112\n",
      "[130,  2800] loss: 2.311734\n",
      "[130,  3000] loss: 2.313756\n",
      "[130,  3200] loss: 2.313042\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[131,   200] loss: 2.309942\n",
      "[131,   400] loss: 2.309899\n",
      "[131,   600] loss: 2.311296\n",
      "[131,   800] loss: 2.311146\n",
      "[131,  1000] loss: 2.312331\n",
      "[131,  1200] loss: 2.311838\n",
      "[131,  1400] loss: 2.311998\n",
      "[131,  1600] loss: 2.312343\n",
      "[131,  1800] loss: 2.311650\n",
      "[131,  2000] loss: 2.312106\n",
      "[131,  2200] loss: 2.312417\n",
      "[131,  2400] loss: 2.311501\n",
      "[131,  2600] loss: 2.311472\n",
      "[131,  2800] loss: 2.313125\n",
      "[131,  3000] loss: 2.311638\n",
      "[131,  3200] loss: 2.310619\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[132,   200] loss: 2.312118\n",
      "[132,   400] loss: 2.310230\n",
      "[132,   600] loss: 2.310080\n",
      "[132,   800] loss: 2.312585\n",
      "[132,  1000] loss: 2.311901\n",
      "[132,  1200] loss: 2.311980\n",
      "[132,  1400] loss: 2.309423\n",
      "[132,  1600] loss: 2.313291\n",
      "[132,  1800] loss: 2.311500\n",
      "[132,  2000] loss: 2.311051\n",
      "[132,  2200] loss: 2.311867\n",
      "[132,  2400] loss: 2.311721\n",
      "[132,  2600] loss: 2.312039\n",
      "[132,  2800] loss: 2.310664\n",
      "[132,  3000] loss: 2.311176\n",
      "[132,  3200] loss: 2.311945\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[133,   200] loss: 2.311084\n",
      "[133,   400] loss: 2.310566\n",
      "[133,   600] loss: 2.310419\n",
      "[133,   800] loss: 2.310795\n",
      "[133,  1000] loss: 2.309171\n",
      "[133,  1200] loss: 2.312842\n",
      "[133,  1400] loss: 2.312711\n",
      "[133,  1600] loss: 2.313070\n",
      "[133,  1800] loss: 2.312362\n",
      "[133,  2000] loss: 2.311713\n",
      "[133,  2200] loss: 2.313112\n",
      "[133,  2400] loss: 2.311224\n",
      "[133,  2600] loss: 2.311922\n",
      "[133,  2800] loss: 2.311386\n",
      "[133,  3000] loss: 2.312856\n",
      "[133,  3200] loss: 2.312727\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[134,   200] loss: 2.310633\n",
      "[134,   400] loss: 2.312325\n",
      "[134,   600] loss: 2.311440\n",
      "[134,   800] loss: 2.311506\n",
      "[134,  1000] loss: 2.310279\n",
      "[134,  1200] loss: 2.312060\n",
      "[134,  1400] loss: 2.310207\n",
      "[134,  1600] loss: 2.311289\n",
      "[134,  1800] loss: 2.310716\n",
      "[134,  2000] loss: 2.313004\n",
      "[134,  2200] loss: 2.313030\n",
      "[134,  2400] loss: 2.311878\n",
      "[134,  2600] loss: 2.309760\n",
      "[134,  2800] loss: 2.308951\n",
      "[134,  3000] loss: 2.311251\n",
      "[134,  3200] loss: 2.311394\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[135,   200] loss: 2.311048\n",
      "[135,   400] loss: 2.309756\n",
      "[135,   600] loss: 2.367731\n",
      "[135,   800] loss: 2.342722\n",
      "[135,  1000] loss: 2.313085\n",
      "[135,  1200] loss: 2.309597\n",
      "[135,  1400] loss: 2.312610\n",
      "[135,  1600] loss: 2.321606\n",
      "[135,  1800] loss: 2.335833\n",
      "[135,  2000] loss: 2.310580\n",
      "[135,  2200] loss: 2.312337\n",
      "[135,  2400] loss: 2.311580\n",
      "[135,  2600] loss: 2.312209\n",
      "[135,  2800] loss: 2.310069\n",
      "[135,  3000] loss: 2.312215\n",
      "[135,  3200] loss: 2.311907\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[136,   200] loss: 2.311336\n",
      "[136,   400] loss: 2.311717\n",
      "[136,   600] loss: 2.309701\n",
      "[136,   800] loss: 2.312032\n",
      "[136,  1000] loss: 2.311159\n",
      "[136,  1200] loss: 2.310761\n",
      "[136,  1400] loss: 2.312173\n",
      "[136,  1600] loss: 2.311608\n",
      "[136,  1800] loss: 2.311294\n",
      "[136,  2000] loss: 2.311427\n",
      "[136,  2200] loss: 2.312598\n",
      "[136,  2400] loss: 2.310922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136,  2600] loss: 2.310713\n",
      "[136,  2800] loss: 2.309886\n",
      "[136,  3000] loss: 2.312325\n",
      "[136,  3200] loss: 2.311509\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[137,   200] loss: 2.312563\n",
      "[137,   400] loss: 2.311981\n",
      "[137,   600] loss: 2.309944\n",
      "[137,   800] loss: 2.311968\n",
      "[137,  1000] loss: 2.313840\n",
      "[137,  1200] loss: 2.311864\n",
      "[137,  1400] loss: 2.312410\n",
      "[137,  1600] loss: 2.311083\n",
      "[137,  1800] loss: 2.312021\n",
      "[137,  2000] loss: 2.312265\n",
      "[137,  2200] loss: 2.312229\n",
      "[137,  2400] loss: 2.312414\n",
      "[137,  2600] loss: 2.312145\n",
      "[137,  2800] loss: 2.312448\n",
      "[137,  3000] loss: 2.312383\n",
      "[137,  3200] loss: 2.312212\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[138,   200] loss: 2.310025\n",
      "[138,   400] loss: 2.311665\n",
      "[138,   600] loss: 2.310938\n",
      "[138,   800] loss: 2.310249\n",
      "[138,  1000] loss: 2.312817\n",
      "[138,  1200] loss: 2.313822\n",
      "[138,  1400] loss: 2.311940\n",
      "[138,  1600] loss: 2.311871\n",
      "[138,  1800] loss: 2.310886\n",
      "[138,  2000] loss: 2.311729\n",
      "[138,  2200] loss: 2.311175\n",
      "[138,  2400] loss: 2.312119\n",
      "[138,  2600] loss: 2.310737\n",
      "[138,  2800] loss: 2.312654\n",
      "[138,  3000] loss: 2.311665\n",
      "[138,  3200] loss: 2.311954\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[139,   200] loss: 2.312253\n",
      "[139,   400] loss: 2.309441\n",
      "[139,   600] loss: 2.311264\n",
      "[139,   800] loss: 2.310732\n",
      "[139,  1000] loss: 2.312318\n",
      "[139,  1200] loss: 2.310720\n",
      "[139,  1400] loss: 2.310046\n",
      "[139,  1600] loss: 2.309926\n",
      "[139,  1800] loss: 2.311128\n",
      "[139,  2000] loss: 2.310968\n",
      "[139,  2200] loss: 2.312913\n",
      "[139,  2400] loss: 2.311062\n",
      "[139,  2600] loss: 2.312736\n",
      "[139,  2800] loss: 2.311794\n",
      "[139,  3000] loss: 2.311914\n",
      "[139,  3200] loss: 2.310558\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[140,   200] loss: 2.311930\n",
      "[140,   400] loss: 2.311184\n",
      "[140,   600] loss: 2.312614\n",
      "[140,   800] loss: 2.310186\n",
      "[140,  1000] loss: 2.311805\n",
      "[140,  1200] loss: 2.312570\n",
      "[140,  1400] loss: 2.309538\n",
      "[140,  1600] loss: 2.312972\n",
      "[140,  1800] loss: 2.311590\n",
      "[140,  2000] loss: 2.311822\n",
      "[140,  2200] loss: 2.309357\n",
      "[140,  2400] loss: 2.310866\n",
      "[140,  2600] loss: 2.312364\n",
      "[140,  2800] loss: 2.312060\n",
      "[140,  3000] loss: 2.311080\n",
      "[140,  3200] loss: 2.311490\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[141,   200] loss: 2.309660\n",
      "[141,   400] loss: 2.313209\n",
      "[141,   600] loss: 2.311164\n",
      "[141,   800] loss: 2.310823\n",
      "[141,  1000] loss: 2.313023\n",
      "[141,  1200] loss: 2.312533\n",
      "[141,  1400] loss: 2.313821\n",
      "[141,  1600] loss: 2.310706\n",
      "[141,  1800] loss: 2.311507\n",
      "[141,  2000] loss: 2.311023\n",
      "[141,  2200] loss: 2.312128\n",
      "[141,  2400] loss: 2.310972\n",
      "[141,  2600] loss: 2.310900\n",
      "[141,  2800] loss: 2.310835\n",
      "[141,  3000] loss: 2.313363\n",
      "[141,  3200] loss: 2.312905\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[142,   200] loss: 2.311462\n",
      "[142,   400] loss: 2.311176\n",
      "[142,   600] loss: 2.311847\n",
      "[142,   800] loss: 2.311777\n",
      "[142,  1000] loss: 2.311741\n",
      "[142,  1200] loss: 2.312956\n",
      "[142,  1400] loss: 2.310959\n",
      "[142,  1600] loss: 2.311097\n",
      "[142,  1800] loss: 2.310156\n",
      "[142,  2000] loss: 2.311362\n",
      "[142,  2200] loss: 2.312132\n",
      "[142,  2400] loss: 2.310027\n",
      "[142,  2600] loss: 2.311520\n",
      "[142,  2800] loss: 2.312761\n",
      "[142,  3000] loss: 2.311046\n",
      "[142,  3200] loss: 2.311917\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[143,   200] loss: 2.310712\n",
      "[143,   400] loss: 2.310534\n",
      "[143,   600] loss: 2.311569\n",
      "[143,   800] loss: 2.310255\n",
      "[143,  1000] loss: 2.311304\n",
      "[143,  1200] loss: 2.311245\n",
      "[143,  1400] loss: 2.311662\n",
      "[143,  1600] loss: 2.311368\n",
      "[143,  1800] loss: 2.310384\n",
      "[143,  2000] loss: 2.311820\n",
      "[143,  2200] loss: 2.309412\n",
      "[143,  2400] loss: 2.309678\n",
      "[143,  2600] loss: 2.311276\n",
      "[143,  2800] loss: 2.310646\n",
      "[143,  3000] loss: 2.312977\n",
      "[143,  3200] loss: 2.310850\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[144,   200] loss: 2.311096\n",
      "[144,   400] loss: 2.311547\n",
      "[144,   600] loss: 2.312098\n",
      "[144,   800] loss: 2.311785\n",
      "[144,  1000] loss: 2.312131\n",
      "[144,  1200] loss: 2.311258\n",
      "[144,  1400] loss: 2.311613\n",
      "[144,  1600] loss: 2.310756\n",
      "[144,  1800] loss: 2.312001\n",
      "[144,  2000] loss: 2.311198\n",
      "[144,  2200] loss: 2.312037\n",
      "[144,  2400] loss: 2.310841\n",
      "[144,  2600] loss: 2.311027\n",
      "[144,  2800] loss: 2.312002\n",
      "[144,  3000] loss: 2.311332\n",
      "[144,  3200] loss: 2.311588\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[145,   200] loss: 2.311618\n",
      "[145,   400] loss: 2.310759\n",
      "[145,   600] loss: 2.310527\n",
      "[145,   800] loss: 2.313077\n",
      "[145,  1000] loss: 2.310868\n",
      "[145,  1200] loss: 2.311475\n",
      "[145,  1400] loss: 2.311157\n",
      "[145,  1600] loss: 2.312620\n",
      "[145,  1800] loss: 2.312088\n",
      "[145,  2000] loss: 2.312341\n",
      "[145,  2200] loss: 2.310980\n",
      "[145,  2400] loss: 2.312463\n",
      "[145,  2600] loss: 2.311139\n",
      "[145,  2800] loss: 2.312451\n",
      "[145,  3000] loss: 2.312495\n",
      "[145,  3200] loss: 2.309706\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[146,   200] loss: 2.310427\n",
      "[146,   400] loss: 2.311113\n",
      "[146,   600] loss: 2.310458\n",
      "[146,   800] loss: 2.311343\n",
      "[146,  1000] loss: 2.310857\n",
      "[146,  1200] loss: 2.313335\n",
      "[146,  1400] loss: 2.310488\n",
      "[146,  1600] loss: 2.312006\n",
      "[146,  1800] loss: 2.309836\n",
      "[146,  2000] loss: 2.311943\n",
      "[146,  2200] loss: 2.311011\n",
      "[146,  2400] loss: 2.311202\n",
      "[146,  2600] loss: 2.310972\n",
      "[146,  2800] loss: 2.310784\n",
      "[146,  3000] loss: 2.310617\n",
      "[146,  3200] loss: 2.312417\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[147,   200] loss: 2.311681\n",
      "[147,   400] loss: 2.310337\n",
      "[147,   600] loss: 2.311902\n",
      "[147,   800] loss: 2.313428\n",
      "[147,  1000] loss: 2.310394\n",
      "[147,  1200] loss: 2.310857\n",
      "[147,  1400] loss: 2.311118\n",
      "[147,  1600] loss: 2.310026\n",
      "[147,  1800] loss: 2.310834\n",
      "[147,  2000] loss: 2.312349\n",
      "[147,  2200] loss: 2.311429\n",
      "[147,  2400] loss: 2.311447\n",
      "[147,  2600] loss: 2.310774\n",
      "[147,  2800] loss: 2.312612\n",
      "[147,  3000] loss: 2.310638\n",
      "[147,  3200] loss: 2.310690\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[148,   200] loss: 2.313141\n",
      "[148,   400] loss: 2.313042\n",
      "[148,   600] loss: 2.312319\n",
      "[148,   800] loss: 2.310629\n",
      "[148,  1000] loss: 2.311641\n",
      "[148,  1200] loss: 2.309432\n",
      "[148,  1400] loss: 2.310983\n",
      "[148,  1600] loss: 2.310120\n",
      "[148,  1800] loss: 2.311638\n",
      "[148,  2000] loss: 2.312627\n",
      "[148,  2200] loss: 2.309961\n",
      "[148,  2400] loss: 2.311559\n",
      "[148,  2600] loss: 2.311969\n",
      "[148,  2800] loss: 2.309566\n",
      "[148,  3000] loss: 2.312056\n",
      "[148,  3200] loss: 2.312149\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[149,   200] loss: 2.310807\n",
      "[149,   400] loss: 2.313062\n",
      "[149,   600] loss: 2.310587\n",
      "[149,   800] loss: 2.310000\n",
      "[149,  1000] loss: 2.312272\n",
      "[149,  1200] loss: 2.312791\n",
      "[149,  1400] loss: 2.311828\n",
      "[149,  1600] loss: 2.309932\n",
      "[149,  1800] loss: 2.312290\n",
      "[149,  2000] loss: 2.312069\n",
      "[149,  2200] loss: 2.311054\n",
      "[149,  2400] loss: 2.310151\n",
      "[149,  2600] loss: 2.312220\n",
      "[149,  2800] loss: 2.312439\n",
      "[149,  3000] loss: 2.311876\n",
      "[149,  3200] loss: 2.311360\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[150,   200] loss: 2.311371\n",
      "[150,   400] loss: 2.311776\n",
      "[150,   600] loss: 2.310672\n",
      "[150,   800] loss: 2.309895\n",
      "[150,  1000] loss: 2.310617\n",
      "[150,  1200] loss: 2.313536\n",
      "[150,  1400] loss: 2.312883\n",
      "[150,  1600] loss: 2.312928\n",
      "[150,  1800] loss: 2.312589\n",
      "[150,  2000] loss: 2.310225\n",
      "[150,  2200] loss: 2.312821\n",
      "[150,  2400] loss: 2.313663\n",
      "[150,  2600] loss: 2.310438\n",
      "[150,  2800] loss: 2.310185\n",
      "[150,  3000] loss: 2.313198\n",
      "[150,  3200] loss: 2.312047\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[151,   200] loss: 2.305327\n",
      "[151,   400] loss: 2.303442\n",
      "[151,   600] loss: 2.303686\n",
      "[151,   800] loss: 2.303386\n",
      "[151,  1000] loss: 2.303195\n",
      "[151,  1200] loss: 2.303435\n",
      "[151,  1400] loss: 2.303648\n",
      "[151,  1600] loss: 2.303153\n",
      "[151,  1800] loss: 2.303387\n",
      "[151,  2000] loss: 2.302663\n",
      "[151,  2200] loss: 2.303603\n",
      "[151,  2400] loss: 2.303401\n",
      "[151,  2600] loss: 2.303337\n",
      "[151,  2800] loss: 2.303625\n",
      "[151,  3000] loss: 2.303378\n",
      "[151,  3200] loss: 2.303485\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[152,   200] loss: 2.303132\n",
      "[152,   400] loss: 2.303686\n",
      "[152,   600] loss: 2.303534\n",
      "[152,   800] loss: 2.302886\n",
      "[152,  1000] loss: 2.304251\n",
      "[152,  1200] loss: 2.303546\n",
      "[152,  1400] loss: 2.303638\n",
      "[152,  1600] loss: 2.302637\n",
      "[152,  1800] loss: 2.303546\n",
      "[152,  2000] loss: 2.303241\n",
      "[152,  2200] loss: 2.303453\n",
      "[152,  2400] loss: 2.303579\n",
      "[152,  2600] loss: 2.302978\n",
      "[152,  2800] loss: 2.303618\n",
      "[152,  3000] loss: 2.303329\n",
      "[152,  3200] loss: 2.303898\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[153,   200] loss: 2.303440\n",
      "[153,   400] loss: 2.303585\n",
      "[153,   600] loss: 2.303711\n",
      "[153,   800] loss: 2.303710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153,  1000] loss: 2.303322\n",
      "[153,  1200] loss: 2.303784\n",
      "[153,  1400] loss: 2.303817\n",
      "[153,  1600] loss: 2.303018\n",
      "[153,  1800] loss: 2.303166\n",
      "[153,  2000] loss: 2.303522\n",
      "[153,  2200] loss: 2.303301\n",
      "[153,  2400] loss: 2.303686\n",
      "[153,  2600] loss: 2.303092\n",
      "[153,  2800] loss: 2.303748\n",
      "[153,  3000] loss: 2.303704\n",
      "[153,  3200] loss: 2.303697\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[154,   200] loss: 2.303425\n",
      "[154,   400] loss: 2.303180\n",
      "[154,   600] loss: 2.303339\n",
      "[154,   800] loss: 2.303532\n",
      "[154,  1000] loss: 2.303453\n",
      "[154,  1200] loss: 2.303683\n",
      "[154,  1400] loss: 2.303106\n",
      "[154,  1600] loss: 2.303566\n",
      "[154,  1800] loss: 2.303715\n",
      "[154,  2000] loss: 2.303476\n",
      "[154,  2200] loss: 2.303298\n",
      "[154,  2400] loss: 2.304059\n",
      "[154,  2600] loss: 2.303838\n",
      "[154,  2800] loss: 2.303256\n",
      "[154,  3000] loss: 2.303337\n",
      "[154,  3200] loss: 2.303166\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[155,   200] loss: 2.303358\n",
      "[155,   400] loss: 2.303709\n",
      "[155,   600] loss: 2.303657\n",
      "[155,   800] loss: 2.303243\n",
      "[155,  1000] loss: 2.302948\n",
      "[155,  1200] loss: 2.303753\n",
      "[155,  1400] loss: 2.303560\n",
      "[155,  1600] loss: 2.303233\n",
      "[155,  1800] loss: 2.303571\n",
      "[155,  2000] loss: 2.303551\n",
      "[155,  2200] loss: 2.303252\n",
      "[155,  2400] loss: 2.303433\n",
      "[155,  2600] loss: 2.303234\n",
      "[155,  2800] loss: 2.303533\n",
      "[155,  3000] loss: 2.303745\n",
      "[155,  3200] loss: 2.303343\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[156,   200] loss: 2.303326\n",
      "[156,   400] loss: 2.303832\n",
      "[156,   600] loss: 2.302773\n",
      "[156,   800] loss: 2.303995\n",
      "[156,  1000] loss: 2.303666\n",
      "[156,  1200] loss: 2.303598\n",
      "[156,  1400] loss: 2.303430\n",
      "[156,  1600] loss: 2.303580\n",
      "[156,  1800] loss: 2.303265\n",
      "[156,  2000] loss: 2.303562\n",
      "[156,  2200] loss: 2.303800\n",
      "[156,  2400] loss: 2.303603\n",
      "[156,  2600] loss: 2.303607\n",
      "[156,  2800] loss: 2.303614\n",
      "[156,  3000] loss: 2.303386\n",
      "[156,  3200] loss: 2.302901\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[157,   200] loss: 2.303468\n",
      "[157,   400] loss: 2.303353\n",
      "[157,   600] loss: 2.303692\n",
      "[157,   800] loss: 2.303431\n",
      "[157,  1000] loss: 2.303286\n",
      "[157,  1200] loss: 2.303115\n",
      "[157,  1400] loss: 2.303768\n",
      "[157,  1600] loss: 2.303797\n",
      "[157,  1800] loss: 2.303422\n",
      "[157,  2000] loss: 2.303582\n",
      "[157,  2200] loss: 2.302973\n",
      "[157,  2400] loss: 2.303640\n",
      "[157,  2600] loss: 2.303544\n",
      "[157,  2800] loss: 2.303138\n",
      "[157,  3000] loss: 2.303466\n",
      "[157,  3200] loss: 2.303678\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[158,   200] loss: 2.303525\n",
      "[158,   400] loss: 2.303079\n",
      "[158,   600] loss: 2.303101\n",
      "[158,   800] loss: 2.303639\n",
      "[158,  1000] loss: 2.303631\n",
      "[158,  1200] loss: 2.303383\n",
      "[158,  1400] loss: 2.303204\n",
      "[158,  1600] loss: 2.303537\n",
      "[158,  1800] loss: 2.303335\n",
      "[158,  2000] loss: 2.303601\n",
      "[158,  2200] loss: 2.303327\n",
      "[158,  2400] loss: 2.303827\n",
      "[158,  2600] loss: 2.303202\n",
      "[158,  2800] loss: 2.303706\n",
      "[158,  3000] loss: 2.303693\n",
      "[158,  3200] loss: 2.303425\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[159,   200] loss: 2.303328\n",
      "[159,   400] loss: 2.303108\n",
      "[159,   600] loss: 2.303816\n",
      "[159,   800] loss: 2.303115\n",
      "[159,  1000] loss: 2.303240\n",
      "[159,  1200] loss: 2.303372\n",
      "[159,  1400] loss: 2.303681\n",
      "[159,  1600] loss: 2.303537\n",
      "[159,  1800] loss: 2.303149\n",
      "[159,  2000] loss: 2.303681\n",
      "[159,  2200] loss: 2.303888\n",
      "[159,  2400] loss: 2.303938\n",
      "[159,  2600] loss: 2.303516\n",
      "[159,  2800] loss: 2.303893\n",
      "[159,  3000] loss: 2.302912\n",
      "[159,  3200] loss: 2.304128\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[160,   200] loss: 2.303501\n",
      "[160,   400] loss: 2.303390\n",
      "[160,   600] loss: 2.303574\n",
      "[160,   800] loss: 2.303389\n",
      "[160,  1000] loss: 2.303215\n",
      "[160,  1200] loss: 2.303286\n",
      "[160,  1400] loss: 2.303162\n",
      "[160,  1600] loss: 2.303724\n",
      "[160,  1800] loss: 2.303741\n",
      "[160,  2000] loss: 2.303429\n",
      "[160,  2200] loss: 2.303530\n",
      "[160,  2400] loss: 2.303386\n",
      "[160,  2600] loss: 2.303149\n",
      "[160,  2800] loss: 2.303509\n",
      "[160,  3000] loss: 2.303460\n",
      "[160,  3200] loss: 2.303900\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[161,   200] loss: 2.303713\n",
      "[161,   400] loss: 2.303543\n",
      "[161,   600] loss: 2.303258\n",
      "[161,   800] loss: 2.303504\n",
      "[161,  1000] loss: 2.303525\n",
      "[161,  1200] loss: 2.303282\n",
      "[161,  1400] loss: 2.303195\n",
      "[161,  1600] loss: 2.303646\n",
      "[161,  1800] loss: 2.303637\n",
      "[161,  2000] loss: 2.303493\n",
      "[161,  2200] loss: 2.303253\n",
      "[161,  2400] loss: 2.303662\n",
      "[161,  2600] loss: 2.304050\n",
      "[161,  2800] loss: 2.303834\n",
      "[161,  3000] loss: 2.303574\n",
      "[161,  3200] loss: 2.303759\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[162,   200] loss: 2.303578\n",
      "[162,   400] loss: 2.303455\n",
      "[162,   600] loss: 2.303763\n",
      "[162,   800] loss: 2.303530\n",
      "[162,  1000] loss: 2.302810\n",
      "[162,  1200] loss: 2.303560\n",
      "[162,  1400] loss: 2.303443\n",
      "[162,  1600] loss: 2.303501\n",
      "[162,  1800] loss: 2.303561\n",
      "[162,  2000] loss: 2.303865\n",
      "[162,  2200] loss: 2.303224\n",
      "[162,  2400] loss: 2.303343\n",
      "[162,  2600] loss: 2.303597\n",
      "[162,  2800] loss: 2.303425\n",
      "[162,  3000] loss: 2.303455\n",
      "[162,  3200] loss: 2.303570\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[163,   200] loss: 2.303146\n",
      "[163,   400] loss: 2.303640\n",
      "[163,   600] loss: 2.303503\n",
      "[163,   800] loss: 2.303646\n",
      "[163,  1000] loss: 2.302951\n",
      "[163,  1200] loss: 2.303071\n",
      "[163,  1400] loss: 2.303601\n",
      "[163,  1600] loss: 2.303481\n",
      "[163,  1800] loss: 2.303170\n",
      "[163,  2000] loss: 2.303660\n",
      "[163,  2200] loss: 2.303018\n",
      "[163,  2400] loss: 2.303979\n",
      "[163,  2600] loss: 2.303415\n",
      "[163,  2800] loss: 2.303513\n",
      "[163,  3000] loss: 2.303284\n",
      "[163,  3200] loss: 2.303379\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[164,   200] loss: 2.303300\n",
      "[164,   400] loss: 2.303639\n",
      "[164,   600] loss: 2.303769\n",
      "[164,   800] loss: 2.303495\n",
      "[164,  1000] loss: 2.303323\n",
      "[164,  1200] loss: 2.303136\n",
      "[164,  1400] loss: 2.303703\n",
      "[164,  1600] loss: 2.303734\n",
      "[164,  1800] loss: 2.303634\n",
      "[164,  2000] loss: 2.303383\n",
      "[164,  2200] loss: 2.303764\n",
      "[164,  2400] loss: 2.303035\n",
      "[164,  2600] loss: 2.303410\n",
      "[164,  2800] loss: 2.303694\n",
      "[164,  3000] loss: 2.303134\n",
      "[164,  3200] loss: 2.303276\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[165,   200] loss: 2.303612\n",
      "[165,   400] loss: 2.303367\n",
      "[165,   600] loss: 2.303497\n",
      "[165,   800] loss: 2.303186\n",
      "[165,  1000] loss: 2.303424\n",
      "[165,  1200] loss: 2.303389\n",
      "[165,  1400] loss: 2.303338\n",
      "[165,  1600] loss: 2.303384\n",
      "[165,  1800] loss: 2.303766\n",
      "[165,  2000] loss: 2.303492\n",
      "[165,  2200] loss: 2.303592\n",
      "[165,  2400] loss: 2.303306\n",
      "[165,  2600] loss: 2.303285\n",
      "[165,  2800] loss: 2.303579\n",
      "[165,  3000] loss: 2.303552\n",
      "[165,  3200] loss: 2.303821\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[166,   200] loss: 2.303097\n",
      "[166,   400] loss: 2.303497\n",
      "[166,   600] loss: 2.303623\n",
      "[166,   800] loss: 2.303518\n",
      "[166,  1000] loss: 2.303888\n",
      "[166,  1200] loss: 2.303496\n",
      "[166,  1400] loss: 2.303346\n",
      "[166,  1600] loss: 2.303856\n",
      "[166,  1800] loss: 2.303086\n",
      "[166,  2000] loss: 2.303663\n",
      "[166,  2200] loss: 2.303462\n",
      "[166,  2400] loss: 2.303256\n",
      "[166,  2600] loss: 2.303272\n",
      "[166,  2800] loss: 2.303543\n",
      "[166,  3000] loss: 2.303783\n",
      "[166,  3200] loss: 2.303544\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[167,   200] loss: 2.303228\n",
      "[167,   400] loss: 2.303507\n",
      "[167,   600] loss: 2.303650\n",
      "[167,   800] loss: 2.303698\n",
      "[167,  1000] loss: 2.303310\n",
      "[167,  1200] loss: 2.303593\n",
      "[167,  1400] loss: 2.303502\n",
      "[167,  1600] loss: 2.303561\n",
      "[167,  1800] loss: 2.303493\n",
      "[167,  2000] loss: 2.303949\n",
      "[167,  2200] loss: 2.303431\n",
      "[167,  2400] loss: 2.302871\n",
      "[167,  2600] loss: 2.303470\n",
      "[167,  2800] loss: 2.303335\n",
      "[167,  3000] loss: 2.303432\n",
      "[167,  3200] loss: 2.303535\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[168,   200] loss: 2.303732\n",
      "[168,   400] loss: 2.303449\n",
      "[168,   600] loss: 2.303431\n",
      "[168,   800] loss: 2.303472\n",
      "[168,  1000] loss: 2.303386\n",
      "[168,  1200] loss: 2.303389\n",
      "[168,  1400] loss: 2.303819\n",
      "[168,  1600] loss: 2.303352\n",
      "[168,  1800] loss: 2.303562\n",
      "[168,  2000] loss: 2.303478\n",
      "[168,  2200] loss: 2.303165\n",
      "[168,  2400] loss: 2.303116\n",
      "[168,  2600] loss: 2.303527\n",
      "[168,  2800] loss: 2.303513\n",
      "[168,  3000] loss: 2.303125\n",
      "[168,  3200] loss: 2.303395\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[169,   200] loss: 2.303616\n",
      "[169,   400] loss: 2.303399\n",
      "[169,   600] loss: 2.303463\n",
      "[169,   800] loss: 2.303513\n",
      "[169,  1000] loss: 2.303289\n",
      "[169,  1200] loss: 2.303297\n",
      "[169,  1400] loss: 2.303127\n",
      "[169,  1600] loss: 2.303308\n",
      "[169,  1800] loss: 2.303914\n",
      "[169,  2000] loss: 2.303118\n",
      "[169,  2200] loss: 2.303953\n",
      "[169,  2400] loss: 2.303398\n",
      "[169,  2600] loss: 2.302993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169,  2800] loss: 2.303654\n",
      "[169,  3000] loss: 2.303504\n",
      "[169,  3200] loss: 2.303539\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[170,   200] loss: 2.303562\n",
      "[170,   400] loss: 2.303197\n",
      "[170,   600] loss: 2.303711\n",
      "[170,   800] loss: 2.303921\n",
      "[170,  1000] loss: 2.303213\n",
      "[170,  1200] loss: 2.303612\n",
      "[170,  1400] loss: 2.303488\n",
      "[170,  1600] loss: 2.303223\n",
      "[170,  1800] loss: 2.303662\n",
      "[170,  2000] loss: 2.303164\n",
      "[170,  2200] loss: 2.303136\n",
      "[170,  2400] loss: 2.303919\n",
      "[170,  2600] loss: 2.303270\n",
      "[170,  2800] loss: 2.303490\n",
      "[170,  3000] loss: 2.303539\n",
      "[170,  3200] loss: 2.303533\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[171,   200] loss: 2.303289\n",
      "[171,   400] loss: 2.303575\n",
      "[171,   600] loss: 2.303328\n",
      "[171,   800] loss: 2.303440\n",
      "[171,  1000] loss: 2.303720\n",
      "[171,  1200] loss: 2.303631\n",
      "[171,  1400] loss: 2.303653\n",
      "[171,  1600] loss: 2.303573\n",
      "[171,  1800] loss: 2.304046\n",
      "[171,  2000] loss: 2.303636\n",
      "[171,  2200] loss: 2.303578\n",
      "[171,  2400] loss: 2.303660\n",
      "[171,  2600] loss: 2.303521\n",
      "[171,  2800] loss: 2.303528\n",
      "[171,  3000] loss: 2.303300\n",
      "[171,  3200] loss: 2.303424\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[172,   200] loss: 2.303847\n",
      "[172,   400] loss: 2.303408\n",
      "[172,   600] loss: 2.303540\n",
      "[172,   800] loss: 2.303777\n",
      "[172,  1000] loss: 2.303550\n",
      "[172,  1200] loss: 2.303823\n",
      "[172,  1400] loss: 2.303193\n",
      "[172,  1600] loss: 2.303549\n",
      "[172,  1800] loss: 2.303646\n",
      "[172,  2000] loss: 2.303374\n",
      "[172,  2200] loss: 2.303896\n",
      "[172,  2400] loss: 2.303341\n",
      "[172,  2600] loss: 2.303434\n",
      "[172,  2800] loss: 2.303515\n",
      "[172,  3000] loss: 2.303238\n",
      "[172,  3200] loss: 2.303907\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[173,   200] loss: 2.303349\n",
      "[173,   400] loss: 2.303275\n",
      "[173,   600] loss: 2.303640\n",
      "[173,   800] loss: 2.303523\n",
      "[173,  1000] loss: 2.303232\n",
      "[173,  1200] loss: 2.303534\n",
      "[173,  1400] loss: 2.303472\n",
      "[173,  1600] loss: 2.303322\n",
      "[173,  1800] loss: 2.303563\n",
      "[173,  2000] loss: 2.303362\n",
      "[173,  2200] loss: 2.303396\n",
      "[173,  2400] loss: 2.303625\n",
      "[173,  2600] loss: 2.303574\n",
      "[173,  2800] loss: 2.303821\n",
      "[173,  3000] loss: 2.303651\n",
      "[173,  3200] loss: 2.303798\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[174,   200] loss: 2.303314\n",
      "[174,   400] loss: 2.303228\n",
      "[174,   600] loss: 2.303230\n",
      "[174,   800] loss: 2.303167\n",
      "[174,  1000] loss: 2.303576\n",
      "[174,  1200] loss: 2.303468\n",
      "[174,  1400] loss: 2.303649\n",
      "[174,  1600] loss: 2.303693\n",
      "[174,  1800] loss: 2.303792\n",
      "[174,  2000] loss: 2.303432\n",
      "[174,  2200] loss: 2.303545\n",
      "[174,  2400] loss: 2.303453\n",
      "[174,  2600] loss: 2.303175\n",
      "[174,  2800] loss: 2.303385\n",
      "[174,  3000] loss: 2.303490\n",
      "[174,  3200] loss: 2.303782\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[175,   200] loss: 2.303086\n",
      "[175,   400] loss: 2.303840\n",
      "[175,   600] loss: 2.303157\n",
      "[175,   800] loss: 2.303442\n",
      "[175,  1000] loss: 2.303699\n",
      "[175,  1200] loss: 2.303694\n",
      "[175,  1400] loss: 2.303663\n",
      "[175,  1600] loss: 2.303626\n",
      "[175,  1800] loss: 2.303135\n",
      "[175,  2000] loss: 2.303612\n",
      "[175,  2200] loss: 2.303267\n",
      "[175,  2400] loss: 2.303494\n",
      "[175,  2600] loss: 2.303568\n",
      "[175,  2800] loss: 2.303383\n",
      "[175,  3000] loss: 2.303707\n",
      "[175,  3200] loss: 2.303517\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[176,   200] loss: 2.303534\n",
      "[176,   400] loss: 2.303461\n",
      "[176,   600] loss: 2.303660\n",
      "[176,   800] loss: 2.303735\n",
      "[176,  1000] loss: 2.303108\n",
      "[176,  1200] loss: 2.303690\n",
      "[176,  1400] loss: 2.303973\n",
      "[176,  1600] loss: 2.303476\n",
      "[176,  1800] loss: 2.303140\n",
      "[176,  2000] loss: 2.303387\n",
      "[176,  2200] loss: 2.303635\n",
      "[176,  2400] loss: 2.303078\n",
      "[176,  2600] loss: 2.303623\n",
      "[176,  2800] loss: 2.303431\n",
      "[176,  3000] loss: 2.303722\n",
      "[176,  3200] loss: 2.303359\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[177,   200] loss: 2.303684\n",
      "[177,   400] loss: 2.303625\n",
      "[177,   600] loss: 2.303461\n",
      "[177,   800] loss: 2.303556\n",
      "[177,  1000] loss: 2.303885\n",
      "[177,  1200] loss: 2.303210\n",
      "[177,  1400] loss: 2.303943\n",
      "[177,  1600] loss: 2.303286\n",
      "[177,  1800] loss: 2.303601\n",
      "[177,  2000] loss: 2.303247\n",
      "[177,  2200] loss: 2.303190\n",
      "[177,  2400] loss: 2.303347\n",
      "[177,  2600] loss: 2.303650\n",
      "[177,  2800] loss: 2.303175\n",
      "[177,  3000] loss: 2.303584\n",
      "[177,  3200] loss: 2.303274\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[178,   200] loss: 2.303981\n",
      "[178,   400] loss: 2.303263\n",
      "[178,   600] loss: 2.303342\n",
      "[178,   800] loss: 2.303500\n",
      "[178,  1000] loss: 2.302969\n",
      "[178,  1200] loss: 2.303425\n",
      "[178,  1400] loss: 2.303814\n",
      "[178,  1600] loss: 2.303431\n",
      "[178,  1800] loss: 2.303508\n",
      "[178,  2000] loss: 2.303233\n",
      "[178,  2200] loss: 2.303716\n",
      "[178,  2400] loss: 2.303057\n",
      "[178,  2600] loss: 2.302901\n",
      "[178,  2800] loss: 2.303498\n",
      "[178,  3000] loss: 2.303479\n",
      "[178,  3200] loss: 2.303574\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[179,   200] loss: 2.303326\n",
      "[179,   400] loss: 2.303701\n",
      "[179,   600] loss: 2.303588\n",
      "[179,   800] loss: 2.303522\n",
      "[179,  1000] loss: 2.303328\n",
      "[179,  1200] loss: 2.303757\n",
      "[179,  1400] loss: 2.303290\n",
      "[179,  1600] loss: 2.303710\n",
      "[179,  1800] loss: 2.303145\n",
      "[179,  2000] loss: 2.303627\n",
      "[179,  2200] loss: 2.303510\n",
      "[179,  2400] loss: 2.303728\n",
      "[179,  2600] loss: 2.303932\n",
      "[179,  2800] loss: 2.303538\n",
      "[179,  3000] loss: 2.303821\n",
      "[179,  3200] loss: 2.303760\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[180,   200] loss: 2.303079\n",
      "[180,   400] loss: 2.303534\n",
      "[180,   600] loss: 2.302868\n",
      "[180,   800] loss: 2.303373\n",
      "[180,  1000] loss: 2.303222\n",
      "[180,  1200] loss: 2.303458\n",
      "[180,  1400] loss: 2.303311\n",
      "[180,  1600] loss: 2.303451\n",
      "[180,  1800] loss: 2.303458\n",
      "[180,  2000] loss: 2.303411\n",
      "[180,  2200] loss: 2.303625\n",
      "[180,  2400] loss: 2.303668\n",
      "[180,  2600] loss: 2.303434\n",
      "[180,  2800] loss: 2.303580\n",
      "[180,  3000] loss: 2.303861\n",
      "[180,  3200] loss: 2.303542\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[181,   200] loss: 2.303228\n",
      "[181,   400] loss: 2.303636\n",
      "[181,   600] loss: 2.303242\n",
      "[181,   800] loss: 2.303174\n",
      "[181,  1000] loss: 2.303287\n",
      "[181,  1200] loss: 2.303096\n",
      "[181,  1400] loss: 2.303544\n",
      "[181,  1600] loss: 2.303554\n",
      "[181,  1800] loss: 2.303647\n",
      "[181,  2000] loss: 2.303864\n",
      "[181,  2200] loss: 2.303586\n",
      "[181,  2400] loss: 2.303889\n",
      "[181,  2600] loss: 2.303294\n",
      "[181,  2800] loss: 2.303054\n",
      "[181,  3000] loss: 2.303498\n",
      "[181,  3200] loss: 2.303586\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[182,   200] loss: 2.303516\n",
      "[182,   400] loss: 2.303738\n",
      "[182,   600] loss: 2.303776\n",
      "[182,   800] loss: 2.303539\n",
      "[182,  1000] loss: 2.303697\n",
      "[182,  1200] loss: 2.303398\n",
      "[182,  1400] loss: 2.303475\n",
      "[182,  1600] loss: 2.303553\n",
      "[182,  1800] loss: 2.303393\n",
      "[182,  2000] loss: 2.303496\n",
      "[182,  2200] loss: 2.303686\n",
      "[182,  2400] loss: 2.303272\n",
      "[182,  2600] loss: 2.303630\n",
      "[182,  2800] loss: 2.303532\n",
      "[182,  3000] loss: 2.303548\n",
      "[182,  3200] loss: 2.303524\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[183,   200] loss: 2.303180\n",
      "[183,   400] loss: 2.303372\n",
      "[183,   600] loss: 2.303405\n",
      "[183,   800] loss: 2.303094\n",
      "[183,  1000] loss: 2.303406\n",
      "[183,  1200] loss: 2.303395\n",
      "[183,  1400] loss: 2.303288\n",
      "[183,  1600] loss: 2.303691\n",
      "[183,  1800] loss: 2.303504\n",
      "[183,  2000] loss: 2.303146\n",
      "[183,  2200] loss: 2.303651\n",
      "[183,  2400] loss: 2.303958\n",
      "[183,  2600] loss: 2.303238\n",
      "[183,  2800] loss: 2.303253\n",
      "[183,  3000] loss: 2.303555\n",
      "[183,  3200] loss: 2.303635\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[184,   200] loss: 2.303380\n",
      "[184,   400] loss: 2.303683\n",
      "[184,   600] loss: 2.303447\n",
      "[184,   800] loss: 2.303818\n",
      "[184,  1000] loss: 2.303425\n",
      "[184,  1200] loss: 2.303411\n",
      "[184,  1400] loss: 2.303661\n",
      "[184,  1600] loss: 2.303705\n",
      "[184,  1800] loss: 2.303071\n",
      "[184,  2000] loss: 2.303609\n",
      "[184,  2200] loss: 2.303504\n",
      "[184,  2400] loss: 2.303432\n",
      "[184,  2600] loss: 2.303209\n",
      "[184,  2800] loss: 2.303844\n",
      "[184,  3000] loss: 2.303465\n",
      "[184,  3200] loss: 2.303255\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[185,   200] loss: 2.303804\n",
      "[185,   400] loss: 2.303685\n",
      "[185,   600] loss: 2.303422\n",
      "[185,   800] loss: 2.304136\n",
      "[185,  1000] loss: 2.303382\n",
      "[185,  1200] loss: 2.303589\n",
      "[185,  1400] loss: 2.303467\n",
      "[185,  1600] loss: 2.303211\n",
      "[185,  1800] loss: 2.303130\n",
      "[185,  2000] loss: 2.303866\n",
      "[185,  2200] loss: 2.303698\n",
      "[185,  2400] loss: 2.303733\n",
      "[185,  2600] loss: 2.303404\n",
      "[185,  2800] loss: 2.303526\n",
      "[185,  3000] loss: 2.303679\n",
      "[185,  3200] loss: 2.303735\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[186,   200] loss: 2.303608\n",
      "[186,   400] loss: 2.303327\n",
      "[186,   600] loss: 2.303527\n",
      "[186,   800] loss: 2.303490\n",
      "[186,  1000] loss: 2.303552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186,  1200] loss: 2.303865\n",
      "[186,  1400] loss: 2.303203\n",
      "[186,  1600] loss: 2.303039\n",
      "[186,  1800] loss: 2.304085\n",
      "[186,  2000] loss: 2.302909\n",
      "[186,  2200] loss: 2.303088\n",
      "[186,  2400] loss: 2.303601\n",
      "[186,  2600] loss: 2.303551\n",
      "[186,  2800] loss: 2.303662\n",
      "[186,  3000] loss: 2.303759\n",
      "[186,  3200] loss: 2.303222\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[187,   200] loss: 2.303788\n",
      "[187,   400] loss: 2.303158\n",
      "[187,   600] loss: 2.303206\n",
      "[187,   800] loss: 2.302927\n",
      "[187,  1000] loss: 2.303420\n",
      "[187,  1200] loss: 2.303463\n",
      "[187,  1400] loss: 2.303373\n",
      "[187,  1600] loss: 2.304013\n",
      "[187,  1800] loss: 2.303504\n",
      "[187,  2000] loss: 2.303025\n",
      "[187,  2200] loss: 2.303687\n",
      "[187,  2400] loss: 2.303673\n",
      "[187,  2600] loss: 2.303522\n",
      "[187,  2800] loss: 2.303554\n",
      "[187,  3000] loss: 2.303271\n",
      "[187,  3200] loss: 2.303597\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[188,   200] loss: 2.303656\n",
      "[188,   400] loss: 2.303523\n",
      "[188,   600] loss: 2.303176\n",
      "[188,   800] loss: 2.303800\n",
      "[188,  1000] loss: 2.303495\n",
      "[188,  1200] loss: 2.303661\n",
      "[188,  1400] loss: 2.303516\n",
      "[188,  1600] loss: 2.303460\n",
      "[188,  1800] loss: 2.303334\n",
      "[188,  2000] loss: 2.303480\n",
      "[188,  2200] loss: 2.303567\n",
      "[188,  2400] loss: 2.303514\n",
      "[188,  2600] loss: 2.303313\n",
      "[188,  2800] loss: 2.303774\n",
      "[188,  3000] loss: 2.303177\n",
      "[188,  3200] loss: 2.303735\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[189,   200] loss: 2.303368\n",
      "[189,   400] loss: 2.303711\n",
      "[189,   600] loss: 2.303025\n",
      "[189,   800] loss: 2.303640\n",
      "[189,  1000] loss: 2.303489\n",
      "[189,  1200] loss: 2.303203\n",
      "[189,  1400] loss: 2.303443\n",
      "[189,  1600] loss: 2.303609\n",
      "[189,  1800] loss: 2.302755\n",
      "[189,  2000] loss: 2.303355\n",
      "[189,  2200] loss: 2.303201\n",
      "[189,  2400] loss: 2.303339\n",
      "[189,  2600] loss: 2.303231\n",
      "[189,  2800] loss: 2.303054\n",
      "[189,  3000] loss: 2.303521\n",
      "[189,  3200] loss: 2.303889\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[190,   200] loss: 2.303690\n",
      "[190,   400] loss: 2.303123\n",
      "[190,   600] loss: 2.303886\n",
      "[190,   800] loss: 2.303629\n",
      "[190,  1000] loss: 2.303648\n",
      "[190,  1200] loss: 2.303195\n",
      "[190,  1400] loss: 2.303641\n",
      "[190,  1600] loss: 2.303675\n",
      "[190,  1800] loss: 2.303329\n",
      "[190,  2000] loss: 2.303818\n",
      "[190,  2200] loss: 2.303259\n",
      "[190,  2400] loss: 2.303216\n",
      "[190,  2600] loss: 2.304080\n",
      "[190,  2800] loss: 2.303631\n",
      "[190,  3000] loss: 2.303339\n",
      "[190,  3200] loss: 2.303155\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[191,   200] loss: 2.303694\n",
      "[191,   400] loss: 2.303562\n",
      "[191,   600] loss: 2.303777\n",
      "[191,   800] loss: 2.303652\n",
      "[191,  1000] loss: 2.303542\n",
      "[191,  1200] loss: 2.303362\n",
      "[191,  1400] loss: 2.303725\n",
      "[191,  1600] loss: 2.303693\n",
      "[191,  1800] loss: 2.303393\n",
      "[191,  2000] loss: 2.303254\n",
      "[191,  2200] loss: 2.303455\n",
      "[191,  2400] loss: 2.303486\n",
      "[191,  2600] loss: 2.303664\n",
      "[191,  2800] loss: 2.303904\n",
      "[191,  3000] loss: 2.303530\n",
      "[191,  3200] loss: 2.303777\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[192,   200] loss: 2.303553\n",
      "[192,   400] loss: 2.303427\n",
      "[192,   600] loss: 2.303210\n",
      "[192,   800] loss: 2.303557\n",
      "[192,  1000] loss: 2.303667\n",
      "[192,  1200] loss: 2.302565\n",
      "[192,  1400] loss: 2.303430\n",
      "[192,  1600] loss: 2.303655\n",
      "[192,  1800] loss: 2.303467\n",
      "[192,  2000] loss: 2.303746\n",
      "[192,  2200] loss: 2.303722\n",
      "[192,  2400] loss: 2.303510\n",
      "[192,  2600] loss: 2.303086\n",
      "[192,  2800] loss: 2.303808\n",
      "[192,  3000] loss: 2.303250\n",
      "[192,  3200] loss: 2.303680\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[193,   200] loss: 2.303393\n",
      "[193,   400] loss: 2.303353\n",
      "[193,   600] loss: 2.303560\n",
      "[193,   800] loss: 2.303762\n",
      "[193,  1000] loss: 2.303825\n",
      "[193,  1200] loss: 2.303146\n",
      "[193,  1400] loss: 2.303710\n",
      "[193,  1600] loss: 2.303448\n",
      "[193,  1800] loss: 2.303423\n",
      "[193,  2000] loss: 2.303743\n",
      "[193,  2200] loss: 2.303501\n",
      "[193,  2400] loss: 2.303174\n",
      "[193,  2600] loss: 2.303596\n",
      "[193,  2800] loss: 2.303447\n",
      "[193,  3000] loss: 2.303725\n",
      "[193,  3200] loss: 2.303530\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[194,   200] loss: 2.303018\n",
      "[194,   400] loss: 2.303736\n",
      "[194,   600] loss: 2.303315\n",
      "[194,   800] loss: 2.303639\n",
      "[194,  1000] loss: 2.303557\n",
      "[194,  1200] loss: 2.303577\n",
      "[194,  1400] loss: 2.303313\n",
      "[194,  1600] loss: 2.303445\n",
      "[194,  1800] loss: 2.303553\n",
      "[194,  2000] loss: 2.303568\n",
      "[194,  2200] loss: 2.303538\n",
      "[194,  2400] loss: 2.303598\n",
      "[194,  2600] loss: 2.303225\n",
      "[194,  2800] loss: 2.303277\n",
      "[194,  3000] loss: 2.303899\n",
      "[194,  3200] loss: 2.303545\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[195,   200] loss: 2.303486\n",
      "[195,   400] loss: 2.303723\n",
      "[195,   600] loss: 2.303629\n",
      "[195,   800] loss: 2.303326\n",
      "[195,  1000] loss: 2.303481\n",
      "[195,  1200] loss: 2.303455\n",
      "[195,  1400] loss: 2.303717\n",
      "[195,  1600] loss: 2.303765\n",
      "[195,  1800] loss: 2.303751\n",
      "[195,  2000] loss: 2.303413\n",
      "[195,  2200] loss: 2.303537\n",
      "[195,  2400] loss: 2.303370\n",
      "[195,  2600] loss: 2.303717\n",
      "[195,  2800] loss: 2.303219\n",
      "[195,  3000] loss: 2.303341\n",
      "[195,  3200] loss: 2.303501\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[196,   200] loss: 2.303291\n",
      "[196,   400] loss: 2.303487\n",
      "[196,   600] loss: 2.303776\n",
      "[196,   800] loss: 2.303453\n",
      "[196,  1000] loss: 2.303406\n",
      "[196,  1200] loss: 2.302857\n",
      "[196,  1400] loss: 2.303644\n",
      "[196,  1600] loss: 2.303305\n",
      "[196,  1800] loss: 2.303320\n",
      "[196,  2000] loss: 2.303280\n",
      "[196,  2200] loss: 2.303470\n",
      "[196,  2400] loss: 2.303577\n",
      "[196,  2600] loss: 2.303500\n",
      "[196,  2800] loss: 2.303529\n",
      "[196,  3000] loss: 2.303528\n",
      "[196,  3200] loss: 2.303649\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[197,   200] loss: 2.303586\n",
      "[197,   400] loss: 2.303274\n",
      "[197,   600] loss: 2.303399\n",
      "[197,   800] loss: 2.303435\n",
      "[197,  1000] loss: 2.303389\n",
      "[197,  1200] loss: 2.303459\n",
      "[197,  1400] loss: 2.303626\n",
      "[197,  1600] loss: 2.303600\n",
      "[197,  1800] loss: 2.303660\n",
      "[197,  2000] loss: 2.303017\n",
      "[197,  2200] loss: 2.303026\n",
      "[197,  2400] loss: 2.303406\n",
      "[197,  2600] loss: 2.304010\n",
      "[197,  2800] loss: 2.303559\n",
      "[197,  3000] loss: 2.303668\n",
      "[197,  3200] loss: 2.303379\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[198,   200] loss: 2.303913\n",
      "[198,   400] loss: 2.303215\n",
      "[198,   600] loss: 2.303595\n",
      "[198,   800] loss: 2.303405\n",
      "[198,  1000] loss: 2.303399\n",
      "[198,  1200] loss: 2.303658\n",
      "[198,  1400] loss: 2.303310\n",
      "[198,  1600] loss: 2.303398\n",
      "[198,  1800] loss: 2.303657\n",
      "[198,  2000] loss: 2.303434\n",
      "[198,  2200] loss: 2.303723\n",
      "[198,  2400] loss: 2.303147\n",
      "[198,  2600] loss: 2.303787\n",
      "[198,  2800] loss: 2.303662\n",
      "[198,  3000] loss: 2.303360\n",
      "[198,  3200] loss: 2.303787\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[199,   200] loss: 2.303318\n",
      "[199,   400] loss: 2.303472\n",
      "[199,   600] loss: 2.303525\n",
      "[199,   800] loss: 2.303580\n",
      "[199,  1000] loss: 2.303465\n",
      "[199,  1200] loss: 2.303714\n",
      "[199,  1400] loss: 2.303618\n",
      "[199,  1600] loss: 2.303695\n",
      "[199,  1800] loss: 2.303674\n",
      "[199,  2000] loss: 2.303295\n",
      "[199,  2200] loss: 2.303571\n",
      "[199,  2400] loss: 2.303532\n",
      "[199,  2600] loss: 2.303384\n",
      "[199,  2800] loss: 2.303355\n",
      "[199,  3000] loss: 2.302786\n",
      "[199,  3200] loss: 2.303512\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[200,   200] loss: 2.304096\n",
      "[200,   400] loss: 2.302953\n",
      "[200,   600] loss: 2.303260\n",
      "[200,   800] loss: 2.303164\n",
      "[200,  1000] loss: 2.303643\n",
      "[200,  1200] loss: 2.303169\n",
      "[200,  1400] loss: 2.303531\n",
      "[200,  1600] loss: 2.303428\n",
      "[200,  1800] loss: 2.303334\n",
      "[200,  2000] loss: 2.303535\n",
      "[200,  2200] loss: 2.303257\n",
      "[200,  2400] loss: 2.303283\n",
      "[200,  2600] loss: 2.303305\n",
      "[200,  2800] loss: 2.303567\n",
      "[200,  3000] loss: 2.303418\n",
      "[200,  3200] loss: 2.303420\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[201,   200] loss: 2.303837\n",
      "[201,   400] loss: 2.303176\n",
      "[201,   600] loss: 2.303310\n",
      "[201,   800] loss: 2.303521\n",
      "[201,  1000] loss: 2.303358\n",
      "[201,  1200] loss: 2.303657\n",
      "[201,  1400] loss: 2.303172\n",
      "[201,  1600] loss: 2.303871\n",
      "[201,  1800] loss: 2.303080\n",
      "[201,  2000] loss: 2.303663\n",
      "[201,  2200] loss: 2.303705\n",
      "[201,  2400] loss: 2.303670\n",
      "[201,  2600] loss: 2.303624\n",
      "[201,  2800] loss: 2.303746\n",
      "[201,  3000] loss: 2.303259\n",
      "[201,  3200] loss: 2.303199\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[202,   200] loss: 2.303398\n",
      "[202,   400] loss: 2.303570\n",
      "[202,   600] loss: 2.303289\n",
      "[202,   800] loss: 2.303257\n",
      "[202,  1000] loss: 2.303354\n",
      "[202,  1200] loss: 2.303398\n",
      "[202,  1400] loss: 2.303171\n",
      "[202,  1600] loss: 2.303589\n",
      "[202,  1800] loss: 2.303666\n",
      "[202,  2000] loss: 2.303473\n",
      "[202,  2200] loss: 2.303881\n",
      "[202,  2400] loss: 2.303674\n",
      "[202,  2600] loss: 2.303376\n",
      "[202,  2800] loss: 2.303154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202,  3000] loss: 2.303003\n",
      "[202,  3200] loss: 2.303674\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[203,   200] loss: 2.303617\n",
      "[203,   400] loss: 2.303332\n",
      "[203,   600] loss: 2.303384\n",
      "[203,   800] loss: 2.304053\n",
      "[203,  1000] loss: 2.303274\n",
      "[203,  1200] loss: 2.303685\n",
      "[203,  1400] loss: 2.303559\n",
      "[203,  1600] loss: 2.303363\n",
      "[203,  1800] loss: 2.302880\n",
      "[203,  2000] loss: 2.303427\n",
      "[203,  2200] loss: 2.303481\n",
      "[203,  2400] loss: 2.303638\n",
      "[203,  2600] loss: 2.303392\n",
      "[203,  2800] loss: 2.303906\n",
      "[203,  3000] loss: 2.303166\n",
      "[203,  3200] loss: 2.303508\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[204,   200] loss: 2.303625\n",
      "[204,   400] loss: 2.303604\n",
      "[204,   600] loss: 2.302680\n",
      "[204,   800] loss: 2.303652\n",
      "[204,  1000] loss: 2.303733\n",
      "[204,  1200] loss: 2.303870\n",
      "[204,  1400] loss: 2.303420\n",
      "[204,  1600] loss: 2.303103\n",
      "[204,  1800] loss: 2.303536\n",
      "[204,  2000] loss: 2.303683\n",
      "[204,  2200] loss: 2.303374\n",
      "[204,  2400] loss: 2.303395\n",
      "[204,  2600] loss: 2.303498\n",
      "[204,  2800] loss: 2.303786\n",
      "[204,  3000] loss: 2.303644\n",
      "[204,  3200] loss: 2.303546\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[205,   200] loss: 2.303133\n",
      "[205,   400] loss: 2.303652\n",
      "[205,   600] loss: 2.303668\n",
      "[205,   800] loss: 2.303748\n",
      "[205,  1000] loss: 2.303285\n",
      "[205,  1200] loss: 2.303758\n",
      "[205,  1400] loss: 2.303343\n",
      "[205,  1600] loss: 2.302962\n",
      "[205,  1800] loss: 2.303556\n",
      "[205,  2000] loss: 2.303742\n",
      "[205,  2200] loss: 2.303456\n",
      "[205,  2400] loss: 2.303232\n",
      "[205,  2600] loss: 2.303701\n",
      "[205,  2800] loss: 2.303469\n",
      "[205,  3000] loss: 2.303568\n",
      "[205,  3200] loss: 2.303416\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[206,   200] loss: 2.303533\n",
      "[206,   400] loss: 2.302868\n",
      "[206,   600] loss: 2.303944\n",
      "[206,   800] loss: 2.303766\n",
      "[206,  1000] loss: 2.303690\n",
      "[206,  1200] loss: 2.303514\n",
      "[206,  1400] loss: 2.303400\n",
      "[206,  1600] loss: 2.303033\n",
      "[206,  1800] loss: 2.303232\n",
      "[206,  2000] loss: 2.303219\n",
      "[206,  2200] loss: 2.303933\n",
      "[206,  2400] loss: 2.303268\n",
      "[206,  2600] loss: 2.303997\n",
      "[206,  2800] loss: 2.303947\n",
      "[206,  3000] loss: 2.303324\n",
      "[206,  3200] loss: 2.303427\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[207,   200] loss: 2.303214\n",
      "[207,   400] loss: 2.303552\n",
      "[207,   600] loss: 2.303581\n",
      "[207,   800] loss: 2.303509\n",
      "[207,  1000] loss: 2.303304\n",
      "[207,  1200] loss: 2.303483\n",
      "[207,  1400] loss: 2.303783\n",
      "[207,  1600] loss: 2.303631\n",
      "[207,  1800] loss: 2.303511\n",
      "[207,  2000] loss: 2.303556\n",
      "[207,  2200] loss: 2.303793\n",
      "[207,  2400] loss: 2.303070\n",
      "[207,  2600] loss: 2.303164\n",
      "[207,  2800] loss: 2.303814\n",
      "[207,  3000] loss: 2.303227\n",
      "[207,  3200] loss: 2.303504\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[208,   200] loss: 2.303652\n",
      "[208,   400] loss: 2.302909\n",
      "[208,   600] loss: 2.303270\n",
      "[208,   800] loss: 2.303675\n",
      "[208,  1000] loss: 2.303361\n",
      "[208,  1200] loss: 2.303362\n",
      "[208,  1400] loss: 2.303626\n",
      "[208,  1600] loss: 2.303431\n",
      "[208,  1800] loss: 2.303402\n",
      "[208,  2000] loss: 2.303485\n",
      "[208,  2200] loss: 2.303318\n",
      "[208,  2400] loss: 2.303581\n",
      "[208,  2600] loss: 2.303515\n",
      "[208,  2800] loss: 2.303920\n",
      "[208,  3000] loss: 2.303469\n",
      "[208,  3200] loss: 2.303147\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[209,   200] loss: 2.303867\n",
      "[209,   400] loss: 2.303678\n",
      "[209,   600] loss: 2.303663\n",
      "[209,   800] loss: 2.303381\n",
      "[209,  1000] loss: 2.303974\n",
      "[209,  1200] loss: 2.303388\n",
      "[209,  1400] loss: 2.302871\n",
      "[209,  1600] loss: 2.303565\n",
      "[209,  1800] loss: 2.303762\n",
      "[209,  2000] loss: 2.303462\n",
      "[209,  2200] loss: 2.303764\n",
      "[209,  2400] loss: 2.303481\n",
      "[209,  2600] loss: 2.303279\n",
      "[209,  2800] loss: 2.303694\n",
      "[209,  3000] loss: 2.303228\n",
      "[209,  3200] loss: 2.303419\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[210,   200] loss: 2.303670\n",
      "[210,   400] loss: 2.303568\n",
      "[210,   600] loss: 2.303245\n",
      "[210,   800] loss: 2.303726\n",
      "[210,  1000] loss: 2.303102\n",
      "[210,  1200] loss: 2.303568\n",
      "[210,  1400] loss: 2.303232\n",
      "[210,  1600] loss: 2.303532\n",
      "[210,  1800] loss: 2.303482\n",
      "[210,  2000] loss: 2.303253\n",
      "[210,  2200] loss: 2.303880\n",
      "[210,  2400] loss: 2.303608\n",
      "[210,  2600] loss: 2.303307\n",
      "[210,  2800] loss: 2.303551\n",
      "[210,  3000] loss: 2.303668\n",
      "[210,  3200] loss: 2.303576\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[211,   200] loss: 2.303425\n",
      "[211,   400] loss: 2.303698\n",
      "[211,   600] loss: 2.303827\n",
      "[211,   800] loss: 2.303590\n",
      "[211,  1000] loss: 2.303636\n",
      "[211,  1200] loss: 2.303856\n",
      "[211,  1400] loss: 2.303425\n",
      "[211,  1600] loss: 2.303616\n",
      "[211,  1800] loss: 2.303306\n",
      "[211,  2000] loss: 2.303471\n",
      "[211,  2200] loss: 2.303757\n",
      "[211,  2400] loss: 2.303692\n",
      "[211,  2600] loss: 2.303750\n",
      "[211,  2800] loss: 2.303396\n",
      "[211,  3000] loss: 2.303749\n",
      "[211,  3200] loss: 2.303721\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[212,   200] loss: 2.303390\n",
      "[212,   400] loss: 2.303762\n",
      "[212,   600] loss: 2.303895\n",
      "[212,   800] loss: 2.302862\n",
      "[212,  1000] loss: 2.303548\n",
      "[212,  1200] loss: 2.303503\n",
      "[212,  1400] loss: 2.303389\n",
      "[212,  1600] loss: 2.303657\n",
      "[212,  1800] loss: 2.304006\n",
      "[212,  2000] loss: 2.303632\n",
      "[212,  2200] loss: 2.303432\n",
      "[212,  2400] loss: 2.303537\n",
      "[212,  2600] loss: 2.303474\n",
      "[212,  2800] loss: 2.303430\n",
      "[212,  3000] loss: 2.302833\n",
      "[212,  3200] loss: 2.303426\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[213,   200] loss: 2.303830\n",
      "[213,   400] loss: 2.303631\n",
      "[213,   600] loss: 2.303317\n",
      "[213,   800] loss: 2.303716\n",
      "[213,  1000] loss: 2.303766\n",
      "[213,  1200] loss: 2.303490\n",
      "[213,  1400] loss: 2.303372\n",
      "[213,  1600] loss: 2.303023\n",
      "[213,  1800] loss: 2.303710\n",
      "[213,  2000] loss: 2.303242\n",
      "[213,  2200] loss: 2.303884\n",
      "[213,  2400] loss: 2.303264\n",
      "[213,  2600] loss: 2.303520\n",
      "[213,  2800] loss: 2.303133\n",
      "[213,  3000] loss: 2.304003\n",
      "[213,  3200] loss: 2.303373\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[214,   200] loss: 2.303303\n",
      "[214,   400] loss: 2.303680\n",
      "[214,   600] loss: 2.304156\n",
      "[214,   800] loss: 2.303516\n",
      "[214,  1000] loss: 2.303279\n",
      "[214,  1200] loss: 2.303506\n",
      "[214,  1400] loss: 2.303384\n",
      "[214,  1600] loss: 2.303405\n",
      "[214,  1800] loss: 2.303552\n",
      "[214,  2000] loss: 2.303563\n",
      "[214,  2200] loss: 2.303249\n",
      "[214,  2400] loss: 2.303635\n",
      "[214,  2600] loss: 2.303050\n",
      "[214,  2800] loss: 2.303732\n",
      "[214,  3000] loss: 2.303556\n",
      "[214,  3200] loss: 2.303640\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[215,   200] loss: 2.303289\n",
      "[215,   400] loss: 2.303462\n",
      "[215,   600] loss: 2.303097\n",
      "[215,   800] loss: 2.303262\n",
      "[215,  1000] loss: 2.303508\n",
      "[215,  1200] loss: 2.303653\n",
      "[215,  1400] loss: 2.303172\n",
      "[215,  1600] loss: 2.303529\n",
      "[215,  1800] loss: 2.303664\n",
      "[215,  2000] loss: 2.303667\n",
      "[215,  2200] loss: 2.303620\n",
      "[215,  2400] loss: 2.303272\n",
      "[215,  2600] loss: 2.303503\n",
      "[215,  2800] loss: 2.303777\n",
      "[215,  3000] loss: 2.303485\n",
      "[215,  3200] loss: 2.303860\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[216,   200] loss: 2.303349\n",
      "[216,   400] loss: 2.303709\n",
      "[216,   600] loss: 2.303840\n",
      "[216,   800] loss: 2.303719\n",
      "[216,  1000] loss: 2.303816\n",
      "[216,  1200] loss: 2.303553\n",
      "[216,  1400] loss: 2.303463\n",
      "[216,  1600] loss: 2.303572\n",
      "[216,  1800] loss: 2.303380\n",
      "[216,  2000] loss: 2.303533\n",
      "[216,  2200] loss: 2.303571\n",
      "[216,  2400] loss: 2.303520\n",
      "[216,  2600] loss: 2.303317\n",
      "[216,  2800] loss: 2.304268\n",
      "[216,  3000] loss: 2.303526\n",
      "[216,  3200] loss: 2.302921\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[217,   200] loss: 2.303923\n",
      "[217,   400] loss: 2.303632\n",
      "[217,   600] loss: 2.303845\n",
      "[217,   800] loss: 2.303799\n",
      "[217,  1000] loss: 2.303176\n",
      "[217,  1200] loss: 2.303623\n",
      "[217,  1400] loss: 2.303114\n",
      "[217,  1600] loss: 2.303744\n",
      "[217,  1800] loss: 2.303525\n",
      "[217,  2000] loss: 2.303633\n",
      "[217,  2200] loss: 2.303283\n",
      "[217,  2400] loss: 2.303359\n",
      "[217,  2600] loss: 2.303288\n",
      "[217,  2800] loss: 2.303674\n",
      "[217,  3000] loss: 2.303690\n",
      "[217,  3200] loss: 2.303566\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[218,   200] loss: 2.303598\n",
      "[218,   400] loss: 2.303597\n",
      "[218,   600] loss: 2.303476\n",
      "[218,   800] loss: 2.303825\n",
      "[218,  1000] loss: 2.303243\n",
      "[218,  1200] loss: 2.303237\n",
      "[218,  1400] loss: 2.303092\n",
      "[218,  1600] loss: 2.303689\n",
      "[218,  1800] loss: 2.303328\n",
      "[218,  2000] loss: 2.303832\n",
      "[218,  2200] loss: 2.303523\n",
      "[218,  2400] loss: 2.303555\n",
      "[218,  2600] loss: 2.303557\n",
      "[218,  2800] loss: 2.303559\n",
      "[218,  3000] loss: 2.303435\n",
      "[218,  3200] loss: 2.303771\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[219,   200] loss: 2.303070\n",
      "[219,   400] loss: 2.303737\n",
      "[219,   600] loss: 2.303481\n",
      "[219,   800] loss: 2.302913\n",
      "[219,  1000] loss: 2.303588\n",
      "[219,  1200] loss: 2.303046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[219,  1400] loss: 2.303140\n",
      "[219,  1600] loss: 2.303945\n",
      "[219,  1800] loss: 2.303587\n",
      "[219,  2000] loss: 2.303794\n",
      "[219,  2200] loss: 2.303594\n",
      "[219,  2400] loss: 2.303380\n",
      "[219,  2600] loss: 2.303735\n",
      "[219,  2800] loss: 2.303383\n",
      "[219,  3000] loss: 2.303643\n",
      "[219,  3200] loss: 2.303312\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[220,   200] loss: 2.303457\n",
      "[220,   400] loss: 2.303735\n",
      "[220,   600] loss: 2.303477\n",
      "[220,   800] loss: 2.303324\n",
      "[220,  1000] loss: 2.303279\n",
      "[220,  1200] loss: 2.303461\n",
      "[220,  1400] loss: 2.303611\n",
      "[220,  1600] loss: 2.303359\n",
      "[220,  1800] loss: 2.303429\n",
      "[220,  2000] loss: 2.303374\n",
      "[220,  2200] loss: 2.303287\n",
      "[220,  2400] loss: 2.303552\n",
      "[220,  2600] loss: 2.303130\n",
      "[220,  2800] loss: 2.303602\n",
      "[220,  3000] loss: 2.303437\n",
      "[220,  3200] loss: 2.303253\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[221,   200] loss: 2.303641\n",
      "[221,   400] loss: 2.302501\n",
      "[221,   600] loss: 2.303035\n",
      "[221,   800] loss: 2.303926\n",
      "[221,  1000] loss: 2.303560\n",
      "[221,  1200] loss: 2.303370\n",
      "[221,  1400] loss: 2.303578\n",
      "[221,  1600] loss: 2.303852\n",
      "[221,  1800] loss: 2.303256\n",
      "[221,  2000] loss: 2.303583\n",
      "[221,  2200] loss: 2.303333\n",
      "[221,  2400] loss: 2.303442\n",
      "[221,  2600] loss: 2.303622\n",
      "[221,  2800] loss: 2.303536\n",
      "[221,  3000] loss: 2.303380\n",
      "[221,  3200] loss: 2.303430\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[222,   200] loss: 2.303565\n",
      "[222,   400] loss: 2.303566\n",
      "[222,   600] loss: 2.303397\n",
      "[222,   800] loss: 2.303523\n",
      "[222,  1000] loss: 2.303489\n",
      "[222,  1200] loss: 2.303602\n",
      "[222,  1400] loss: 2.303370\n",
      "[222,  1600] loss: 2.303611\n",
      "[222,  1800] loss: 2.303868\n",
      "[222,  2000] loss: 2.303502\n",
      "[222,  2200] loss: 2.303001\n",
      "[222,  2400] loss: 2.303282\n",
      "[222,  2600] loss: 2.303509\n",
      "[222,  2800] loss: 2.303411\n",
      "[222,  3000] loss: 2.303579\n",
      "[222,  3200] loss: 2.303589\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[223,   200] loss: 2.303780\n",
      "[223,   400] loss: 2.303469\n",
      "[223,   600] loss: 2.303722\n",
      "[223,   800] loss: 2.303538\n",
      "[223,  1000] loss: 2.303724\n",
      "[223,  1200] loss: 2.303367\n",
      "[223,  1400] loss: 2.303290\n",
      "[223,  1600] loss: 2.303474\n",
      "[223,  1800] loss: 2.303347\n",
      "[223,  2000] loss: 2.303559\n",
      "[223,  2200] loss: 2.303829\n",
      "[223,  2400] loss: 2.303175\n",
      "[223,  2600] loss: 2.303374\n",
      "[223,  2800] loss: 2.303659\n",
      "[223,  3000] loss: 2.303774\n",
      "[223,  3200] loss: 2.303420\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[224,   200] loss: 2.303709\n",
      "[224,   400] loss: 2.303735\n",
      "[224,   600] loss: 2.303652\n",
      "[224,   800] loss: 2.303662\n",
      "[224,  1000] loss: 2.303457\n",
      "[224,  1200] loss: 2.302763\n",
      "[224,  1400] loss: 2.303822\n",
      "[224,  1600] loss: 2.303664\n",
      "[224,  1800] loss: 2.303312\n",
      "[224,  2000] loss: 2.303693\n",
      "[224,  2200] loss: 2.303433\n",
      "[224,  2400] loss: 2.303199\n",
      "[224,  2600] loss: 2.303326\n",
      "[224,  2800] loss: 2.304030\n",
      "[224,  3000] loss: 2.303517\n",
      "[224,  3200] loss: 2.304094\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[225,   200] loss: 2.303750\n",
      "[225,   400] loss: 2.303524\n",
      "[225,   600] loss: 2.303288\n",
      "[225,   800] loss: 2.303489\n",
      "[225,  1000] loss: 2.303617\n",
      "[225,  1200] loss: 2.303498\n",
      "[225,  1400] loss: 2.303258\n",
      "[225,  1600] loss: 2.303305\n",
      "[225,  1800] loss: 2.303240\n",
      "[225,  2000] loss: 2.302764\n",
      "[225,  2200] loss: 2.303096\n",
      "[225,  2400] loss: 2.303360\n",
      "[225,  2600] loss: 2.303663\n",
      "[225,  2800] loss: 2.303460\n",
      "[225,  3000] loss: 2.303320\n",
      "[225,  3200] loss: 2.303696\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[226,   200] loss: 2.303479\n",
      "[226,   400] loss: 2.302902\n",
      "[226,   600] loss: 2.303825\n",
      "[226,   800] loss: 2.303078\n",
      "[226,  1000] loss: 2.303670\n",
      "[226,  1200] loss: 2.303400\n",
      "[226,  1400] loss: 2.303534\n",
      "[226,  1600] loss: 2.303880\n",
      "[226,  1800] loss: 2.302586\n",
      "[226,  2000] loss: 2.303943\n",
      "[226,  2200] loss: 2.302883\n",
      "[226,  2400] loss: 2.304018\n",
      "[226,  2600] loss: 2.303218\n",
      "[226,  2800] loss: 2.302928\n",
      "[226,  3000] loss: 2.303269\n",
      "[226,  3200] loss: 2.303565\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[227,   200] loss: 2.303590\n",
      "[227,   400] loss: 2.303712\n",
      "[227,   600] loss: 2.303471\n",
      "[227,   800] loss: 2.303447\n",
      "[227,  1000] loss: 2.303834\n",
      "[227,  1200] loss: 2.303321\n",
      "[227,  1400] loss: 2.303453\n",
      "[227,  1600] loss: 2.303638\n",
      "[227,  1800] loss: 2.303518\n",
      "[227,  2000] loss: 2.303824\n",
      "[227,  2200] loss: 2.303372\n",
      "[227,  2400] loss: 2.302675\n",
      "[227,  2600] loss: 2.303104\n",
      "[227,  2800] loss: 2.303558\n",
      "[227,  3000] loss: 2.303130\n",
      "[227,  3200] loss: 2.303451\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[228,   200] loss: 2.303479\n",
      "[228,   400] loss: 2.303479\n",
      "[228,   600] loss: 2.303530\n",
      "[228,   800] loss: 2.303677\n",
      "[228,  1000] loss: 2.303430\n",
      "[228,  1200] loss: 2.303163\n",
      "[228,  1400] loss: 2.303508\n",
      "[228,  1600] loss: 2.303489\n",
      "[228,  1800] loss: 2.303782\n",
      "[228,  2000] loss: 2.303151\n",
      "[228,  2200] loss: 2.303397\n",
      "[228,  2400] loss: 2.303208\n",
      "[228,  2600] loss: 2.303718\n",
      "[228,  2800] loss: 2.303468\n",
      "[228,  3000] loss: 2.303128\n",
      "[228,  3200] loss: 2.303753\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[229,   200] loss: 2.303838\n",
      "[229,   400] loss: 2.303331\n",
      "[229,   600] loss: 2.303514\n",
      "[229,   800] loss: 2.303493\n",
      "[229,  1000] loss: 2.303887\n",
      "[229,  1200] loss: 2.303340\n",
      "[229,  1400] loss: 2.303462\n",
      "[229,  1600] loss: 2.303188\n",
      "[229,  1800] loss: 2.303597\n",
      "[229,  2000] loss: 2.303608\n",
      "[229,  2200] loss: 2.303345\n",
      "[229,  2400] loss: 2.303453\n",
      "[229,  2600] loss: 2.303606\n",
      "[229,  2800] loss: 2.303953\n",
      "[229,  3000] loss: 2.303671\n",
      "[229,  3200] loss: 2.303721\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[230,   200] loss: 2.303670\n",
      "[230,   400] loss: 2.303079\n",
      "[230,   600] loss: 2.303708\n",
      "[230,   800] loss: 2.303533\n",
      "[230,  1000] loss: 2.303368\n",
      "[230,  1200] loss: 2.303099\n",
      "[230,  1400] loss: 2.303601\n",
      "[230,  1600] loss: 2.303764\n",
      "[230,  1800] loss: 2.303744\n",
      "[230,  2000] loss: 2.303819\n",
      "[230,  2200] loss: 2.303289\n",
      "[230,  2400] loss: 2.303598\n",
      "[230,  2600] loss: 2.303613\n",
      "[230,  2800] loss: 2.303105\n",
      "[230,  3000] loss: 2.304083\n",
      "[230,  3200] loss: 2.303429\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[231,   200] loss: 2.303251\n",
      "[231,   400] loss: 2.303443\n",
      "[231,   600] loss: 2.303285\n",
      "[231,   800] loss: 2.303564\n",
      "[231,  1000] loss: 2.303960\n",
      "[231,  1200] loss: 2.302841\n",
      "[231,  1400] loss: 2.303438\n",
      "[231,  1600] loss: 2.303819\n",
      "[231,  1800] loss: 2.303550\n",
      "[231,  2000] loss: 2.303155\n",
      "[231,  2200] loss: 2.303831\n",
      "[231,  2400] loss: 2.303395\n",
      "[231,  2600] loss: 2.303493\n",
      "[231,  2800] loss: 2.303424\n",
      "[231,  3000] loss: 2.303894\n",
      "[231,  3200] loss: 2.303127\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[232,   200] loss: 2.303010\n",
      "[232,   400] loss: 2.303358\n",
      "[232,   600] loss: 2.303561\n",
      "[232,   800] loss: 2.303296\n",
      "[232,  1000] loss: 2.303338\n",
      "[232,  1200] loss: 2.303620\n",
      "[232,  1400] loss: 2.303842\n",
      "[232,  1600] loss: 2.303267\n",
      "[232,  1800] loss: 2.303675\n",
      "[232,  2000] loss: 2.303697\n",
      "[232,  2200] loss: 2.303528\n",
      "[232,  2400] loss: 2.303417\n",
      "[232,  2600] loss: 2.303767\n",
      "[232,  2800] loss: 2.303185\n",
      "[232,  3000] loss: 2.303755\n",
      "[232,  3200] loss: 2.303063\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[233,   200] loss: 2.303518\n",
      "[233,   400] loss: 2.303302\n",
      "[233,   600] loss: 2.303663\n",
      "[233,   800] loss: 2.304059\n",
      "[233,  1000] loss: 2.303374\n",
      "[233,  1200] loss: 2.303486\n",
      "[233,  1400] loss: 2.303136\n",
      "[233,  1600] loss: 2.303608\n",
      "[233,  1800] loss: 2.303447\n",
      "[233,  2000] loss: 2.303706\n",
      "[233,  2200] loss: 2.303616\n",
      "[233,  2400] loss: 2.303555\n",
      "[233,  2600] loss: 2.303688\n",
      "[233,  2800] loss: 2.303509\n",
      "[233,  3000] loss: 2.303749\n",
      "[233,  3200] loss: 2.303272\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[234,   200] loss: 2.302845\n",
      "[234,   400] loss: 2.303508\n",
      "[234,   600] loss: 2.303262\n",
      "[234,   800] loss: 2.303604\n",
      "[234,  1000] loss: 2.303481\n",
      "[234,  1200] loss: 2.303369\n",
      "[234,  1400] loss: 2.303703\n",
      "[234,  1600] loss: 2.303135\n",
      "[234,  1800] loss: 2.303543\n",
      "[234,  2000] loss: 2.303116\n",
      "[234,  2200] loss: 2.303314\n",
      "[234,  2400] loss: 2.303143\n",
      "[234,  2600] loss: 2.303802\n",
      "[234,  2800] loss: 2.303073\n",
      "[234,  3000] loss: 2.303497\n",
      "[234,  3200] loss: 2.303454\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[235,   200] loss: 2.303523\n",
      "[235,   400] loss: 2.303444\n",
      "[235,   600] loss: 2.303564\n",
      "[235,   800] loss: 2.303539\n",
      "[235,  1000] loss: 2.303749\n",
      "[235,  1200] loss: 2.303502\n",
      "[235,  1400] loss: 2.303247\n",
      "[235,  1600] loss: 2.303335\n",
      "[235,  1800] loss: 2.303439\n",
      "[235,  2000] loss: 2.303423\n",
      "[235,  2200] loss: 2.303377\n",
      "[235,  2400] loss: 2.303635\n",
      "[235,  2600] loss: 2.303859\n",
      "[235,  2800] loss: 2.303507\n",
      "[235,  3000] loss: 2.302786\n",
      "[235,  3200] loss: 2.303904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[236,   200] loss: 2.303278\n",
      "[236,   400] loss: 2.303216\n",
      "[236,   600] loss: 2.303358\n",
      "[236,   800] loss: 2.303394\n",
      "[236,  1000] loss: 2.303702\n",
      "[236,  1200] loss: 2.303428\n",
      "[236,  1400] loss: 2.303396\n",
      "[236,  1600] loss: 2.303683\n",
      "[236,  1800] loss: 2.303331\n",
      "[236,  2000] loss: 2.303303\n",
      "[236,  2200] loss: 2.303985\n",
      "[236,  2400] loss: 2.303375\n",
      "[236,  2600] loss: 2.303566\n",
      "[236,  2800] loss: 2.303427\n",
      "[236,  3000] loss: 2.303247\n",
      "[236,  3200] loss: 2.303699\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[237,   200] loss: 2.303457\n",
      "[237,   400] loss: 2.303596\n",
      "[237,   600] loss: 2.303637\n",
      "[237,   800] loss: 2.303484\n",
      "[237,  1000] loss: 2.303633\n",
      "[237,  1200] loss: 2.303350\n",
      "[237,  1400] loss: 2.303273\n",
      "[237,  1600] loss: 2.303474\n",
      "[237,  1800] loss: 2.303372\n",
      "[237,  2000] loss: 2.303524\n",
      "[237,  2200] loss: 2.304153\n",
      "[237,  2400] loss: 2.303237\n",
      "[237,  2600] loss: 2.303523\n",
      "[237,  2800] loss: 2.303374\n",
      "[237,  3000] loss: 2.303871\n",
      "[237,  3200] loss: 2.303131\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[238,   200] loss: 2.303906\n",
      "[238,   400] loss: 2.303348\n",
      "[238,   600] loss: 2.303853\n",
      "[238,   800] loss: 2.303435\n",
      "[238,  1000] loss: 2.303271\n",
      "[238,  1200] loss: 2.303368\n",
      "[238,  1400] loss: 2.303742\n",
      "[238,  1600] loss: 2.303512\n",
      "[238,  1800] loss: 2.303692\n",
      "[238,  2000] loss: 2.303054\n",
      "[238,  2200] loss: 2.303494\n",
      "[238,  2400] loss: 2.303504\n",
      "[238,  2600] loss: 2.303461\n",
      "[238,  2800] loss: 2.303422\n",
      "[238,  3000] loss: 2.303417\n",
      "[238,  3200] loss: 2.303513\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[239,   200] loss: 2.303305\n",
      "[239,   400] loss: 2.303431\n",
      "[239,   600] loss: 2.303387\n",
      "[239,   800] loss: 2.303438\n",
      "[239,  1000] loss: 2.303598\n",
      "[239,  1200] loss: 2.303593\n",
      "[239,  1400] loss: 2.303412\n",
      "[239,  1600] loss: 2.303817\n",
      "[239,  1800] loss: 2.303540\n",
      "[239,  2000] loss: 2.303471\n",
      "[239,  2200] loss: 2.303259\n",
      "[239,  2400] loss: 2.303554\n",
      "[239,  2600] loss: 2.303679\n",
      "[239,  2800] loss: 2.303085\n",
      "[239,  3000] loss: 2.303224\n",
      "[239,  3200] loss: 2.303834\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[240,   200] loss: 2.303348\n",
      "[240,   400] loss: 2.303696\n",
      "[240,   600] loss: 2.303755\n",
      "[240,   800] loss: 2.303446\n",
      "[240,  1000] loss: 2.303824\n",
      "[240,  1200] loss: 2.303489\n",
      "[240,  1400] loss: 2.303838\n",
      "[240,  1600] loss: 2.303613\n",
      "[240,  1800] loss: 2.303363\n",
      "[240,  2000] loss: 2.303040\n",
      "[240,  2200] loss: 2.303587\n",
      "[240,  2400] loss: 2.303640\n",
      "[240,  2600] loss: 2.303714\n",
      "[240,  2800] loss: 2.303584\n",
      "[240,  3000] loss: 2.303732\n",
      "[240,  3200] loss: 2.303379\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[241,   200] loss: 2.302988\n",
      "[241,   400] loss: 2.303761\n",
      "[241,   600] loss: 2.303243\n",
      "[241,   800] loss: 2.303527\n",
      "[241,  1000] loss: 2.303409\n",
      "[241,  1200] loss: 2.303342\n",
      "[241,  1400] loss: 2.303570\n",
      "[241,  1600] loss: 2.303651\n",
      "[241,  1800] loss: 2.303316\n",
      "[241,  2000] loss: 2.303250\n",
      "[241,  2200] loss: 2.303398\n",
      "[241,  2400] loss: 2.303366\n",
      "[241,  2600] loss: 2.303180\n",
      "[241,  2800] loss: 2.303236\n",
      "[241,  3000] loss: 2.303295\n",
      "[241,  3200] loss: 2.303490\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[242,   200] loss: 2.303272\n",
      "[242,   400] loss: 2.303605\n",
      "[242,   600] loss: 2.303209\n",
      "[242,   800] loss: 2.303261\n",
      "[242,  1000] loss: 2.303220\n",
      "[242,  1200] loss: 2.303567\n",
      "[242,  1400] loss: 2.303779\n",
      "[242,  1600] loss: 2.303622\n",
      "[242,  1800] loss: 2.303475\n",
      "[242,  2000] loss: 2.303624\n",
      "[242,  2200] loss: 2.303058\n",
      "[242,  2400] loss: 2.303509\n",
      "[242,  2600] loss: 2.303611\n",
      "[242,  2800] loss: 2.303576\n",
      "[242,  3000] loss: 2.303037\n",
      "[242,  3200] loss: 2.303639\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[243,   200] loss: 2.303320\n",
      "[243,   400] loss: 2.303608\n",
      "[243,   600] loss: 2.303331\n",
      "[243,   800] loss: 2.303929\n",
      "[243,  1000] loss: 2.303569\n",
      "[243,  1200] loss: 2.303273\n",
      "[243,  1400] loss: 2.302572\n",
      "[243,  1600] loss: 2.303972\n",
      "[243,  1800] loss: 2.303430\n",
      "[243,  2000] loss: 2.303397\n",
      "[243,  2200] loss: 2.303467\n",
      "[243,  2400] loss: 2.303443\n",
      "[243,  2600] loss: 2.303206\n",
      "[243,  2800] loss: 2.302944\n",
      "[243,  3000] loss: 2.303962\n",
      "[243,  3200] loss: 2.303790\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[244,   200] loss: 2.303956\n",
      "[244,   400] loss: 2.303505\n",
      "[244,   600] loss: 2.303407\n",
      "[244,   800] loss: 2.303662\n",
      "[244,  1000] loss: 2.303768\n",
      "[244,  1200] loss: 2.303097\n",
      "[244,  1400] loss: 2.303596\n",
      "[244,  1600] loss: 2.303950\n",
      "[244,  1800] loss: 2.303684\n",
      "[244,  2000] loss: 2.303323\n",
      "[244,  2200] loss: 2.303333\n",
      "[244,  2400] loss: 2.303681\n",
      "[244,  2600] loss: 2.303689\n",
      "[244,  2800] loss: 2.302589\n",
      "[244,  3000] loss: 2.303757\n",
      "[244,  3200] loss: 2.303256\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[245,   200] loss: 2.303266\n",
      "[245,   400] loss: 2.303187\n",
      "[245,   600] loss: 2.304049\n",
      "[245,   800] loss: 2.303785\n",
      "[245,  1000] loss: 2.303708\n",
      "[245,  1200] loss: 2.303081\n",
      "[245,  1400] loss: 2.303702\n",
      "[245,  1600] loss: 2.303687\n",
      "[245,  1800] loss: 2.303673\n",
      "[245,  2000] loss: 2.303724\n",
      "[245,  2200] loss: 2.303444\n",
      "[245,  2400] loss: 2.303413\n",
      "[245,  2600] loss: 2.303679\n",
      "[245,  2800] loss: 2.303459\n",
      "[245,  3000] loss: 2.303640\n",
      "[245,  3200] loss: 2.303203\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[246,   200] loss: 2.303537\n",
      "[246,   400] loss: 2.303224\n",
      "[246,   600] loss: 2.303330\n",
      "[246,   800] loss: 2.303487\n",
      "[246,  1000] loss: 2.303615\n",
      "[246,  1200] loss: 2.303376\n",
      "[246,  1400] loss: 2.304006\n",
      "[246,  1600] loss: 2.303304\n",
      "[246,  1800] loss: 2.303610\n",
      "[246,  2000] loss: 2.303764\n",
      "[246,  2200] loss: 2.303795\n",
      "[246,  2400] loss: 2.303643\n",
      "[246,  2600] loss: 2.303151\n",
      "[246,  2800] loss: 2.303324\n",
      "[246,  3000] loss: 2.303335\n",
      "[246,  3200] loss: 2.303326\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[247,   200] loss: 2.303272\n",
      "[247,   400] loss: 2.303593\n",
      "[247,   600] loss: 2.303848\n",
      "[247,   800] loss: 2.303274\n",
      "[247,  1000] loss: 2.303509\n",
      "[247,  1200] loss: 2.303169\n",
      "[247,  1400] loss: 2.303397\n",
      "[247,  1600] loss: 2.303466\n",
      "[247,  1800] loss: 2.302920\n",
      "[247,  2000] loss: 2.303270\n",
      "[247,  2200] loss: 2.303413\n",
      "[247,  2400] loss: 2.303309\n",
      "[247,  2600] loss: 2.303352\n",
      "[247,  2800] loss: 2.303568\n",
      "[247,  3000] loss: 2.303441\n",
      "[247,  3200] loss: 2.303443\n",
      "Got 422 / 4000 correct (10.55)\n",
      "skip model saving\n",
      "[248,   200] loss: 2.303159\n",
      "[248,   400] loss: 2.303577\n",
      "[248,   600] loss: 2.303619\n",
      "[248,   800] loss: 2.303508\n",
      "[248,  1000] loss: 2.303415\n",
      "[248,  1200] loss: 2.303092\n",
      "[248,  1400] loss: 2.302914\n",
      "[248,  1600] loss: 2.303541\n",
      "[248,  1800] loss: 2.303679\n",
      "[248,  2000] loss: 2.303411\n",
      "[248,  2200] loss: 2.303283\n",
      "[248,  2400] loss: 2.303198\n",
      "[248,  2600] loss: 2.303410\n",
      "[248,  2800] loss: 2.303494\n",
      "[248,  3000] loss: 2.303393\n",
      "[248,  3200] loss: 2.303599\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[249,   200] loss: 2.303740\n",
      "[249,   400] loss: 2.303278\n",
      "[249,   600] loss: 2.303986\n",
      "[249,   800] loss: 2.303523\n",
      "[249,  1000] loss: 2.303377\n",
      "[249,  1200] loss: 2.303321\n",
      "[249,  1400] loss: 2.303873\n",
      "[249,  1600] loss: 2.303460\n",
      "[249,  1800] loss: 2.303882\n",
      "[249,  2000] loss: 2.303338\n",
      "[249,  2200] loss: 2.302758\n",
      "[249,  2400] loss: 2.303770\n",
      "[249,  2600] loss: 2.303649\n",
      "[249,  2800] loss: 2.303628\n",
      "[249,  3000] loss: 2.302952\n",
      "[249,  3200] loss: 2.303422\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[250,   200] loss: 2.302997\n",
      "[250,   400] loss: 2.303717\n",
      "[250,   600] loss: 2.303478\n",
      "[250,   800] loss: 2.303244\n",
      "[250,  1000] loss: 2.303557\n",
      "[250,  1200] loss: 2.303119\n",
      "[250,  1400] loss: 2.303463\n",
      "[250,  1600] loss: 2.303736\n",
      "[250,  1800] loss: 2.303911\n",
      "[250,  2000] loss: 2.303626\n",
      "[250,  2200] loss: 2.303604\n",
      "[250,  2400] loss: 2.303487\n",
      "[250,  2600] loss: 2.303702\n",
      "[250,  2800] loss: 2.303464\n",
      "[250,  3000] loss: 2.303347\n",
      "[250,  3200] loss: 2.303814\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[251,   200] loss: 2.303387\n",
      "[251,   400] loss: 2.302876\n",
      "[251,   600] loss: 2.302509\n",
      "[251,   800] loss: 2.302600\n",
      "[251,  1000] loss: 2.302383\n",
      "[251,  1200] loss: 2.302784\n",
      "[251,  1400] loss: 2.302580\n",
      "[251,  1600] loss: 2.302615\n",
      "[251,  1800] loss: 2.302699\n",
      "[251,  2000] loss: 2.302663\n",
      "[251,  2200] loss: 2.302729\n",
      "[251,  2400] loss: 2.302631\n",
      "[251,  2600] loss: 2.302739\n",
      "[251,  2800] loss: 2.302651\n",
      "[251,  3000] loss: 2.302579\n",
      "[251,  3200] loss: 2.302596\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[252,   200] loss: 2.302585\n",
      "[252,   400] loss: 2.302756\n",
      "[252,   600] loss: 2.302521\n",
      "[252,   800] loss: 2.302646\n",
      "[252,  1000] loss: 2.302647\n",
      "[252,  1200] loss: 2.302720\n",
      "[252,  1400] loss: 2.302740\n",
      "[252,  1600] loss: 2.302668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[252,  1800] loss: 2.302477\n",
      "[252,  2000] loss: 2.302668\n",
      "[252,  2200] loss: 2.302867\n",
      "[252,  2400] loss: 2.302547\n",
      "[252,  2600] loss: 2.302794\n",
      "[252,  2800] loss: 2.302594\n",
      "[252,  3000] loss: 2.302658\n",
      "[252,  3200] loss: 2.302647\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[253,   200] loss: 2.302682\n",
      "[253,   400] loss: 2.302760\n",
      "[253,   600] loss: 2.302672\n",
      "[253,   800] loss: 2.302640\n",
      "[253,  1000] loss: 2.302510\n",
      "[253,  1200] loss: 2.302561\n",
      "[253,  1400] loss: 2.302572\n",
      "[253,  1600] loss: 2.302355\n",
      "[253,  1800] loss: 2.302673\n",
      "[253,  2000] loss: 2.302879\n",
      "[253,  2200] loss: 2.302550\n",
      "[253,  2400] loss: 2.302370\n",
      "[253,  2600] loss: 2.302972\n",
      "[253,  2800] loss: 2.302588\n",
      "[253,  3000] loss: 2.302754\n",
      "[253,  3200] loss: 2.302674\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[254,   200] loss: 2.302616\n",
      "[254,   400] loss: 2.302718\n",
      "[254,   600] loss: 2.302773\n",
      "[254,   800] loss: 2.302493\n",
      "[254,  1000] loss: 2.302677\n",
      "[254,  1200] loss: 2.302642\n",
      "[254,  1400] loss: 2.302663\n",
      "[254,  1600] loss: 2.302681\n",
      "[254,  1800] loss: 2.302765\n",
      "[254,  2000] loss: 2.302607\n",
      "[254,  2200] loss: 2.302433\n",
      "[254,  2400] loss: 2.302416\n",
      "[254,  2600] loss: 2.302756\n",
      "[254,  2800] loss: 2.302379\n",
      "[254,  3000] loss: 2.302856\n",
      "[254,  3200] loss: 2.302793\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[255,   200] loss: 2.302646\n",
      "[255,   400] loss: 2.302641\n",
      "[255,   600] loss: 2.302429\n",
      "[255,   800] loss: 2.302855\n",
      "[255,  1000] loss: 2.302522\n",
      "[255,  1200] loss: 2.302877\n",
      "[255,  1400] loss: 2.302605\n",
      "[255,  1600] loss: 2.302750\n",
      "[255,  1800] loss: 2.302669\n",
      "[255,  2000] loss: 2.302695\n",
      "[255,  2200] loss: 2.302468\n",
      "[255,  2400] loss: 2.302654\n",
      "[255,  2600] loss: 2.302457\n",
      "[255,  2800] loss: 2.302932\n",
      "[255,  3000] loss: 2.302567\n",
      "[255,  3200] loss: 2.302630\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[256,   200] loss: 2.302756\n",
      "[256,   400] loss: 2.302457\n",
      "[256,   600] loss: 2.302676\n",
      "[256,   800] loss: 2.302798\n",
      "[256,  1000] loss: 2.302568\n",
      "[256,  1200] loss: 2.302651\n",
      "[256,  1400] loss: 2.302613\n",
      "[256,  1600] loss: 2.302732\n",
      "[256,  1800] loss: 2.302742\n",
      "[256,  2000] loss: 2.302677\n",
      "[256,  2200] loss: 2.302615\n",
      "[256,  2400] loss: 2.302698\n",
      "[256,  2600] loss: 2.302598\n",
      "[256,  2800] loss: 2.302762\n",
      "[256,  3000] loss: 2.302627\n",
      "[256,  3200] loss: 2.302746\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[257,   200] loss: 2.302699\n",
      "[257,   400] loss: 2.302689\n",
      "[257,   600] loss: 2.302443\n",
      "[257,   800] loss: 2.302699\n",
      "[257,  1000] loss: 2.302791\n",
      "[257,  1200] loss: 2.302678\n",
      "[257,  1400] loss: 2.302594\n",
      "[257,  1600] loss: 2.302672\n",
      "[257,  1800] loss: 2.302250\n",
      "[257,  2000] loss: 2.302879\n",
      "[257,  2200] loss: 2.302470\n",
      "[257,  2400] loss: 2.302861\n",
      "[257,  2600] loss: 2.302490\n",
      "[257,  2800] loss: 2.302585\n",
      "[257,  3000] loss: 2.302891\n",
      "[257,  3200] loss: 2.302690\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[258,   200] loss: 2.302522\n",
      "[258,   400] loss: 2.302409\n",
      "[258,   600] loss: 2.302794\n",
      "[258,   800] loss: 2.302762\n",
      "[258,  1000] loss: 2.302611\n",
      "[258,  1200] loss: 2.302356\n",
      "[258,  1400] loss: 2.302732\n",
      "[258,  1600] loss: 2.302806\n",
      "[258,  1800] loss: 2.302688\n",
      "[258,  2000] loss: 2.302466\n",
      "[258,  2200] loss: 2.302839\n",
      "[258,  2400] loss: 2.302610\n",
      "[258,  2600] loss: 2.302516\n",
      "[258,  2800] loss: 2.302788\n",
      "[258,  3000] loss: 2.302719\n",
      "[258,  3200] loss: 2.302643\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[259,   200] loss: 2.302749\n",
      "[259,   400] loss: 2.302589\n",
      "[259,   600] loss: 2.302551\n",
      "[259,   800] loss: 2.302310\n",
      "[259,  1000] loss: 2.302796\n",
      "[259,  1200] loss: 2.302617\n",
      "[259,  1400] loss: 2.302742\n",
      "[259,  1600] loss: 2.302593\n",
      "[259,  1800] loss: 2.302600\n",
      "[259,  2000] loss: 2.302651\n",
      "[259,  2200] loss: 2.302756\n",
      "[259,  2400] loss: 2.302764\n",
      "[259,  2600] loss: 2.302666\n",
      "[259,  2800] loss: 2.302751\n",
      "[259,  3000] loss: 2.302597\n",
      "[259,  3200] loss: 2.302648\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[260,   200] loss: 2.302474\n",
      "[260,   400] loss: 2.302652\n",
      "[260,   600] loss: 2.302552\n",
      "[260,   800] loss: 2.302810\n",
      "[260,  1000] loss: 2.302786\n",
      "[260,  1200] loss: 2.302648\n",
      "[260,  1400] loss: 2.302598\n",
      "[260,  1600] loss: 2.302642\n",
      "[260,  1800] loss: 2.302843\n",
      "[260,  2000] loss: 2.302530\n",
      "[260,  2200] loss: 2.302786\n",
      "[260,  2400] loss: 2.302641\n",
      "[260,  2600] loss: 2.302581\n",
      "[260,  2800] loss: 2.302479\n",
      "[260,  3000] loss: 2.302563\n",
      "[260,  3200] loss: 2.302776\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[261,   200] loss: 2.302528\n",
      "[261,   400] loss: 2.302519\n",
      "[261,   600] loss: 2.302772\n",
      "[261,   800] loss: 2.302634\n",
      "[261,  1000] loss: 2.302646\n",
      "[261,  1200] loss: 2.302401\n",
      "[261,  1400] loss: 2.302969\n",
      "[261,  1600] loss: 2.302665\n",
      "[261,  1800] loss: 2.302552\n",
      "[261,  2000] loss: 2.302713\n",
      "[261,  2200] loss: 2.302661\n",
      "[261,  2400] loss: 2.302700\n",
      "[261,  2600] loss: 2.302319\n",
      "[261,  2800] loss: 2.302627\n",
      "[261,  3000] loss: 2.302784\n",
      "[261,  3200] loss: 2.302632\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[262,   200] loss: 2.302552\n",
      "[262,   400] loss: 2.302549\n",
      "[262,   600] loss: 2.302776\n",
      "[262,   800] loss: 2.302723\n",
      "[262,  1000] loss: 2.302711\n",
      "[262,  1200] loss: 2.302715\n",
      "[262,  1400] loss: 2.302685\n",
      "[262,  1600] loss: 2.302548\n",
      "[262,  1800] loss: 2.302666\n",
      "[262,  2000] loss: 2.302697\n",
      "[262,  2200] loss: 2.302543\n",
      "[262,  2400] loss: 2.302504\n",
      "[262,  2600] loss: 2.302730\n",
      "[262,  2800] loss: 2.302558\n",
      "[262,  3000] loss: 2.302817\n",
      "[262,  3200] loss: 2.302660\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[263,   200] loss: 2.302790\n",
      "[263,   400] loss: 2.302748\n",
      "[263,   600] loss: 2.302741\n",
      "[263,   800] loss: 2.302471\n",
      "[263,  1000] loss: 2.302425\n",
      "[263,  1200] loss: 2.302550\n",
      "[263,  1400] loss: 2.302562\n",
      "[263,  1600] loss: 2.302934\n",
      "[263,  1800] loss: 2.302685\n",
      "[263,  2000] loss: 2.302601\n",
      "[263,  2200] loss: 2.302809\n",
      "[263,  2400] loss: 2.302585\n",
      "[263,  2600] loss: 2.302480\n",
      "[263,  2800] loss: 2.302777\n",
      "[263,  3000] loss: 2.302726\n",
      "[263,  3200] loss: 2.302522\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[264,   200] loss: 2.302747\n",
      "[264,   400] loss: 2.302633\n",
      "[264,   600] loss: 2.302799\n",
      "[264,   800] loss: 2.302644\n",
      "[264,  1000] loss: 2.302688\n",
      "[264,  1200] loss: 2.302506\n",
      "[264,  1400] loss: 2.302589\n",
      "[264,  1600] loss: 2.302383\n",
      "[264,  1800] loss: 2.302479\n",
      "[264,  2000] loss: 2.302852\n",
      "[264,  2200] loss: 2.302766\n",
      "[264,  2400] loss: 2.302801\n",
      "[264,  2600] loss: 2.302666\n",
      "[264,  2800] loss: 2.302456\n",
      "[264,  3000] loss: 2.302474\n",
      "[264,  3200] loss: 2.302693\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[265,   200] loss: 2.302708\n",
      "[265,   400] loss: 2.302620\n",
      "[265,   600] loss: 2.302367\n",
      "[265,   800] loss: 2.302989\n",
      "[265,  1000] loss: 2.302427\n",
      "[265,  1200] loss: 2.302673\n",
      "[265,  1400] loss: 2.302685\n",
      "[265,  1600] loss: 2.302452\n",
      "[265,  1800] loss: 2.302731\n",
      "[265,  2000] loss: 2.302552\n",
      "[265,  2200] loss: 2.302825\n",
      "[265,  2400] loss: 2.302694\n",
      "[265,  2600] loss: 2.302476\n",
      "[265,  2800] loss: 2.302741\n",
      "[265,  3000] loss: 2.302676\n",
      "[265,  3200] loss: 2.302728\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[266,   200] loss: 2.302672\n",
      "[266,   400] loss: 2.302680\n",
      "[266,   600] loss: 2.302637\n",
      "[266,   800] loss: 2.302616\n",
      "[266,  1000] loss: 2.302573\n",
      "[266,  1200] loss: 2.302620\n",
      "[266,  1400] loss: 2.302687\n",
      "[266,  1600] loss: 2.302376\n",
      "[266,  1800] loss: 2.302696\n",
      "[266,  2000] loss: 2.302560\n",
      "[266,  2200] loss: 2.302681\n",
      "[266,  2400] loss: 2.302809\n",
      "[266,  2600] loss: 2.302681\n",
      "[266,  2800] loss: 2.302717\n",
      "[266,  3000] loss: 2.302539\n",
      "[266,  3200] loss: 2.302771\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[267,   200] loss: 2.302648\n",
      "[267,   400] loss: 2.302665\n",
      "[267,   600] loss: 2.302432\n",
      "[267,   800] loss: 2.302435\n",
      "[267,  1000] loss: 2.302546\n",
      "[267,  1200] loss: 2.302819\n",
      "[267,  1400] loss: 2.302731\n",
      "[267,  1600] loss: 2.302692\n",
      "[267,  1800] loss: 2.302819\n",
      "[267,  2000] loss: 2.302652\n",
      "[267,  2200] loss: 2.302756\n",
      "[267,  2400] loss: 2.302550\n",
      "[267,  2600] loss: 2.302542\n",
      "[267,  2800] loss: 2.302660\n",
      "[267,  3000] loss: 2.302621\n",
      "[267,  3200] loss: 2.302606\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[268,   200] loss: 2.302544\n",
      "[268,   400] loss: 2.302694\n",
      "[268,   600] loss: 2.302730\n",
      "[268,   800] loss: 2.302611\n",
      "[268,  1000] loss: 2.302659\n",
      "[268,  1200] loss: 2.302635\n",
      "[268,  1400] loss: 2.302588\n",
      "[268,  1600] loss: 2.302495\n",
      "[268,  1800] loss: 2.302687\n",
      "[268,  2000] loss: 2.302401\n",
      "[268,  2200] loss: 2.302724\n",
      "[268,  2400] loss: 2.302710\n",
      "[268,  2600] loss: 2.302331\n",
      "[268,  2800] loss: 2.302802\n",
      "[268,  3000] loss: 2.302955\n",
      "[268,  3200] loss: 2.302563\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269,   200] loss: 2.302644\n",
      "[269,   400] loss: 2.302828\n",
      "[269,   600] loss: 2.302673\n",
      "[269,   800] loss: 2.302615\n",
      "[269,  1000] loss: 2.302539\n",
      "[269,  1200] loss: 2.302472\n",
      "[269,  1400] loss: 2.302909\n",
      "[269,  1600] loss: 2.302360\n",
      "[269,  1800] loss: 2.302633\n",
      "[269,  2000] loss: 2.302562\n",
      "[269,  2200] loss: 2.302660\n",
      "[269,  2400] loss: 2.302796\n",
      "[269,  2600] loss: 2.302520\n",
      "[269,  2800] loss: 2.302743\n",
      "[269,  3000] loss: 2.302646\n",
      "[269,  3200] loss: 2.302584\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[270,   200] loss: 2.302774\n",
      "[270,   400] loss: 2.302440\n",
      "[270,   600] loss: 2.302557\n",
      "[270,   800] loss: 2.302565\n",
      "[270,  1000] loss: 2.302629\n",
      "[270,  1200] loss: 2.302307\n",
      "[270,  1400] loss: 2.302891\n",
      "[270,  1600] loss: 2.302686\n",
      "[270,  1800] loss: 2.302571\n",
      "[270,  2000] loss: 2.302771\n",
      "[270,  2200] loss: 2.302449\n",
      "[270,  2400] loss: 2.302227\n",
      "[270,  2600] loss: 2.303019\n",
      "[270,  2800] loss: 2.302805\n",
      "[270,  3000] loss: 2.302694\n",
      "[270,  3200] loss: 2.302626\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[271,   200] loss: 2.302520\n",
      "[271,   400] loss: 2.302474\n",
      "[271,   600] loss: 2.302790\n",
      "[271,   800] loss: 2.302533\n",
      "[271,  1000] loss: 2.302628\n",
      "[271,  1200] loss: 2.302716\n",
      "[271,  1400] loss: 2.302673\n",
      "[271,  1600] loss: 2.302441\n",
      "[271,  1800] loss: 2.302755\n",
      "[271,  2000] loss: 2.302514\n",
      "[271,  2200] loss: 2.302551\n",
      "[271,  2400] loss: 2.302776\n",
      "[271,  2600] loss: 2.302576\n",
      "[271,  2800] loss: 2.302696\n",
      "[271,  3000] loss: 2.302604\n",
      "[271,  3200] loss: 2.302791\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[272,   200] loss: 2.302756\n",
      "[272,   400] loss: 2.302356\n",
      "[272,   600] loss: 2.302574\n",
      "[272,   800] loss: 2.302631\n",
      "[272,  1000] loss: 2.302702\n",
      "[272,  1200] loss: 2.302741\n",
      "[272,  1400] loss: 2.302691\n",
      "[272,  1600] loss: 2.302760\n",
      "[272,  1800] loss: 2.302738\n",
      "[272,  2000] loss: 2.302596\n",
      "[272,  2200] loss: 2.302678\n",
      "[272,  2400] loss: 2.302676\n",
      "[272,  2600] loss: 2.302629\n",
      "[272,  2800] loss: 2.302727\n",
      "[272,  3000] loss: 2.302620\n",
      "[272,  3200] loss: 2.302523\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[273,   200] loss: 2.302598\n",
      "[273,   400] loss: 2.302772\n",
      "[273,   600] loss: 2.302680\n",
      "[273,   800] loss: 2.302398\n",
      "[273,  1000] loss: 2.302366\n",
      "[273,  1200] loss: 2.302782\n",
      "[273,  1400] loss: 2.302967\n",
      "[273,  1600] loss: 2.302611\n",
      "[273,  1800] loss: 2.302702\n",
      "[273,  2000] loss: 2.302753\n",
      "[273,  2200] loss: 2.302386\n",
      "[273,  2400] loss: 2.302550\n",
      "[273,  2600] loss: 2.302615\n",
      "[273,  2800] loss: 2.302686\n",
      "[273,  3000] loss: 2.302743\n",
      "[273,  3200] loss: 2.302677\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[274,   200] loss: 2.302595\n",
      "[274,   400] loss: 2.302726\n",
      "[274,   600] loss: 2.302557\n",
      "[274,   800] loss: 2.302475\n",
      "[274,  1000] loss: 2.302921\n",
      "[274,  1200] loss: 2.302531\n",
      "[274,  1400] loss: 2.302460\n",
      "[274,  1600] loss: 2.302696\n",
      "[274,  1800] loss: 2.302677\n",
      "[274,  2000] loss: 2.302734\n",
      "[274,  2200] loss: 2.302636\n",
      "[274,  2400] loss: 2.302683\n",
      "[274,  2600] loss: 2.302769\n",
      "[274,  2800] loss: 2.302734\n",
      "[274,  3000] loss: 2.302637\n",
      "[274,  3200] loss: 2.302809\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[275,   200] loss: 2.302705\n",
      "[275,   400] loss: 2.302431\n",
      "[275,   600] loss: 2.302709\n",
      "[275,   800] loss: 2.302743\n",
      "[275,  1000] loss: 2.302562\n",
      "[275,  1200] loss: 2.302522\n",
      "[275,  1400] loss: 2.302769\n",
      "[275,  1600] loss: 2.302580\n",
      "[275,  1800] loss: 2.302724\n",
      "[275,  2000] loss: 2.302643\n",
      "[275,  2200] loss: 2.302590\n",
      "[275,  2400] loss: 2.302735\n",
      "[275,  2600] loss: 2.302516\n",
      "[275,  2800] loss: 2.302691\n",
      "[275,  3000] loss: 2.302696\n",
      "[275,  3200] loss: 2.302716\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[276,   200] loss: 2.302732\n",
      "[276,   400] loss: 2.302752\n",
      "[276,   600] loss: 2.302606\n",
      "[276,   800] loss: 2.302558\n",
      "[276,  1000] loss: 2.302725\n",
      "[276,  1200] loss: 2.302745\n",
      "[276,  1400] loss: 2.302833\n",
      "[276,  1600] loss: 2.302628\n",
      "[276,  1800] loss: 2.302481\n",
      "[276,  2000] loss: 2.302642\n",
      "[276,  2200] loss: 2.302686\n",
      "[276,  2400] loss: 2.302499\n",
      "[276,  2600] loss: 2.302687\n",
      "[276,  2800] loss: 2.302499\n",
      "[276,  3000] loss: 2.302644\n",
      "[276,  3200] loss: 2.302614\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[277,   200] loss: 2.302724\n",
      "[277,   400] loss: 2.302517\n",
      "[277,   600] loss: 2.302817\n",
      "[277,   800] loss: 2.302575\n",
      "[277,  1000] loss: 2.302714\n",
      "[277,  1200] loss: 2.302556\n",
      "[277,  1400] loss: 2.302721\n",
      "[277,  1600] loss: 2.302769\n",
      "[277,  1800] loss: 2.302680\n",
      "[277,  2000] loss: 2.302721\n",
      "[277,  2200] loss: 2.302468\n",
      "[277,  2400] loss: 2.302759\n",
      "[277,  2600] loss: 2.302507\n",
      "[277,  2800] loss: 2.302481\n",
      "[277,  3000] loss: 2.302695\n",
      "[277,  3200] loss: 2.302650\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[278,   200] loss: 2.302703\n",
      "[278,   400] loss: 2.302528\n",
      "[278,   600] loss: 2.302487\n",
      "[278,   800] loss: 2.302713\n",
      "[278,  1000] loss: 2.302796\n",
      "[278,  1200] loss: 2.302665\n",
      "[278,  1400] loss: 2.302320\n",
      "[278,  1600] loss: 2.302860\n",
      "[278,  1800] loss: 2.302792\n",
      "[278,  2000] loss: 2.302748\n",
      "[278,  2200] loss: 2.302556\n",
      "[278,  2400] loss: 2.302722\n",
      "[278,  2600] loss: 2.302633\n",
      "[278,  2800] loss: 2.302685\n",
      "[278,  3000] loss: 2.302719\n",
      "[278,  3200] loss: 2.302663\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[279,   200] loss: 2.302761\n",
      "[279,   400] loss: 2.302263\n",
      "[279,   600] loss: 2.302632\n",
      "[279,   800] loss: 2.302619\n",
      "[279,  1000] loss: 2.302695\n",
      "[279,  1200] loss: 2.302668\n",
      "[279,  1400] loss: 2.302657\n",
      "[279,  1600] loss: 2.302629\n",
      "[279,  1800] loss: 2.302833\n",
      "[279,  2000] loss: 2.302578\n",
      "[279,  2200] loss: 2.302797\n",
      "[279,  2400] loss: 2.302691\n",
      "[279,  2600] loss: 2.302678\n",
      "[279,  2800] loss: 2.302542\n",
      "[279,  3000] loss: 2.302619\n",
      "[279,  3200] loss: 2.302611\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[280,   200] loss: 2.302817\n",
      "[280,   400] loss: 2.302599\n",
      "[280,   600] loss: 2.302761\n",
      "[280,   800] loss: 2.302738\n",
      "[280,  1000] loss: 2.302588\n",
      "[280,  1200] loss: 2.302747\n",
      "[280,  1400] loss: 2.302539\n",
      "[280,  1600] loss: 2.302784\n",
      "[280,  1800] loss: 2.302588\n",
      "[280,  2000] loss: 2.302492\n",
      "[280,  2200] loss: 2.302749\n",
      "[280,  2400] loss: 2.302650\n",
      "[280,  2600] loss: 2.302641\n",
      "[280,  2800] loss: 2.302704\n",
      "[280,  3000] loss: 2.302610\n",
      "[280,  3200] loss: 2.302702\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[281,   200] loss: 2.302702\n",
      "[281,   400] loss: 2.302663\n",
      "[281,   600] loss: 2.302679\n",
      "[281,   800] loss: 2.302709\n",
      "[281,  1000] loss: 2.302645\n",
      "[281,  1200] loss: 2.302732\n",
      "[281,  1400] loss: 2.302594\n",
      "[281,  1600] loss: 2.302629\n",
      "[281,  1800] loss: 2.302586\n",
      "[281,  2000] loss: 2.302707\n",
      "[281,  2200] loss: 2.302379\n",
      "[281,  2400] loss: 2.302846\n",
      "[281,  2600] loss: 2.302584\n",
      "[281,  2800] loss: 2.302507\n",
      "[281,  3000] loss: 2.302952\n",
      "[281,  3200] loss: 2.302524\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[282,   200] loss: 2.302650\n",
      "[282,   400] loss: 2.302678\n",
      "[282,   600] loss: 2.302707\n",
      "[282,   800] loss: 2.302684\n",
      "[282,  1000] loss: 2.302515\n",
      "[282,  1200] loss: 2.302688\n",
      "[282,  1400] loss: 2.302561\n",
      "[282,  1600] loss: 2.302681\n",
      "[282,  1800] loss: 2.302851\n",
      "[282,  2000] loss: 2.302457\n",
      "[282,  2200] loss: 2.302772\n",
      "[282,  2400] loss: 2.302523\n",
      "[282,  2600] loss: 2.302647\n",
      "[282,  2800] loss: 2.302345\n",
      "[282,  3000] loss: 2.302782\n",
      "[282,  3200] loss: 2.302618\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[283,   200] loss: 2.302504\n",
      "[283,   400] loss: 2.302648\n",
      "[283,   600] loss: 2.302511\n",
      "[283,   800] loss: 2.302576\n",
      "[283,  1000] loss: 2.302818\n",
      "[283,  1200] loss: 2.302718\n",
      "[283,  1400] loss: 2.302617\n",
      "[283,  1600] loss: 2.302594\n",
      "[283,  1800] loss: 2.302822\n",
      "[283,  2000] loss: 2.302404\n",
      "[283,  2200] loss: 2.302905\n",
      "[283,  2400] loss: 2.302388\n",
      "[283,  2600] loss: 2.302945\n",
      "[283,  2800] loss: 2.302629\n",
      "[283,  3000] loss: 2.302789\n",
      "[283,  3200] loss: 2.302677\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[284,   200] loss: 2.302717\n",
      "[284,   400] loss: 2.302634\n",
      "[284,   600] loss: 2.302791\n",
      "[284,   800] loss: 2.302630\n",
      "[284,  1000] loss: 2.302584\n",
      "[284,  1200] loss: 2.302562\n",
      "[284,  1400] loss: 2.302924\n",
      "[284,  1600] loss: 2.302736\n",
      "[284,  1800] loss: 2.302573\n",
      "[284,  2000] loss: 2.302510\n",
      "[284,  2200] loss: 2.302569\n",
      "[284,  2400] loss: 2.302414\n",
      "[284,  2600] loss: 2.302507\n",
      "[284,  2800] loss: 2.302678\n",
      "[284,  3000] loss: 2.302720\n",
      "[284,  3200] loss: 2.302492\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[285,   200] loss: 2.302466\n",
      "[285,   400] loss: 2.302822\n",
      "[285,   600] loss: 2.302754\n",
      "[285,   800] loss: 2.302673\n",
      "[285,  1000] loss: 2.302682\n",
      "[285,  1200] loss: 2.302789\n",
      "[285,  1400] loss: 2.302606\n",
      "[285,  1600] loss: 2.302626\n",
      "[285,  1800] loss: 2.302668\n",
      "[285,  2000] loss: 2.302672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285,  2200] loss: 2.302643\n",
      "[285,  2400] loss: 2.302779\n",
      "[285,  2600] loss: 2.302803\n",
      "[285,  2800] loss: 2.302427\n",
      "[285,  3000] loss: 2.302689\n",
      "[285,  3200] loss: 2.302500\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[286,   200] loss: 2.302418\n",
      "[286,   400] loss: 2.302592\n",
      "[286,   600] loss: 2.302635\n",
      "[286,   800] loss: 2.302446\n",
      "[286,  1000] loss: 2.302818\n",
      "[286,  1200] loss: 2.302750\n",
      "[286,  1400] loss: 2.302710\n",
      "[286,  1600] loss: 2.302753\n",
      "[286,  1800] loss: 2.302779\n",
      "[286,  2000] loss: 2.302630\n",
      "[286,  2200] loss: 2.302627\n",
      "[286,  2400] loss: 2.302581\n",
      "[286,  2600] loss: 2.302655\n",
      "[286,  2800] loss: 2.302628\n",
      "[286,  3000] loss: 2.302710\n",
      "[286,  3200] loss: 2.302604\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[287,   200] loss: 2.302575\n",
      "[287,   400] loss: 2.302567\n",
      "[287,   600] loss: 2.302636\n",
      "[287,   800] loss: 2.302659\n",
      "[287,  1000] loss: 2.302574\n",
      "[287,  1200] loss: 2.302806\n",
      "[287,  1400] loss: 2.302281\n",
      "[287,  1600] loss: 2.302869\n",
      "[287,  1800] loss: 2.302428\n",
      "[287,  2000] loss: 2.302748\n",
      "[287,  2200] loss: 2.302549\n",
      "[287,  2400] loss: 2.302844\n",
      "[287,  2600] loss: 2.302607\n",
      "[287,  2800] loss: 2.302656\n",
      "[287,  3000] loss: 2.302766\n",
      "[287,  3200] loss: 2.302605\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[288,   200] loss: 2.302464\n",
      "[288,   400] loss: 2.302860\n",
      "[288,   600] loss: 2.302532\n",
      "[288,   800] loss: 2.302633\n",
      "[288,  1000] loss: 2.302707\n",
      "[288,  1200] loss: 2.302732\n",
      "[288,  1400] loss: 2.302623\n",
      "[288,  1600] loss: 2.302777\n",
      "[288,  1800] loss: 2.302638\n",
      "[288,  2000] loss: 2.302480\n",
      "[288,  2200] loss: 2.302673\n",
      "[288,  2400] loss: 2.302650\n",
      "[288,  2600] loss: 2.302570\n",
      "[288,  2800] loss: 2.302919\n",
      "[288,  3000] loss: 2.302500\n",
      "[288,  3200] loss: 2.302815\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[289,   200] loss: 2.302598\n",
      "[289,   400] loss: 2.302620\n",
      "[289,   600] loss: 2.302774\n",
      "[289,   800] loss: 2.302631\n",
      "[289,  1000] loss: 2.302762\n",
      "[289,  1200] loss: 2.302564\n",
      "[289,  1400] loss: 2.302686\n",
      "[289,  1600] loss: 2.302676\n",
      "[289,  1800] loss: 2.302731\n",
      "[289,  2000] loss: 2.302514\n",
      "[289,  2200] loss: 2.302759\n",
      "[289,  2400] loss: 2.302679\n",
      "[289,  2600] loss: 2.302550\n",
      "[289,  2800] loss: 2.302595\n",
      "[289,  3000] loss: 2.302208\n",
      "[289,  3200] loss: 2.302741\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[290,   200] loss: 2.302553\n",
      "[290,   400] loss: 2.302718\n",
      "[290,   600] loss: 2.302458\n",
      "[290,   800] loss: 2.302706\n",
      "[290,  1000] loss: 2.302676\n",
      "[290,  1200] loss: 2.302777\n",
      "[290,  1400] loss: 2.302628\n",
      "[290,  1600] loss: 2.302514\n",
      "[290,  1800] loss: 2.302755\n",
      "[290,  2000] loss: 2.302668\n",
      "[290,  2200] loss: 2.302467\n",
      "[290,  2400] loss: 2.302677\n",
      "[290,  2600] loss: 2.302612\n",
      "[290,  2800] loss: 2.302653\n",
      "[290,  3000] loss: 2.302695\n",
      "[290,  3200] loss: 2.302738\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[291,   200] loss: 2.302624\n",
      "[291,   400] loss: 2.302850\n",
      "[291,   600] loss: 2.302620\n",
      "[291,   800] loss: 2.302473\n",
      "[291,  1000] loss: 2.302661\n",
      "[291,  1200] loss: 2.302721\n",
      "[291,  1400] loss: 2.302567\n",
      "[291,  1600] loss: 2.302785\n",
      "[291,  1800] loss: 2.302634\n",
      "[291,  2000] loss: 2.302716\n",
      "[291,  2200] loss: 2.302261\n",
      "[291,  2400] loss: 2.302801\n",
      "[291,  2600] loss: 2.302664\n",
      "[291,  2800] loss: 2.302607\n",
      "[291,  3000] loss: 2.302291\n",
      "[291,  3200] loss: 2.302951\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[292,   200] loss: 2.302791\n",
      "[292,   400] loss: 2.302650\n",
      "[292,   600] loss: 2.302525\n",
      "[292,   800] loss: 2.302537\n",
      "[292,  1000] loss: 2.302973\n",
      "[292,  1200] loss: 2.302571\n",
      "[292,  1400] loss: 2.302550\n",
      "[292,  1600] loss: 2.302522\n",
      "[292,  1800] loss: 2.302488\n",
      "[292,  2000] loss: 2.302628\n",
      "[292,  2200] loss: 2.302826\n",
      "[292,  2400] loss: 2.302562\n",
      "[292,  2600] loss: 2.302758\n",
      "[292,  2800] loss: 2.302681\n",
      "[292,  3000] loss: 2.302387\n",
      "[292,  3200] loss: 2.302690\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[293,   200] loss: 2.302581\n",
      "[293,   400] loss: 2.302369\n",
      "[293,   600] loss: 2.302585\n",
      "[293,   800] loss: 2.302571\n",
      "[293,  1000] loss: 2.302615\n",
      "[293,  1200] loss: 2.302465\n",
      "[293,  1400] loss: 2.302739\n",
      "[293,  1600] loss: 2.302599\n",
      "[293,  1800] loss: 2.302820\n",
      "[293,  2000] loss: 2.302721\n",
      "[293,  2200] loss: 2.302634\n",
      "[293,  2400] loss: 2.302614\n",
      "[293,  2600] loss: 2.302560\n",
      "[293,  2800] loss: 2.302749\n",
      "[293,  3000] loss: 2.302598\n",
      "[293,  3200] loss: 2.302669\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[294,   200] loss: 2.302585\n",
      "[294,   400] loss: 2.302640\n",
      "[294,   600] loss: 2.302820\n",
      "[294,   800] loss: 2.302545\n",
      "[294,  1000] loss: 2.302762\n",
      "[294,  1200] loss: 2.302674\n",
      "[294,  1400] loss: 2.302649\n",
      "[294,  1600] loss: 2.302735\n",
      "[294,  1800] loss: 2.302770\n",
      "[294,  2000] loss: 2.302483\n",
      "[294,  2200] loss: 2.302469\n",
      "[294,  2400] loss: 2.302583\n",
      "[294,  2600] loss: 2.302899\n",
      "[294,  2800] loss: 2.302680\n",
      "[294,  3000] loss: 2.302381\n",
      "[294,  3200] loss: 2.302568\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[295,   200] loss: 2.302708\n",
      "[295,   400] loss: 2.302783\n",
      "[295,   600] loss: 2.302587\n",
      "[295,   800] loss: 2.302489\n",
      "[295,  1000] loss: 2.302570\n",
      "[295,  1200] loss: 2.302738\n",
      "[295,  1400] loss: 2.302728\n",
      "[295,  1600] loss: 2.302782\n",
      "[295,  1800] loss: 2.302488\n",
      "[295,  2000] loss: 2.302518\n",
      "[295,  2200] loss: 2.302707\n",
      "[295,  2400] loss: 2.302652\n",
      "[295,  2600] loss: 2.302571\n",
      "[295,  2800] loss: 2.302626\n",
      "[295,  3000] loss: 2.302475\n",
      "[295,  3200] loss: 2.302763\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[296,   200] loss: 2.302603\n",
      "[296,   400] loss: 2.302514\n",
      "[296,   600] loss: 2.302803\n",
      "[296,   800] loss: 2.302869\n",
      "[296,  1000] loss: 2.302671\n",
      "[296,  1200] loss: 2.302706\n",
      "[296,  1400] loss: 2.302673\n",
      "[296,  1600] loss: 2.302744\n",
      "[296,  1800] loss: 2.302381\n",
      "[296,  2000] loss: 2.302624\n",
      "[296,  2200] loss: 2.302579\n",
      "[296,  2400] loss: 2.302356\n",
      "[296,  2600] loss: 2.302927\n",
      "[296,  2800] loss: 2.302475\n",
      "[296,  3000] loss: 2.302614\n",
      "[296,  3200] loss: 2.302852\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[297,   200] loss: 2.302805\n",
      "[297,   400] loss: 2.302642\n",
      "[297,   600] loss: 2.302628\n",
      "[297,   800] loss: 2.302615\n",
      "[297,  1000] loss: 2.302659\n",
      "[297,  1200] loss: 2.302741\n",
      "[297,  1400] loss: 2.302573\n",
      "[297,  1600] loss: 2.302589\n",
      "[297,  1800] loss: 2.302789\n",
      "[297,  2000] loss: 2.302464\n",
      "[297,  2200] loss: 2.302804\n",
      "[297,  2400] loss: 2.302741\n",
      "[297,  2600] loss: 2.302564\n",
      "[297,  2800] loss: 2.302696\n",
      "[297,  3000] loss: 2.302590\n",
      "[297,  3200] loss: 2.302578\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[298,   200] loss: 2.302533\n",
      "[298,   400] loss: 2.302730\n",
      "[298,   600] loss: 2.302745\n",
      "[298,   800] loss: 2.302491\n",
      "[298,  1000] loss: 2.302561\n",
      "[298,  1200] loss: 2.302912\n",
      "[298,  1400] loss: 2.302591\n",
      "[298,  1600] loss: 2.302813\n",
      "[298,  1800] loss: 2.302491\n",
      "[298,  2000] loss: 2.302613\n",
      "[298,  2200] loss: 2.302535\n",
      "[298,  2400] loss: 2.302524\n",
      "[298,  2600] loss: 2.302633\n",
      "[298,  2800] loss: 2.302559\n",
      "[298,  3000] loss: 2.302999\n",
      "[298,  3200] loss: 2.302597\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[299,   200] loss: 2.302381\n",
      "[299,   400] loss: 2.302330\n",
      "[299,   600] loss: 2.302904\n",
      "[299,   800] loss: 2.302319\n",
      "[299,  1000] loss: 2.302759\n",
      "[299,  1200] loss: 2.302771\n",
      "[299,  1400] loss: 2.302690\n",
      "[299,  1600] loss: 2.302534\n",
      "[299,  1800] loss: 2.302891\n",
      "[299,  2000] loss: 2.302508\n",
      "[299,  2200] loss: 2.302632\n",
      "[299,  2400] loss: 2.302782\n",
      "[299,  2600] loss: 2.302683\n",
      "[299,  2800] loss: 2.302654\n",
      "[299,  3000] loss: 2.302837\n",
      "[299,  3200] loss: 2.302597\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[300,   200] loss: 2.302463\n",
      "[300,   400] loss: 2.302448\n",
      "[300,   600] loss: 2.302630\n",
      "[300,   800] loss: 2.302584\n",
      "[300,  1000] loss: 2.302751\n",
      "[300,  1200] loss: 2.302664\n",
      "[300,  1400] loss: 2.302616\n",
      "[300,  1600] loss: 2.302803\n",
      "[300,  1800] loss: 2.302755\n",
      "[300,  2000] loss: 2.302459\n",
      "[300,  2200] loss: 2.302641\n",
      "[300,  2400] loss: 2.302634\n",
      "[300,  2600] loss: 2.302701\n",
      "[300,  2800] loss: 2.302754\n",
      "[300,  3000] loss: 2.302582\n",
      "[300,  3200] loss: 2.302610\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[301,   200] loss: 2.302802\n",
      "[301,   400] loss: 2.302674\n",
      "[301,   600] loss: 2.302493\n",
      "[301,   800] loss: 2.302497\n",
      "[301,  1000] loss: 2.302892\n",
      "[301,  1200] loss: 2.302520\n",
      "[301,  1400] loss: 2.302656\n",
      "[301,  1600] loss: 2.302655\n",
      "[301,  1800] loss: 2.302594\n",
      "[301,  2000] loss: 2.302660\n",
      "[301,  2200] loss: 2.302751\n",
      "[301,  2400] loss: 2.302719\n",
      "[301,  2600] loss: 2.302574\n",
      "[301,  2800] loss: 2.302723\n",
      "[301,  3000] loss: 2.302749\n",
      "[301,  3200] loss: 2.302783\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[302,   200] loss: 2.302719\n",
      "[302,   400] loss: 2.302647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302,   600] loss: 2.302647\n",
      "[302,   800] loss: 2.302455\n",
      "[302,  1000] loss: 2.302708\n",
      "[302,  1200] loss: 2.302669\n",
      "[302,  1400] loss: 2.302656\n",
      "[302,  1600] loss: 2.302746\n",
      "[302,  1800] loss: 2.302497\n",
      "[302,  2000] loss: 2.302897\n",
      "[302,  2200] loss: 2.302723\n",
      "[302,  2400] loss: 2.302764\n",
      "[302,  2600] loss: 2.302696\n",
      "[302,  2800] loss: 2.302481\n",
      "[302,  3000] loss: 2.302533\n",
      "[302,  3200] loss: 2.302632\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[303,   200] loss: 2.302741\n",
      "[303,   400] loss: 2.302737\n",
      "[303,   600] loss: 2.302700\n",
      "[303,   800] loss: 2.302742\n",
      "[303,  1000] loss: 2.302625\n",
      "[303,  1200] loss: 2.302542\n",
      "[303,  1400] loss: 2.302472\n",
      "[303,  1600] loss: 2.302742\n",
      "[303,  1800] loss: 2.302711\n",
      "[303,  2000] loss: 2.302340\n",
      "[303,  2200] loss: 2.302772\n",
      "[303,  2400] loss: 2.302554\n",
      "[303,  2600] loss: 2.302596\n",
      "[303,  2800] loss: 2.302613\n",
      "[303,  3000] loss: 2.302727\n",
      "[303,  3200] loss: 2.302685\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[304,   200] loss: 2.302575\n",
      "[304,   400] loss: 2.302701\n",
      "[304,   600] loss: 2.302582\n",
      "[304,   800] loss: 2.302801\n",
      "[304,  1000] loss: 2.302644\n",
      "[304,  1200] loss: 2.302596\n",
      "[304,  1400] loss: 2.302801\n",
      "[304,  1600] loss: 2.302619\n",
      "[304,  1800] loss: 2.302732\n",
      "[304,  2000] loss: 2.302598\n",
      "[304,  2200] loss: 2.302682\n",
      "[304,  2400] loss: 2.302788\n",
      "[304,  2600] loss: 2.302703\n",
      "[304,  2800] loss: 2.302580\n",
      "[304,  3000] loss: 2.302776\n",
      "[304,  3200] loss: 2.302388\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[305,   200] loss: 2.302834\n",
      "[305,   400] loss: 2.302514\n",
      "[305,   600] loss: 2.302556\n",
      "[305,   800] loss: 2.302824\n",
      "[305,  1000] loss: 2.302480\n",
      "[305,  1200] loss: 2.302506\n",
      "[305,  1400] loss: 2.302706\n",
      "[305,  1600] loss: 2.302681\n",
      "[305,  1800] loss: 2.302587\n",
      "[305,  2000] loss: 2.302824\n",
      "[305,  2200] loss: 2.302590\n",
      "[305,  2400] loss: 2.302461\n",
      "[305,  2600] loss: 2.302586\n",
      "[305,  2800] loss: 2.302772\n",
      "[305,  3000] loss: 2.302617\n",
      "[305,  3200] loss: 2.302566\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[306,   200] loss: 2.302612\n",
      "[306,   400] loss: 2.302790\n",
      "[306,   600] loss: 2.302509\n",
      "[306,   800] loss: 2.302789\n",
      "[306,  1000] loss: 2.302664\n",
      "[306,  1200] loss: 2.302420\n",
      "[306,  1400] loss: 2.302732\n",
      "[306,  1600] loss: 2.302329\n",
      "[306,  1800] loss: 2.302505\n",
      "[306,  2000] loss: 2.302598\n",
      "[306,  2200] loss: 2.302692\n",
      "[306,  2400] loss: 2.302842\n",
      "[306,  2600] loss: 2.302665\n",
      "[306,  2800] loss: 2.302370\n",
      "[306,  3000] loss: 2.302651\n",
      "[306,  3200] loss: 2.302588\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[307,   200] loss: 2.302808\n",
      "[307,   400] loss: 2.302513\n",
      "[307,   600] loss: 2.302594\n",
      "[307,   800] loss: 2.302874\n",
      "[307,  1000] loss: 2.302693\n",
      "[307,  1200] loss: 2.302560\n",
      "[307,  1400] loss: 2.302539\n",
      "[307,  1600] loss: 2.302601\n",
      "[307,  1800] loss: 2.302720\n",
      "[307,  2000] loss: 2.302653\n",
      "[307,  2200] loss: 2.302759\n",
      "[307,  2400] loss: 2.302412\n",
      "[307,  2600] loss: 2.302912\n",
      "[307,  2800] loss: 2.302552\n",
      "[307,  3000] loss: 2.302744\n",
      "[307,  3200] loss: 2.302459\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[308,   200] loss: 2.302391\n",
      "[308,   400] loss: 2.302707\n",
      "[308,   600] loss: 2.302797\n",
      "[308,   800] loss: 2.302708\n",
      "[308,  1000] loss: 2.302550\n",
      "[308,  1200] loss: 2.302706\n",
      "[308,  1400] loss: 2.302767\n",
      "[308,  1600] loss: 2.302493\n",
      "[308,  1800] loss: 2.302717\n",
      "[308,  2000] loss: 2.302670\n",
      "[308,  2200] loss: 2.302305\n",
      "[308,  2400] loss: 2.302741\n",
      "[308,  2600] loss: 2.302929\n",
      "[308,  2800] loss: 2.302361\n",
      "[308,  3000] loss: 2.302536\n",
      "[308,  3200] loss: 2.302761\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[309,   200] loss: 2.302803\n",
      "[309,   400] loss: 2.302660\n",
      "[309,   600] loss: 2.302565\n",
      "[309,   800] loss: 2.302637\n",
      "[309,  1000] loss: 2.302573\n",
      "[309,  1200] loss: 2.302259\n",
      "[309,  1400] loss: 2.302847\n",
      "[309,  1600] loss: 2.302608\n",
      "[309,  1800] loss: 2.302674\n",
      "[309,  2000] loss: 2.302637\n",
      "[309,  2200] loss: 2.302720\n",
      "[309,  2400] loss: 2.302497\n",
      "[309,  2600] loss: 2.302778\n",
      "[309,  2800] loss: 2.302631\n",
      "[309,  3000] loss: 2.302627\n",
      "[309,  3200] loss: 2.302774\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[310,   200] loss: 2.302654\n",
      "[310,   400] loss: 2.302496\n",
      "[310,   600] loss: 2.302835\n",
      "[310,   800] loss: 2.302513\n",
      "[310,  1000] loss: 2.302438\n",
      "[310,  1200] loss: 2.302455\n",
      "[310,  1400] loss: 2.302429\n",
      "[310,  1600] loss: 2.302548\n",
      "[310,  1800] loss: 2.302663\n",
      "[310,  2000] loss: 2.302600\n",
      "[310,  2200] loss: 2.302771\n",
      "[310,  2400] loss: 2.302705\n",
      "[310,  2600] loss: 2.302755\n",
      "[310,  2800] loss: 2.302699\n",
      "[310,  3000] loss: 2.302627\n",
      "[310,  3200] loss: 2.302534\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[311,   200] loss: 2.302631\n",
      "[311,   400] loss: 2.302601\n",
      "[311,   600] loss: 2.302537\n",
      "[311,   800] loss: 2.302561\n",
      "[311,  1000] loss: 2.302791\n",
      "[311,  1200] loss: 2.302656\n",
      "[311,  1400] loss: 2.302456\n",
      "[311,  1600] loss: 2.302842\n",
      "[311,  1800] loss: 2.302550\n",
      "[311,  2000] loss: 2.302530\n",
      "[311,  2200] loss: 2.302771\n",
      "[311,  2400] loss: 2.302659\n",
      "[311,  2600] loss: 2.302817\n",
      "[311,  2800] loss: 2.302482\n",
      "[311,  3000] loss: 2.302845\n",
      "[311,  3200] loss: 2.302660\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[312,   200] loss: 2.302671\n",
      "[312,   400] loss: 2.302733\n",
      "[312,   600] loss: 2.302624\n",
      "[312,   800] loss: 2.302589\n",
      "[312,  1000] loss: 2.302698\n",
      "[312,  1200] loss: 2.302684\n",
      "[312,  1400] loss: 2.302496\n",
      "[312,  1600] loss: 2.302728\n",
      "[312,  1800] loss: 2.302575\n",
      "[312,  2000] loss: 2.302711\n",
      "[312,  2200] loss: 2.302579\n",
      "[312,  2400] loss: 2.302617\n",
      "[312,  2600] loss: 2.302511\n",
      "[312,  2800] loss: 2.302648\n",
      "[312,  3000] loss: 2.302813\n",
      "[312,  3200] loss: 2.302641\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[313,   200] loss: 2.302455\n",
      "[313,   400] loss: 2.302708\n",
      "[313,   600] loss: 2.302680\n",
      "[313,   800] loss: 2.302661\n",
      "[313,  1000] loss: 2.302784\n",
      "[313,  1200] loss: 2.302488\n",
      "[313,  1400] loss: 2.302860\n",
      "[313,  1600] loss: 2.302585\n",
      "[313,  1800] loss: 2.302795\n",
      "[313,  2000] loss: 2.302741\n",
      "[313,  2200] loss: 2.302526\n",
      "[313,  2400] loss: 2.302695\n",
      "[313,  2600] loss: 2.302486\n",
      "[313,  2800] loss: 2.302210\n",
      "[313,  3000] loss: 2.302997\n",
      "[313,  3200] loss: 2.302552\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[314,   200] loss: 2.302420\n",
      "[314,   400] loss: 2.302878\n",
      "[314,   600] loss: 2.302456\n",
      "[314,   800] loss: 2.302779\n",
      "[314,  1000] loss: 2.302650\n",
      "[314,  1200] loss: 2.302604\n",
      "[314,  1400] loss: 2.302649\n",
      "[314,  1600] loss: 2.302584\n",
      "[314,  1800] loss: 2.302645\n",
      "[314,  2000] loss: 2.302727\n",
      "[314,  2200] loss: 2.302724\n",
      "[314,  2400] loss: 2.302637\n",
      "[314,  2600] loss: 2.302573\n",
      "[314,  2800] loss: 2.302445\n",
      "[314,  3000] loss: 2.302823\n",
      "[314,  3200] loss: 2.302687\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[315,   200] loss: 2.302710\n",
      "[315,   400] loss: 2.302367\n",
      "[315,   600] loss: 2.302763\n",
      "[315,   800] loss: 2.302609\n",
      "[315,  1000] loss: 2.302838\n",
      "[315,  1200] loss: 2.302534\n",
      "[315,  1400] loss: 2.302612\n",
      "[315,  1600] loss: 2.302682\n",
      "[315,  1800] loss: 2.302585\n",
      "[315,  2000] loss: 2.302898\n",
      "[315,  2200] loss: 2.302626\n",
      "[315,  2400] loss: 2.302739\n",
      "[315,  2600] loss: 2.302692\n",
      "[315,  2800] loss: 2.302446\n",
      "[315,  3000] loss: 2.302668\n",
      "[315,  3200] loss: 2.302515\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[316,   200] loss: 2.302698\n",
      "[316,   400] loss: 2.302444\n",
      "[316,   600] loss: 2.302693\n",
      "[316,   800] loss: 2.302714\n",
      "[316,  1000] loss: 2.302567\n",
      "[316,  1200] loss: 2.302682\n",
      "[316,  1400] loss: 2.302671\n",
      "[316,  1600] loss: 2.302578\n",
      "[316,  1800] loss: 2.302655\n",
      "[316,  2000] loss: 2.302714\n",
      "[316,  2200] loss: 2.302444\n",
      "[316,  2400] loss: 2.302644\n",
      "[316,  2600] loss: 2.302829\n",
      "[316,  2800] loss: 2.302662\n",
      "[316,  3000] loss: 2.302774\n",
      "[316,  3200] loss: 2.302612\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[317,   200] loss: 2.302597\n",
      "[317,   400] loss: 2.302751\n",
      "[317,   600] loss: 2.302363\n",
      "[317,   800] loss: 2.302791\n",
      "[317,  1000] loss: 2.302553\n",
      "[317,  1200] loss: 2.302728\n",
      "[317,  1400] loss: 2.302639\n",
      "[317,  1600] loss: 2.302765\n",
      "[317,  1800] loss: 2.302593\n",
      "[317,  2000] loss: 2.302749\n",
      "[317,  2200] loss: 2.302611\n",
      "[317,  2400] loss: 2.302744\n",
      "[317,  2600] loss: 2.302638\n",
      "[317,  2800] loss: 2.302542\n",
      "[317,  3000] loss: 2.302505\n",
      "[317,  3200] loss: 2.302705\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[318,   200] loss: 2.302600\n",
      "[318,   400] loss: 2.302493\n",
      "[318,   600] loss: 2.302839\n",
      "[318,   800] loss: 2.302592\n",
      "[318,  1000] loss: 2.302789\n",
      "[318,  1200] loss: 2.302617\n",
      "[318,  1400] loss: 2.302585\n",
      "[318,  1600] loss: 2.302868\n",
      "[318,  1800] loss: 2.302701\n",
      "[318,  2000] loss: 2.302765\n",
      "[318,  2200] loss: 2.302643\n",
      "[318,  2400] loss: 2.302593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318,  2600] loss: 2.302591\n",
      "[318,  2800] loss: 2.302521\n",
      "[318,  3000] loss: 2.302698\n",
      "[318,  3200] loss: 2.302648\n",
      "Got 401 / 4000 correct (10.03)\n",
      "skip model saving\n",
      "[319,   200] loss: 2.302708\n",
      "[319,   400] loss: 2.302724\n",
      "[319,   600] loss: 2.302544\n",
      "[319,   800] loss: 2.302677\n",
      "[319,  1000] loss: 2.302539\n",
      "[319,  1200] loss: 2.302615\n",
      "[319,  1400] loss: 2.302674\n",
      "[319,  1600] loss: 2.302652\n",
      "[319,  1800] loss: 2.302633\n",
      "[319,  2000] loss: 2.302554\n",
      "[319,  2200] loss: 2.302742\n",
      "[319,  2400] loss: 2.302577\n",
      "[319,  2600] loss: 2.302714\n",
      "[319,  2800] loss: 2.302573\n",
      "[319,  3000] loss: 2.302631\n",
      "[319,  3200] loss: 2.302600\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[320,   200] loss: 2.302366\n",
      "[320,   400] loss: 2.302779\n",
      "[320,   600] loss: 2.302556\n",
      "[320,   800] loss: 2.302784\n",
      "[320,  1000] loss: 2.302596\n",
      "[320,  1200] loss: 2.302732\n",
      "[320,  1400] loss: 2.302809\n",
      "[320,  1600] loss: 2.302555\n",
      "[320,  1800] loss: 2.302832\n",
      "[320,  2000] loss: 2.302681\n",
      "[320,  2200] loss: 2.302594\n",
      "[320,  2400] loss: 2.302500\n",
      "[320,  2600] loss: 2.302785\n",
      "[320,  2800] loss: 2.302822\n",
      "[320,  3000] loss: 2.302429\n",
      "[320,  3200] loss: 2.302566\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[321,   200] loss: 2.302706\n",
      "[321,   400] loss: 2.302592\n",
      "[321,   600] loss: 2.302708\n",
      "[321,   800] loss: 2.302731\n",
      "[321,  1000] loss: 2.302564\n",
      "[321,  1200] loss: 2.302762\n",
      "[321,  1400] loss: 2.302529\n",
      "[321,  1600] loss: 2.302717\n",
      "[321,  1800] loss: 2.302582\n",
      "[321,  2000] loss: 2.302699\n",
      "[321,  2200] loss: 2.302598\n",
      "[321,  2400] loss: 2.302671\n",
      "[321,  2600] loss: 2.302731\n",
      "[321,  2800] loss: 2.302694\n",
      "[321,  3000] loss: 2.302486\n",
      "[321,  3200] loss: 2.302488\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[322,   200] loss: 2.302527\n",
      "[322,   400] loss: 2.302766\n",
      "[322,   600] loss: 2.302751\n",
      "[322,   800] loss: 2.302660\n",
      "[322,  1000] loss: 2.302554\n",
      "[322,  1200] loss: 2.302586\n",
      "[322,  1400] loss: 2.302742\n",
      "[322,  1600] loss: 2.302466\n",
      "[322,  1800] loss: 2.302881\n",
      "[322,  2000] loss: 2.302445\n",
      "[322,  2200] loss: 2.302444\n",
      "[322,  2400] loss: 2.302791\n",
      "[322,  2600] loss: 2.302448\n",
      "[322,  2800] loss: 2.302847\n",
      "[322,  3000] loss: 2.302506\n",
      "[322,  3200] loss: 2.302557\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[323,   200] loss: 2.302820\n",
      "[323,   400] loss: 2.302643\n",
      "[323,   600] loss: 2.302594\n",
      "[323,   800] loss: 2.302631\n",
      "[323,  1000] loss: 2.302672\n",
      "[323,  1200] loss: 2.302712\n",
      "[323,  1400] loss: 2.302715\n",
      "[323,  1600] loss: 2.302755\n",
      "[323,  1800] loss: 2.302562\n",
      "[323,  2000] loss: 2.302393\n",
      "[323,  2200] loss: 2.302609\n",
      "[323,  2400] loss: 2.302865\n",
      "[323,  2600] loss: 2.302468\n",
      "[323,  2800] loss: 2.302747\n",
      "[323,  3000] loss: 2.302688\n",
      "[323,  3200] loss: 2.302416\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[324,   200] loss: 2.302691\n",
      "[324,   400] loss: 2.302781\n",
      "[324,   600] loss: 2.302474\n",
      "[324,   800] loss: 2.302738\n",
      "[324,  1000] loss: 2.302632\n",
      "[324,  1200] loss: 2.302773\n",
      "[324,  1400] loss: 2.302496\n",
      "[324,  1600] loss: 2.302618\n",
      "[324,  1800] loss: 2.302798\n",
      "[324,  2000] loss: 2.302643\n",
      "[324,  2200] loss: 2.302559\n",
      "[324,  2400] loss: 2.302334\n",
      "[324,  2600] loss: 2.302562\n",
      "[324,  2800] loss: 2.302880\n",
      "[324,  3000] loss: 2.302608\n",
      "[324,  3200] loss: 2.302687\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[325,   200] loss: 2.302656\n",
      "[325,   400] loss: 2.302718\n",
      "[325,   600] loss: 2.302664\n",
      "[325,   800] loss: 2.302523\n",
      "[325,  1000] loss: 2.302659\n",
      "[325,  1200] loss: 2.302805\n",
      "[325,  1400] loss: 2.302637\n",
      "[325,  1600] loss: 2.302526\n",
      "[325,  1800] loss: 2.302641\n",
      "[325,  2000] loss: 2.302525\n",
      "[325,  2200] loss: 2.302698\n",
      "[325,  2400] loss: 2.302718\n",
      "[325,  2600] loss: 2.302388\n",
      "[325,  2800] loss: 2.302874\n",
      "[325,  3000] loss: 2.302574\n",
      "[325,  3200] loss: 2.302538\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[326,   200] loss: 2.302455\n",
      "[326,   400] loss: 2.302702\n",
      "[326,   600] loss: 2.302651\n",
      "[326,   800] loss: 2.302585\n",
      "[326,  1000] loss: 2.302846\n",
      "[326,  1200] loss: 2.302696\n",
      "[326,  1400] loss: 2.302778\n",
      "[326,  1600] loss: 2.302666\n",
      "[326,  1800] loss: 2.302699\n",
      "[326,  2000] loss: 2.302567\n",
      "[326,  2200] loss: 2.302660\n",
      "[326,  2400] loss: 2.302633\n",
      "[326,  2600] loss: 2.302752\n",
      "[326,  2800] loss: 2.302489\n",
      "[326,  3000] loss: 2.302493\n",
      "[326,  3200] loss: 2.302490\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[327,   200] loss: 2.302496\n",
      "[327,   400] loss: 2.302643\n",
      "[327,   600] loss: 2.302596\n",
      "[327,   800] loss: 2.302834\n",
      "[327,  1000] loss: 2.302500\n",
      "[327,  1200] loss: 2.302706\n",
      "[327,  1400] loss: 2.302675\n",
      "[327,  1600] loss: 2.302620\n",
      "[327,  1800] loss: 2.302663\n",
      "[327,  2000] loss: 2.302696\n",
      "[327,  2200] loss: 2.302786\n",
      "[327,  2400] loss: 2.302624\n",
      "[327,  2600] loss: 2.302700\n",
      "[327,  2800] loss: 2.302547\n",
      "[327,  3000] loss: 2.302691\n",
      "[327,  3200] loss: 2.302506\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[328,   200] loss: 2.302657\n",
      "[328,   400] loss: 2.302667\n",
      "[328,   600] loss: 2.302592\n",
      "[328,   800] loss: 2.302837\n",
      "[328,  1000] loss: 2.302533\n",
      "[328,  1200] loss: 2.302499\n",
      "[328,  1400] loss: 2.302629\n",
      "[328,  1600] loss: 2.302500\n",
      "[328,  1800] loss: 2.302714\n",
      "[328,  2000] loss: 2.302772\n",
      "[328,  2200] loss: 2.302547\n",
      "[328,  2400] loss: 2.302835\n",
      "[328,  2600] loss: 2.302509\n",
      "[328,  2800] loss: 2.302648\n",
      "[328,  3000] loss: 2.302718\n",
      "[328,  3200] loss: 2.302634\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[329,   200] loss: 2.302647\n",
      "[329,   400] loss: 2.302382\n",
      "[329,   600] loss: 2.302638\n",
      "[329,   800] loss: 2.302992\n",
      "[329,  1000] loss: 2.302767\n",
      "[329,  1200] loss: 2.302649\n",
      "[329,  1400] loss: 2.302520\n",
      "[329,  1600] loss: 2.302645\n",
      "[329,  1800] loss: 2.302605\n",
      "[329,  2000] loss: 2.302627\n",
      "[329,  2200] loss: 2.302387\n",
      "[329,  2400] loss: 2.302698\n",
      "[329,  2600] loss: 2.302645\n",
      "[329,  2800] loss: 2.302583\n",
      "[329,  3000] loss: 2.302702\n",
      "[329,  3200] loss: 2.302603\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[330,   200] loss: 2.302811\n",
      "[330,   400] loss: 2.302531\n",
      "[330,   600] loss: 2.302649\n",
      "[330,   800] loss: 2.302614\n",
      "[330,  1000] loss: 2.302658\n",
      "[330,  1200] loss: 2.302695\n",
      "[330,  1400] loss: 2.302575\n",
      "[330,  1600] loss: 2.302714\n",
      "[330,  1800] loss: 2.302736\n",
      "[330,  2000] loss: 2.302620\n",
      "[330,  2200] loss: 2.302485\n",
      "[330,  2400] loss: 2.302798\n",
      "[330,  2600] loss: 2.302474\n",
      "[330,  2800] loss: 2.302839\n",
      "[330,  3000] loss: 2.302735\n",
      "[330,  3200] loss: 2.302570\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[331,   200] loss: 2.302856\n",
      "[331,   400] loss: 2.302604\n",
      "[331,   600] loss: 2.302796\n",
      "[331,   800] loss: 2.302639\n",
      "[331,  1000] loss: 2.302680\n",
      "[331,  1200] loss: 2.302737\n",
      "[331,  1400] loss: 2.302326\n",
      "[331,  1600] loss: 2.302694\n",
      "[331,  1800] loss: 2.302560\n",
      "[331,  2000] loss: 2.302715\n",
      "[331,  2200] loss: 2.302513\n",
      "[331,  2400] loss: 2.302722\n",
      "[331,  2600] loss: 2.302535\n",
      "[331,  2800] loss: 2.302595\n",
      "[331,  3000] loss: 2.302640\n",
      "[331,  3200] loss: 2.302708\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[332,   200] loss: 2.302374\n",
      "[332,   400] loss: 2.302632\n",
      "[332,   600] loss: 2.302570\n",
      "[332,   800] loss: 2.302723\n",
      "[332,  1000] loss: 2.302632\n",
      "[332,  1200] loss: 2.302562\n",
      "[332,  1400] loss: 2.302632\n",
      "[332,  1600] loss: 2.302518\n",
      "[332,  1800] loss: 2.302818\n",
      "[332,  2000] loss: 2.302658\n",
      "[332,  2200] loss: 2.302680\n",
      "[332,  2400] loss: 2.302810\n",
      "[332,  2600] loss: 2.302532\n",
      "[332,  2800] loss: 2.302627\n",
      "[332,  3000] loss: 2.302740\n",
      "[332,  3200] loss: 2.302706\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[333,   200] loss: 2.302477\n",
      "[333,   400] loss: 2.302731\n",
      "[333,   600] loss: 2.302716\n",
      "[333,   800] loss: 2.302667\n",
      "[333,  1000] loss: 2.302508\n",
      "[333,  1200] loss: 2.302680\n",
      "[333,  1400] loss: 2.302768\n",
      "[333,  1600] loss: 2.302796\n",
      "[333,  1800] loss: 2.302648\n",
      "[333,  2000] loss: 2.302592\n",
      "[333,  2200] loss: 2.302793\n",
      "[333,  2400] loss: 2.302693\n",
      "[333,  2600] loss: 2.302604\n",
      "[333,  2800] loss: 2.302731\n",
      "[333,  3000] loss: 2.302646\n",
      "[333,  3200] loss: 2.302574\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[334,   200] loss: 2.302896\n",
      "[334,   400] loss: 2.302728\n",
      "[334,   600] loss: 2.302588\n",
      "[334,   800] loss: 2.302738\n",
      "[334,  1000] loss: 2.302648\n",
      "[334,  1200] loss: 2.302503\n",
      "[334,  1400] loss: 2.302661\n",
      "[334,  1600] loss: 2.302741\n",
      "[334,  1800] loss: 2.302562\n",
      "[334,  2000] loss: 2.302599\n",
      "[334,  2200] loss: 2.302575\n",
      "[334,  2400] loss: 2.302441\n",
      "[334,  2600] loss: 2.302831\n",
      "[334,  2800] loss: 2.302783\n",
      "[334,  3000] loss: 2.302634\n",
      "[334,  3200] loss: 2.302449\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[335,   200] loss: 2.302783\n",
      "[335,   400] loss: 2.302738\n",
      "[335,   600] loss: 2.302640\n",
      "[335,   800] loss: 2.302581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335,  1000] loss: 2.302874\n",
      "[335,  1200] loss: 2.302647\n",
      "[335,  1400] loss: 2.302810\n",
      "[335,  1600] loss: 2.302614\n",
      "[335,  1800] loss: 2.302731\n",
      "[335,  2000] loss: 2.302618\n",
      "[335,  2200] loss: 2.302601\n",
      "[335,  2400] loss: 2.302675\n",
      "[335,  2600] loss: 2.302612\n",
      "[335,  2800] loss: 2.302518\n",
      "[335,  3000] loss: 2.302634\n",
      "[335,  3200] loss: 2.302331\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[336,   200] loss: 2.302883\n",
      "[336,   400] loss: 2.302707\n",
      "[336,   600] loss: 2.302526\n",
      "[336,   800] loss: 2.302849\n",
      "[336,  1000] loss: 2.302560\n",
      "[336,  1200] loss: 2.302592\n",
      "[336,  1400] loss: 2.302496\n",
      "[336,  1600] loss: 2.302728\n",
      "[336,  1800] loss: 2.302715\n",
      "[336,  2000] loss: 2.302579\n",
      "[336,  2200] loss: 2.302611\n",
      "[336,  2400] loss: 2.302636\n",
      "[336,  2600] loss: 2.302690\n",
      "[336,  2800] loss: 2.302575\n",
      "[336,  3000] loss: 2.302699\n",
      "[336,  3200] loss: 2.302518\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[337,   200] loss: 2.302515\n",
      "[337,   400] loss: 2.302869\n",
      "[337,   600] loss: 2.302571\n",
      "[337,   800] loss: 2.302762\n",
      "[337,  1000] loss: 2.302459\n",
      "[337,  1200] loss: 2.302733\n",
      "[337,  1400] loss: 2.302471\n",
      "[337,  1600] loss: 2.302325\n",
      "[337,  1800] loss: 2.302981\n",
      "[337,  2000] loss: 2.302503\n",
      "[337,  2200] loss: 2.302676\n",
      "[337,  2400] loss: 2.302672\n",
      "[337,  2600] loss: 2.302767\n",
      "[337,  2800] loss: 2.302664\n",
      "[337,  3000] loss: 2.302624\n",
      "[337,  3200] loss: 2.302379\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[338,   200] loss: 2.302706\n",
      "[338,   400] loss: 2.302549\n",
      "[338,   600] loss: 2.302606\n",
      "[338,   800] loss: 2.302710\n",
      "[338,  1000] loss: 2.302583\n",
      "[338,  1200] loss: 2.302736\n",
      "[338,  1400] loss: 2.302628\n",
      "[338,  1600] loss: 2.302698\n",
      "[338,  1800] loss: 2.302526\n",
      "[338,  2000] loss: 2.302742\n",
      "[338,  2200] loss: 2.302690\n",
      "[338,  2400] loss: 2.302703\n",
      "[338,  2600] loss: 2.302773\n",
      "[338,  2800] loss: 2.302644\n",
      "[338,  3000] loss: 2.302528\n",
      "[338,  3200] loss: 2.302465\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[339,   200] loss: 2.302548\n",
      "[339,   400] loss: 2.302739\n",
      "[339,   600] loss: 2.302567\n",
      "[339,   800] loss: 2.302699\n",
      "[339,  1000] loss: 2.302615\n",
      "[339,  1200] loss: 2.302537\n",
      "[339,  1400] loss: 2.302684\n",
      "[339,  1600] loss: 2.302592\n",
      "[339,  1800] loss: 2.302709\n",
      "[339,  2000] loss: 2.302568\n",
      "[339,  2200] loss: 2.302635\n",
      "[339,  2400] loss: 2.302785\n",
      "[339,  2600] loss: 2.302554\n",
      "[339,  2800] loss: 2.302581\n",
      "[339,  3000] loss: 2.302618\n",
      "[339,  3200] loss: 2.302700\n",
      "Got 394 / 4000 correct (9.85)\n",
      "skip model saving\n",
      "[340,   200] loss: 2.302701\n",
      "[340,   400] loss: 2.302794\n",
      "[340,   600] loss: 2.302422\n",
      "[340,   800] loss: 2.302574\n",
      "[340,  1000] loss: 2.302625\n",
      "[340,  1200] loss: 2.302588\n",
      "[340,  1400] loss: 2.302686\n",
      "[340,  1600] loss: 2.302539\n",
      "[340,  1800] loss: 2.302817\n",
      "[340,  2000] loss: 2.302684\n",
      "[340,  2200] loss: 2.302653\n",
      "[340,  2400] loss: 2.302513\n",
      "[340,  2600] loss: 2.302474\n",
      "[340,  2800] loss: 2.302655\n",
      "[340,  3000] loss: 2.302793\n",
      "[340,  3200] loss: 2.302764\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[341,   200] loss: 2.302474\n",
      "[341,   400] loss: 2.302862\n",
      "[341,   600] loss: 2.302693\n",
      "[341,   800] loss: 2.302628\n",
      "[341,  1000] loss: 2.302576\n",
      "[341,  1200] loss: 2.302633\n",
      "[341,  1400] loss: 2.302714\n",
      "[341,  1600] loss: 2.302566\n",
      "[341,  1800] loss: 2.302826\n",
      "[341,  2000] loss: 2.302624\n",
      "[341,  2200] loss: 2.302779\n",
      "[341,  2400] loss: 2.302569\n",
      "[341,  2600] loss: 2.302656\n",
      "[341,  2800] loss: 2.302645\n",
      "[341,  3000] loss: 2.302592\n",
      "[341,  3200] loss: 2.302682\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[342,   200] loss: 2.302683\n",
      "[342,   400] loss: 2.302577\n",
      "[342,   600] loss: 2.302460\n",
      "[342,   800] loss: 2.302839\n",
      "[342,  1000] loss: 2.302698\n",
      "[342,  1200] loss: 2.302809\n",
      "[342,  1400] loss: 2.302545\n",
      "[342,  1600] loss: 2.302613\n",
      "[342,  1800] loss: 2.302605\n",
      "[342,  2000] loss: 2.302531\n",
      "[342,  2200] loss: 2.302423\n",
      "[342,  2400] loss: 2.302594\n",
      "[342,  2600] loss: 2.302747\n",
      "[342,  2800] loss: 2.302684\n",
      "[342,  3000] loss: 2.302517\n",
      "[342,  3200] loss: 2.302786\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[343,   200] loss: 2.302584\n",
      "[343,   400] loss: 2.302558\n",
      "[343,   600] loss: 2.302504\n",
      "[343,   800] loss: 2.302917\n",
      "[343,  1000] loss: 2.302656\n",
      "[343,  1200] loss: 2.302423\n",
      "[343,  1400] loss: 2.302775\n",
      "[343,  1600] loss: 2.302751\n",
      "[343,  1800] loss: 2.302817\n",
      "[343,  2000] loss: 2.302658\n",
      "[343,  2200] loss: 2.302758\n",
      "[343,  2400] loss: 2.302629\n",
      "[343,  2600] loss: 2.302728\n",
      "[343,  2800] loss: 2.302569\n",
      "[343,  3000] loss: 2.302634\n",
      "[343,  3200] loss: 2.302759\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[344,   200] loss: 2.302567\n",
      "[344,   400] loss: 2.302782\n",
      "[344,   600] loss: 2.302693\n",
      "[344,   800] loss: 2.302501\n",
      "[344,  1000] loss: 2.302678\n",
      "[344,  1200] loss: 2.302644\n",
      "[344,  1400] loss: 2.302698\n",
      "[344,  1600] loss: 2.302525\n",
      "[344,  1800] loss: 2.302589\n",
      "[344,  2000] loss: 2.302637\n",
      "[344,  2200] loss: 2.302762\n",
      "[344,  2400] loss: 2.302649\n",
      "[344,  2600] loss: 2.302554\n",
      "[344,  2800] loss: 2.302766\n",
      "[344,  3000] loss: 2.302718\n",
      "[344,  3200] loss: 2.302630\n",
      "Got 415 / 4000 correct (10.38)\n",
      "skip model saving\n",
      "[345,   200] loss: 2.302497\n",
      "[345,   400] loss: 2.302607\n",
      "[345,   600] loss: 2.302605\n",
      "[345,   800] loss: 2.302691\n",
      "[345,  1000] loss: 2.302439\n",
      "[345,  1200] loss: 2.302810\n",
      "[345,  1400] loss: 2.302596\n",
      "[345,  1600] loss: 2.302841\n",
      "[345,  1800] loss: 2.302658\n",
      "[345,  2000] loss: 2.302535\n",
      "[345,  2200] loss: 2.302601\n",
      "[345,  2400] loss: 2.302565\n",
      "[345,  2600] loss: 2.302748\n",
      "[345,  2800] loss: 2.302719\n",
      "[345,  3000] loss: 2.302757\n",
      "[345,  3200] loss: 2.302602\n",
      "Got 405 / 4000 correct (10.12)\n",
      "skip model saving\n",
      "[346,   200] loss: 2.302548\n",
      "[346,   400] loss: 2.302800\n",
      "[346,   600] loss: 2.302456\n",
      "[346,   800] loss: 2.302465\n",
      "[346,  1000] loss: 2.302889\n",
      "[346,  1200] loss: 2.302629\n",
      "[346,  1400] loss: 2.302464\n",
      "[346,  1600] loss: 2.302852\n",
      "[346,  1800] loss: 2.302648\n",
      "[346,  2000] loss: 2.302738\n",
      "[346,  2200] loss: 2.302687\n",
      "[346,  2400] loss: 2.302516\n",
      "[346,  2600] loss: 2.302577\n",
      "[346,  2800] loss: 2.302890\n",
      "[346,  3000] loss: 2.302624\n",
      "[346,  3200] loss: 2.302455\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[347,   200] loss: 2.302599\n",
      "[347,   400] loss: 2.302526\n",
      "[347,   600] loss: 2.302680\n",
      "[347,   800] loss: 2.302658\n",
      "[347,  1000] loss: 2.302600\n",
      "[347,  1200] loss: 2.302641\n",
      "[347,  1400] loss: 2.302695\n",
      "[347,  1600] loss: 2.302726\n",
      "[347,  1800] loss: 2.302777\n",
      "[347,  2000] loss: 2.302498\n",
      "[347,  2200] loss: 2.302835\n",
      "[347,  2400] loss: 2.302681\n",
      "[347,  2600] loss: 2.302560\n",
      "[347,  2800] loss: 2.302605\n",
      "[347,  3000] loss: 2.302814\n",
      "[347,  3200] loss: 2.302515\n",
      "Got 396 / 4000 correct (9.90)\n",
      "skip model saving\n",
      "[348,   200] loss: 2.302754\n",
      "[348,   400] loss: 2.302562\n",
      "[348,   600] loss: 2.302699\n",
      "[348,   800] loss: 2.302724\n",
      "[348,  1000] loss: 2.302445\n",
      "[348,  1200] loss: 2.302667\n",
      "[348,  1400] loss: 2.302614\n",
      "[348,  1600] loss: 2.302682\n",
      "[348,  1800] loss: 2.302691\n",
      "[348,  2000] loss: 2.302658\n",
      "[348,  2200] loss: 2.302707\n",
      "[348,  2400] loss: 2.302580\n",
      "[348,  2600] loss: 2.302537\n",
      "[348,  2800] loss: 2.302660\n",
      "[348,  3000] loss: 2.302611\n",
      "[348,  3200] loss: 2.302714\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n",
      "[349,   200] loss: 2.302627\n",
      "[349,   400] loss: 2.302741\n",
      "[349,   600] loss: 2.302674\n",
      "[349,   800] loss: 2.302581\n",
      "[349,  1000] loss: 2.302567\n",
      "[349,  1200] loss: 2.302617\n",
      "[349,  1400] loss: 2.302713\n",
      "[349,  1600] loss: 2.302714\n",
      "[349,  1800] loss: 2.302572\n",
      "[349,  2000] loss: 2.302621\n",
      "[349,  2200] loss: 2.302821\n",
      "[349,  2400] loss: 2.302699\n",
      "[349,  2600] loss: 2.302522\n",
      "[349,  2800] loss: 2.302657\n",
      "[349,  3000] loss: 2.302366\n",
      "[349,  3200] loss: 2.302697\n",
      "Got 383 / 4000 correct (9.57)\n",
      "skip model saving\n",
      "[350,   200] loss: 2.302532\n",
      "[350,   400] loss: 2.302841\n",
      "[350,   600] loss: 2.302604\n",
      "[350,   800] loss: 2.302593\n",
      "[350,  1000] loss: 2.302649\n",
      "[350,  1200] loss: 2.302811\n",
      "[350,  1400] loss: 2.302429\n",
      "[350,  1600] loss: 2.302364\n",
      "[350,  1800] loss: 2.302777\n",
      "[350,  2000] loss: 2.302477\n",
      "[350,  2200] loss: 2.302848\n",
      "[350,  2400] loss: 2.302589\n",
      "[350,  2600] loss: 2.302690\n",
      "[350,  2800] loss: 2.302758\n",
      "[350,  3000] loss: 2.302548\n",
      "[350,  3200] loss: 2.302826\n",
      "Got 381 / 4000 correct (9.53)\n",
      "skip model saving\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# define and train the network\n",
    "stylised_model_path_torch = './cifar32_style_torch.pth'\n",
    "stylised_model_torch = torchvision.models.resnet50().to(device)\n",
    "lr=0.1\n",
    "stylised_optimizer_torch = optim.Adam(stylised_model_torch.parameters(), lr=lr)\n",
    "stylised_lr_scheduler_torch = optim.lr_scheduler.MultiStepLR(stylised_optimizer_torch, milestones=[150, 250])\n",
    "train_part(stylised_model_torch, data_loader_train_style, data_loader_val_style, stylised_model_path_torch, stylised_optimizer_torch, stylised_lr_scheduler_torch, epochs = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised ResNet50 Torch on Normal CIFAR10 Dataset\n",
    "\n",
    "The below code tests the stylised ResNet50 torch model on the normal CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 998 / 10000 correct, accuracy of the dataset is: 9.980 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "stylised_model_torch = torchvision.models.resnet50().to(device)\n",
    "stylised_model_torch.load_state_dict(torch.load('./cifar32_style_torch.pth'))\n",
    "stylised_model_torch.to(device)\n",
    "check_accuracy(data_loader_test, stylised_model_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stylised ResNet50 Torch on Stylised CIFAR10 Dataset\n",
    "\n",
    "The below code tests the stylised ResNet50 torch model on the stylised CIFAR10 test set and prints out the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1000 / 10000 correct, accuracy of the dataset is: 10.000 %\n"
     ]
    }
   ],
   "source": [
    "# report test set accuracy\n",
    "stylised_model_torch = torchvision.models.resnet50().to(device)\n",
    "stylised_model_torch.load_state_dict(torch.load('./cifar32_style_torch.pth'))\n",
    "stylised_model_torch.to(device)\n",
    "check_accuracy(data_loader_test_style, stylised_model_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
